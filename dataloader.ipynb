{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/home/hadoop/trainingandtestdata'\n",
    "trainfile = 'training.1600000.processed.noemoticon.csv'\n",
    "testfile = 'testdata.manual.2009.06.14.csv'\n",
    "s = ['UNK', 't', '—', '*', 'f', '#', '(', '4', 'a', '”', '{', '!', 'm', 's', ':', 'n', \n",
    "     'k', 'z', '}', '@', ')', 'h', '8', '0', '/', 'u', 'o', 'x', '6', '“', 'e', \n",
    "     'p', 'i', 'b', '2', '&', ' ', \"'\", '$', 'r', 'l', '.', '`', '_', 'y', 'c', \n",
    "     'w', '?', '1', '~', ';', ']', '+', '^', '%', 'v', '9', 'g', 'q', 'j', '[',\n",
    "     ',', 'd', '-']\n",
    "c_voc = {c:idx for (idx, c) in enumerate(s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/trainingandtestdata/training.1600000.processed.noemoticon.csv\n"
     ]
    }
   ],
   "source": [
    "trainset, trainlabel = load_data(os.path.join(dirpath, trainfile), c_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/trainingandtestdata/testdata.manual.2009.06.14.csv\n"
     ]
    }
   ],
   "source": [
    "testset, testlabel = load_data(os.path.join(dirpath, testfile), c_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_num_cnt = {(i+1):0 for i in range(41)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mycomicshop.com/search?tid\n",
      "sent: ['@nchokkan', 'https://www.mycomicshop.com/search?tid', 'but', 'all', 'says', 'not', 'in', 'stock']\n",
      "loooooooooooooooooooooooooooooooonnnnnnngggggggg\n",
      "sent: [\"it's\", 'going', 'to', 'be', 'a', 'loooooooooooooooooooooooooooooooonnnnnnngggggggg', 'night', 'at', 'work']\n",
      "alfhdjkghvsfdghvifduhbgvkjsdfhglsfdkghvfjlkdsghsdflkgh\n",
      "sent: ['alfhdjkghvsfdghvifduhbgvkjsdfhglsfdkghvfjlkdsghsdflkgh', 'today', 'sucks']\n",
      "soon.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnooooooooooooooooooooooooooooooooooooo\n",
      "sent: ['going', 'to', 'school', 'soon.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnooooooooooooooooooooooooooooooooooooo']\n",
      "[7, 25, 31, 12, 58, 7, 31, 25, 58, 12, 19, 45, 31, 19, 18, 5, 31, 10, 18, 19, 5, 33, 21, 47, 55, 22, 33, None, 47, 55, 21, 47, 33, None, 21, 47, 33, 55, None, 10, 18, 34, 7, 11, 30, 49, 2, 5, 18, 34, 7, 11, 30, 49, 10, 34, 7, 11, 30, 49, 5, 47, 55, None, None, None, 22, 55, 33, 47, 33, 47, None, 22, 21, 55, 33, 5, 2, 19, 24, 4, 5, 4, 19, 10, 19, 5, 33, None, 47, 22, 55, 18, 19, 34, 7, 11, 30, 49, 5, 33, None, 22, 47, 19, 34, 7, 11, 30]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-67625d11acf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_char_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-67625d11acf4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_char_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "max_char_num = 0\n",
    "for data in [trainset, testset]:\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if max_char_num < len(word):\n",
    "                max_char_num = len(word)\n",
    "                if max_char_num > 21:\n",
    "                    try:\n",
    "                        print(''.join([s[idx] for idx in word]))\n",
    "                    except TypeError:\n",
    "                        print(word)\n",
    "                        raise\n",
    "                    print(\"sent:\", [''.join([s[idx] for idx in w]) for w in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 12],\n",
       " [24, 30, 12, 29, 0],\n",
       " [0, 20, 7, 0],\n",
       " [20, 29],\n",
       " [44, 7, 14, 36, 0],\n",
       " [24, 30, 61, 7, 0, 29],\n",
       " [20, 31, 12],\n",
       " [3, 7, 44, 29, 32, 25, 25, 15],\n",
       " [32, 43],\n",
       " [0, 29, 26, 0, 31, 14, 56],\n",
       " [31, 0],\n",
       " [7, 14, 61],\n",
       " [11, 31, 56, 20, 0],\n",
       " [44, 38, 43],\n",
       " [7, 12],\n",
       " [7],\n",
       " [38, 29, 12, 24, 39, 0],\n",
       " [12, 44, 20, 25, 25, 39],\n",
       " [0, 25, 61, 7, 43],\n",
       " [7, 39, 12, 25],\n",
       " [32, 39, 7, 20]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, 29,  7, 61, 31, 14, 56,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [11, 43,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15, 31, 14, 61, 39, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [39, 25, 54, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [39, 29, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [44, 20, 31, 39, 61, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [31, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [56, 25, 25, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [38, 29,  7, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.preprocessing.sequence.pad_sequences(testset[0], maxlen=41, dtype='int32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_num = max([max(len(word) for word in sent) for sent in testset])\n",
    "char_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainingBatch(batchId, batchsize, embedding_dim):\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(trainset):\n",
    "        startIdx = startIdx%len(trainset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(trainset):\n",
    "            mts = mts%len(trainset)\n",
    "        data_y[i][trainlabel[mts]/2] = 1\n",
    "        for j in range(len(trainset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[trainset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", FLAGS.batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n",
    "def GetTestData(batchId, batchsize, embedding_dim):\n",
    "    data_x = np.zeros([data_x = np.zeros([batchsize, max_sent_len, embedding_dim], dtype=np.float32)\n",
    "    data_y = np.zeros([batch_size, 3], dtype=np.int32)])\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(testset):\n",
    "        startIdx = startIdx%len(testset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(testset):\n",
    "            mts = mts%len(testset)\n",
    "        data_y[i][testlabel[mts]/2] = 1\n",
    "        for j in range(len(testset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[testset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class SentiDataLoader:\n",
    "    def __init__(self, dirpath, trainfile, testfile, char_list):\n",
    "        self.dirpath = dirpath\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.s = char_list\n",
    "\n",
    "    def load_data(self): # charVocab is a char-to-idx dictionary:{char:idx}\n",
    "        def transIrregularWord(word):\n",
    "            if not word:\n",
    "                return ''\n",
    "            pattern1 = \"[^A-Za-z]*$\" #punctuation at the end of sentence\n",
    "            pattern2 = \"^[^A-Za-z@#]*\" #punctuation at the start of sentence\n",
    "            word = re.sub(pattern2, \"\", re.sub(pattern1, \"\", word))\n",
    "            pattern3 = '(.*)http(.?)://(.*)' # url\n",
    "            pattern4 = '^[0-9]+.?[0-9]+$' # number\n",
    "            if not word:\n",
    "                return ''\n",
    "        #     elif word.__contains__('@'):\n",
    "        #         return 'person'\n",
    "        #     elif word.__contains__('#'):\n",
    "        #         return 'topic'\n",
    "            elif re.match(r'(.*)http?://(.*)', word, re.M|re.I|re.S):    \n",
    "                return 'links'\n",
    "#             elif re.match(pattern4, word, re.M|re.I):\n",
    "#                 print(\"word:\", word)\n",
    "#                 return 'number'\n",
    "            else:\n",
    "                return  word.lower()\n",
    "\n",
    "        def sentence2words(line):\n",
    "            words = re.split('([,\\n ]+)', line.strip() )\n",
    "            words = list( filter(lambda s: len(s)>0, [transIrregularWord(word) for word in words]) )\n",
    "            return words\n",
    "\n",
    "        def word2chars(word, charVocab):\n",
    "            rst = [charVocab.get(char) for char in word]\n",
    "            for i in range(len(rst)):\n",
    "                if rst[i] is None:\n",
    "                    print(\"Unknown char:\", word[i])\n",
    "                    print(\"word:\", word)\n",
    "                    rst[i] = 0\n",
    "            return rst\n",
    "\n",
    "        def CSVFile2Dataset(filepath):\n",
    "            print(filepath)\n",
    "            df = pd.read_csv(filepath,encoding='latin-1')\n",
    "            instances = [(line[-1], line[0]) for line in df.values]\n",
    "            del df\n",
    "            texts = [sentence2words(instance[0]) for instance in instances]\n",
    "            labels = [instance[1] for instance in instances]\n",
    "            return texts, labels\n",
    "        \n",
    "        charVocab = {c:idx for (idx, c) in enumerate(self.s)}\n",
    "        texts, self.train_label = CSVFile2Dataset(os.path.join(self.dirpath, self.trainfile))\n",
    "        self.train_data = [list(word2chars(word, charVocab) for word in sentence) for sentence in texts]\n",
    "        texts, self.test_label = CSVFile2Dataset(os.path.join(self.dirpath, self.testfile))\n",
    "        self.test_data = [list(word2chars(word, charVocab) for word in sentence) for sentence in texts]\n",
    "        del texts\n",
    "        self.max_sent_len = max([max(len(sent) for sent in texts) for texts in [self.train_data, self.test_data]])\n",
    "        self.max_char_num = max(\n",
    "                                max([max(len(word) for word in sent) for sent in self.test_data]), \n",
    "                                max([max(len(word) for word in sent) for sent in self.train_data])\n",
    "                               )\n",
    "\n",
    "    def GetTrainingBatch(self, batchId, batchsize):\n",
    "        startIdx = batchId*batchsize\n",
    "        data_y = np.zeros([batchsize, 3], dtype=np.int32)\n",
    "        ids = [idx%len(self.train_data) for idx in range(startIdx, startIdx+batchsize, 1)]\n",
    "        sentences = [self.train_data[x] for x in ids]\n",
    "        data_x = keras.preprocessing.sequence.pad_sequences(testset[0], maxlen=41, dtype='int32', padding='post', \n",
    "                                                            truncating='post', value=0.0)\n",
    "        for i in range(batchsize):\n",
    "            mts = startIdx + i\n",
    "            if mts >= len(trainset):\n",
    "                mts = mts%len(trainset)\n",
    "            data_y[i][trainlabel[mts]/2] = 1\n",
    "            for j in range(len(trainset[mts])):\n",
    "                try:\n",
    "                    data_x[i][j] = word2vec[trainset[mts][j]]\n",
    "                except KeyError:\n",
    "                    print(\"word:\", m_word)\n",
    "                    miss_vec += 1\n",
    "                except IndexError:\n",
    "                    print(\"i, j, k:\", FLAGS.batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                    print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                    raise\n",
    "                else:\n",
    "                    hit_vec += 1\n",
    "        print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "        return data_x, data_y\n",
    "    \n",
    "    def GetTestData(self, batchId, batchsize):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/home/hadoop/trainingandtestdata'\n",
    "# trainfile = 'training.1600000.processed.noemoticon.csv'\n",
    "testfile = 'testdata.manual.2009.06.14.csv'\n",
    "trainfile = 'testdata.manual.2009.06.14.csv'\n",
    "s = ['UNK', 't', '—', '*', 'f', '#', '(', 'a', '”', '{', '!', 'm', 's', ':', 'n', \n",
    "     'k', 'z', '}', '@', ')', 'h', '/', 'u', 'o', 'x', '“', 'e', \n",
    "     'p', 'i', 'b', '&', ' ', \"'\", '$', 'r', 'l', '.', '`', '_', 'y', 'c', \n",
    "     'w', '?', '~', ';', ']', '+', '^', '%', 'v', 'g', 'q', 'j', '[',\n",
    "     ',', 'd', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "loader = SentiDataLoader(dirpath, trainfile, testfile, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/trainingandtestdata/testdata.manual.2009.06.14.csv\n",
      "/home/hadoop/trainingandtestdata/testdata.manual.2009.06.14.csv\n"
     ]
    }
   ],
   "source": [
    "loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.tinyurl.com/m595fk\n",
      "sent: ['time', 'warner', 'cable', 'pulls', 'the', 'plug', 'on', 'the', 'girlfriend', 'experience', 'www.tinyurl.com/m595fk']\n",
      "#socialentrepreneurship\n",
      "sent: ['ethics', 'and', 'nonprofits', 'links', '#stanford', '#socialentrepreneurship']\n",
      "#flockofseagullsweregeopoliticallycorrect\n",
      "sent: ['trouble', 'in', 'iran', 'i', 'see', 'hmm', 'iran', 'iran', 'so', 'far', 'away', '#flockofseagullsweregeopoliticallycorrect']\n"
     ]
    }
   ],
   "source": [
    "max_char_num = 0\n",
    "for data in [loader.test_data]:\n",
    "    for sent in data:\n",
    "        for word in sent:\n",
    "            if max_char_num < len(word):\n",
    "                max_char_num = len(word)\n",
    "                if max_char_num > 21:\n",
    "                    try:\n",
    "                        print(''.join([s[idx] for idx in word]))\n",
    "                    except TypeError:\n",
    "                        print(word)\n",
    "                        raise\n",
    "                    print(\"sent:\", [''.join([s[idx] for idx in w]) for w in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_char_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.test_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
