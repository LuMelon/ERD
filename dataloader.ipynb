{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def transIrregularWord(word):\n",
    "    if not word:\n",
    "        return ''\n",
    "    pattern1 = \"[^A-Za-z]*$\" #punctuation at the end of sentence\n",
    "    pattern2 = \"^[^A-Za-z@#]*\" #punctuation at the start of sentence\n",
    "    word = re.sub(pattern2, \"\", re.sub(pattern1, \"\", word))\n",
    "    pattern3 = '(.*)http(.?)://(.*)' # url\n",
    "    pattern4 = '^[0-9]+.?[0-9]+$' # number\n",
    "    if not word:\n",
    "        return ''\n",
    "#     elif word.__contains__('@'):\n",
    "#         return 'person'\n",
    "#     elif word.__contains__('#'):\n",
    "#         return 'topic'\n",
    "    elif re.match(r'(.*)http?://(.*)', word, re.M|re.I|re.S):    \n",
    "        return 'links'\n",
    "    elif re.match(pattern4, word, re.M|re.I):\n",
    "        return 'number'\n",
    "    else:\n",
    "        return  word.lower()\n",
    "def sentence2words(line):\n",
    "    words = re.split('([,\\n ]+)', line.strip() )\n",
    "    words = list( filter(lambda s: len(s)>0, [transIrregularWord(word) for word in words]) )\n",
    "    return words\n",
    "\n",
    "def word2chars(word, charVocab):\n",
    "    return [charVocab.get(char) for char in word]\n",
    "\n",
    "def CSVFile2Dataset(filepath):\n",
    "    print(filepath)\n",
    "    df = pd.read_csv(filepath,encoding='latin-1')\n",
    "    instances = [(line[-1], line[0]) for line in df.values]\n",
    "    del df\n",
    "    texts = [sentence2words(instance[0]) for instance in instances]\n",
    "    labels = [instance[1] for instance in instances]\n",
    "    return texts, labels\n",
    "\n",
    "def load_data(data_dir, charVocab): # charVocab is a char-to-idx dictionary:{char:idx}\n",
    "    texts, label = CSVFile2Dataset(data_dir)\n",
    "    encode_data = [list(word2chars(word, charVocab) for word in sentence) for sentence in texts]\n",
    "    return encode_data, label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/home/hadoop/trainingandtestdata'\n",
    "trainfile = 'training.1600000.processed.noemoticon.csv'\n",
    "testfile = 'testdata.manual.2009.06.14.csv'\n",
    "s = ['t', '—', '*', 'f', '#', '(', '4', 'a', '”', '{', '!', 'm', 's', ':', 'n', \n",
    "     'k', 'z', '}', '@', ')', 'h', '8', '0', '/', 'u', 'o', 'x', '6', '“', 'e', \n",
    "     'p', 'i', 'b', '2', '&', ' ', \"'\", '$', 'r', 'l', '.', '`', '_', 'y', 'c', \n",
    "     'w', '?', '1', '~', ';', ']', '+', '^', '%', 'v', '9', 'g', 'q', 'j', '[',\n",
    "     ',', 'd', '-']\n",
    "c_voc = {c:idx for (idx, c) in enumerate(s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dirpath, testfile), encoding='latin-1')\n",
    "instances = [(line[-1], line[0]) for line in df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ok, first assesment of the #kindle2 ...it fucking rocks!!!', 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop/trainingandtestdata/testdata.manual.2009.06.14.csv\n"
     ]
    }
   ],
   "source": [
    "trainset, trainlabel = load_data(os.path.join(dirpath, testfile), c_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497,\n",
       " [38, 29, 7, 61, 31, 14, 56],\n",
       " [['o', 'k'],\n",
       "  ['f', 'i', 'r', 's', 't'],\n",
       "  ['a', 's', 's', 'e', 's', 'm', 'e', 'n', 't'],\n",
       "  ['o', 'f'],\n",
       "  ['t', 'h', 'e'],\n",
       "  ['#', 'k', 'i', 'n', 'd', 'l', 'e'],\n",
       "  ['i', 't'],\n",
       "  ['f', 'u', 'c', 'k', 'i', 'n', 'g'],\n",
       "  ['r', 'o', 'c', 'k', 's']])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), trainset[0][0], [list(s[idx] for idx in trainset[1][i]) for i in range(len(trainset[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testset, testlabel = CSVFile2Dataset(os.path.join(dirpath, testfile))\n",
    "max_sent_len = max([(len(sent) for sent in texts) for texts in [trainset, testset]])\n",
    "print(max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/Users/lumenglong/word2vec.model')\n",
    "\n",
    "def GetTrainingBatch(batchId, batchsize, embedding_dim):\n",
    "    data_x = np.zeros([data_x = np.zeros([batch, max_sent_len, embedding_dim], dtype=np.float32)\n",
    "    data_y = np.zeros([batch_size, 3], dtype=np.int32)])\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(trainset):\n",
    "        startIdx = startIdx%len(trainset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(trainset):\n",
    "            mts = mts%len(trainset)\n",
    "        data_y[i][trainlabel[mts]/2] = 1\n",
    "        for j in range(len(trainset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[trainset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", FLAGS.batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n",
    "def GetTestData(batchId, batchsize, embedding_dim):\n",
    "    data_x = np.zeros([data_x = np.zeros([batchsize, max_sent_len, embedding_dim], dtype=np.float32)\n",
    "    data_y = np.zeros([batch_size, 3], dtype=np.int32)])\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(testset):\n",
    "        startIdx = startIdx%len(testset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(testset):\n",
    "            mts = mts%len(testset)\n",
    "        data_y[i][testlabel[mts]/2] = 1\n",
    "        for j in range(len(testset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[testset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Defend the harmony of Yuen Long, protect local peace. ——6th village of Yuen Long. 1899, British army invaded Yuen Long, and went into the heavy resistance of local people. 1941, Japanese army invaded Yuen Long, and went into the local guerrillas 2014, when the rioters of occupying central invaded Yuen Long, they got kicked away by locals. 2019, the rioters wanted to take over Yuen Long again, and got defeated one more time. Yuen Long people are native HK people. Because of the unification, defeated invaders one time after the other. As a Yuen Long resident, I sincerely invite people living in other regions to move here. (I am not sure about the last sentence, since its Cantonese). #Yuen Long people are true HK people #I love Yuen Long. Protect Yuen Long, protect homeland. Put the fight aside, strive for HK. The hope of HK is in Yuen Long. Tonight, the Yuen Long people which are mainly hakka wearing white shirts, holding Chinese flags, pushed all the way to Admiralty, and had encountered the HK separatist who are mainly South Asian immigrant and wearing black shirts. Most black shirts ran away. Some of them were concentrated to the middle section, and avoid being beaten up by the local police. (my comment: If you remember those “protecters” surrounding police HQ) Earlier this day, those black shirts, by the support of foreign intelligence, assaulted LOCPG office and paint insulting words such as “chink” on the walls.!@qwertyuiop[]\\asdfghjkl;'zxcvbnm,.?/{}+_-)(*&^%$#@!~`)\"\n",
    "s_set = list(set(s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " '—',\n",
       " '*',\n",
       " 'f',\n",
       " '#',\n",
       " '(',\n",
       " '4',\n",
       " 'a',\n",
       " '”',\n",
       " '{',\n",
       " '!',\n",
       " 'm',\n",
       " 's',\n",
       " ':',\n",
       " 'n',\n",
       " 'k',\n",
       " 'z',\n",
       " '}',\n",
       " '@',\n",
       " ')',\n",
       " 'h',\n",
       " '8',\n",
       " '0',\n",
       " '/',\n",
       " 'u',\n",
       " 'o',\n",
       " 'x',\n",
       " '6',\n",
       " '“',\n",
       " 'e',\n",
       " 'p',\n",
       " 'i',\n",
       " 'b',\n",
       " '2',\n",
       " '&',\n",
       " ' ',\n",
       " \"'\",\n",
       " '$',\n",
       " 'r',\n",
       " '\\x07',\n",
       " 'l',\n",
       " '.',\n",
       " '`',\n",
       " '_',\n",
       " 'y',\n",
       " 'c',\n",
       " 'w',\n",
       " '?',\n",
       " '1',\n",
       " '~',\n",
       " ';',\n",
       " ']',\n",
       " '+',\n",
       " '^',\n",
       " '%',\n",
       " 'v',\n",
       " '9',\n",
       " 'g',\n",
       " 'q',\n",
       " 'j',\n",
       " '[',\n",
       " ',',\n",
       " 'd',\n",
       " '-']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['t', '—', '*', 'f', '#', '(', '4', 'a', '”', '{', '!', 'm', 's', ':', 'n', 'k', 'z', '}', '@', ')', 'h', '8', '0', '/', 'u', 'o', 'x', '6', '“', 'e', 'p', 'i', 'b', '2', '&', ' ', \"'\", '$', 'r', 'l', '.', '`', '_', 'y', 'c', 'w', '?', '1', '~', ';', ']', '+', '^', '%', 'v', '9', 'g', 'q', 'j', '[', ',', 'd', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
