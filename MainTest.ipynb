{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 20:05:18.601061 140117901190976 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"ERDMain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 20:05:33.144768 140117901190976 logger.py:24] (300, 200, 101, 31, 2, 2)\n",
      "I0830 20:05:33.145292 140117901190976 logger.py:24] 2019-08-30 20:05:33 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 200 101 31 2 2\n",
      "2019-08-30 20:05:33 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/hadoop/ERD/model.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(lstm_char_cnn)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32)\n",
      "input_: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(?, 1100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w2v = lstm_char_cnn.WordEmbedding(\n",
    "                max_word_length = FLAGS.max_char_num , \n",
    "                char_vocab_size = char_vocab.size, \n",
    "                char_embed_size = FLAGS.char_embed_size, \n",
    "                kernels = eval(FLAGS.kernels), \n",
    "                kernel_features = eval(FLAGS.kernel_features), \n",
    "                num_highway_layers = FLAGS.highway_layers,\n",
    "                embedding_dim = FLAGS.embedding_dim\n",
    "            )\n",
    "lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "            batch_size = FLAGS.batch_size, \n",
    "            num_unroll_steps = FLAGS.max_sent_len, \n",
    "            rnn_size = FLAGS.embedding_dim, \n",
    "            num_rnn_layers = FLAGS.rnn_layers, \n",
    "            word_vocab_size = word_vocab.size\n",
    "        )\n",
    "\n",
    "char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                    w2v, lstm_lm, \n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    max_word_length = FLAGS.max_char_num, \n",
    "                    learning_rate = FLAGS.learning_rate,\n",
    "                    max_grad_norm = FLAGS.max_grad_norm\n",
    "                 )\n",
    "#sentiment analysis model\n",
    "s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "senti_train_graph = model.InferSentiTrainGraph(\n",
    "                        w2v, \n",
    "                        lstm_lm, \n",
    "                        s_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        sent_num = FLAGS.sent_num,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/hadoop/ERD/model.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(lstm_char_cnn)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_reshape: Tensor(\"Reshape_2:0\", shape=(2020, 31, 21), dtype=int32)\n",
      "input_: Tensor(\"Reshape_2:0\", shape=(2020, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_3/CNN_OUT/add_7:0\", shape=(62620, 1100), dtype=float32)\n",
      "x_embedding: Tensor(\"Embedding_3/CNN_OUT/add_7:0\", shape=(62620, 1100), dtype=float32)\n",
      "cnn_outs: Tensor(\"Reshape_3:0\", shape=(2020, 31, 1100), dtype=float32)\n",
      "RDM words_embedding: Tensor(\"transpose_1:0\", shape=(2020, 31, 300), dtype=float32)\n",
      "x_senti: Tensor(\"SentiModel_1/Max:0\", shape=(2020, 200), dtype=float32)\n",
      "pooled_rl_input: Tensor(\"Reshape_5:0\", shape=(?, 200), dtype=float32)\n",
      "rl_state: Tensor(\"rl_states:0\", shape=(?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df model\n",
    "rdm_model = model.RDM_Model(\n",
    "                max_seq_len = FLAGS.max_seq_len, \n",
    "                max_word_num = FLAGS.max_sent_len, \n",
    "                embedding_dim = FLAGS.embedding_dim, \n",
    "                hidden_dim = FLAGS.hidden_dim\n",
    "            )\n",
    "rdm_train_graph = model.InferRDMTrainGraph(\n",
    "                        w2v, lstm_lm, s_model, rdm_model, \n",
    "                        batchsize=FLAGS.batch_size,\n",
    "                        max_seq_len = FLAGS.max_seq_len, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        embedding_dim = FLAGS.embedding_dim,\n",
    "                        class_num = FLAGS.class_num\n",
    "                )\n",
    "\n",
    "# rl model\n",
    "cm_model = model.CM_Model(\n",
    "                    max_word_num = FLAGS.max_sent_len, \n",
    "                    embedding_dim = FLAGS.embedding_dim, \n",
    "                    hidden_dim = FLAGS.hidden_dim, \n",
    "                    action_num = FLAGS.action_num\n",
    "            )\n",
    "cm_train_graph = model.InferCMTrainGraph(\n",
    "                        w2v, s_model, rdm_model, cm_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        embedding_dim = FLAGS.embedding_dim, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        action_num = FLAGS.action_num\n",
    "                    )\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lstm_char_cnn' from '/home/hadoop/ERD/lstm_char_cnn.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(lstm_char_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     5: 0 [    5/ 1499], train_loss/perplexity = 9.15729713/9483.3896484 secs/batch = 0.5930s, grad.norm=0.12490036\n",
      "    10: 0 [   10/ 1499], train_loss/perplexity = 9.08498096/8821.7978516 secs/batch = 0.6202s, grad.norm=0.13333045\n",
      "    15: 0 [   15/ 1499], train_loss/perplexity = 8.98046017/7946.2880859 secs/batch = 0.7101s, grad.norm=0.18570276\n",
      "    20: 0 [   20/ 1499], train_loss/perplexity = 8.63869190/5645.9394531 secs/batch = 0.5504s, grad.norm=0.43776885\n",
      "    25: 0 [   25/ 1499], train_loss/perplexity = 7.53612280/1874.5479736 secs/batch = 0.6260s, grad.norm=0.47992179\n",
      "    30: 0 [   30/ 1499], train_loss/perplexity = 7.29143143/1467.6700439 secs/batch = 0.5361s, grad.norm=1.12251890\n",
      "    35: 0 [   35/ 1499], train_loss/perplexity = 7.44433117/1710.1411133 secs/batch = 0.5397s, grad.norm=1.29667437\n",
      "    40: 0 [   40/ 1499], train_loss/perplexity = 7.20252466/1342.8166504 secs/batch = 0.5403s, grad.norm=1.38146412\n",
      "    45: 0 [   45/ 1499], train_loss/perplexity = 7.23605251/1388.6016846 secs/batch = 0.5417s, grad.norm=0.60844481\n",
      "    50: 0 [   50/ 1499], train_loss/perplexity = 6.85240221/946.1510620 secs/batch = 0.5473s, grad.norm=0.86468434\n",
      "    55: 0 [   55/ 1499], train_loss/perplexity = 7.01791763/1116.4593506 secs/batch = 0.5437s, grad.norm=0.94763744\n",
      "    60: 0 [   60/ 1499], train_loss/perplexity = 7.07743979/1184.9309082 secs/batch = 0.5955s, grad.norm=0.66798860\n",
      "    65: 0 [   65/ 1499], train_loss/perplexity = 6.84144497/935.8403931 secs/batch = 0.5381s, grad.norm=0.82884061\n",
      "    70: 0 [   70/ 1499], train_loss/perplexity = 6.95450068/1047.8552246 secs/batch = 0.5412s, grad.norm=0.69485897\n",
      "    75: 0 [   75/ 1499], train_loss/perplexity = 6.74375486/848.7416992 secs/batch = 0.5385s, grad.norm=0.65387583\n",
      "    80: 0 [   80/ 1499], train_loss/perplexity = 6.66470098/784.2289429 secs/batch = 0.5437s, grad.norm=0.80050129\n",
      "    85: 0 [   85/ 1499], train_loss/perplexity = 6.73466873/841.0648193 secs/batch = 0.5459s, grad.norm=0.68331861\n",
      "    90: 0 [   90/ 1499], train_loss/perplexity = 7.22064877/1367.3758545 secs/batch = 0.5444s, grad.norm=0.65160012\n",
      "    95: 0 [   95/ 1499], train_loss/perplexity = 6.86706161/960.1232300 secs/batch = 0.5418s, grad.norm=2.23444128\n",
      "   100: 0 [  100/ 1499], train_loss/perplexity = 6.78442764/883.9739990 secs/batch = 0.5418s, grad.norm=0.48371723\n",
      "   105: 0 [  105/ 1499], train_loss/perplexity = 6.59414434/730.8032837 secs/batch = 0.5450s, grad.norm=0.64565563\n",
      "   110: 0 [  110/ 1499], train_loss/perplexity = 6.79803514/896.0848999 secs/batch = 0.5452s, grad.norm=0.64021891\n",
      "   115: 0 [  115/ 1499], train_loss/perplexity = 6.71722078/826.5172729 secs/batch = 0.5436s, grad.norm=0.52544338\n",
      "   120: 0 [  120/ 1499], train_loss/perplexity = 6.80204439/899.6846924 secs/batch = 0.5386s, grad.norm=0.50281554\n",
      "   125: 0 [  125/ 1499], train_loss/perplexity = 6.84960556/943.5086670 secs/batch = 0.5871s, grad.norm=0.69262564\n",
      "   130: 0 [  130/ 1499], train_loss/perplexity = 6.66587210/785.1478882 secs/batch = 0.5418s, grad.norm=0.66292650\n",
      "   135: 0 [  135/ 1499], train_loss/perplexity = 6.69437218/807.8466187 secs/batch = 0.5372s, grad.norm=0.56834054\n",
      "   140: 0 [  140/ 1499], train_loss/perplexity = 6.94751072/1040.5562744 secs/batch = 0.5415s, grad.norm=2.66790605\n",
      "   145: 0 [  145/ 1499], train_loss/perplexity = 6.89192533/984.2946777 secs/batch = 0.5467s, grad.norm=0.56570035\n",
      "   150: 0 [  150/ 1499], train_loss/perplexity = 6.72081041/829.4894409 secs/batch = 0.5445s, grad.norm=0.63384122\n",
      "   155: 0 [  155/ 1499], train_loss/perplexity = 6.67456198/792.0004883 secs/batch = 0.5489s, grad.norm=0.53302711\n",
      "   160: 0 [  160/ 1499], train_loss/perplexity = 6.79824162/896.2698975 secs/batch = 0.5404s, grad.norm=0.51960140\n",
      "   165: 0 [  165/ 1499], train_loss/perplexity = 6.54727268/697.3397217 secs/batch = 0.5411s, grad.norm=0.81722927\n",
      "   170: 0 [  170/ 1499], train_loss/perplexity = 6.68854046/803.1491699 secs/batch = 0.5394s, grad.norm=0.46655107\n",
      "   175: 0 [  175/ 1499], train_loss/perplexity = 6.52728939/683.5429077 secs/batch = 0.5377s, grad.norm=0.60712826\n",
      "   180: 0 [  180/ 1499], train_loss/perplexity = 6.84294033/937.2409058 secs/batch = 0.5404s, grad.norm=0.48056698\n",
      "   185: 0 [  185/ 1499], train_loss/perplexity = 6.61576891/746.7786865 secs/batch = 0.5419s, grad.norm=0.99304110\n",
      "   190: 0 [  190/ 1499], train_loss/perplexity = 6.87636948/969.1016235 secs/batch = 0.5519s, grad.norm=0.72139060\n",
      "   195: 0 [  195/ 1499], train_loss/perplexity = 6.72306252/831.3596802 secs/batch = 0.5514s, grad.norm=0.63136309\n",
      "   200: 0 [  200/ 1499], train_loss/perplexity = 6.72166538/830.1989746 secs/batch = 0.5374s, grad.norm=0.58111489\n",
      "   205: 0 [  205/ 1499], train_loss/perplexity = 6.84986305/943.7516479 secs/batch = 0.5409s, grad.norm=0.51003706\n",
      "   210: 0 [  210/ 1499], train_loss/perplexity = 6.58339691/722.9910889 secs/batch = 0.5372s, grad.norm=0.67135221\n",
      "   215: 0 [  215/ 1499], train_loss/perplexity = 6.66603374/785.2748413 secs/batch = 0.5393s, grad.norm=1.17223775\n",
      "   220: 0 [  220/ 1499], train_loss/perplexity = 6.74933624/853.4920654 secs/batch = 0.5442s, grad.norm=0.60290414\n",
      "   225: 0 [  225/ 1499], train_loss/perplexity = 6.80556345/902.8563232 secs/batch = 0.5374s, grad.norm=0.47272044\n",
      "   230: 0 [  230/ 1499], train_loss/perplexity = 6.70113182/813.3258667 secs/batch = 0.5396s, grad.norm=0.55341256\n",
      "   235: 0 [  235/ 1499], train_loss/perplexity = 6.91401625/1006.2805786 secs/batch = 0.5395s, grad.norm=0.49683353\n",
      "   240: 0 [  240/ 1499], train_loss/perplexity = 6.85481119/948.4330444 secs/batch = 0.5440s, grad.norm=0.53636760\n",
      "   245: 0 [  245/ 1499], train_loss/perplexity = 6.56626654/710.7114868 secs/batch = 0.5418s, grad.norm=0.44845966\n",
      "   250: 0 [  250/ 1499], train_loss/perplexity = 6.56709003/711.2969971 secs/batch = 0.5386s, grad.norm=0.42867157\n",
      "   255: 0 [  255/ 1499], train_loss/perplexity = 6.44371700/628.7395020 secs/batch = 0.5366s, grad.norm=0.42719221\n",
      "   260: 0 [  260/ 1499], train_loss/perplexity = 6.74554110/850.2590942 secs/batch = 0.5460s, grad.norm=0.45383269\n",
      "   265: 0 [  265/ 1499], train_loss/perplexity = 6.73745823/843.4142456 secs/batch = 0.5352s, grad.norm=1.08042061\n",
      "   270: 0 [  270/ 1499], train_loss/perplexity = 6.85704136/950.5505371 secs/batch = 0.5479s, grad.norm=0.51162636\n",
      "   275: 0 [  275/ 1499], train_loss/perplexity = 6.48904991/657.8980103 secs/batch = 0.5351s, grad.norm=0.43326911\n",
      "   280: 0 [  280/ 1499], train_loss/perplexity = 6.56894588/712.6182861 secs/batch = 0.5935s, grad.norm=0.62724084\n",
      "   285: 0 [  285/ 1499], train_loss/perplexity = 6.88734102/979.7927246 secs/batch = 0.5388s, grad.norm=0.42678773\n",
      "   290: 0 [  290/ 1499], train_loss/perplexity = 6.77013826/871.4323730 secs/batch = 0.5414s, grad.norm=0.41208357\n",
      "   295: 0 [  295/ 1499], train_loss/perplexity = 6.53362989/687.8906250 secs/batch = 0.5435s, grad.norm=0.42331758\n",
      "   300: 0 [  300/ 1499], train_loss/perplexity = 6.48760366/656.9472046 secs/batch = 0.5453s, grad.norm=0.46934214\n",
      "   305: 0 [  305/ 1499], train_loss/perplexity = 6.78201771/881.8462524 secs/batch = 0.5449s, grad.norm=0.56416482\n",
      "   310: 0 [  310/ 1499], train_loss/perplexity = 6.87790298/970.5888672 secs/batch = 0.5356s, grad.norm=0.39976019\n",
      "   315: 0 [  315/ 1499], train_loss/perplexity = 6.70414257/815.7782593 secs/batch = 0.5421s, grad.norm=0.41139778\n",
      "   320: 0 [  320/ 1499], train_loss/perplexity = 6.72836208/835.7772217 secs/batch = 0.5362s, grad.norm=0.46116558\n",
      "   325: 0 [  325/ 1499], train_loss/perplexity = 6.63636827/762.3214111 secs/batch = 0.5365s, grad.norm=0.50728679\n",
      "   330: 0 [  330/ 1499], train_loss/perplexity = 6.70524216/816.6757812 secs/batch = 0.5432s, grad.norm=1.39834237\n",
      "   335: 0 [  335/ 1499], train_loss/perplexity = 6.76682186/868.5471191 secs/batch = 0.5408s, grad.norm=0.67249143\n",
      "   340: 0 [  340/ 1499], train_loss/perplexity = 6.51874590/677.7279053 secs/batch = 0.5395s, grad.norm=0.41724956\n",
      "   345: 0 [  345/ 1499], train_loss/perplexity = 6.47745848/650.3160400 secs/batch = 0.5889s, grad.norm=0.51632887\n",
      "   350: 0 [  350/ 1499], train_loss/perplexity = 6.61782551/748.3161011 secs/batch = 0.5508s, grad.norm=0.41249710\n",
      "   355: 0 [  355/ 1499], train_loss/perplexity = 6.42453003/616.7908936 secs/batch = 0.5400s, grad.norm=0.40182540\n",
      "   360: 0 [  360/ 1499], train_loss/perplexity = 6.66702461/786.0532837 secs/batch = 0.5408s, grad.norm=0.52969933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   365: 0 [  365/ 1499], train_loss/perplexity = 6.63771820/763.3511963 secs/batch = 0.5380s, grad.norm=1.15613115\n",
      "   370: 0 [  370/ 1499], train_loss/perplexity = 6.57711983/718.4670410 secs/batch = 0.5411s, grad.norm=0.44019940\n",
      "   375: 0 [  375/ 1499], train_loss/perplexity = 6.43539858/623.5310669 secs/batch = 0.5414s, grad.norm=0.33791822\n",
      "   380: 0 [  380/ 1499], train_loss/perplexity = 6.64196396/766.5990601 secs/batch = 0.5434s, grad.norm=0.52224058\n",
      "   385: 0 [  385/ 1499], train_loss/perplexity = 6.61221409/744.1287842 secs/batch = 0.5400s, grad.norm=0.50121397\n",
      "   390: 0 [  390/ 1499], train_loss/perplexity = 6.81604147/912.3662109 secs/batch = 0.5370s, grad.norm=0.35354772\n",
      "   395: 0 [  395/ 1499], train_loss/perplexity = 6.80333662/900.8480835 secs/batch = 0.5420s, grad.norm=0.41359380\n",
      "   400: 0 [  400/ 1499], train_loss/perplexity = 6.71813107/827.2699585 secs/batch = 0.5535s, grad.norm=0.49285099\n",
      "   405: 0 [  405/ 1499], train_loss/perplexity = 6.58374310/723.2414551 secs/batch = 0.5451s, grad.norm=0.29595456\n",
      "   410: 0 [  410/ 1499], train_loss/perplexity = 6.44414663/629.0096436 secs/batch = 0.5399s, grad.norm=0.38925210\n",
      "   415: 0 [  415/ 1499], train_loss/perplexity = 6.71315145/823.1607056 secs/batch = 0.5382s, grad.norm=0.33190441\n",
      "   420: 0 [  420/ 1499], train_loss/perplexity = 6.59542894/731.7426758 secs/batch = 0.5413s, grad.norm=0.37358764\n",
      "   425: 0 [  425/ 1499], train_loss/perplexity = 6.78271008/882.4570312 secs/batch = 0.5414s, grad.norm=0.33966643\n",
      "   430: 0 [  430/ 1499], train_loss/perplexity = 6.52289391/680.5449829 secs/batch = 0.5376s, grad.norm=0.36335975\n",
      "   435: 0 [  435/ 1499], train_loss/perplexity = 6.59673595/732.6997070 secs/batch = 0.5426s, grad.norm=0.51651639\n",
      "   440: 0 [  440/ 1499], train_loss/perplexity = 6.59202242/729.2542114 secs/batch = 0.5418s, grad.norm=0.34399137\n",
      "   445: 0 [  445/ 1499], train_loss/perplexity = 6.61462164/745.9224243 secs/batch = 0.5483s, grad.norm=0.42661843\n",
      "   450: 0 [  450/ 1499], train_loss/perplexity = 6.64990139/772.7081299 secs/batch = 0.5356s, grad.norm=0.55940378\n",
      "   455: 0 [  455/ 1499], train_loss/perplexity = 6.66344261/783.2426758 secs/batch = 0.5352s, grad.norm=0.44745365\n",
      "   460: 0 [  460/ 1499], train_loss/perplexity = 6.79700184/895.1594238 secs/batch = 0.5410s, grad.norm=0.92568392\n",
      "   465: 0 [  465/ 1499], train_loss/perplexity = 6.67458868/792.0216064 secs/batch = 0.5384s, grad.norm=0.36509123\n",
      "   470: 0 [  470/ 1499], train_loss/perplexity = 6.79398727/892.4649658 secs/batch = 0.5370s, grad.norm=0.42790800\n",
      "   475: 0 [  475/ 1499], train_loss/perplexity = 6.59852171/734.0092773 secs/batch = 0.5353s, grad.norm=0.43085268\n",
      "   480: 0 [  480/ 1499], train_loss/perplexity = 6.60608721/739.5834961 secs/batch = 0.5413s, grad.norm=0.32275870\n",
      "   485: 0 [  485/ 1499], train_loss/perplexity = 6.66746092/786.3963623 secs/batch = 0.5431s, grad.norm=0.31361005\n",
      "   490: 0 [  490/ 1499], train_loss/perplexity = 6.62887478/756.6303101 secs/batch = 0.5473s, grad.norm=0.69130373\n",
      "   495: 0 [  495/ 1499], train_loss/perplexity = 6.64208412/766.6912231 secs/batch = 0.5380s, grad.norm=0.40845504\n",
      "   500: 0 [  500/ 1499], train_loss/perplexity = 6.73645163/842.5656738 secs/batch = 0.5916s, grad.norm=0.64608341\n",
      "   505: 0 [  505/ 1499], train_loss/perplexity = 6.46131802/639.9039307 secs/batch = 0.5403s, grad.norm=0.35556227\n",
      "   510: 0 [  510/ 1499], train_loss/perplexity = 6.75880146/861.6088867 secs/batch = 0.5429s, grad.norm=0.30980957\n",
      "   515: 0 [  515/ 1499], train_loss/perplexity = 6.57657194/718.0734863 secs/batch = 0.5408s, grad.norm=0.45310661\n",
      "   520: 0 [  520/ 1499], train_loss/perplexity = 6.79362631/892.1428833 secs/batch = 0.5444s, grad.norm=0.35592324\n",
      "   525: 0 [  525/ 1499], train_loss/perplexity = 6.91666603/1008.9505615 secs/batch = 0.5461s, grad.norm=0.50687110\n",
      "   530: 0 [  530/ 1499], train_loss/perplexity = 6.65585470/777.3220215 secs/batch = 0.5388s, grad.norm=0.49948555\n",
      "   535: 0 [  535/ 1499], train_loss/perplexity = 6.58111000/721.3395996 secs/batch = 0.5422s, grad.norm=0.41596952\n",
      "   540: 0 [  540/ 1499], train_loss/perplexity = 6.56899071/712.6502075 secs/batch = 0.5380s, grad.norm=0.46233657\n",
      "   545: 0 [  545/ 1499], train_loss/perplexity = 6.42729998/618.5017090 secs/batch = 0.5395s, grad.norm=0.65435636\n",
      "   550: 0 [  550/ 1499], train_loss/perplexity = 6.64267111/767.1413574 secs/batch = 0.5474s, grad.norm=0.43825257\n",
      "   555: 0 [  555/ 1499], train_loss/perplexity = 6.48377800/654.4387817 secs/batch = 0.5440s, grad.norm=0.41596040\n",
      "   560: 0 [  560/ 1499], train_loss/perplexity = 6.87060452/963.5308838 secs/batch = 0.5417s, grad.norm=0.61804181\n",
      "   565: 0 [  565/ 1499], train_loss/perplexity = 6.58342600/723.0121460 secs/batch = 0.5803s, grad.norm=0.32403567\n",
      "   570: 0 [  570/ 1499], train_loss/perplexity = 6.53397369/688.1271973 secs/batch = 0.5390s, grad.norm=0.28501704\n",
      "   575: 0 [  575/ 1499], train_loss/perplexity = 6.75026608/854.2860107 secs/batch = 0.5419s, grad.norm=1.09746063\n",
      "   580: 0 [  580/ 1499], train_loss/perplexity = 6.48975992/658.3652954 secs/batch = 0.5453s, grad.norm=0.37865958\n",
      "   585: 0 [  585/ 1499], train_loss/perplexity = 6.52872038/684.5217285 secs/batch = 0.5413s, grad.norm=0.42048693\n",
      "   590: 0 [  590/ 1499], train_loss/perplexity = 6.69236231/806.2245483 secs/batch = 0.5390s, grad.norm=0.64849412\n",
      "   595: 0 [  595/ 1499], train_loss/perplexity = 6.56055260/706.6621094 secs/batch = 0.5444s, grad.norm=0.26578006\n",
      "   600: 0 [  600/ 1499], train_loss/perplexity = 6.66821671/786.9909058 secs/batch = 0.5477s, grad.norm=0.49405050\n",
      "   605: 0 [  605/ 1499], train_loss/perplexity = 6.47536135/648.9536743 secs/batch = 0.5416s, grad.norm=0.34858578\n",
      "   610: 0 [  610/ 1499], train_loss/perplexity = 6.70364714/815.3742065 secs/batch = 0.5417s, grad.norm=0.44766417\n",
      "   615: 0 [  615/ 1499], train_loss/perplexity = 6.71979189/828.6450195 secs/batch = 0.5433s, grad.norm=0.48043135\n",
      "   620: 0 [  620/ 1499], train_loss/perplexity = 6.54954195/698.9239502 secs/batch = 0.5407s, grad.norm=0.30389974\n",
      "   625: 0 [  625/ 1499], train_loss/perplexity = 6.60607195/739.5722046 secs/batch = 0.5429s, grad.norm=0.36760679\n",
      "   630: 0 [  630/ 1499], train_loss/perplexity = 6.70804024/818.9641113 secs/batch = 0.5382s, grad.norm=0.36008716\n",
      "   635: 0 [  635/ 1499], train_loss/perplexity = 6.58663368/725.3350220 secs/batch = 0.5436s, grad.norm=0.70500648\n",
      "   640: 0 [  640/ 1499], train_loss/perplexity = 6.64935732/772.2878418 secs/batch = 0.5426s, grad.norm=0.30387124\n",
      "   645: 0 [  645/ 1499], train_loss/perplexity = 6.48686981/656.4652710 secs/batch = 0.5349s, grad.norm=0.31664306\n",
      "   650: 0 [  650/ 1499], train_loss/perplexity = 6.46839714/644.4499512 secs/batch = 0.5519s, grad.norm=0.53280395\n",
      "   655: 0 [  655/ 1499], train_loss/perplexity = 6.70073223/813.0009155 secs/batch = 0.5424s, grad.norm=0.47132611\n",
      "   660: 0 [  660/ 1499], train_loss/perplexity = 6.78019238/880.2380371 secs/batch = 0.5447s, grad.norm=0.36796808\n",
      "   665: 0 [  665/ 1499], train_loss/perplexity = 6.78560829/885.0182495 secs/batch = 0.5397s, grad.norm=0.35987845\n",
      "   670: 0 [  670/ 1499], train_loss/perplexity = 6.67328405/790.9890137 secs/batch = 0.5412s, grad.norm=0.40938026\n",
      "   675: 0 [  675/ 1499], train_loss/perplexity = 6.58391523/723.3659668 secs/batch = 0.5432s, grad.norm=0.35206249\n",
      "   680: 0 [  680/ 1499], train_loss/perplexity = 6.62061644/750.4075317 secs/batch = 0.5377s, grad.norm=0.41547358\n",
      "   685: 0 [  685/ 1499], train_loss/perplexity = 6.66847992/787.1981201 secs/batch = 0.5442s, grad.norm=0.37866428\n",
      "   690: 0 [  690/ 1499], train_loss/perplexity = 6.63233137/759.2501831 secs/batch = 0.5416s, grad.norm=0.42819425\n",
      "   695: 0 [  695/ 1499], train_loss/perplexity = 6.51923847/678.0618286 secs/batch = 0.5373s, grad.norm=0.38235751\n",
      "   700: 0 [  700/ 1499], train_loss/perplexity = 6.72567081/833.5309448 secs/batch = 0.5444s, grad.norm=0.34418362\n",
      "   705: 0 [  705/ 1499], train_loss/perplexity = 6.48960781/658.2651367 secs/batch = 0.5452s, grad.norm=0.36929053\n",
      "   710: 0 [  710/ 1499], train_loss/perplexity = 6.53926182/691.7757568 secs/batch = 0.5469s, grad.norm=0.26262283\n",
      "   715: 0 [  715/ 1499], train_loss/perplexity = 6.58596420/724.8496094 secs/batch = 0.5413s, grad.norm=0.37130007\n",
      "   720: 0 [  720/ 1499], train_loss/perplexity = 6.65871334/779.5473022 secs/batch = 0.5428s, grad.norm=0.31422433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   725: 0 [  725/ 1499], train_loss/perplexity = 6.52330303/680.8234863 secs/batch = 0.5357s, grad.norm=0.67174274\n",
      "   730: 0 [  730/ 1499], train_loss/perplexity = 6.60094213/735.7880859 secs/batch = 0.5373s, grad.norm=0.45806772\n",
      "   735: 0 [  735/ 1499], train_loss/perplexity = 6.57344103/715.8287964 secs/batch = 0.5337s, grad.norm=0.41944647\n",
      "   740: 0 [  740/ 1499], train_loss/perplexity = 6.57568359/717.4359131 secs/batch = 0.5410s, grad.norm=0.30755678\n",
      "   745: 0 [  745/ 1499], train_loss/perplexity = 6.48124552/652.7835083 secs/batch = 0.5372s, grad.norm=0.41048518\n",
      "   750: 0 [  750/ 1499], train_loss/perplexity = 6.62926912/756.9287720 secs/batch = 0.5467s, grad.norm=0.34821105\n",
      "   755: 0 [  755/ 1499], train_loss/perplexity = 6.50807333/670.5332642 secs/batch = 0.5429s, grad.norm=0.32785997\n",
      "   760: 0 [  760/ 1499], train_loss/perplexity = 6.28847456/538.3314819 secs/batch = 0.5420s, grad.norm=0.33857116\n",
      "   765: 0 [  765/ 1499], train_loss/perplexity = 6.53475618/688.6658325 secs/batch = 0.5373s, grad.norm=0.62272656\n",
      "   770: 0 [  770/ 1499], train_loss/perplexity = 6.59699631/732.8905029 secs/batch = 0.5846s, grad.norm=0.41805804\n",
      "   775: 0 [  775/ 1499], train_loss/perplexity = 6.79882669/896.7944336 secs/batch = 0.5398s, grad.norm=0.34699312\n",
      "   780: 0 [  780/ 1499], train_loss/perplexity = 6.47716522/650.1253662 secs/batch = 0.5428s, grad.norm=0.30478594\n",
      "   785: 0 [  785/ 1499], train_loss/perplexity = 6.56705618/711.2728882 secs/batch = 0.5556s, grad.norm=0.35426587\n",
      "   790: 0 [  790/ 1499], train_loss/perplexity = 6.37903118/589.3564453 secs/batch = 0.5374s, grad.norm=0.42432195\n",
      "   795: 0 [  795/ 1499], train_loss/perplexity = 6.63196516/758.9722290 secs/batch = 0.5839s, grad.norm=0.40839258\n",
      "   800: 0 [  800/ 1499], train_loss/perplexity = 6.51381111/674.3917236 secs/batch = 0.5438s, grad.norm=0.44693363\n",
      "   805: 0 [  805/ 1499], train_loss/perplexity = 6.58475256/723.9718628 secs/batch = 0.5463s, grad.norm=0.25474101\n",
      "   810: 0 [  810/ 1499], train_loss/perplexity = 6.61577129/746.7805176 secs/batch = 0.5392s, grad.norm=0.53808671\n",
      "   815: 0 [  815/ 1499], train_loss/perplexity = 6.61725283/747.8876953 secs/batch = 0.5422s, grad.norm=0.34571561\n",
      "   820: 0 [  820/ 1499], train_loss/perplexity = 6.40768576/606.4884644 secs/batch = 0.5449s, grad.norm=0.39519373\n",
      "   825: 0 [  825/ 1499], train_loss/perplexity = 6.62536049/753.9759521 secs/batch = 0.5393s, grad.norm=0.32359615\n",
      "   830: 0 [  830/ 1499], train_loss/perplexity = 6.59115744/728.6237183 secs/batch = 0.5401s, grad.norm=0.34478220\n",
      "   835: 0 [  835/ 1499], train_loss/perplexity = 6.56107807/707.0335083 secs/batch = 0.5372s, grad.norm=0.43927562\n",
      "   840: 0 [  840/ 1499], train_loss/perplexity = 6.48138285/652.8731689 secs/batch = 0.5392s, grad.norm=0.35346717\n",
      "   845: 0 [  845/ 1499], train_loss/perplexity = 6.52360678/681.0302734 secs/batch = 0.5400s, grad.norm=0.50119430\n",
      "   850: 0 [  850/ 1499], train_loss/perplexity = 6.56607103/710.5725098 secs/batch = 0.5381s, grad.norm=0.33522642\n",
      "   855: 0 [  855/ 1499], train_loss/perplexity = 6.66649818/785.6395874 secs/batch = 0.5384s, grad.norm=0.61077034\n",
      "   860: 0 [  860/ 1499], train_loss/perplexity = 6.32161283/556.4697876 secs/batch = 0.5343s, grad.norm=0.38085365\n",
      "   865: 0 [  865/ 1499], train_loss/perplexity = 6.54567003/696.2230225 secs/batch = 0.5351s, grad.norm=0.28946796\n",
      "   870: 0 [  870/ 1499], train_loss/perplexity = 6.43795347/625.1261597 secs/batch = 0.5365s, grad.norm=0.31877077\n",
      "   875: 0 [  875/ 1499], train_loss/perplexity = 6.58191824/721.9228516 secs/batch = 0.5407s, grad.norm=0.27921546\n",
      "   880: 0 [  880/ 1499], train_loss/perplexity = 6.44575262/630.0206909 secs/batch = 0.5447s, grad.norm=0.26821834\n",
      "   885: 0 [  885/ 1499], train_loss/perplexity = 6.38283062/591.5999146 secs/batch = 0.5419s, grad.norm=0.32863075\n",
      "   890: 0 [  890/ 1499], train_loss/perplexity = 6.54885054/698.4408569 secs/batch = 0.5431s, grad.norm=0.32203588\n",
      "   895: 0 [  895/ 1499], train_loss/perplexity = 6.52295732/680.5881348 secs/batch = 0.5409s, grad.norm=0.33574080\n",
      "   900: 0 [  900/ 1499], train_loss/perplexity = 6.53401709/688.1570435 secs/batch = 0.5420s, grad.norm=0.37119970\n",
      "   905: 0 [  905/ 1499], train_loss/perplexity = 6.56147718/707.3157349 secs/batch = 0.5392s, grad.norm=0.29058722\n",
      "   910: 0 [  910/ 1499], train_loss/perplexity = 6.70255566/814.4847412 secs/batch = 0.5446s, grad.norm=0.41764465\n",
      "   915: 0 [  915/ 1499], train_loss/perplexity = 6.43795824/625.1291504 secs/batch = 0.5406s, grad.norm=0.32782191\n",
      "   920: 0 [  920/ 1499], train_loss/perplexity = 6.29035234/539.3433228 secs/batch = 0.5442s, grad.norm=0.33519420\n",
      "   925: 0 [  925/ 1499], train_loss/perplexity = 6.65359592/775.5681763 secs/batch = 0.5461s, grad.norm=0.41569617\n",
      "   930: 0 [  930/ 1499], train_loss/perplexity = 6.57865810/719.5730591 secs/batch = 0.5376s, grad.norm=0.39121410\n",
      "   935: 0 [  935/ 1499], train_loss/perplexity = 6.61121225/743.3836670 secs/batch = 0.5407s, grad.norm=0.40881988\n",
      "   940: 0 [  940/ 1499], train_loss/perplexity = 6.34317446/568.5984497 secs/batch = 0.5421s, grad.norm=0.44135702\n",
      "   945: 0 [  945/ 1499], train_loss/perplexity = 6.61273527/744.5166626 secs/batch = 0.5418s, grad.norm=0.40684047\n",
      "   950: 0 [  950/ 1499], train_loss/perplexity = 6.59620047/732.3074951 secs/batch = 0.5366s, grad.norm=0.32767108\n",
      "   955: 0 [  955/ 1499], train_loss/perplexity = 6.67173576/789.7652588 secs/batch = 0.5499s, grad.norm=0.31711659\n",
      "   960: 0 [  960/ 1499], train_loss/perplexity = 6.54306316/694.4104004 secs/batch = 0.5406s, grad.norm=0.48935288\n",
      "   965: 0 [  965/ 1499], train_loss/perplexity = 6.51451683/674.8677979 secs/batch = 0.5410s, grad.norm=0.34123066\n",
      "   970: 0 [  970/ 1499], train_loss/perplexity = 6.72119379/829.8075562 secs/batch = 0.5442s, grad.norm=0.41939557\n",
      "   975: 0 [  975/ 1499], train_loss/perplexity = 6.57145977/714.4119873 secs/batch = 0.5397s, grad.norm=0.29637882\n",
      "   980: 0 [  980/ 1499], train_loss/perplexity = 6.57464266/716.6894531 secs/batch = 0.5403s, grad.norm=0.47710451\n",
      "   985: 0 [  985/ 1499], train_loss/perplexity = 6.41620827/611.6793823 secs/batch = 0.5430s, grad.norm=0.37993297\n",
      "   990: 0 [  990/ 1499], train_loss/perplexity = 6.57299614/715.5103760 secs/batch = 0.5347s, grad.norm=0.30635786\n",
      "   995: 0 [  995/ 1499], train_loss/perplexity = 6.73920584/844.8894653 secs/batch = 0.5403s, grad.norm=0.54763091\n",
      "  1000: 0 [ 1000/ 1499], train_loss/perplexity = 6.41316032/609.8178711 secs/batch = 0.5494s, grad.norm=0.30161265\n",
      "  1005: 0 [ 1005/ 1499], train_loss/perplexity = 6.71881628/827.8369751 secs/batch = 0.5453s, grad.norm=0.41859004\n",
      "  1010: 0 [ 1010/ 1499], train_loss/perplexity = 6.58723879/725.7741089 secs/batch = 0.5458s, grad.norm=0.30422357\n",
      "  1015: 0 [ 1015/ 1499], train_loss/perplexity = 6.44031096/626.6016235 secs/batch = 0.5408s, grad.norm=0.33926561\n",
      "  1020: 0 [ 1020/ 1499], train_loss/perplexity = 6.51318169/673.9673462 secs/batch = 0.5377s, grad.norm=0.46465868\n",
      "  1025: 0 [ 1025/ 1499], train_loss/perplexity = 6.44410896/628.9859619 secs/batch = 0.5416s, grad.norm=0.38345328\n",
      "  1030: 0 [ 1030/ 1499], train_loss/perplexity = 6.50059128/665.5350342 secs/batch = 0.5413s, grad.norm=0.30027229\n",
      "  1035: 0 [ 1035/ 1499], train_loss/perplexity = 6.86328840/956.5072632 secs/batch = 0.5409s, grad.norm=0.76152450\n",
      "  1040: 0 [ 1040/ 1499], train_loss/perplexity = 6.40522909/605.0003662 secs/batch = 0.5432s, grad.norm=0.31572387\n",
      "  1045: 0 [ 1045/ 1499], train_loss/perplexity = 6.60709333/740.3280029 secs/batch = 0.5388s, grad.norm=0.31082109\n",
      "  1050: 0 [ 1050/ 1499], train_loss/perplexity = 6.24729443/516.6132202 secs/batch = 0.5364s, grad.norm=0.39073661\n",
      "  1055: 0 [ 1055/ 1499], train_loss/perplexity = 6.55137396/700.2055664 secs/batch = 0.5440s, grad.norm=0.51979113\n",
      "  1060: 0 [ 1060/ 1499], train_loss/perplexity = 6.67375326/791.3602295 secs/batch = 0.5390s, grad.norm=0.32597333\n",
      "  1065: 0 [ 1065/ 1499], train_loss/perplexity = 6.46338463/641.2277222 secs/batch = 0.5929s, grad.norm=0.33962789\n",
      "  1070: 0 [ 1070/ 1499], train_loss/perplexity = 6.65898180/779.7565918 secs/batch = 0.5379s, grad.norm=0.42435664\n",
      "  1075: 0 [ 1075/ 1499], train_loss/perplexity = 6.56385469/708.9993896 secs/batch = 0.5410s, grad.norm=0.27427500\n",
      "  1080: 0 [ 1080/ 1499], train_loss/perplexity = 6.70603895/817.3267212 secs/batch = 0.5372s, grad.norm=0.32566512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1085: 0 [ 1085/ 1499], train_loss/perplexity = 6.68645906/801.4792480 secs/batch = 0.5370s, grad.norm=0.32147256\n",
      "  1090: 0 [ 1090/ 1499], train_loss/perplexity = 6.43140650/621.0468140 secs/batch = 0.5377s, grad.norm=0.34510538\n",
      "  1095: 0 [ 1095/ 1499], train_loss/perplexity = 6.62986803/757.3822021 secs/batch = 0.5432s, grad.norm=0.32318753\n",
      "  1100: 0 [ 1100/ 1499], train_loss/perplexity = 6.62116766/750.8212891 secs/batch = 0.5441s, grad.norm=0.29048184\n",
      "  1105: 0 [ 1105/ 1499], train_loss/perplexity = 6.53903818/691.6210327 secs/batch = 0.5483s, grad.norm=0.34881800\n",
      "  1110: 0 [ 1110/ 1499], train_loss/perplexity = 6.60372019/737.8349609 secs/batch = 0.5392s, grad.norm=0.28734133\n",
      "  1115: 0 [ 1115/ 1499], train_loss/perplexity = 6.36170435/579.2327271 secs/batch = 0.5519s, grad.norm=0.32726750\n",
      "  1120: 0 [ 1120/ 1499], train_loss/perplexity = 6.60577869/739.3553467 secs/batch = 0.5395s, grad.norm=0.40353474\n",
      "  1125: 0 [ 1125/ 1499], train_loss/perplexity = 6.54308033/694.4223633 secs/batch = 0.5429s, grad.norm=0.58018655\n",
      "  1130: 0 [ 1130/ 1499], train_loss/perplexity = 6.41749382/612.4662476 secs/batch = 0.5374s, grad.norm=0.33600625\n",
      "  1135: 0 [ 1135/ 1499], train_loss/perplexity = 6.74568415/850.3807373 secs/batch = 0.5324s, grad.norm=0.36664549\n",
      "  1140: 0 [ 1140/ 1499], train_loss/perplexity = 6.62472963/753.5004883 secs/batch = 0.5392s, grad.norm=0.31622428\n",
      "  1145: 0 [ 1145/ 1499], train_loss/perplexity = 6.57149601/714.4378662 secs/batch = 0.5457s, grad.norm=0.32059860\n",
      "  1150: 0 [ 1150/ 1499], train_loss/perplexity = 6.59649801/732.5253906 secs/batch = 0.5422s, grad.norm=0.35328507\n",
      "  1155: 0 [ 1155/ 1499], train_loss/perplexity = 6.37645006/587.8372192 secs/batch = 0.5482s, grad.norm=0.28386909\n",
      "  1160: 0 [ 1160/ 1499], train_loss/perplexity = 6.56001139/706.2797241 secs/batch = 0.5364s, grad.norm=0.40525278\n",
      "  1165: 0 [ 1165/ 1499], train_loss/perplexity = 6.38176060/590.9672241 secs/batch = 0.5424s, grad.norm=0.42688641\n",
      "  1170: 0 [ 1170/ 1499], train_loss/perplexity = 6.64684105/770.3469849 secs/batch = 0.5379s, grad.norm=0.32655734\n",
      "  1175: 0 [ 1175/ 1499], train_loss/perplexity = 6.33173609/562.1316528 secs/batch = 0.5390s, grad.norm=0.62986588\n",
      "  1180: 0 [ 1180/ 1499], train_loss/perplexity = 6.48464680/655.0075684 secs/batch = 0.5404s, grad.norm=0.42801502\n",
      "  1185: 0 [ 1185/ 1499], train_loss/perplexity = 6.45718956/637.2675171 secs/batch = 0.5382s, grad.norm=0.36655727\n",
      "  1190: 0 [ 1190/ 1499], train_loss/perplexity = 6.66610050/785.3272705 secs/batch = 0.5406s, grad.norm=0.31236479\n",
      "  1195: 0 [ 1195/ 1499], train_loss/perplexity = 6.59815741/733.7419434 secs/batch = 0.5435s, grad.norm=0.31136206\n",
      "  1200: 0 [ 1200/ 1499], train_loss/perplexity = 6.60423899/738.2178345 secs/batch = 0.5395s, grad.norm=0.42569974\n",
      "  1205: 0 [ 1205/ 1499], train_loss/perplexity = 6.48854017/657.5627441 secs/batch = 0.5462s, grad.norm=0.39152771\n",
      "  1210: 0 [ 1210/ 1499], train_loss/perplexity = 6.60719013/740.3996582 secs/batch = 0.5410s, grad.norm=0.60007781\n",
      "  1215: 0 [ 1215/ 1499], train_loss/perplexity = 6.22581577/505.6353455 secs/batch = 0.5421s, grad.norm=0.34048963\n",
      "  1220: 0 [ 1220/ 1499], train_loss/perplexity = 6.56840801/712.2350464 secs/batch = 0.5417s, grad.norm=0.33681569\n",
      "  1225: 0 [ 1225/ 1499], train_loss/perplexity = 6.28391886/535.8845825 secs/batch = 0.5857s, grad.norm=0.55357170\n",
      "  1230: 0 [ 1230/ 1499], train_loss/perplexity = 6.59212637/729.3300171 secs/batch = 0.5415s, grad.norm=0.42106202\n",
      "  1235: 0 [ 1235/ 1499], train_loss/perplexity = 6.40765715/606.4711304 secs/batch = 0.5383s, grad.norm=0.48425704\n",
      "  1240: 0 [ 1240/ 1499], train_loss/perplexity = 6.34663916/570.5718994 secs/batch = 0.5423s, grad.norm=0.34719381\n",
      "  1245: 0 [ 1245/ 1499], train_loss/perplexity = 6.57979059/720.3884888 secs/batch = 0.5378s, grad.norm=0.49055436\n",
      "  1250: 0 [ 1250/ 1499], train_loss/perplexity = 6.87499285/967.7684326 secs/batch = 0.5384s, grad.norm=0.31444463\n",
      "  1255: 0 [ 1255/ 1499], train_loss/perplexity = 6.50397158/667.7885742 secs/batch = 0.5439s, grad.norm=0.43614560\n",
      "  1260: 0 [ 1260/ 1499], train_loss/perplexity = 6.45196486/633.9466553 secs/batch = 0.5444s, grad.norm=0.33134216\n",
      "  1265: 0 [ 1265/ 1499], train_loss/perplexity = 6.60533905/739.0303955 secs/batch = 0.5478s, grad.norm=0.39092708\n",
      "  1270: 0 [ 1270/ 1499], train_loss/perplexity = 6.59262657/729.6949463 secs/batch = 0.5381s, grad.norm=0.36290774\n",
      "  1275: 0 [ 1275/ 1499], train_loss/perplexity = 6.58671856/725.3966064 secs/batch = 0.5461s, grad.norm=0.29321238\n",
      "  1280: 0 [ 1280/ 1499], train_loss/perplexity = 6.36700201/582.3094482 secs/batch = 0.5390s, grad.norm=0.49194983\n",
      "  1285: 0 [ 1285/ 1499], train_loss/perplexity = 6.60551119/739.1575928 secs/batch = 0.5440s, grad.norm=0.38744891\n",
      "  1290: 0 [ 1290/ 1499], train_loss/perplexity = 6.59258032/729.6611938 secs/batch = 0.5408s, grad.norm=0.36363137\n",
      "  1295: 0 [ 1295/ 1499], train_loss/perplexity = 6.39227295/597.2124634 secs/batch = 0.5361s, grad.norm=0.31721663\n",
      "  1300: 0 [ 1300/ 1499], train_loss/perplexity = 6.58470869/723.9401245 secs/batch = 0.5421s, grad.norm=0.41156068\n",
      "  1305: 0 [ 1305/ 1499], train_loss/perplexity = 6.52914667/684.8135986 secs/batch = 0.5496s, grad.norm=0.36026448\n",
      "  1310: 0 [ 1310/ 1499], train_loss/perplexity = 6.58108568/721.3220215 secs/batch = 0.5427s, grad.norm=0.34871238\n",
      "  1315: 0 [ 1315/ 1499], train_loss/perplexity = 6.43844271/625.4320679 secs/batch = 0.5355s, grad.norm=0.46074113\n",
      "  1320: 0 [ 1320/ 1499], train_loss/perplexity = 6.22717047/506.3208008 secs/batch = 0.5405s, grad.norm=0.29584298\n",
      "  1325: 0 [ 1325/ 1499], train_loss/perplexity = 6.52095318/679.2255249 secs/batch = 0.5428s, grad.norm=0.34046340\n",
      "  1330: 0 [ 1330/ 1499], train_loss/perplexity = 6.66480827/784.3130493 secs/batch = 0.5425s, grad.norm=0.30832905\n",
      "  1335: 0 [ 1335/ 1499], train_loss/perplexity = 6.46028948/639.2460938 secs/batch = 0.5894s, grad.norm=0.30788878\n",
      "  1340: 0 [ 1340/ 1499], train_loss/perplexity = 6.22136879/503.3917847 secs/batch = 0.5375s, grad.norm=0.41857082\n",
      "  1345: 0 [ 1345/ 1499], train_loss/perplexity = 6.31332874/551.8789673 secs/batch = 0.5385s, grad.norm=0.55141312\n",
      "  1350: 0 [ 1350/ 1499], train_loss/perplexity = 6.37138748/584.8687744 secs/batch = 0.5349s, grad.norm=0.45115727\n",
      "  1355: 0 [ 1355/ 1499], train_loss/perplexity = 6.16299009/474.8457947 secs/batch = 0.5535s, grad.norm=0.51879746\n",
      "  1360: 0 [ 1360/ 1499], train_loss/perplexity = 6.19069242/488.1840210 secs/batch = 0.5410s, grad.norm=0.62076747\n",
      "  1365: 0 [ 1365/ 1499], train_loss/perplexity = 6.26826429/527.5609131 secs/batch = 0.5371s, grad.norm=0.42707396\n",
      "  1370: 0 [ 1370/ 1499], train_loss/perplexity = 6.15751171/472.2515259 secs/batch = 0.5336s, grad.norm=0.33093607\n",
      "  1375: 0 [ 1375/ 1499], train_loss/perplexity = 6.22398520/504.7106018 secs/batch = 0.5388s, grad.norm=0.44423828\n",
      "  1380: 0 [ 1380/ 1499], train_loss/perplexity = 6.37190628/585.1722412 secs/batch = 0.5440s, grad.norm=0.45024595\n",
      "  1385: 0 [ 1385/ 1499], train_loss/perplexity = 6.34768248/571.1674805 secs/batch = 0.5419s, grad.norm=0.31128785\n",
      "  1390: 0 [ 1390/ 1499], train_loss/perplexity = 6.34310722/568.5602417 secs/batch = 0.5379s, grad.norm=0.58506179\n",
      "  1395: 0 [ 1395/ 1499], train_loss/perplexity = 6.39023113/595.9943237 secs/batch = 0.5349s, grad.norm=0.33352447\n",
      "  1400: 0 [ 1400/ 1499], train_loss/perplexity = 6.53998804/692.2783203 secs/batch = 0.5392s, grad.norm=0.52294397\n",
      "  1405: 0 [ 1405/ 1499], train_loss/perplexity = 6.32278252/557.1210327 secs/batch = 0.5507s, grad.norm=0.40532157\n",
      "  1410: 0 [ 1410/ 1499], train_loss/perplexity = 6.45905542/638.4577026 secs/batch = 0.5422s, grad.norm=0.44780421\n",
      "  1415: 0 [ 1415/ 1499], train_loss/perplexity = 6.37016582/584.1546631 secs/batch = 0.5375s, grad.norm=0.37836337\n",
      "  1420: 0 [ 1420/ 1499], train_loss/perplexity = 6.44845390/631.7248535 secs/batch = 0.5396s, grad.norm=0.38613677\n",
      "  1425: 0 [ 1425/ 1499], train_loss/perplexity = 6.37768459/588.5633545 secs/batch = 0.5372s, grad.norm=0.49662238\n",
      "  1430: 0 [ 1430/ 1499], train_loss/perplexity = 6.51781178/677.0951538 secs/batch = 0.5411s, grad.norm=0.57620710\n",
      "  1435: 0 [ 1435/ 1499], train_loss/perplexity = 6.26004457/523.2422485 secs/batch = 0.5393s, grad.norm=0.31711039\n",
      "  1440: 0 [ 1440/ 1499], train_loss/perplexity = 5.99387503/400.9653625 secs/batch = 0.5399s, grad.norm=0.33909395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1445: 0 [ 1445/ 1499], train_loss/perplexity = 6.31693411/553.8722534 secs/batch = 0.5748s, grad.norm=0.35198790\n",
      "  1450: 0 [ 1450/ 1499], train_loss/perplexity = 6.31810999/554.5239258 secs/batch = 0.5382s, grad.norm=0.29216608\n",
      "  1455: 0 [ 1455/ 1499], train_loss/perplexity = 6.54954863/698.9286499 secs/batch = 0.5433s, grad.norm=0.37018383\n",
      "  1460: 0 [ 1460/ 1499], train_loss/perplexity = 6.58060026/720.9719849 secs/batch = 0.5425s, grad.norm=0.38409078\n",
      "  1465: 0 [ 1465/ 1499], train_loss/perplexity = 6.81263685/909.2652588 secs/batch = 0.5421s, grad.norm=0.37625739\n",
      "  1470: 0 [ 1470/ 1499], train_loss/perplexity = 6.41194677/609.0782471 secs/batch = 0.5395s, grad.norm=0.46633723\n",
      "  1475: 0 [ 1475/ 1499], train_loss/perplexity = 6.37993383/589.8886719 secs/batch = 0.5380s, grad.norm=0.33547100\n",
      "  1480: 0 [ 1480/ 1499], train_loss/perplexity = 6.52528381/682.1733398 secs/batch = 0.5355s, grad.norm=0.33582419\n",
      "  1485: 0 [ 1485/ 1499], train_loss/perplexity = 6.23986483/512.7891846 secs/batch = 0.5363s, grad.norm=0.33655781\n",
      "  1490: 0 [ 1490/ 1499], train_loss/perplexity = 6.31130648/550.7640381 secs/batch = 0.5429s, grad.norm=0.37318173\n",
      "  1495: 0 [ 1495/ 1499], train_loss/perplexity = 6.59807110/733.6786499 secs/batch = 0.5365s, grad.norm=0.37567669\n",
      "Epoch training time: 818.0175364017487\n",
      "Saved char model cv/epoch000_6.4272.model\n",
      "  1504: 1 [    5/ 1499], train_loss/perplexity = 6.41237879/609.3414307 secs/batch = 0.5404s, grad.norm=0.34353438\n",
      "  1509: 1 [   10/ 1499], train_loss/perplexity = 6.47241545/647.0447388 secs/batch = 0.5390s, grad.norm=0.42311916\n",
      "  1514: 1 [   15/ 1499], train_loss/perplexity = 6.38949871/595.5579834 secs/batch = 0.5476s, grad.norm=0.31941402\n",
      "  1519: 1 [   20/ 1499], train_loss/perplexity = 6.31641340/553.5839233 secs/batch = 0.5465s, grad.norm=0.32075945\n",
      "  1524: 1 [   25/ 1499], train_loss/perplexity = 6.53500366/688.8363037 secs/batch = 0.5357s, grad.norm=0.33869284\n",
      "  1529: 1 [   30/ 1499], train_loss/perplexity = 6.50931168/671.3641357 secs/batch = 0.5371s, grad.norm=0.43728051\n",
      "  1534: 1 [   35/ 1499], train_loss/perplexity = 6.38650513/593.7777710 secs/batch = 0.5421s, grad.norm=0.38907242\n",
      "  1539: 1 [   40/ 1499], train_loss/perplexity = 6.45487738/635.7957764 secs/batch = 0.5376s, grad.norm=0.43733782\n",
      "  1544: 1 [   45/ 1499], train_loss/perplexity = 6.51425552/674.6914673 secs/batch = 0.5466s, grad.norm=0.43923029\n",
      "  1549: 1 [   50/ 1499], train_loss/perplexity = 6.27851295/532.9954834 secs/batch = 0.5435s, grad.norm=0.38089606\n",
      "  1554: 1 [   55/ 1499], train_loss/perplexity = 6.30651760/548.1328125 secs/batch = 0.5513s, grad.norm=0.38162139\n",
      "  1559: 1 [   60/ 1499], train_loss/perplexity = 6.32541609/558.5902100 secs/batch = 0.5415s, grad.norm=0.36381239\n",
      "  1564: 1 [   65/ 1499], train_loss/perplexity = 6.28550482/536.7351685 secs/batch = 0.5429s, grad.norm=0.39015490\n",
      "  1569: 1 [   70/ 1499], train_loss/perplexity = 6.28369713/535.7658081 secs/batch = 0.5469s, grad.norm=0.36839753\n",
      "  1574: 1 [   75/ 1499], train_loss/perplexity = 6.11838293/454.1297302 secs/batch = 0.5440s, grad.norm=0.36615428\n",
      "  1579: 1 [   80/ 1499], train_loss/perplexity = 6.16768551/477.0806274 secs/batch = 0.5345s, grad.norm=0.46023542\n",
      "  1584: 1 [   85/ 1499], train_loss/perplexity = 6.29615116/542.4799805 secs/batch = 0.5350s, grad.norm=0.34657952\n",
      "  1589: 1 [   90/ 1499], train_loss/perplexity = 6.36337042/580.1985474 secs/batch = 0.5417s, grad.norm=0.32423595\n",
      "  1594: 1 [   95/ 1499], train_loss/perplexity = 6.24862862/517.3029175 secs/batch = 0.5466s, grad.norm=0.60316974\n",
      "  1599: 1 [  100/ 1499], train_loss/perplexity = 6.24522495/515.5451660 secs/batch = 0.5387s, grad.norm=0.41163567\n",
      "  1604: 1 [  105/ 1499], train_loss/perplexity = 6.18180227/483.8632202 secs/batch = 0.5460s, grad.norm=0.39819500\n",
      "  1609: 1 [  110/ 1499], train_loss/perplexity = 6.13358593/461.0866089 secs/batch = 0.5466s, grad.norm=0.45565766\n",
      "  1614: 1 [  115/ 1499], train_loss/perplexity = 6.31076574/550.4663086 secs/batch = 0.5474s, grad.norm=0.35894278\n",
      "  1619: 1 [  120/ 1499], train_loss/perplexity = 6.19540977/490.4924011 secs/batch = 0.5427s, grad.norm=0.34805313\n",
      "  1624: 1 [  125/ 1499], train_loss/perplexity = 6.49443340/661.4493408 secs/batch = 0.5385s, grad.norm=0.56194562\n",
      "  1629: 1 [  130/ 1499], train_loss/perplexity = 6.29124832/539.8267822 secs/batch = 0.5386s, grad.norm=0.45132688\n",
      "  1634: 1 [  135/ 1499], train_loss/perplexity = 6.32408571/557.8475342 secs/batch = 0.5374s, grad.norm=0.33711112\n",
      "  1639: 1 [  140/ 1499], train_loss/perplexity = 6.31250715/551.4257202 secs/batch = 0.5373s, grad.norm=0.82540536\n",
      "  1644: 1 [  145/ 1499], train_loss/perplexity = 6.18595552/485.8770142 secs/batch = 0.5382s, grad.norm=0.42925227\n",
      "  1649: 1 [  150/ 1499], train_loss/perplexity = 6.38310957/591.7649536 secs/batch = 0.5384s, grad.norm=0.45615390\n",
      "  1654: 1 [  155/ 1499], train_loss/perplexity = 6.43258905/621.7816772 secs/batch = 0.5907s, grad.norm=0.43953520\n",
      "  1659: 1 [  160/ 1499], train_loss/perplexity = 6.52975225/685.2284546 secs/batch = 0.5410s, grad.norm=0.30693910\n",
      "  1664: 1 [  165/ 1499], train_loss/perplexity = 6.19223547/488.9378967 secs/batch = 0.5733s, grad.norm=0.49805364\n",
      "  1669: 1 [  170/ 1499], train_loss/perplexity = 6.49552727/662.1732788 secs/batch = 0.5398s, grad.norm=0.74555516\n",
      "  1674: 1 [  175/ 1499], train_loss/perplexity = 6.28403234/535.9454346 secs/batch = 0.5426s, grad.norm=0.37172604\n",
      "  1679: 1 [  180/ 1499], train_loss/perplexity = 6.55641079/703.7412720 secs/batch = 0.5381s, grad.norm=0.38325155\n",
      "  1684: 1 [  185/ 1499], train_loss/perplexity = 6.25449276/520.3453979 secs/batch = 0.5479s, grad.norm=0.47497094\n",
      "  1689: 1 [  190/ 1499], train_loss/perplexity = 6.61774921/748.2590332 secs/batch = 0.5445s, grad.norm=0.43040901\n",
      "  1694: 1 [  195/ 1499], train_loss/perplexity = 6.46728039/643.7306519 secs/batch = 0.5362s, grad.norm=0.41948330\n",
      "  1699: 1 [  200/ 1499], train_loss/perplexity = 6.48968410/658.3153687 secs/batch = 0.5406s, grad.norm=0.40494585\n",
      "  1704: 1 [  205/ 1499], train_loss/perplexity = 6.42513561/617.1644897 secs/batch = 0.5379s, grad.norm=0.30016392\n",
      "  1709: 1 [  210/ 1499], train_loss/perplexity = 6.21666050/501.0272522 secs/batch = 0.5484s, grad.norm=0.32051069\n",
      "  1714: 1 [  215/ 1499], train_loss/perplexity = 6.33491898/563.9237061 secs/batch = 0.5419s, grad.norm=0.41849229\n",
      "  1719: 1 [  220/ 1499], train_loss/perplexity = 6.25422430/520.2056885 secs/batch = 0.5413s, grad.norm=0.39090386\n",
      "  1724: 1 [  225/ 1499], train_loss/perplexity = 6.39437485/598.4690552 secs/batch = 0.5417s, grad.norm=0.48567331\n",
      "  1729: 1 [  230/ 1499], train_loss/perplexity = 6.35397768/574.7744141 secs/batch = 0.5403s, grad.norm=0.34521252\n",
      "  1734: 1 [  235/ 1499], train_loss/perplexity = 6.41809273/612.8331299 secs/batch = 0.5373s, grad.norm=0.51466984\n",
      "  1739: 1 [  240/ 1499], train_loss/perplexity = 6.48255730/653.6403809 secs/batch = 0.5399s, grad.norm=0.35949147\n",
      "  1744: 1 [  245/ 1499], train_loss/perplexity = 6.24616194/516.0284424 secs/batch = 0.5374s, grad.norm=0.42233419\n",
      "  1749: 1 [  250/ 1499], train_loss/perplexity = 6.28907728/538.6560669 secs/batch = 0.5383s, grad.norm=0.32567221\n",
      "  1754: 1 [  255/ 1499], train_loss/perplexity = 6.14484453/466.3071594 secs/batch = 0.5386s, grad.norm=0.37880716\n",
      "  1759: 1 [  260/ 1499], train_loss/perplexity = 6.40088987/602.3808594 secs/batch = 0.5383s, grad.norm=0.37388965\n",
      "  1764: 1 [  265/ 1499], train_loss/perplexity = 6.36900139/583.4748535 secs/batch = 0.5494s, grad.norm=0.55846167\n",
      "  1769: 1 [  270/ 1499], train_loss/perplexity = 6.48585606/655.8001099 secs/batch = 0.5437s, grad.norm=0.31238875\n",
      "  1774: 1 [  275/ 1499], train_loss/perplexity = 6.10617447/448.6192322 secs/batch = 0.5430s, grad.norm=0.39919320\n",
      "  1779: 1 [  280/ 1499], train_loss/perplexity = 6.23663139/511.1337891 secs/batch = 0.5384s, grad.norm=0.35409871\n",
      "  1784: 1 [  285/ 1499], train_loss/perplexity = 6.64016294/765.2196655 secs/batch = 0.5412s, grad.norm=0.37575129\n",
      "  1789: 1 [  290/ 1499], train_loss/perplexity = 6.57549620/717.3014526 secs/batch = 0.5383s, grad.norm=0.44355667\n",
      "  1794: 1 [  295/ 1499], train_loss/perplexity = 6.27798700/532.7152100 secs/batch = 0.5349s, grad.norm=0.39700335\n",
      "  1799: 1 [  300/ 1499], train_loss/perplexity = 6.16371632/475.1907654 secs/batch = 0.5463s, grad.norm=0.35092208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1804: 1 [  305/ 1499], train_loss/perplexity = 6.37815762/588.8418579 secs/batch = 0.5367s, grad.norm=0.47976202\n",
      "  1809: 1 [  310/ 1499], train_loss/perplexity = 6.56866121/712.4154053 secs/batch = 0.5357s, grad.norm=0.43348306\n",
      "  1814: 1 [  315/ 1499], train_loss/perplexity = 6.42409563/616.5230103 secs/batch = 0.5370s, grad.norm=0.40078473\n",
      "  1819: 1 [  320/ 1499], train_loss/perplexity = 6.45274687/634.4426270 secs/batch = 0.5451s, grad.norm=0.35786355\n",
      "  1824: 1 [  325/ 1499], train_loss/perplexity = 6.31093979/550.5621338 secs/batch = 0.5966s, grad.norm=0.42508742\n",
      "  1829: 1 [  330/ 1499], train_loss/perplexity = 6.24469614/515.2726440 secs/batch = 0.5464s, grad.norm=0.53714508\n",
      "  1834: 1 [  335/ 1499], train_loss/perplexity = 6.43552542/623.6101685 secs/batch = 0.5372s, grad.norm=0.43948698\n",
      "  1839: 1 [  340/ 1499], train_loss/perplexity = 6.12510490/457.1926575 secs/batch = 0.5387s, grad.norm=0.35895768\n",
      "  1844: 1 [  345/ 1499], train_loss/perplexity = 6.11412573/452.2005310 secs/batch = 0.5404s, grad.norm=0.50958914\n",
      "  1849: 1 [  350/ 1499], train_loss/perplexity = 6.31561995/553.1448975 secs/batch = 0.5370s, grad.norm=0.50424719\n",
      "  1854: 1 [  355/ 1499], train_loss/perplexity = 6.10525942/448.2088928 secs/batch = 0.5423s, grad.norm=0.44678313\n",
      "  1859: 1 [  360/ 1499], train_loss/perplexity = 6.28682327/537.4432983 secs/batch = 0.5387s, grad.norm=0.42424592\n",
      "  1864: 1 [  365/ 1499], train_loss/perplexity = 6.22603846/505.7479553 secs/batch = 0.5374s, grad.norm=1.09072065\n",
      "  1869: 1 [  370/ 1499], train_loss/perplexity = 6.20383596/494.6428528 secs/batch = 0.5390s, grad.norm=0.33605602\n",
      "  1874: 1 [  375/ 1499], train_loss/perplexity = 6.15926361/473.0795898 secs/batch = 0.5525s, grad.norm=0.41974729\n",
      "  1879: 1 [  380/ 1499], train_loss/perplexity = 6.38117647/590.6221313 secs/batch = 0.5451s, grad.norm=0.47637731\n",
      "  1884: 1 [  385/ 1499], train_loss/perplexity = 6.39430571/598.4276733 secs/batch = 0.5394s, grad.norm=0.35853603\n",
      "  1889: 1 [  390/ 1499], train_loss/perplexity = 6.59736109/733.1578979 secs/batch = 0.5318s, grad.norm=0.36643121\n",
      "  1894: 1 [  395/ 1499], train_loss/perplexity = 6.57641935/717.9639282 secs/batch = 0.5346s, grad.norm=0.33338967\n",
      "  1899: 1 [  400/ 1499], train_loss/perplexity = 6.48152494/652.9659424 secs/batch = 0.5390s, grad.norm=0.48339805\n",
      "  1904: 1 [  405/ 1499], train_loss/perplexity = 6.35302591/574.2276611 secs/batch = 0.5463s, grad.norm=0.41897696\n",
      "  1909: 1 [  410/ 1499], train_loss/perplexity = 6.06119013/428.8855591 secs/batch = 0.5415s, grad.norm=0.48271427\n",
      "  1914: 1 [  415/ 1499], train_loss/perplexity = 6.47336626/647.6602783 secs/batch = 0.5446s, grad.norm=0.36020434\n",
      "  1919: 1 [  420/ 1499], train_loss/perplexity = 6.28358412/535.7052612 secs/batch = 0.5369s, grad.norm=0.36525899\n",
      "  1924: 1 [  425/ 1499], train_loss/perplexity = 6.53251219/687.1222534 secs/batch = 0.5400s, grad.norm=0.36233675\n",
      "  1929: 1 [  430/ 1499], train_loss/perplexity = 6.19171238/488.6821899 secs/batch = 0.5380s, grad.norm=0.36112922\n",
      "  1934: 1 [  435/ 1499], train_loss/perplexity = 6.27797794/532.7103882 secs/batch = 0.5464s, grad.norm=0.62255710\n",
      "  1939: 1 [  440/ 1499], train_loss/perplexity = 6.38097191/590.5013428 secs/batch = 0.5425s, grad.norm=0.34075409\n",
      "  1944: 1 [  445/ 1499], train_loss/perplexity = 6.34723234/570.9104614 secs/batch = 0.5429s, grad.norm=0.36457750\n",
      "  1949: 1 [  450/ 1499], train_loss/perplexity = 6.26669645/526.7344360 secs/batch = 0.5430s, grad.norm=0.44282371\n",
      "  1954: 1 [  455/ 1499], train_loss/perplexity = 6.40294838/603.6221313 secs/batch = 0.5428s, grad.norm=0.41063157\n",
      "  1959: 1 [  460/ 1499], train_loss/perplexity = 6.43519974/623.4071045 secs/batch = 0.5495s, grad.norm=0.59032148\n",
      "  1964: 1 [  465/ 1499], train_loss/perplexity = 6.33558559/564.2997437 secs/batch = 0.5365s, grad.norm=0.40649563\n",
      "  1969: 1 [  470/ 1499], train_loss/perplexity = 6.45328140/634.7818604 secs/batch = 0.5458s, grad.norm=0.43878603\n",
      "  1974: 1 [  475/ 1499], train_loss/perplexity = 6.31069183/550.4255981 secs/batch = 0.5444s, grad.norm=0.46039465\n",
      "  1979: 1 [  480/ 1499], train_loss/perplexity = 6.33980417/566.6853027 secs/batch = 0.5415s, grad.norm=0.38458648\n",
      "  1984: 1 [  485/ 1499], train_loss/perplexity = 6.39305973/597.6825562 secs/batch = 0.5419s, grad.norm=0.42168778\n",
      "  1989: 1 [  490/ 1499], train_loss/perplexity = 6.22753477/506.5052795 secs/batch = 0.5382s, grad.norm=0.40309170\n",
      "  1994: 1 [  495/ 1499], train_loss/perplexity = 6.28988218/539.0897827 secs/batch = 0.5350s, grad.norm=0.44036070\n",
      "  1999: 1 [  500/ 1499], train_loss/perplexity = 6.39352655/597.9616089 secs/batch = 0.5379s, grad.norm=0.43770427\n",
      "  2004: 1 [  505/ 1499], train_loss/perplexity = 6.10725737/449.1052856 secs/batch = 0.5356s, grad.norm=0.42275888\n",
      "  2009: 1 [  510/ 1499], train_loss/perplexity = 6.48372412/654.4035034 secs/batch = 0.5370s, grad.norm=0.37940782\n",
      "  2014: 1 [  515/ 1499], train_loss/perplexity = 6.25222921/519.1688843 secs/batch = 0.5430s, grad.norm=0.41121438\n",
      "  2019: 1 [  520/ 1499], train_loss/perplexity = 6.54807138/697.8969116 secs/batch = 0.5449s, grad.norm=0.51346993\n",
      "  2024: 1 [  525/ 1499], train_loss/perplexity = 6.49158716/659.5693970 secs/batch = 0.5389s, grad.norm=0.39026901\n",
      "  2029: 1 [  530/ 1499], train_loss/perplexity = 6.36932802/583.6654663 secs/batch = 0.5422s, grad.norm=0.43672255\n",
      "  2034: 1 [  535/ 1499], train_loss/perplexity = 6.29295874/540.7509155 secs/batch = 0.5394s, grad.norm=0.40596136\n",
      "  2039: 1 [  540/ 1499], train_loss/perplexity = 6.11246443/451.4499207 secs/batch = 0.5367s, grad.norm=0.34986347\n",
      "  2044: 1 [  545/ 1499], train_loss/perplexity = 6.01938772/411.3266602 secs/batch = 0.5389s, grad.norm=0.72701967\n",
      "  2049: 1 [  550/ 1499], train_loss/perplexity = 6.39786005/600.5584717 secs/batch = 0.5449s, grad.norm=0.43297386\n",
      "  2054: 1 [  555/ 1499], train_loss/perplexity = 6.18072081/483.3402405 secs/batch = 0.5368s, grad.norm=0.39013284\n",
      "  2059: 1 [  560/ 1499], train_loss/perplexity = 6.53634739/689.7625122 secs/batch = 0.5342s, grad.norm=0.42131996\n",
      "  2064: 1 [  565/ 1499], train_loss/perplexity = 6.39367819/598.0523071 secs/batch = 0.5362s, grad.norm=0.46101013\n",
      "  2069: 1 [  570/ 1499], train_loss/perplexity = 6.28502703/536.4787598 secs/batch = 0.5498s, grad.norm=0.36509490\n",
      "  2074: 1 [  575/ 1499], train_loss/perplexity = 6.45761156/637.5364990 secs/batch = 0.5440s, grad.norm=0.91378045\n",
      "  2079: 1 [  580/ 1499], train_loss/perplexity = 6.11862278/454.2386780 secs/batch = 0.5405s, grad.norm=0.42634895\n",
      "  2084: 1 [  585/ 1499], train_loss/perplexity = 6.13016176/459.5104980 secs/batch = 0.5422s, grad.norm=0.38096055\n",
      "  2089: 1 [  590/ 1499], train_loss/perplexity = 6.39392996/598.2028809 secs/batch = 0.5421s, grad.norm=0.67683870\n",
      "  2094: 1 [  595/ 1499], train_loss/perplexity = 6.29371119/541.1579590 secs/batch = 0.5920s, grad.norm=0.36537528\n",
      "  2099: 1 [  600/ 1499], train_loss/perplexity = 6.05394983/425.7915039 secs/batch = 0.5325s, grad.norm=0.43834972\n",
      "  2104: 1 [  605/ 1499], train_loss/perplexity = 6.05496597/426.2243958 secs/batch = 0.5376s, grad.norm=0.35471302\n",
      "  2109: 1 [  610/ 1499], train_loss/perplexity = 6.37961817/589.7025146 secs/batch = 0.5615s, grad.norm=0.49130705\n",
      "  2114: 1 [  615/ 1499], train_loss/perplexity = 6.41325665/609.8765869 secs/batch = 0.5376s, grad.norm=0.57691550\n",
      "  2119: 1 [  620/ 1499], train_loss/perplexity = 6.13508320/461.7775269 secs/batch = 0.5919s, grad.norm=0.36740181\n",
      "  2124: 1 [  625/ 1499], train_loss/perplexity = 6.23488474/510.2417908 secs/batch = 0.5481s, grad.norm=0.46724570\n",
      "  2129: 1 [  630/ 1499], train_loss/perplexity = 6.34881163/571.8128052 secs/batch = 0.5462s, grad.norm=0.39036858\n",
      "  2134: 1 [  635/ 1499], train_loss/perplexity = 6.22497559/505.2106934 secs/batch = 0.5368s, grad.norm=0.74104726\n",
      "  2139: 1 [  640/ 1499], train_loss/perplexity = 6.34814739/571.4331055 secs/batch = 0.5462s, grad.norm=0.37380949\n",
      "  2144: 1 [  645/ 1499], train_loss/perplexity = 6.08944702/441.1773682 secs/batch = 0.5421s, grad.norm=0.35026628\n",
      "  2149: 1 [  650/ 1499], train_loss/perplexity = 6.07150698/433.3332214 secs/batch = 0.5445s, grad.norm=0.51243746\n",
      "  2154: 1 [  655/ 1499], train_loss/perplexity = 6.33205509/562.3110352 secs/batch = 0.5404s, grad.norm=0.40358728\n",
      "  2159: 1 [  660/ 1499], train_loss/perplexity = 6.49913931/664.5693970 secs/batch = 0.5478s, grad.norm=0.46860430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2164: 1 [  665/ 1499], train_loss/perplexity = 6.49079609/659.0477905 secs/batch = 0.5390s, grad.norm=0.40604243\n",
      "  2169: 1 [  670/ 1499], train_loss/perplexity = 6.37791872/588.7011719 secs/batch = 0.5393s, grad.norm=0.44745278\n",
      "  2174: 1 [  675/ 1499], train_loss/perplexity = 6.32105255/556.1580811 secs/batch = 0.5428s, grad.norm=0.34707427\n",
      "  2179: 1 [  680/ 1499], train_loss/perplexity = 6.34775639/571.2097168 secs/batch = 0.5458s, grad.norm=0.38400230\n",
      "  2184: 1 [  685/ 1499], train_loss/perplexity = 6.42425442/616.6209106 secs/batch = 0.5393s, grad.norm=0.52731264\n",
      "  2189: 1 [  690/ 1499], train_loss/perplexity = 6.30655146/548.1513672 secs/batch = 0.5392s, grad.norm=0.40353149\n",
      "  2194: 1 [  695/ 1499], train_loss/perplexity = 6.18162537/483.7776184 secs/batch = 0.5379s, grad.norm=0.42706722\n",
      "  2199: 1 [  700/ 1499], train_loss/perplexity = 6.43365002/622.4417114 secs/batch = 0.5449s, grad.norm=0.37177816\n",
      "  2204: 1 [  705/ 1499], train_loss/perplexity = 6.07863140/436.4314880 secs/batch = 0.5452s, grad.norm=0.39054787\n",
      "  2209: 1 [  710/ 1499], train_loss/perplexity = 6.27431154/530.7608643 secs/batch = 0.5415s, grad.norm=0.44969496\n",
      "  2214: 1 [  715/ 1499], train_loss/perplexity = 6.28527308/536.6107788 secs/batch = 0.5488s, grad.norm=0.38626161\n",
      "  2219: 1 [  720/ 1499], train_loss/perplexity = 6.32839012/560.2539062 secs/batch = 0.5404s, grad.norm=0.48531809\n",
      "  2224: 1 [  725/ 1499], train_loss/perplexity = 6.18302202/484.4537659 secs/batch = 0.5438s, grad.norm=0.66649657\n",
      "  2229: 1 [  730/ 1499], train_loss/perplexity = 6.30014944/544.6533203 secs/batch = 0.5351s, grad.norm=0.48209414\n",
      "  2234: 1 [  735/ 1499], train_loss/perplexity = 6.16212177/474.4336548 secs/batch = 0.5346s, grad.norm=0.37134394\n",
      "  2239: 1 [  740/ 1499], train_loss/perplexity = 6.20903492/497.2211609 secs/batch = 0.5387s, grad.norm=0.40398186\n",
      "  2244: 1 [  745/ 1499], train_loss/perplexity = 6.06181622/429.1541748 secs/batch = 0.5345s, grad.norm=0.49420217\n",
      "  2249: 1 [  750/ 1499], train_loss/perplexity = 6.32579374/558.8012085 secs/batch = 0.5379s, grad.norm=0.35332900\n",
      "  2254: 1 [  755/ 1499], train_loss/perplexity = 6.06314993/429.7269287 secs/batch = 0.5416s, grad.norm=0.39521906\n",
      "  2259: 1 [  760/ 1499], train_loss/perplexity = 5.81737280/336.0879211 secs/batch = 0.5384s, grad.norm=0.42869404\n",
      "  2264: 1 [  765/ 1499], train_loss/perplexity = 6.11026049/450.4560242 secs/batch = 0.5360s, grad.norm=0.49302483\n",
      "  2269: 1 [  770/ 1499], train_loss/perplexity = 6.22381592/504.6251831 secs/batch = 0.5494s, grad.norm=0.40310776\n",
      "  2274: 1 [  775/ 1499], train_loss/perplexity = 6.50408363/667.8634033 secs/batch = 0.5399s, grad.norm=0.40589100\n",
      "  2279: 1 [  780/ 1499], train_loss/perplexity = 6.16046238/473.6470337 secs/batch = 0.5367s, grad.norm=0.37631297\n",
      "  2284: 1 [  785/ 1499], train_loss/perplexity = 6.16824532/477.3477783 secs/batch = 0.5398s, grad.norm=0.37865597\n",
      "  2289: 1 [  790/ 1499], train_loss/perplexity = 5.84350157/344.9852295 secs/batch = 0.5396s, grad.norm=0.45145679\n",
      "  2294: 1 [  795/ 1499], train_loss/perplexity = 6.36695290/582.2808228 secs/batch = 0.5410s, grad.norm=0.61313093\n",
      "  2299: 1 [  800/ 1499], train_loss/perplexity = 6.11573935/452.9307861 secs/batch = 0.5444s, grad.norm=0.43572894\n",
      "  2304: 1 [  805/ 1499], train_loss/perplexity = 6.20394659/494.6975708 secs/batch = 0.5438s, grad.norm=0.32676879\n",
      "  2309: 1 [  810/ 1499], train_loss/perplexity = 6.18301439/484.4500732 secs/batch = 0.5392s, grad.norm=0.44073677\n",
      "  2314: 1 [  815/ 1499], train_loss/perplexity = 6.23164034/508.5890503 secs/batch = 0.5396s, grad.norm=0.36784181\n",
      "  2319: 1 [  820/ 1499], train_loss/perplexity = 5.92623615/374.7413940 secs/batch = 0.5457s, grad.norm=0.54166102\n",
      "  2324: 1 [  825/ 1499], train_loss/perplexity = 6.09773016/444.8468933 secs/batch = 0.5400s, grad.norm=0.37008491\n",
      "  2329: 1 [  830/ 1499], train_loss/perplexity = 6.25375366/519.9609375 secs/batch = 0.5670s, grad.norm=0.37371027\n",
      "  2334: 1 [  835/ 1499], train_loss/perplexity = 6.21028519/497.8432007 secs/batch = 0.5435s, grad.norm=0.49810684\n",
      "  2339: 1 [  840/ 1499], train_loss/perplexity = 6.11479473/452.5031433 secs/batch = 0.5383s, grad.norm=0.42811173\n",
      "  2344: 1 [  845/ 1499], train_loss/perplexity = 6.13181877/460.2725220 secs/batch = 0.5373s, grad.norm=0.56625110\n",
      "  2349: 1 [  850/ 1499], train_loss/perplexity = 6.21708393/501.2394409 secs/batch = 0.5442s, grad.norm=0.37850738\n",
      "  2354: 1 [  855/ 1499], train_loss/perplexity = 6.26522827/525.9616089 secs/batch = 0.5361s, grad.norm=0.42780602\n",
      "  2359: 1 [  860/ 1499], train_loss/perplexity = 5.99928093/403.1387939 secs/batch = 0.5385s, grad.norm=0.68698305\n",
      "  2364: 1 [  865/ 1499], train_loss/perplexity = 6.16805220/477.2556152 secs/batch = 0.5402s, grad.norm=0.46477434\n",
      "  2369: 1 [  870/ 1499], train_loss/perplexity = 6.07639360/435.4559326 secs/batch = 0.5417s, grad.norm=0.37976074\n",
      "  2374: 1 [  875/ 1499], train_loss/perplexity = 6.19881964/492.1677551 secs/batch = 0.5422s, grad.norm=0.44327927\n",
      "  2379: 1 [  880/ 1499], train_loss/perplexity = 6.06337881/429.8252869 secs/batch = 0.5420s, grad.norm=0.32692832\n",
      "  2384: 1 [  885/ 1499], train_loss/perplexity = 5.96637392/390.0886230 secs/batch = 0.5360s, grad.norm=0.40459540\n",
      "  2389: 1 [  890/ 1499], train_loss/perplexity = 6.17403078/480.1174622 secs/batch = 0.5782s, grad.norm=0.35333541\n",
      "  2394: 1 [  895/ 1499], train_loss/perplexity = 6.19928980/492.3992004 secs/batch = 0.5421s, grad.norm=0.39208835\n",
      "  2399: 1 [  900/ 1499], train_loss/perplexity = 6.16269779/474.7070007 secs/batch = 0.5403s, grad.norm=0.44879752\n",
      "  2404: 1 [  905/ 1499], train_loss/perplexity = 6.22094870/503.1803589 secs/batch = 0.5439s, grad.norm=0.56227273\n",
      "  2409: 1 [  910/ 1499], train_loss/perplexity = 6.36138344/579.0468750 secs/batch = 0.5363s, grad.norm=0.40965748\n",
      "  2414: 1 [  915/ 1499], train_loss/perplexity = 6.08311081/438.3908386 secs/batch = 0.5423s, grad.norm=0.40123242\n",
      "  2419: 1 [  920/ 1499], train_loss/perplexity = 5.85989809/350.6884155 secs/batch = 0.5409s, grad.norm=0.36198592\n",
      "  2424: 1 [  925/ 1499], train_loss/perplexity = 6.23217535/508.8612366 secs/batch = 0.5395s, grad.norm=0.49849918\n",
      "  2429: 1 [  930/ 1499], train_loss/perplexity = 6.17409849/480.1499634 secs/batch = 0.5445s, grad.norm=0.39702690\n",
      "  2434: 1 [  935/ 1499], train_loss/perplexity = 6.24240971/514.0958252 secs/batch = 0.5400s, grad.norm=0.40118763\n",
      "  2439: 1 [  940/ 1499], train_loss/perplexity = 5.89688492/363.9021301 secs/batch = 0.5373s, grad.norm=0.65659446\n",
      "  2444: 1 [  945/ 1499], train_loss/perplexity = 6.21931171/502.3573608 secs/batch = 0.5410s, grad.norm=0.40377674\n",
      "  2449: 1 [  950/ 1499], train_loss/perplexity = 6.15342093/470.3235779 secs/batch = 0.5437s, grad.norm=0.38103265\n",
      "  2454: 1 [  955/ 1499], train_loss/perplexity = 6.34608173/570.2539062 secs/batch = 0.5411s, grad.norm=0.37359843\n",
      "  2459: 1 [  960/ 1499], train_loss/perplexity = 6.12533426/457.2975464 secs/batch = 0.5413s, grad.norm=0.41670412\n",
      "  2464: 1 [  965/ 1499], train_loss/perplexity = 6.10348892/447.4160461 secs/batch = 0.5327s, grad.norm=0.45767942\n",
      "  2469: 1 [  970/ 1499], train_loss/perplexity = 6.41491890/610.8912354 secs/batch = 0.5411s, grad.norm=0.55273926\n",
      "  2474: 1 [  975/ 1499], train_loss/perplexity = 6.24352217/514.6680908 secs/batch = 0.5448s, grad.norm=0.43760553\n",
      "  2479: 1 [  980/ 1499], train_loss/perplexity = 6.27765226/532.5369263 secs/batch = 0.5427s, grad.norm=0.57607824\n",
      "  2484: 1 [  985/ 1499], train_loss/perplexity = 6.00550079/405.6540833 secs/batch = 0.5398s, grad.norm=0.41544682\n",
      "  2489: 1 [  990/ 1499], train_loss/perplexity = 6.13475227/461.6247253 secs/batch = 0.5389s, grad.norm=0.38731655\n",
      "  2494: 1 [  995/ 1499], train_loss/perplexity = 6.32811689/560.1008911 secs/batch = 0.5421s, grad.norm=0.48794636\n",
      "  2499: 1 [ 1000/ 1499], train_loss/perplexity = 6.00216389/404.3027039 secs/batch = 0.5372s, grad.norm=0.36731407\n",
      "  2504: 1 [ 1005/ 1499], train_loss/perplexity = 6.33321905/562.9658813 secs/batch = 0.5365s, grad.norm=0.47606307\n",
      "  2509: 1 [ 1010/ 1499], train_loss/perplexity = 6.23033714/507.9266968 secs/batch = 0.5388s, grad.norm=0.48298943\n",
      "  2514: 1 [ 1015/ 1499], train_loss/perplexity = 6.02616739/414.1248169 secs/batch = 0.5374s, grad.norm=0.41294244\n",
      "  2519: 1 [ 1020/ 1499], train_loss/perplexity = 6.13318348/460.9010925 secs/batch = 0.5441s, grad.norm=0.54546815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2524: 1 [ 1025/ 1499], train_loss/perplexity = 6.13661146/462.4837646 secs/batch = 0.5435s, grad.norm=0.51283818\n",
      "  2529: 1 [ 1030/ 1499], train_loss/perplexity = 6.14385366/465.8453369 secs/batch = 0.5430s, grad.norm=0.32038638\n",
      "  2534: 1 [ 1035/ 1499], train_loss/perplexity = 6.56315756/708.5053101 secs/batch = 0.5384s, grad.norm=0.63205379\n",
      "  2539: 1 [ 1040/ 1499], train_loss/perplexity = 5.99668694/402.0944214 secs/batch = 0.5445s, grad.norm=0.38887426\n",
      "  2544: 1 [ 1045/ 1499], train_loss/perplexity = 6.17535591/480.7540894 secs/batch = 0.5361s, grad.norm=0.42275581\n",
      "  2549: 1 [ 1050/ 1499], train_loss/perplexity = 5.85937023/350.5033264 secs/batch = 0.5822s, grad.norm=0.59220517\n",
      "  2554: 1 [ 1055/ 1499], train_loss/perplexity = 6.16229153/474.5141907 secs/batch = 0.5403s, grad.norm=0.49304575\n",
      "  2559: 1 [ 1060/ 1499], train_loss/perplexity = 6.36348391/580.2644043 secs/batch = 0.5377s, grad.norm=0.55010331\n",
      "  2564: 1 [ 1065/ 1499], train_loss/perplexity = 5.94577312/382.1346741 secs/batch = 0.5444s, grad.norm=0.38997859\n",
      "  2569: 1 [ 1070/ 1499], train_loss/perplexity = 6.35491753/575.3148804 secs/batch = 0.5367s, grad.norm=0.50880897\n",
      "  2574: 1 [ 1075/ 1499], train_loss/perplexity = 6.21004343/497.7228699 secs/batch = 0.5431s, grad.norm=0.36827722\n",
      "  2579: 1 [ 1080/ 1499], train_loss/perplexity = 6.31937027/555.2232666 secs/batch = 0.5465s, grad.norm=0.39601204\n",
      "  2584: 1 [ 1085/ 1499], train_loss/perplexity = 6.41765499/612.5649414 secs/batch = 0.5414s, grad.norm=0.40905374\n",
      "  2589: 1 [ 1090/ 1499], train_loss/perplexity = 6.04341412/421.3290405 secs/batch = 0.5354s, grad.norm=0.36460307\n",
      "  2594: 1 [ 1095/ 1499], train_loss/perplexity = 6.21537447/500.3833313 secs/batch = 0.5356s, grad.norm=0.38501018\n",
      "  2599: 1 [ 1100/ 1499], train_loss/perplexity = 6.24371767/514.7686768 secs/batch = 0.5407s, grad.norm=0.49441308\n",
      "  2604: 1 [ 1105/ 1499], train_loss/perplexity = 6.07742310/435.9044495 secs/batch = 0.5431s, grad.norm=0.46042976\n",
      "  2609: 1 [ 1110/ 1499], train_loss/perplexity = 6.19185257/488.7507019 secs/batch = 0.5401s, grad.norm=0.37835017\n",
      "  2614: 1 [ 1115/ 1499], train_loss/perplexity = 5.83723450/342.8299255 secs/batch = 0.5403s, grad.norm=0.43845496\n",
      "  2619: 1 [ 1120/ 1499], train_loss/perplexity = 6.11959219/454.6792297 secs/batch = 0.5386s, grad.norm=0.55718422\n",
      "  2624: 1 [ 1125/ 1499], train_loss/perplexity = 6.03371525/417.2623901 secs/batch = 0.5472s, grad.norm=0.49884689\n",
      "  2629: 1 [ 1130/ 1499], train_loss/perplexity = 5.93171167/376.7989197 secs/batch = 0.5403s, grad.norm=0.46895474\n",
      "  2634: 1 [ 1135/ 1499], train_loss/perplexity = 6.32758570/559.8034058 secs/batch = 0.5428s, grad.norm=0.43577445\n",
      "  2639: 1 [ 1140/ 1499], train_loss/perplexity = 6.21725845/501.3269348 secs/batch = 0.5387s, grad.norm=0.38251954\n",
      "  2644: 1 [ 1145/ 1499], train_loss/perplexity = 6.28736067/537.7321777 secs/batch = 0.5324s, grad.norm=0.42459053\n",
      "  2649: 1 [ 1150/ 1499], train_loss/perplexity = 6.24787760/516.9145508 secs/batch = 0.5446s, grad.norm=0.41230595\n",
      "  2654: 1 [ 1155/ 1499], train_loss/perplexity = 5.97511292/393.5125427 secs/batch = 0.5403s, grad.norm=0.53352267\n",
      "  2659: 1 [ 1160/ 1499], train_loss/perplexity = 6.18225956/484.0845337 secs/batch = 0.5956s, grad.norm=0.39547464\n",
      "  2664: 1 [ 1165/ 1499], train_loss/perplexity = 6.01671267/410.2278137 secs/batch = 0.5426s, grad.norm=0.43800321\n",
      "  2669: 1 [ 1170/ 1499], train_loss/perplexity = 6.29316568/540.8627930 secs/batch = 0.5424s, grad.norm=0.37484699\n",
      "  2674: 1 [ 1175/ 1499], train_loss/perplexity = 5.80542660/332.0968323 secs/batch = 0.5463s, grad.norm=0.41228184\n",
      "  2679: 1 [ 1180/ 1499], train_loss/perplexity = 6.07491732/434.8135376 secs/batch = 0.5452s, grad.norm=0.45774993\n",
      "  2684: 1 [ 1185/ 1499], train_loss/perplexity = 5.99729300/402.3381958 secs/batch = 0.5369s, grad.norm=0.52043730\n",
      "  2689: 1 [ 1190/ 1499], train_loss/perplexity = 6.21167803/498.5371094 secs/batch = 0.5390s, grad.norm=0.35142624\n",
      "  2694: 1 [ 1195/ 1499], train_loss/perplexity = 6.26754236/527.1801758 secs/batch = 0.5351s, grad.norm=0.42066705\n",
      "  2699: 1 [ 1200/ 1499], train_loss/perplexity = 6.14999580/468.7154236 secs/batch = 0.5441s, grad.norm=0.53004831\n",
      "  2704: 1 [ 1205/ 1499], train_loss/perplexity = 6.03062057/415.9730835 secs/batch = 0.5399s, grad.norm=0.53145278\n",
      "  2709: 1 [ 1210/ 1499], train_loss/perplexity = 6.12490940/457.1033020 secs/batch = 0.5441s, grad.norm=0.60254323\n",
      "  2714: 1 [ 1215/ 1499], train_loss/perplexity = 5.76253986/318.1553650 secs/batch = 0.5377s, grad.norm=0.58823568\n",
      "  2719: 1 [ 1220/ 1499], train_loss/perplexity = 6.17790890/481.9830322 secs/batch = 0.5415s, grad.norm=0.46575481\n",
      "  2724: 1 [ 1225/ 1499], train_loss/perplexity = 5.68804550/295.3158569 secs/batch = 0.5450s, grad.norm=0.58443999\n",
      "  2729: 1 [ 1230/ 1499], train_loss/perplexity = 6.17589855/481.0150452 secs/batch = 0.5452s, grad.norm=0.42274302\n",
      "  2734: 1 [ 1235/ 1499], train_loss/perplexity = 5.97631598/393.9862366 secs/batch = 0.5441s, grad.norm=0.57908171\n",
      "  2739: 1 [ 1240/ 1499], train_loss/perplexity = 5.92133570/372.9094849 secs/batch = 0.5398s, grad.norm=0.43001893\n",
      "  2744: 1 [ 1245/ 1499], train_loss/perplexity = 6.18779182/486.7700500 secs/batch = 0.5394s, grad.norm=0.44242528\n",
      "  2749: 1 [ 1250/ 1499], train_loss/perplexity = 6.46872711/644.6625977 secs/batch = 0.5400s, grad.norm=0.38731915\n",
      "  2754: 1 [ 1255/ 1499], train_loss/perplexity = 6.04278469/421.0639343 secs/batch = 0.5438s, grad.norm=0.42387566\n",
      "  2759: 1 [ 1260/ 1499], train_loss/perplexity = 6.07568216/435.1462402 secs/batch = 0.5409s, grad.norm=0.39904702\n",
      "  2764: 1 [ 1265/ 1499], train_loss/perplexity = 6.22129345/503.3538818 secs/batch = 0.5415s, grad.norm=0.54290998\n",
      "  2769: 1 [ 1270/ 1499], train_loss/perplexity = 6.24038219/513.0545654 secs/batch = 0.5447s, grad.norm=0.43270648\n",
      "  2774: 1 [ 1275/ 1499], train_loss/perplexity = 6.20158005/493.5282288 secs/batch = 0.5436s, grad.norm=0.43468091\n",
      "  2779: 1 [ 1280/ 1499], train_loss/perplexity = 5.88626194/360.0568542 secs/batch = 0.5439s, grad.norm=0.56259638\n",
      "  2784: 1 [ 1285/ 1499], train_loss/perplexity = 6.19639254/490.9746704 secs/batch = 0.5353s, grad.norm=0.47372892\n",
      "  2789: 1 [ 1290/ 1499], train_loss/perplexity = 6.17670059/481.4010010 secs/batch = 0.5379s, grad.norm=0.39568275\n",
      "  2794: 1 [ 1295/ 1499], train_loss/perplexity = 6.03064966/415.9851990 secs/batch = 0.5422s, grad.norm=0.45040253\n",
      "  2799: 1 [ 1300/ 1499], train_loss/perplexity = 6.26723433/527.0178223 secs/batch = 0.5372s, grad.norm=0.54350460\n",
      "  2804: 1 [ 1305/ 1499], train_loss/perplexity = 6.14618778/466.9339294 secs/batch = 0.5411s, grad.norm=0.58320898\n",
      "  2809: 1 [ 1310/ 1499], train_loss/perplexity = 6.18563652/485.7220459 secs/batch = 0.5435s, grad.norm=0.35527754\n",
      "  2814: 1 [ 1315/ 1499], train_loss/perplexity = 6.04078913/420.2245178 secs/batch = 0.5435s, grad.norm=0.41509095\n",
      "  2819: 1 [ 1320/ 1499], train_loss/perplexity = 5.77636623/322.5848694 secs/batch = 0.5475s, grad.norm=0.36514372\n",
      "  2824: 1 [ 1325/ 1499], train_loss/perplexity = 6.15281487/470.0386353 secs/batch = 0.5407s, grad.norm=0.37361526\n",
      "  2829: 1 [ 1330/ 1499], train_loss/perplexity = 6.27998066/533.7783203 secs/batch = 0.5420s, grad.norm=0.37275329\n",
      "  2834: 1 [ 1335/ 1499], train_loss/perplexity = 6.06063175/428.6461487 secs/batch = 0.5404s, grad.norm=0.40044454\n",
      "  2839: 1 [ 1340/ 1499], train_loss/perplexity = 5.64508963/282.8988953 secs/batch = 0.5422s, grad.norm=0.51083422\n",
      "  2844: 1 [ 1345/ 1499], train_loss/perplexity = 5.82395315/338.3067932 secs/batch = 0.5381s, grad.norm=0.50328350\n",
      "  2849: 1 [ 1350/ 1499], train_loss/perplexity = 5.90772200/367.8671875 secs/batch = 0.5377s, grad.norm=0.52758914\n",
      "  2854: 1 [ 1355/ 1499], train_loss/perplexity = 5.69596672/297.6643982 secs/batch = 0.5450s, grad.norm=0.51630628\n",
      "  2859: 1 [ 1360/ 1499], train_loss/perplexity = 5.70935726/301.6770935 secs/batch = 0.5362s, grad.norm=0.57278699\n",
      "  2864: 1 [ 1365/ 1499], train_loss/perplexity = 5.73740768/310.2590637 secs/batch = 0.5406s, grad.norm=0.36599162\n",
      "  2869: 1 [ 1370/ 1499], train_loss/perplexity = 5.65934134/286.9595642 secs/batch = 0.5331s, grad.norm=0.42662016\n",
      "  2874: 1 [ 1375/ 1499], train_loss/perplexity = 5.66909122/289.7710876 secs/batch = 0.5444s, grad.norm=0.46369776\n",
      "  2879: 1 [ 1380/ 1499], train_loss/perplexity = 6.05239248/425.1289368 secs/batch = 0.5418s, grad.norm=0.50955784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2884: 1 [ 1385/ 1499], train_loss/perplexity = 5.96261358/388.6245117 secs/batch = 0.5397s, grad.norm=0.51529950\n",
      "  2889: 1 [ 1390/ 1499], train_loss/perplexity = 5.95312214/384.9533386 secs/batch = 0.5415s, grad.norm=0.48251107\n",
      "  2894: 1 [ 1395/ 1499], train_loss/perplexity = 6.02153826/412.2121887 secs/batch = 0.5459s, grad.norm=0.37718141\n",
      "  2899: 1 [ 1400/ 1499], train_loss/perplexity = 6.05875349/427.8417969 secs/batch = 0.5370s, grad.norm=0.44711286\n",
      "  2904: 1 [ 1405/ 1499], train_loss/perplexity = 5.96048021/387.7962952 secs/batch = 0.5416s, grad.norm=0.48467934\n",
      "  2909: 1 [ 1410/ 1499], train_loss/perplexity = 6.11054468/450.5840759 secs/batch = 0.5383s, grad.norm=0.39748555\n",
      "  2914: 1 [ 1415/ 1499], train_loss/perplexity = 6.06038570/428.5406799 secs/batch = 0.5386s, grad.norm=0.57142246\n",
      "  2919: 1 [ 1420/ 1499], train_loss/perplexity = 6.12184954/455.7067566 secs/batch = 0.5406s, grad.norm=0.41772982\n",
      "  2924: 1 [ 1425/ 1499], train_loss/perplexity = 5.98669147/398.0953064 secs/batch = 0.5470s, grad.norm=0.55109268\n",
      "  2929: 1 [ 1430/ 1499], train_loss/perplexity = 6.14450312/466.1479797 secs/batch = 0.5801s, grad.norm=0.41273829\n",
      "  2934: 1 [ 1435/ 1499], train_loss/perplexity = 5.89332104/362.6075134 secs/batch = 0.5428s, grad.norm=0.41694385\n",
      "  2939: 1 [ 1440/ 1499], train_loss/perplexity = 5.57530737/263.8306274 secs/batch = 0.5442s, grad.norm=0.44814026\n",
      "  2944: 1 [ 1445/ 1499], train_loss/perplexity = 6.02066946/411.8542175 secs/batch = 0.5421s, grad.norm=0.43159011\n",
      "  2949: 1 [ 1450/ 1499], train_loss/perplexity = 5.95684958/386.3908997 secs/batch = 0.5410s, grad.norm=0.40419778\n",
      "  2954: 1 [ 1455/ 1499], train_loss/perplexity = 6.26388836/525.2573853 secs/batch = 0.5421s, grad.norm=0.41482934\n",
      "  2959: 1 [ 1460/ 1499], train_loss/perplexity = 6.36394262/580.5306396 secs/batch = 0.5347s, grad.norm=0.50609076\n",
      "  2964: 1 [ 1465/ 1499], train_loss/perplexity = 6.49912930/664.5627441 secs/batch = 0.5368s, grad.norm=0.41296878\n",
      "  2969: 1 [ 1470/ 1499], train_loss/perplexity = 6.11757183/453.7615356 secs/batch = 0.5373s, grad.norm=0.59998399\n",
      "  2974: 1 [ 1475/ 1499], train_loss/perplexity = 6.08593225/439.6294556 secs/batch = 0.5511s, grad.norm=0.45488870\n",
      "  2979: 1 [ 1480/ 1499], train_loss/perplexity = 6.17672396/481.4122314 secs/batch = 0.5361s, grad.norm=0.42847553\n",
      "  2984: 1 [ 1485/ 1499], train_loss/perplexity = 5.85168791/347.8209839 secs/batch = 0.5416s, grad.norm=0.40201899\n",
      "  2989: 1 [ 1490/ 1499], train_loss/perplexity = 5.99140787/399.9773254 secs/batch = 0.5381s, grad.norm=0.46418256\n",
      "  2994: 1 [ 1495/ 1499], train_loss/perplexity = 6.27928638/533.4078979 secs/batch = 0.5469s, grad.norm=0.52964014\n",
      "Epoch training time: 815.3354334831238\n",
      "Saved char model cv/epoch001_6.0870.model\n",
      "  3003: 2 [    5/ 1499], train_loss/perplexity = 6.11625624/453.1649780 secs/batch = 0.5373s, grad.norm=0.39551008\n",
      "  3008: 2 [   10/ 1499], train_loss/perplexity = 6.14985609/468.6499329 secs/batch = 0.5418s, grad.norm=0.47211760\n",
      "  3013: 2 [   15/ 1499], train_loss/perplexity = 6.04963493/423.9582214 secs/batch = 0.5432s, grad.norm=0.44561040\n",
      "  3018: 2 [   20/ 1499], train_loss/perplexity = 5.89330101/362.6002502 secs/batch = 0.5386s, grad.norm=0.34511730\n",
      "  3023: 2 [   25/ 1499], train_loss/perplexity = 6.24301386/514.4065552 secs/batch = 0.5430s, grad.norm=0.41977373\n",
      "  3028: 2 [   30/ 1499], train_loss/perplexity = 6.21502876/500.2103882 secs/batch = 0.5440s, grad.norm=0.41419071\n",
      "  3033: 2 [   35/ 1499], train_loss/perplexity = 6.03074312/416.0240784 secs/batch = 0.5439s, grad.norm=0.42778352\n",
      "  3038: 2 [   40/ 1499], train_loss/perplexity = 6.06953430/432.4792175 secs/batch = 0.5419s, grad.norm=0.46157807\n",
      "  3043: 2 [   45/ 1499], train_loss/perplexity = 6.12692499/458.0255737 secs/batch = 0.5398s, grad.norm=0.45331353\n",
      "  3048: 2 [   50/ 1499], train_loss/perplexity = 5.92929316/375.8887329 secs/batch = 0.5428s, grad.norm=0.46166569\n",
      "  3053: 2 [   55/ 1499], train_loss/perplexity = 5.98179531/396.1509399 secs/batch = 0.5422s, grad.norm=0.54602629\n",
      "  3058: 2 [   60/ 1499], train_loss/perplexity = 5.93714476/378.8516846 secs/batch = 0.5374s, grad.norm=0.43798426\n",
      "  3063: 2 [   65/ 1499], train_loss/perplexity = 5.89524698/363.3065491 secs/batch = 0.5437s, grad.norm=0.46609241\n",
      "  3068: 2 [   70/ 1499], train_loss/perplexity = 5.92416763/373.9670410 secs/batch = 0.5370s, grad.norm=0.63123184\n",
      "  3073: 2 [   75/ 1499], train_loss/perplexity = 5.72141123/305.3355103 secs/batch = 0.5366s, grad.norm=0.46599960\n",
      "  3078: 2 [   80/ 1499], train_loss/perplexity = 5.80127907/330.7223206 secs/batch = 0.5453s, grad.norm=0.40707240\n",
      "  3083: 2 [   85/ 1499], train_loss/perplexity = 5.91691160/371.2633362 secs/batch = 0.5428s, grad.norm=0.59253484\n",
      "  3088: 2 [   90/ 1499], train_loss/perplexity = 6.11186600/451.1798401 secs/batch = 0.5386s, grad.norm=0.60153067\n",
      "  3093: 2 [   95/ 1499], train_loss/perplexity = 5.86277390/351.6983643 secs/batch = 0.5422s, grad.norm=0.47411147\n",
      "  3098: 2 [  100/ 1499], train_loss/perplexity = 5.85311460/348.3175659 secs/batch = 0.5419s, grad.norm=0.40808788\n",
      "  3103: 2 [  105/ 1499], train_loss/perplexity = 5.78059864/323.9530640 secs/batch = 0.5380s, grad.norm=0.57867521\n",
      "  3108: 2 [  110/ 1499], train_loss/perplexity = 5.76831627/319.9984741 secs/batch = 0.5360s, grad.norm=0.45818987\n",
      "  3113: 2 [  115/ 1499], train_loss/perplexity = 5.95520306/385.7552185 secs/batch = 0.5457s, grad.norm=0.46054026\n",
      "  3118: 2 [  120/ 1499], train_loss/perplexity = 5.82056761/337.1633911 secs/batch = 0.5376s, grad.norm=0.49515939\n",
      "  3123: 2 [  125/ 1499], train_loss/perplexity = 6.08640623/439.8378906 secs/batch = 0.5386s, grad.norm=0.53125805\n",
      "  3128: 2 [  130/ 1499], train_loss/perplexity = 6.03909445/419.5129700 secs/batch = 0.5448s, grad.norm=0.56931651\n",
      "  3133: 2 [  135/ 1499], train_loss/perplexity = 5.99879217/402.9418030 secs/batch = 0.5520s, grad.norm=0.56025928\n",
      "  3138: 2 [  140/ 1499], train_loss/perplexity = 5.92170525/373.0473022 secs/batch = 0.5398s, grad.norm=0.58590043\n",
      "  3143: 2 [  145/ 1499], train_loss/perplexity = 5.85556698/349.1728210 secs/batch = 0.5407s, grad.norm=0.64224887\n",
      "  3148: 2 [  150/ 1499], train_loss/perplexity = 6.00369215/404.9210815 secs/batch = 0.5410s, grad.norm=0.55821449\n",
      "  3153: 2 [  155/ 1499], train_loss/perplexity = 6.07878017/436.4964294 secs/batch = 0.5430s, grad.norm=0.44080764\n",
      "  3158: 2 [  160/ 1499], train_loss/perplexity = 6.24155807/513.6582031 secs/batch = 0.5400s, grad.norm=0.41137591\n",
      "  3163: 2 [  165/ 1499], train_loss/perplexity = 5.84024334/343.8630066 secs/batch = 0.5368s, grad.norm=0.55240488\n",
      "  3168: 2 [  170/ 1499], train_loss/perplexity = 6.19349766/489.5554199 secs/batch = 0.5391s, grad.norm=0.71966577\n",
      "  3173: 2 [  175/ 1499], train_loss/perplexity = 5.91263533/369.6791077 secs/batch = 0.5447s, grad.norm=0.37649712\n",
      "  3178: 2 [  180/ 1499], train_loss/perplexity = 6.11384487/452.0735474 secs/batch = 0.5536s, grad.norm=0.43218005\n",
      "  3183: 2 [  185/ 1499], train_loss/perplexity = 5.90936756/368.4730530 secs/batch = 0.5458s, grad.norm=0.41394094\n",
      "  3188: 2 [  190/ 1499], train_loss/perplexity = 6.30552959/547.5914917 secs/batch = 0.5383s, grad.norm=0.58422923\n",
      "  3193: 2 [  195/ 1499], train_loss/perplexity = 6.17509937/480.6307983 secs/batch = 0.5401s, grad.norm=0.51187164\n",
      "  3198: 2 [  200/ 1499], train_loss/perplexity = 6.10485458/448.0274963 secs/batch = 0.5845s, grad.norm=0.39108226\n",
      "  3203: 2 [  205/ 1499], train_loss/perplexity = 6.06708622/431.4217834 secs/batch = 0.5447s, grad.norm=0.37462905\n",
      "  3208: 2 [  210/ 1499], train_loss/perplexity = 5.84566116/345.7310486 secs/batch = 0.5427s, grad.norm=0.37869024\n",
      "  3213: 2 [  215/ 1499], train_loss/perplexity = 5.97015667/391.5670166 secs/batch = 0.5372s, grad.norm=0.57270700\n",
      "  3218: 2 [  220/ 1499], train_loss/perplexity = 5.83645964/342.5643921 secs/batch = 0.5416s, grad.norm=0.41049579\n",
      "  3223: 2 [  225/ 1499], train_loss/perplexity = 5.99894762/403.0044556 secs/batch = 0.5938s, grad.norm=0.48488510\n",
      "  3228: 2 [  230/ 1499], train_loss/perplexity = 6.05482340/426.1636353 secs/batch = 0.5353s, grad.norm=0.44432268\n",
      "  3233: 2 [  235/ 1499], train_loss/perplexity = 5.96752691/390.5386353 secs/batch = 0.5412s, grad.norm=0.48764160\n",
      "  3238: 2 [  240/ 1499], train_loss/perplexity = 6.15568590/471.3900452 secs/batch = 0.5446s, grad.norm=0.41961038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3243: 2 [  245/ 1499], train_loss/perplexity = 5.85004902/347.2514038 secs/batch = 0.5453s, grad.norm=0.40856382\n",
      "  3248: 2 [  250/ 1499], train_loss/perplexity = 5.96549988/389.7478027 secs/batch = 0.5469s, grad.norm=0.49447820\n",
      "  3253: 2 [  255/ 1499], train_loss/perplexity = 5.80752420/332.7941895 secs/batch = 0.5422s, grad.norm=0.42155397\n",
      "  3258: 2 [  260/ 1499], train_loss/perplexity = 6.11057281/450.5967407 secs/batch = 0.5371s, grad.norm=0.53382552\n",
      "  3263: 2 [  265/ 1499], train_loss/perplexity = 6.00235653/404.3806152 secs/batch = 0.5352s, grad.norm=0.44103488\n",
      "  3268: 2 [  270/ 1499], train_loss/perplexity = 6.14621401/466.9461670 secs/batch = 0.5421s, grad.norm=0.56417680\n",
      "  3273: 2 [  275/ 1499], train_loss/perplexity = 5.65832615/286.6683960 secs/batch = 0.5437s, grad.norm=0.43526846\n",
      "  3278: 2 [  280/ 1499], train_loss/perplexity = 5.88303900/358.8982849 secs/batch = 0.5475s, grad.norm=0.61070144\n",
      "  3283: 2 [  285/ 1499], train_loss/perplexity = 6.29367256/541.1370239 secs/batch = 0.5482s, grad.norm=0.41550770\n",
      "  3288: 2 [  290/ 1499], train_loss/perplexity = 6.21551371/500.4530029 secs/batch = 0.5437s, grad.norm=0.54265684\n",
      "  3293: 2 [  295/ 1499], train_loss/perplexity = 5.91441774/370.3386230 secs/batch = 0.5371s, grad.norm=0.44354203\n",
      "  3298: 2 [  300/ 1499], train_loss/perplexity = 5.80747652/332.7783203 secs/batch = 0.5441s, grad.norm=0.41979396\n",
      "  3303: 2 [  305/ 1499], train_loss/perplexity = 6.07202625/433.5582886 secs/batch = 0.5714s, grad.norm=0.45641550\n",
      "  3308: 2 [  310/ 1499], train_loss/perplexity = 6.18774080/486.7452087 secs/batch = 0.5879s, grad.norm=0.47427887\n",
      "  3313: 2 [  315/ 1499], train_loss/perplexity = 6.08767033/440.3942261 secs/batch = 0.6870s, grad.norm=0.42239115\n",
      "  3318: 2 [  320/ 1499], train_loss/perplexity = 6.10603952/448.5586853 secs/batch = 0.5714s, grad.norm=0.43509138\n",
      "  3323: 2 [  325/ 1499], train_loss/perplexity = 5.96121264/388.0804443 secs/batch = 0.5660s, grad.norm=0.48778477\n",
      "  3328: 2 [  330/ 1499], train_loss/perplexity = 5.86754704/353.3810730 secs/batch = 0.6170s, grad.norm=0.45010513\n",
      "  3333: 2 [  335/ 1499], train_loss/perplexity = 6.03899240/419.4701538 secs/batch = 0.6007s, grad.norm=0.54857153\n",
      "  3338: 2 [  340/ 1499], train_loss/perplexity = 5.79347610/328.1517334 secs/batch = 0.5514s, grad.norm=0.61497116\n",
      "  3343: 2 [  345/ 1499], train_loss/perplexity = 5.73285723/308.8504639 secs/batch = 0.5429s, grad.norm=0.40768027\n",
      "  3348: 2 [  350/ 1499], train_loss/perplexity = 5.92311478/373.5735168 secs/batch = 0.5429s, grad.norm=0.46347815\n",
      "  3353: 2 [  355/ 1499], train_loss/perplexity = 5.70175838/299.3934021 secs/batch = 0.5367s, grad.norm=0.42477363\n",
      "  3358: 2 [  360/ 1499], train_loss/perplexity = 5.86914349/353.9456787 secs/batch = 0.5402s, grad.norm=0.51759255\n",
      "  3363: 2 [  365/ 1499], train_loss/perplexity = 5.71930742/304.6938171 secs/batch = 0.5461s, grad.norm=0.50661939\n",
      "  3368: 2 [  370/ 1499], train_loss/perplexity = 5.82668638/339.2327271 secs/batch = 0.5494s, grad.norm=0.49134460\n",
      "  3373: 2 [  375/ 1499], train_loss/perplexity = 5.84105206/344.1412048 secs/batch = 0.5633s, grad.norm=0.42670390\n",
      "  3378: 2 [  380/ 1499], train_loss/perplexity = 6.00607252/405.8860779 secs/batch = 0.5357s, grad.norm=0.50349826\n",
      "  3383: 2 [  385/ 1499], train_loss/perplexity = 6.12659502/457.8744507 secs/batch = 0.5396s, grad.norm=0.41922960\n",
      "  3388: 2 [  390/ 1499], train_loss/perplexity = 6.24220037/513.9882202 secs/batch = 0.5377s, grad.norm=0.50395674\n",
      "  3393: 2 [  395/ 1499], train_loss/perplexity = 6.27630758/531.8213501 secs/batch = 0.5387s, grad.norm=0.50271702\n",
      "  3398: 2 [  400/ 1499], train_loss/perplexity = 6.13268185/460.6699524 secs/batch = 0.5414s, grad.norm=0.55234206\n",
      "  3403: 2 [  405/ 1499], train_loss/perplexity = 6.05973721/428.2628784 secs/batch = 0.5486s, grad.norm=0.55169028\n",
      "  3408: 2 [  410/ 1499], train_loss/perplexity = 5.63246775/279.3506470 secs/batch = 0.5457s, grad.norm=0.54420799\n",
      "  3413: 2 [  415/ 1499], train_loss/perplexity = 6.13300180/460.8173828 secs/batch = 0.5408s, grad.norm=0.47114784\n",
      "  3418: 2 [  420/ 1499], train_loss/perplexity = 5.87013626/354.2972412 secs/batch = 0.5433s, grad.norm=0.45407534\n",
      "  3423: 2 [  425/ 1499], train_loss/perplexity = 6.21428919/499.8405762 secs/batch = 0.5467s, grad.norm=0.43568912\n",
      "  3428: 2 [  430/ 1499], train_loss/perplexity = 5.85783005/349.9639282 secs/batch = 0.5627s, grad.norm=0.42688552\n",
      "  3433: 2 [  435/ 1499], train_loss/perplexity = 5.86568069/352.7221680 secs/batch = 0.5385s, grad.norm=0.46272019\n",
      "  3438: 2 [  440/ 1499], train_loss/perplexity = 6.03793669/419.0275574 secs/batch = 0.5410s, grad.norm=0.56145293\n",
      "  3443: 2 [  445/ 1499], train_loss/perplexity = 6.01800632/410.7588501 secs/batch = 0.5420s, grad.norm=0.42429903\n",
      "  3448: 2 [  450/ 1499], train_loss/perplexity = 5.91539288/370.6999207 secs/batch = 0.5423s, grad.norm=0.53409088\n",
      "  3453: 2 [  455/ 1499], train_loss/perplexity = 6.07258654/433.8012695 secs/batch = 0.5369s, grad.norm=0.42034441\n",
      "  3458: 2 [  460/ 1499], train_loss/perplexity = 6.11584616/452.9791870 secs/batch = 0.5431s, grad.norm=0.59069014\n",
      "  3463: 2 [  465/ 1499], train_loss/perplexity = 6.02753496/414.6915283 secs/batch = 0.5429s, grad.norm=0.51176834\n",
      "  3468: 2 [  470/ 1499], train_loss/perplexity = 6.11637020/453.2166138 secs/batch = 0.5414s, grad.norm=0.53848457\n",
      "  3473: 2 [  475/ 1499], train_loss/perplexity = 5.92153692/372.9845276 secs/batch = 0.5422s, grad.norm=0.45352945\n",
      "  3478: 2 [  480/ 1499], train_loss/perplexity = 6.03813601/419.1110840 secs/batch = 0.5380s, grad.norm=0.45220593\n",
      "  3483: 2 [  485/ 1499], train_loss/perplexity = 6.04588318/422.3706360 secs/batch = 0.5436s, grad.norm=0.47754246\n",
      "  3488: 2 [  490/ 1499], train_loss/perplexity = 5.92677975/374.9451599 secs/batch = 0.5416s, grad.norm=0.56548256\n",
      "  3493: 2 [  495/ 1499], train_loss/perplexity = 5.94318104/381.1454468 secs/batch = 0.5364s, grad.norm=0.45035768\n",
      "  3498: 2 [  500/ 1499], train_loss/perplexity = 6.08627129/439.7785339 secs/batch = 0.5444s, grad.norm=0.44691131\n",
      "  3503: 2 [  505/ 1499], train_loss/perplexity = 5.78288078/324.6932068 secs/batch = 0.5387s, grad.norm=0.59097487\n",
      "  3508: 2 [  510/ 1499], train_loss/perplexity = 6.16705275/476.7788391 secs/batch = 0.5435s, grad.norm=0.45077640\n",
      "  3513: 2 [  515/ 1499], train_loss/perplexity = 5.87147093/354.7704468 secs/batch = 0.5460s, grad.norm=0.51978284\n",
      "  3518: 2 [  520/ 1499], train_loss/perplexity = 6.15852976/472.7325439 secs/batch = 0.5472s, grad.norm=0.53774655\n",
      "  3523: 2 [  525/ 1499], train_loss/perplexity = 6.16347313/475.0751953 secs/batch = 0.5343s, grad.norm=0.57170695\n",
      "  3528: 2 [  530/ 1499], train_loss/perplexity = 6.05801487/427.5259094 secs/batch = 0.5406s, grad.norm=0.54771531\n",
      "  3533: 2 [  535/ 1499], train_loss/perplexity = 5.95350170/385.0994873 secs/batch = 0.5334s, grad.norm=0.47353330\n",
      "  3538: 2 [  540/ 1499], train_loss/perplexity = 5.79176044/327.5892334 secs/batch = 0.5440s, grad.norm=0.43960688\n",
      "  3543: 2 [  545/ 1499], train_loss/perplexity = 5.63609648/280.3661499 secs/batch = 0.5427s, grad.norm=0.82844526\n",
      "  3548: 2 [  550/ 1499], train_loss/perplexity = 6.06041718/428.5541687 secs/batch = 0.5416s, grad.norm=0.56484610\n",
      "  3553: 2 [  555/ 1499], train_loss/perplexity = 5.82659245/339.2008667 secs/batch = 0.5412s, grad.norm=0.47793749\n",
      "  3558: 2 [  560/ 1499], train_loss/perplexity = 6.18672371/486.2503967 secs/batch = 0.5358s, grad.norm=0.45096806\n",
      "  3563: 2 [  565/ 1499], train_loss/perplexity = 6.03559637/418.0480347 secs/batch = 0.5490s, grad.norm=0.43599325\n",
      "  3568: 2 [  570/ 1499], train_loss/perplexity = 5.98770523/398.4990845 secs/batch = 0.5497s, grad.norm=0.40038389\n",
      "  3573: 2 [  575/ 1499], train_loss/perplexity = 6.03401136/417.3859558 secs/batch = 0.5362s, grad.norm=0.48095274\n",
      "  3578: 2 [  580/ 1499], train_loss/perplexity = 5.81308222/334.6489868 secs/batch = 0.5423s, grad.norm=0.52599245\n",
      "  3583: 2 [  585/ 1499], train_loss/perplexity = 5.72580099/306.6788025 secs/batch = 0.5389s, grad.norm=0.49377868\n",
      "  3588: 2 [  590/ 1499], train_loss/perplexity = 5.98447895/397.2154846 secs/batch = 0.5817s, grad.norm=0.52635795\n",
      "  3593: 2 [  595/ 1499], train_loss/perplexity = 5.91115236/369.1312866 secs/batch = 0.5429s, grad.norm=0.43188906\n",
      "  3598: 2 [  600/ 1499], train_loss/perplexity = 5.71807051/304.3171692 secs/batch = 0.5419s, grad.norm=0.45441660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3603: 2 [  605/ 1499], train_loss/perplexity = 5.72216511/305.5657959 secs/batch = 0.5413s, grad.norm=0.43465331\n",
      "  3608: 2 [  610/ 1499], train_loss/perplexity = 6.01751614/410.5575562 secs/batch = 0.5408s, grad.norm=0.54842395\n",
      "  3613: 2 [  615/ 1499], train_loss/perplexity = 6.01689196/410.3013916 secs/batch = 0.5374s, grad.norm=0.42902750\n",
      "  3618: 2 [  620/ 1499], train_loss/perplexity = 5.84651470/346.0262756 secs/batch = 0.5388s, grad.norm=0.46556053\n",
      "  3623: 2 [  625/ 1499], train_loss/perplexity = 5.86388540/352.0895081 secs/batch = 0.5428s, grad.norm=0.48113179\n",
      "  3628: 2 [  630/ 1499], train_loss/perplexity = 5.96937752/391.2620544 secs/batch = 0.5344s, grad.norm=0.45657605\n",
      "  3633: 2 [  635/ 1499], train_loss/perplexity = 5.80281544/331.2308044 secs/batch = 0.5414s, grad.norm=0.78509980\n",
      "  3638: 2 [  640/ 1499], train_loss/perplexity = 6.02418709/413.3055115 secs/batch = 0.5407s, grad.norm=0.43782437\n",
      "  3643: 2 [  645/ 1499], train_loss/perplexity = 5.68557405/294.5869141 secs/batch = 0.5418s, grad.norm=0.47737569\n",
      "  3648: 2 [  650/ 1499], train_loss/perplexity = 5.70778942/301.2044983 secs/batch = 0.5746s, grad.norm=0.62055385\n",
      "  3653: 2 [  655/ 1499], train_loss/perplexity = 6.05433369/425.9549866 secs/batch = 0.5376s, grad.norm=0.44546178\n",
      "  3658: 2 [  660/ 1499], train_loss/perplexity = 6.15242386/469.8548584 secs/batch = 0.5384s, grad.norm=0.48170149\n",
      "  3663: 2 [  665/ 1499], train_loss/perplexity = 6.11233616/451.3919983 secs/batch = 0.5966s, grad.norm=0.46369022\n",
      "  3668: 2 [  670/ 1499], train_loss/perplexity = 6.09024382/441.5290527 secs/batch = 0.5381s, grad.norm=0.48175317\n",
      "  3673: 2 [  675/ 1499], train_loss/perplexity = 6.00341702/404.8096619 secs/batch = 0.5413s, grad.norm=0.44050634\n",
      "  3678: 2 [  680/ 1499], train_loss/perplexity = 6.00988531/407.4365845 secs/batch = 0.5384s, grad.norm=0.49149758\n",
      "  3683: 2 [  685/ 1499], train_loss/perplexity = 6.07988977/436.9810181 secs/batch = 0.5440s, grad.norm=0.45205095\n",
      "  3688: 2 [  690/ 1499], train_loss/perplexity = 5.95798159/386.8285522 secs/batch = 0.5416s, grad.norm=0.44286293\n",
      "  3693: 2 [  695/ 1499], train_loss/perplexity = 5.87956476/357.6535339 secs/batch = 0.5397s, grad.norm=0.47411054\n",
      "  3698: 2 [  700/ 1499], train_loss/perplexity = 6.15101290/469.1923828 secs/batch = 0.5404s, grad.norm=0.50297260\n",
      "  3703: 2 [  705/ 1499], train_loss/perplexity = 5.68680429/294.9495544 secs/batch = 0.5384s, grad.norm=0.47775224\n",
      "  3708: 2 [  710/ 1499], train_loss/perplexity = 5.89983559/364.9774475 secs/batch = 0.5406s, grad.norm=0.41626719\n",
      "  3713: 2 [  715/ 1499], train_loss/perplexity = 5.91865873/371.9125366 secs/batch = 0.5449s, grad.norm=0.44588810\n",
      "  3718: 2 [  720/ 1499], train_loss/perplexity = 6.00511026/405.4956970 secs/batch = 0.5464s, grad.norm=0.57167411\n",
      "  3723: 2 [  725/ 1499], train_loss/perplexity = 5.81891537/336.6067505 secs/batch = 0.5404s, grad.norm=0.58585250\n",
      "  3728: 2 [  730/ 1499], train_loss/perplexity = 5.97583294/393.7959595 secs/batch = 0.5374s, grad.norm=0.58169812\n",
      "  3733: 2 [  735/ 1499], train_loss/perplexity = 5.83151913/340.8761292 secs/batch = 0.5428s, grad.norm=0.49734735\n",
      "  3738: 2 [  740/ 1499], train_loss/perplexity = 5.79572153/328.8894043 secs/batch = 0.5375s, grad.norm=0.43704948\n",
      "  3743: 2 [  745/ 1499], train_loss/perplexity = 5.64609623/283.1838074 secs/batch = 0.5405s, grad.norm=0.44887316\n",
      "  3748: 2 [  750/ 1499], train_loss/perplexity = 5.93596888/378.4064331 secs/batch = 0.5417s, grad.norm=0.46101820\n",
      "  3753: 2 [  755/ 1499], train_loss/perplexity = 5.67353916/291.0628357 secs/batch = 0.5405s, grad.norm=0.49849111\n",
      "  3758: 2 [  760/ 1499], train_loss/perplexity = 5.52539396/250.9851990 secs/batch = 0.5357s, grad.norm=0.84569734\n",
      "  3763: 2 [  765/ 1499], train_loss/perplexity = 5.78145075/324.2292175 secs/batch = 0.5430s, grad.norm=0.53856164\n",
      "  3768: 2 [  770/ 1499], train_loss/perplexity = 5.88795996/360.6687622 secs/batch = 0.5466s, grad.norm=0.47618717\n",
      "  3773: 2 [  775/ 1499], train_loss/perplexity = 6.19856501/492.0424500 secs/batch = 0.5415s, grad.norm=0.52421623\n",
      "  3778: 2 [  780/ 1499], train_loss/perplexity = 5.73590183/309.7922363 secs/batch = 0.5406s, grad.norm=0.41913730\n",
      "  3783: 2 [  785/ 1499], train_loss/perplexity = 5.87493515/356.0015869 secs/batch = 0.5401s, grad.norm=0.47874665\n",
      "  3788: 2 [  790/ 1499], train_loss/perplexity = 5.52969551/252.0671539 secs/batch = 0.5426s, grad.norm=0.64725757\n",
      "  3793: 2 [  795/ 1499], train_loss/perplexity = 5.97899342/395.0425110 secs/batch = 0.5418s, grad.norm=0.46130267\n",
      "  3798: 2 [  800/ 1499], train_loss/perplexity = 5.76535034/319.0508118 secs/batch = 0.5335s, grad.norm=0.50635850\n",
      "  3803: 2 [  805/ 1499], train_loss/perplexity = 5.85870361/350.2697754 secs/batch = 0.5372s, grad.norm=0.39564964\n",
      "  3808: 2 [  810/ 1499], train_loss/perplexity = 5.78219748/324.4714355 secs/batch = 0.5501s, grad.norm=0.42612460\n",
      "  3813: 2 [  815/ 1499], train_loss/perplexity = 5.87922335/357.5314636 secs/batch = 0.5553s, grad.norm=0.41124171\n",
      "  3818: 2 [  820/ 1499], train_loss/perplexity = 5.55344248/258.1246033 secs/batch = 0.5436s, grad.norm=0.52585781\n",
      "  3823: 2 [  825/ 1499], train_loss/perplexity = 5.67840624/292.4829102 secs/batch = 0.5405s, grad.norm=0.46477377\n",
      "  3828: 2 [  830/ 1499], train_loss/perplexity = 5.92140436/372.9350891 secs/batch = 0.5374s, grad.norm=0.48186326\n",
      "  3833: 2 [  835/ 1499], train_loss/perplexity = 5.90991592/368.6751709 secs/batch = 0.5399s, grad.norm=0.54262531\n",
      "  3838: 2 [  840/ 1499], train_loss/perplexity = 5.79143524/327.4826965 secs/batch = 0.5377s, grad.norm=0.49537909\n",
      "  3843: 2 [  845/ 1499], train_loss/perplexity = 5.73273420/308.8124695 secs/batch = 0.5362s, grad.norm=0.49395335\n",
      "  3848: 2 [  850/ 1499], train_loss/perplexity = 5.78398180/325.0509033 secs/batch = 0.5382s, grad.norm=0.45482701\n",
      "  3853: 2 [  855/ 1499], train_loss/perplexity = 5.94747925/382.7872009 secs/batch = 0.5407s, grad.norm=0.49856007\n",
      "  3858: 2 [  860/ 1499], train_loss/perplexity = 5.62788725/278.0740051 secs/batch = 0.5404s, grad.norm=0.74418497\n",
      "  3863: 2 [  865/ 1499], train_loss/perplexity = 5.78651619/325.8757629 secs/batch = 0.5452s, grad.norm=0.42432410\n",
      "  3868: 2 [  870/ 1499], train_loss/perplexity = 5.74941826/314.0079346 secs/batch = 0.5451s, grad.norm=0.49362567\n",
      "  3873: 2 [  875/ 1499], train_loss/perplexity = 5.80621862/332.3599548 secs/batch = 0.5408s, grad.norm=0.52104348\n",
      "  3878: 2 [  880/ 1499], train_loss/perplexity = 5.70225811/299.5430298 secs/batch = 0.5440s, grad.norm=0.49183989\n",
      "  3883: 2 [  885/ 1499], train_loss/perplexity = 5.63315725/279.5433044 secs/batch = 0.5367s, grad.norm=0.54879713\n",
      "  3888: 2 [  890/ 1499], train_loss/perplexity = 5.90568733/367.1194763 secs/batch = 0.5393s, grad.norm=0.41580606\n",
      "  3893: 2 [  895/ 1499], train_loss/perplexity = 5.91888285/371.9959106 secs/batch = 0.5403s, grad.norm=0.43277314\n",
      "  3898: 2 [  900/ 1499], train_loss/perplexity = 5.74378014/312.2424927 secs/batch = 0.5435s, grad.norm=0.44716769\n",
      "  3903: 2 [  905/ 1499], train_loss/perplexity = 5.79280615/327.9319458 secs/batch = 0.5504s, grad.norm=0.59084195\n",
      "  3908: 2 [  910/ 1499], train_loss/perplexity = 5.98623323/397.9129333 secs/batch = 0.5410s, grad.norm=0.49542695\n",
      "  3913: 2 [  915/ 1499], train_loss/perplexity = 5.72242928/305.6465149 secs/batch = 0.5386s, grad.norm=0.56661451\n",
      "  3918: 2 [  920/ 1499], train_loss/perplexity = 5.47048950/237.5764618 secs/batch = 0.5443s, grad.norm=0.42610285\n",
      "  3923: 2 [  925/ 1499], train_loss/perplexity = 5.91492701/370.5272522 secs/batch = 0.5406s, grad.norm=0.68731964\n",
      "  3928: 2 [  930/ 1499], train_loss/perplexity = 5.85458136/348.8288269 secs/batch = 0.5393s, grad.norm=0.51379740\n",
      "  3933: 2 [  935/ 1499], train_loss/perplexity = 5.89180756/362.0591431 secs/batch = 0.5816s, grad.norm=0.43283629\n",
      "  3938: 2 [  940/ 1499], train_loss/perplexity = 5.52682447/251.3444977 secs/batch = 0.5400s, grad.norm=0.51743495\n",
      "  3943: 2 [  945/ 1499], train_loss/perplexity = 5.90227699/365.8695984 secs/batch = 0.5365s, grad.norm=0.52150875\n",
      "  3948: 2 [  950/ 1499], train_loss/perplexity = 5.83819771/343.1603088 secs/batch = 0.5361s, grad.norm=0.48423833\n",
      "  3953: 2 [  955/ 1499], train_loss/perplexity = 5.97498608/393.4626160 secs/batch = 0.5392s, grad.norm=0.40736976\n",
      "  3958: 2 [  960/ 1499], train_loss/perplexity = 5.74666977/313.1460876 secs/batch = 0.5441s, grad.norm=0.48821676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3963: 2 [  965/ 1499], train_loss/perplexity = 5.83567142/342.2944946 secs/batch = 0.5433s, grad.norm=0.70064616\n",
      "  3968: 2 [  970/ 1499], train_loss/perplexity = 6.01706362/410.3718262 secs/batch = 0.5416s, grad.norm=0.45476413\n",
      "  3973: 2 [  975/ 1499], train_loss/perplexity = 5.86966133/354.1290283 secs/batch = 0.5447s, grad.norm=0.63425863\n",
      "  3978: 2 [  980/ 1499], train_loss/perplexity = 5.93623257/378.5062561 secs/batch = 0.5419s, grad.norm=0.58096004\n",
      "  3983: 2 [  985/ 1499], train_loss/perplexity = 5.62039375/275.9980469 secs/batch = 0.5452s, grad.norm=0.50643891\n",
      "  3988: 2 [  990/ 1499], train_loss/perplexity = 5.80965281/333.5033264 secs/batch = 0.5382s, grad.norm=0.46288157\n",
      "  3993: 2 [  995/ 1499], train_loss/perplexity = 6.00993824/407.4581604 secs/batch = 0.5429s, grad.norm=0.58836323\n",
      "  3998: 2 [ 1000/ 1499], train_loss/perplexity = 5.70917606/301.6224365 secs/batch = 0.5402s, grad.norm=0.50464469\n",
      "  4003: 2 [ 1005/ 1499], train_loss/perplexity = 6.02266216/412.6757507 secs/batch = 0.5397s, grad.norm=0.49785522\n",
      "  4008: 2 [ 1010/ 1499], train_loss/perplexity = 5.90657425/367.4452209 secs/batch = 0.5404s, grad.norm=0.44574574\n",
      "  4013: 2 [ 1015/ 1499], train_loss/perplexity = 5.66790199/289.4266663 secs/batch = 0.5491s, grad.norm=0.46328974\n",
      "  4018: 2 [ 1020/ 1499], train_loss/perplexity = 5.83124161/340.7815247 secs/batch = 0.5405s, grad.norm=0.67606091\n",
      "  4023: 2 [ 1025/ 1499], train_loss/perplexity = 5.75835037/316.8252563 secs/batch = 0.5414s, grad.norm=0.46572134\n",
      "  4028: 2 [ 1030/ 1499], train_loss/perplexity = 5.86656284/353.0334473 secs/batch = 0.5472s, grad.norm=0.43097636\n",
      "  4033: 2 [ 1035/ 1499], train_loss/perplexity = 6.19721746/491.3798523 secs/batch = 0.5455s, grad.norm=0.56477761\n",
      "  4038: 2 [ 1040/ 1499], train_loss/perplexity = 5.64716625/283.4869995 secs/batch = 0.5403s, grad.norm=0.44522315\n",
      "  4043: 2 [ 1045/ 1499], train_loss/perplexity = 5.75285864/315.0900879 secs/batch = 0.5420s, grad.norm=0.52413827\n",
      "  4048: 2 [ 1050/ 1499], train_loss/perplexity = 5.44678164/232.0102692 secs/batch = 0.5368s, grad.norm=0.48008266\n",
      "  4053: 2 [ 1055/ 1499], train_loss/perplexity = 5.82835102/339.7978821 secs/batch = 0.5385s, grad.norm=0.51440245\n",
      "  4058: 2 [ 1060/ 1499], train_loss/perplexity = 6.03547478/417.9972229 secs/batch = 0.5507s, grad.norm=0.57606983\n",
      "  4063: 2 [ 1065/ 1499], train_loss/perplexity = 5.58310890/265.8969727 secs/batch = 0.5483s, grad.norm=0.45145577\n",
      "  4068: 2 [ 1070/ 1499], train_loss/perplexity = 6.02747679/414.6674194 secs/batch = 0.5350s, grad.norm=0.44575030\n",
      "  4073: 2 [ 1075/ 1499], train_loss/perplexity = 5.88873100/360.9469604 secs/batch = 0.5407s, grad.norm=0.47348222\n",
      "  4078: 2 [ 1080/ 1499], train_loss/perplexity = 6.07809877/436.1990967 secs/batch = 0.5391s, grad.norm=0.51382440\n",
      "  4083: 2 [ 1085/ 1499], train_loss/perplexity = 6.15606403/471.5683289 secs/batch = 0.5447s, grad.norm=0.49837491\n",
      "  4088: 2 [ 1090/ 1499], train_loss/perplexity = 5.70686245/300.9254150 secs/batch = 0.5542s, grad.norm=0.43024874\n",
      "  4093: 2 [ 1095/ 1499], train_loss/perplexity = 5.92041731/372.5671692 secs/batch = 0.5362s, grad.norm=0.55217344\n",
      "  4098: 2 [ 1100/ 1499], train_loss/perplexity = 5.91647625/371.1017456 secs/batch = 0.5435s, grad.norm=0.56997138\n",
      "  4103: 2 [ 1105/ 1499], train_loss/perplexity = 5.66103220/287.4451904 secs/batch = 0.5403s, grad.norm=0.48721021\n",
      "  4108: 2 [ 1110/ 1499], train_loss/perplexity = 5.88013363/357.8570557 secs/batch = 0.5470s, grad.norm=0.50494605\n",
      "  4113: 2 [ 1115/ 1499], train_loss/perplexity = 5.51112318/247.4288788 secs/batch = 0.5413s, grad.norm=0.47447285\n",
      "  4118: 2 [ 1120/ 1499], train_loss/perplexity = 5.72367048/306.0261230 secs/batch = 0.5430s, grad.norm=0.50245309\n",
      "  4123: 2 [ 1125/ 1499], train_loss/perplexity = 5.70095396/299.1526489 secs/batch = 0.5465s, grad.norm=0.49507579\n",
      "  4128: 2 [ 1130/ 1499], train_loss/perplexity = 5.52279282/250.3331909 secs/batch = 0.5788s, grad.norm=0.51635098\n",
      "  4133: 2 [ 1135/ 1499], train_loss/perplexity = 6.07060194/432.9412231 secs/batch = 0.5437s, grad.norm=0.50958419\n",
      "  4138: 2 [ 1140/ 1499], train_loss/perplexity = 5.94220781/380.7746887 secs/batch = 0.5836s, grad.norm=0.48111612\n",
      "  4143: 2 [ 1145/ 1499], train_loss/perplexity = 6.01120806/407.9758911 secs/batch = 0.5458s, grad.norm=0.51508331\n",
      "  4148: 2 [ 1150/ 1499], train_loss/perplexity = 5.90809488/368.0043945 secs/batch = 0.5879s, grad.norm=0.54483199\n",
      "  4153: 2 [ 1155/ 1499], train_loss/perplexity = 5.61732006/275.1510010 secs/batch = 0.5450s, grad.norm=0.53718150\n",
      "  4158: 2 [ 1160/ 1499], train_loss/perplexity = 5.80097246/330.6209106 secs/batch = 0.5872s, grad.norm=0.48307297\n",
      "  4163: 2 [ 1165/ 1499], train_loss/perplexity = 5.67518568/291.5424805 secs/batch = 0.5637s, grad.norm=0.54276597\n",
      "  4168: 2 [ 1170/ 1499], train_loss/perplexity = 5.97617006/393.9287415 secs/batch = 0.6016s, grad.norm=0.50678605\n",
      "  4173: 2 [ 1175/ 1499], train_loss/perplexity = 5.42986250/228.1178741 secs/batch = 0.5392s, grad.norm=0.54133499\n",
      "  4178: 2 [ 1180/ 1499], train_loss/perplexity = 5.68193388/293.5165100 secs/batch = 0.6031s, grad.norm=0.47649294\n",
      "  4183: 2 [ 1185/ 1499], train_loss/perplexity = 5.58941889/267.5800781 secs/batch = 0.6145s, grad.norm=0.58068997\n",
      "  4188: 2 [ 1190/ 1499], train_loss/perplexity = 5.86691380/353.1573792 secs/batch = 0.5971s, grad.norm=0.45468459\n",
      "  4193: 2 [ 1195/ 1499], train_loss/perplexity = 5.89621496/363.6583862 secs/batch = 0.6139s, grad.norm=0.52477729\n",
      "  4198: 2 [ 1200/ 1499], train_loss/perplexity = 5.73682642/310.0787964 secs/batch = 0.6092s, grad.norm=0.44820631\n",
      "  4203: 2 [ 1205/ 1499], train_loss/perplexity = 5.71299362/302.7761230 secs/batch = 0.6671s, grad.norm=0.56905740\n",
      "  4208: 2 [ 1210/ 1499], train_loss/perplexity = 5.68980742/295.8366394 secs/batch = 0.5815s, grad.norm=0.54729646\n",
      "  4213: 2 [ 1215/ 1499], train_loss/perplexity = 5.34262753/209.0613098 secs/batch = 0.7173s, grad.norm=0.65393329\n",
      "  4218: 2 [ 1220/ 1499], train_loss/perplexity = 5.72492933/306.4116211 secs/batch = 0.6767s, grad.norm=0.46871895\n",
      "  4223: 2 [ 1225/ 1499], train_loss/perplexity = 5.23089361/186.9597931 secs/batch = 0.6574s, grad.norm=0.59197098\n",
      "  4228: 2 [ 1230/ 1499], train_loss/perplexity = 5.81119204/334.0170593 secs/batch = 0.5942s, grad.norm=0.62681222\n",
      "  4233: 2 [ 1235/ 1499], train_loss/perplexity = 5.54417372/255.7431793 secs/batch = 0.6148s, grad.norm=0.48458391\n",
      "  4238: 2 [ 1240/ 1499], train_loss/perplexity = 5.61239624/273.7995300 secs/batch = 0.6940s, grad.norm=0.48038024\n",
      "  4243: 2 [ 1245/ 1499], train_loss/perplexity = 5.89188910/362.0886536 secs/batch = 0.6082s, grad.norm=0.46762106\n",
      "  4248: 2 [ 1250/ 1499], train_loss/perplexity = 6.13537931/461.9142761 secs/batch = 0.5682s, grad.norm=0.47274497\n",
      "  4253: 2 [ 1255/ 1499], train_loss/perplexity = 5.65643215/286.1259766 secs/batch = 0.5884s, grad.norm=0.46915302\n",
      "  4258: 2 [ 1260/ 1499], train_loss/perplexity = 5.78090906/324.0536499 secs/batch = 0.6950s, grad.norm=0.47149244\n",
      "  4263: 2 [ 1265/ 1499], train_loss/perplexity = 5.82084560/337.2571106 secs/batch = 0.6714s, grad.norm=0.45651174\n",
      "  4268: 2 [ 1270/ 1499], train_loss/perplexity = 5.85719681/349.7423706 secs/batch = 0.5744s, grad.norm=0.45090386\n",
      "  4273: 2 [ 1275/ 1499], train_loss/perplexity = 6.01748562/410.5450439 secs/batch = 0.5960s, grad.norm=0.96912897\n",
      "  4278: 2 [ 1280/ 1499], train_loss/perplexity = 5.51600742/248.6403351 secs/batch = 0.5422s, grad.norm=0.56061816\n",
      "  4283: 2 [ 1285/ 1499], train_loss/perplexity = 5.86930561/354.0030823 secs/batch = 0.5416s, grad.norm=0.48928356\n",
      "  4288: 2 [ 1290/ 1499], train_loss/perplexity = 5.89491892/363.1873779 secs/batch = 0.5436s, grad.norm=0.55401111\n",
      "  4293: 2 [ 1295/ 1499], train_loss/perplexity = 5.68921375/295.6610718 secs/batch = 0.5981s, grad.norm=0.46754193\n",
      "  4298: 2 [ 1300/ 1499], train_loss/perplexity = 5.93050385/376.3440857 secs/batch = 0.6849s, grad.norm=0.46757746\n",
      "  4303: 2 [ 1305/ 1499], train_loss/perplexity = 5.77787352/323.0714417 secs/batch = 0.5748s, grad.norm=0.50979871\n",
      "  4308: 2 [ 1310/ 1499], train_loss/perplexity = 5.86522818/352.5625916 secs/batch = 0.6971s, grad.norm=0.47331417\n",
      "  4313: 2 [ 1315/ 1499], train_loss/perplexity = 5.72475529/306.3582764 secs/batch = 0.5929s, grad.norm=0.61983556\n",
      "  4318: 2 [ 1320/ 1499], train_loss/perplexity = 5.42209959/226.3538818 secs/batch = 0.5744s, grad.norm=0.45122465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4323: 2 [ 1325/ 1499], train_loss/perplexity = 5.77604628/322.4816589 secs/batch = 0.6355s, grad.norm=0.48283646\n",
      "  4328: 2 [ 1330/ 1499], train_loss/perplexity = 5.89789295/364.2691345 secs/batch = 0.5746s, grad.norm=0.45466971\n",
      "  4333: 2 [ 1335/ 1499], train_loss/perplexity = 5.70828009/301.3523254 secs/batch = 0.5436s, grad.norm=0.49445704\n",
      "  4338: 2 [ 1340/ 1499], train_loss/perplexity = 5.24642229/189.8856964 secs/batch = 0.5548s, grad.norm=0.50596249\n",
      "  4343: 2 [ 1345/ 1499], train_loss/perplexity = 5.40404892/222.3046875 secs/batch = 0.6403s, grad.norm=0.52940959\n",
      "  4348: 2 [ 1350/ 1499], train_loss/perplexity = 5.58526039/266.4696655 secs/batch = 0.5333s, grad.norm=0.63639933\n",
      "  4353: 2 [ 1355/ 1499], train_loss/perplexity = 5.30879450/202.1064453 secs/batch = 0.5415s, grad.norm=0.57513338\n",
      "  4358: 2 [ 1360/ 1499], train_loss/perplexity = 5.37441492/215.8135681 secs/batch = 0.5988s, grad.norm=0.53514719\n",
      "  4363: 2 [ 1365/ 1499], train_loss/perplexity = 5.35850668/212.4075165 secs/batch = 0.5432s, grad.norm=0.44452083\n",
      "  4368: 2 [ 1370/ 1499], train_loss/perplexity = 5.28412676/197.1819153 secs/batch = 0.5799s, grad.norm=0.49704826\n",
      "  4373: 2 [ 1375/ 1499], train_loss/perplexity = 5.32806778/206.0394745 secs/batch = 0.5763s, grad.norm=0.53794503\n",
      "  4378: 2 [ 1380/ 1499], train_loss/perplexity = 5.80502272/331.9627380 secs/batch = 0.6553s, grad.norm=0.55534834\n",
      "  4383: 2 [ 1385/ 1499], train_loss/perplexity = 5.59279776/268.4857178 secs/batch = 0.6316s, grad.norm=0.47267109\n",
      "  4388: 2 [ 1390/ 1499], train_loss/perplexity = 5.56439257/260.9666443 secs/batch = 0.6372s, grad.norm=0.46326602\n",
      "  4393: 2 [ 1395/ 1499], train_loss/perplexity = 5.70087004/299.1275330 secs/batch = 0.6081s, grad.norm=0.45518368\n",
      "  4398: 2 [ 1400/ 1499], train_loss/perplexity = 5.76870728/320.1236267 secs/batch = 0.6137s, grad.norm=0.51491094\n",
      "  4403: 2 [ 1405/ 1499], train_loss/perplexity = 5.56651068/261.5199890 secs/batch = 0.6316s, grad.norm=0.48366320\n",
      "  4408: 2 [ 1410/ 1499], train_loss/perplexity = 5.81367683/334.8480530 secs/batch = 0.5400s, grad.norm=0.43769479\n",
      "  4413: 2 [ 1415/ 1499], train_loss/perplexity = 5.69772196/298.1873474 secs/batch = 0.5434s, grad.norm=0.49929631\n",
      "  4418: 2 [ 1420/ 1499], train_loss/perplexity = 5.71289158/302.7452087 secs/batch = 0.5438s, grad.norm=0.45826352\n",
      "  4423: 2 [ 1425/ 1499], train_loss/perplexity = 5.63475084/279.9891357 secs/batch = 0.5394s, grad.norm=0.59170544\n",
      "  4428: 2 [ 1430/ 1499], train_loss/perplexity = 5.84628201/345.9457703 secs/batch = 0.5457s, grad.norm=0.48161644\n",
      "  4433: 2 [ 1435/ 1499], train_loss/perplexity = 5.56160355/260.2398071 secs/batch = 0.5445s, grad.norm=0.57937837\n",
      "  4438: 2 [ 1440/ 1499], train_loss/perplexity = 5.38863420/218.9042053 secs/batch = 0.5420s, grad.norm=0.84808308\n",
      "  4443: 2 [ 1445/ 1499], train_loss/perplexity = 5.66434622/288.3993835 secs/batch = 0.5535s, grad.norm=0.43996036\n",
      "  4448: 2 [ 1450/ 1499], train_loss/perplexity = 5.68957281/295.7672424 secs/batch = 0.5350s, grad.norm=0.45204261\n",
      "  4453: 2 [ 1455/ 1499], train_loss/perplexity = 6.00013018/403.4813232 secs/batch = 0.5450s, grad.norm=0.45799240\n",
      "  4458: 2 [ 1460/ 1499], train_loss/perplexity = 5.98465347/397.2848206 secs/batch = 0.5416s, grad.norm=0.46483889\n",
      "  4463: 2 [ 1465/ 1499], train_loss/perplexity = 6.14684439/467.2406311 secs/batch = 0.5398s, grad.norm=0.47606641\n",
      "  4468: 2 [ 1470/ 1499], train_loss/perplexity = 5.80771780/332.8586121 secs/batch = 0.5440s, grad.norm=0.60259962\n",
      "  4473: 2 [ 1475/ 1499], train_loss/perplexity = 5.79866457/329.8587646 secs/batch = 0.5423s, grad.norm=0.47644007\n",
      "  4478: 2 [ 1480/ 1499], train_loss/perplexity = 5.82750940/339.5120239 secs/batch = 0.5513s, grad.norm=0.45868671\n",
      "  4483: 2 [ 1485/ 1499], train_loss/perplexity = 5.54657507/256.3580322 secs/batch = 0.5446s, grad.norm=0.47494313\n",
      "  4488: 2 [ 1490/ 1499], train_loss/perplexity = 5.67105818/290.3416138 secs/batch = 0.5427s, grad.norm=0.47241637\n",
      "  4493: 2 [ 1495/ 1499], train_loss/perplexity = 5.93360996/377.5148621 secs/batch = 0.5403s, grad.norm=0.50085980\n",
      "Epoch training time: 832.8266475200653\n",
      "Saved char model cv/epoch002_5.7780.model\n",
      "  4502: 3 [    5/ 1499], train_loss/perplexity = 5.84393978/345.1364136 secs/batch = 0.5417s, grad.norm=0.45096976\n",
      "  4507: 3 [   10/ 1499], train_loss/perplexity = 5.86090708/351.0424194 secs/batch = 0.5411s, grad.norm=0.50958407\n",
      "  4512: 3 [   15/ 1499], train_loss/perplexity = 5.69251442/296.6385498 secs/batch = 0.5389s, grad.norm=0.53020269\n",
      "  4517: 3 [   20/ 1499], train_loss/perplexity = 5.53995275/254.6659698 secs/batch = 0.5435s, grad.norm=0.43480808\n",
      "  4522: 3 [   25/ 1499], train_loss/perplexity = 5.94380331/381.3826904 secs/batch = 0.5338s, grad.norm=0.49294233\n",
      "  4527: 3 [   30/ 1499], train_loss/perplexity = 5.91848660/371.8485413 secs/batch = 0.5387s, grad.norm=0.52424800\n",
      "  4532: 3 [   35/ 1499], train_loss/perplexity = 5.70510864/300.3981018 secs/batch = 0.5584s, grad.norm=0.47681960\n",
      "  4537: 3 [   40/ 1499], train_loss/perplexity = 5.77096367/320.8467712 secs/batch = 0.5622s, grad.norm=0.49107400\n",
      "  4542: 3 [   45/ 1499], train_loss/perplexity = 5.74984598/314.1422729 secs/batch = 0.5469s, grad.norm=0.48386607\n",
      "  4547: 3 [   50/ 1499], train_loss/perplexity = 5.63931704/281.2705688 secs/batch = 0.5418s, grad.norm=0.50987291\n",
      "  4552: 3 [   55/ 1499], train_loss/perplexity = 5.56853533/262.0499878 secs/batch = 0.5639s, grad.norm=0.47833726\n",
      "  4557: 3 [   60/ 1499], train_loss/perplexity = 5.55812693/259.3366394 secs/batch = 0.5453s, grad.norm=0.47084484\n",
      "  4562: 3 [   65/ 1499], train_loss/perplexity = 5.63698673/280.6158752 secs/batch = 0.5434s, grad.norm=0.57340133\n",
      "  4567: 3 [   70/ 1499], train_loss/perplexity = 5.57954502/264.9510193 secs/batch = 0.5440s, grad.norm=0.54529876\n",
      "  4572: 3 [   75/ 1499], train_loss/perplexity = 5.37154102/215.1942291 secs/batch = 0.5473s, grad.norm=0.56521577\n",
      "  4577: 3 [   80/ 1499], train_loss/perplexity = 5.53664780/253.8256989 secs/batch = 0.5419s, grad.norm=0.51733136\n",
      "  4582: 3 [   85/ 1499], train_loss/perplexity = 5.52520227/250.9370880 secs/batch = 0.5457s, grad.norm=0.60872847\n",
      "  4587: 3 [   90/ 1499], train_loss/perplexity = 5.69345331/296.9172058 secs/batch = 0.5401s, grad.norm=0.51918721\n",
      "  4592: 3 [   95/ 1499], train_loss/perplexity = 5.44399595/231.3648682 secs/batch = 0.5445s, grad.norm=0.53247452\n",
      "  4597: 3 [  100/ 1499], train_loss/perplexity = 5.52385902/250.6002502 secs/batch = 0.5525s, grad.norm=0.50348890\n",
      "  4602: 3 [  105/ 1499], train_loss/perplexity = 5.43691921/229.7333374 secs/batch = 0.5427s, grad.norm=0.49850574\n",
      "  4607: 3 [  110/ 1499], train_loss/perplexity = 5.45009613/232.7805481 secs/batch = 0.5357s, grad.norm=0.63452846\n",
      "  4612: 3 [  115/ 1499], train_loss/perplexity = 5.66085339/287.3937988 secs/batch = 0.5399s, grad.norm=0.50049704\n",
      "  4617: 3 [  120/ 1499], train_loss/perplexity = 5.46031570/235.1716614 secs/batch = 0.5402s, grad.norm=0.44484439\n",
      "  4622: 3 [  125/ 1499], train_loss/perplexity = 5.71795654/304.2825012 secs/batch = 0.5402s, grad.norm=0.56279260\n",
      "  4627: 3 [  130/ 1499], train_loss/perplexity = 5.66452789/288.4517822 secs/batch = 0.5367s, grad.norm=0.54919571\n",
      "  4632: 3 [  135/ 1499], train_loss/perplexity = 5.65197802/284.8543701 secs/batch = 0.5404s, grad.norm=0.49419636\n",
      "  4637: 3 [  140/ 1499], train_loss/perplexity = 5.65491581/285.6924438 secs/batch = 0.5375s, grad.norm=0.57487977\n",
      "  4642: 3 [  145/ 1499], train_loss/perplexity = 5.43420458/229.1105347 secs/batch = 0.5456s, grad.norm=0.57138515\n",
      "  4647: 3 [  150/ 1499], train_loss/perplexity = 5.67770147/292.2768555 secs/batch = 0.5421s, grad.norm=0.66613793\n",
      "  4652: 3 [  155/ 1499], train_loss/perplexity = 5.74172401/311.6011658 secs/batch = 0.5368s, grad.norm=0.45318919\n",
      "  4657: 3 [  160/ 1499], train_loss/perplexity = 5.99713135/402.2731628 secs/batch = 0.5437s, grad.norm=0.45736271\n",
      "  4662: 3 [  165/ 1499], train_loss/perplexity = 5.47657490/239.0266113 secs/batch = 0.5662s, grad.norm=0.46512410\n",
      "  4667: 3 [  170/ 1499], train_loss/perplexity = 5.91297054/369.8030396 secs/batch = 0.5438s, grad.norm=0.56130821\n",
      "  4672: 3 [  175/ 1499], train_loss/perplexity = 5.62915993/278.4281311 secs/batch = 0.5427s, grad.norm=0.45177200\n",
      "  4677: 3 [  180/ 1499], train_loss/perplexity = 5.82773399/339.5882874 secs/batch = 0.5373s, grad.norm=0.45986333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4682: 3 [  185/ 1499], train_loss/perplexity = 5.57537842/263.8493958 secs/batch = 0.5401s, grad.norm=0.46138558\n",
      "  4687: 3 [  190/ 1499], train_loss/perplexity = 5.85907793/350.4009094 secs/batch = 0.5362s, grad.norm=0.49702737\n",
      "  4692: 3 [  195/ 1499], train_loss/perplexity = 5.87798166/357.0877991 secs/batch = 0.5459s, grad.norm=0.48084006\n",
      "  4697: 3 [  200/ 1499], train_loss/perplexity = 5.83228970/341.1388855 secs/batch = 0.5455s, grad.norm=0.58503973\n",
      "  4702: 3 [  205/ 1499], train_loss/perplexity = 5.72810173/307.3852234 secs/batch = 0.5445s, grad.norm=0.44629699\n",
      "  4707: 3 [  210/ 1499], train_loss/perplexity = 5.52109480/249.9084930 secs/batch = 0.5395s, grad.norm=0.50454909\n",
      "  4712: 3 [  215/ 1499], train_loss/perplexity = 5.69145441/296.3242798 secs/batch = 0.5895s, grad.norm=0.55369788\n",
      "  4717: 3 [  220/ 1499], train_loss/perplexity = 5.51014423/247.1867828 secs/batch = 0.5530s, grad.norm=0.49501511\n",
      "  4722: 3 [  225/ 1499], train_loss/perplexity = 5.69811869/298.3056641 secs/batch = 0.5365s, grad.norm=0.53946352\n",
      "  4727: 3 [  230/ 1499], train_loss/perplexity = 5.75318861/315.1940918 secs/batch = 0.5455s, grad.norm=0.56809342\n",
      "  4732: 3 [  235/ 1499], train_loss/perplexity = 5.69197893/296.4797668 secs/batch = 0.5357s, grad.norm=0.51824772\n",
      "  4737: 3 [  240/ 1499], train_loss/perplexity = 5.84859037/346.7452698 secs/batch = 0.5424s, grad.norm=0.44840011\n",
      "  4742: 3 [  245/ 1499], train_loss/perplexity = 5.56874466/262.1048584 secs/batch = 0.5416s, grad.norm=0.65508479\n",
      "  4747: 3 [  250/ 1499], train_loss/perplexity = 5.60786581/272.5619202 secs/batch = 0.5412s, grad.norm=0.46557504\n",
      "  4752: 3 [  255/ 1499], train_loss/perplexity = 5.45622969/234.2127075 secs/batch = 0.5572s, grad.norm=0.49214926\n",
      "  4757: 3 [  260/ 1499], train_loss/perplexity = 5.75754976/316.5717163 secs/batch = 0.5432s, grad.norm=0.50884652\n",
      "  4762: 3 [  265/ 1499], train_loss/perplexity = 5.72186613/305.4744568 secs/batch = 0.5429s, grad.norm=0.55766481\n",
      "  4767: 3 [  270/ 1499], train_loss/perplexity = 5.78884077/326.6341553 secs/batch = 0.5320s, grad.norm=0.48475018\n",
      "  4772: 3 [  275/ 1499], train_loss/perplexity = 5.37679815/216.3285217 secs/batch = 0.5454s, grad.norm=0.58341932\n",
      "  4777: 3 [  280/ 1499], train_loss/perplexity = 5.55456161/258.4136658 secs/batch = 0.5478s, grad.norm=0.57590055\n",
      "  4782: 3 [  285/ 1499], train_loss/perplexity = 6.00100517/403.8345032 secs/batch = 0.5411s, grad.norm=0.49032256\n",
      "  4787: 3 [  290/ 1499], train_loss/perplexity = 5.87213850/355.0073547 secs/batch = 0.5360s, grad.norm=0.45396268\n",
      "  4792: 3 [  295/ 1499], train_loss/perplexity = 5.64637089/283.2615967 secs/batch = 0.5492s, grad.norm=0.48465732\n",
      "  4797: 3 [  300/ 1499], train_loss/perplexity = 5.56906080/262.1877441 secs/batch = 0.5424s, grad.norm=0.47735065\n",
      "  4802: 3 [  305/ 1499], train_loss/perplexity = 5.80253029/331.1363831 secs/batch = 0.5512s, grad.norm=0.51517409\n",
      "  4807: 3 [  310/ 1499], train_loss/perplexity = 5.91417313/370.2480164 secs/batch = 0.5370s, grad.norm=0.53225762\n",
      "  4812: 3 [  315/ 1499], train_loss/perplexity = 5.80016088/330.3526917 secs/batch = 0.5462s, grad.norm=0.44222081\n",
      "  4817: 3 [  320/ 1499], train_loss/perplexity = 5.82004833/336.9883423 secs/batch = 0.5415s, grad.norm=0.73226649\n",
      "  4822: 3 [  325/ 1499], train_loss/perplexity = 5.67512703/291.5253601 secs/batch = 0.5407s, grad.norm=0.54639030\n",
      "  4827: 3 [  330/ 1499], train_loss/perplexity = 5.60968351/273.0578003 secs/batch = 0.5389s, grad.norm=0.57557607\n",
      "  4832: 3 [  335/ 1499], train_loss/perplexity = 5.72461939/306.3166504 secs/batch = 0.5393s, grad.norm=0.56981182\n",
      "  4837: 3 [  340/ 1499], train_loss/perplexity = 5.43976736/230.3885803 secs/batch = 0.5401s, grad.norm=0.44962788\n",
      "  4842: 3 [  345/ 1499], train_loss/perplexity = 5.39837122/221.0460815 secs/batch = 0.5415s, grad.norm=0.46440655\n",
      "  4847: 3 [  350/ 1499], train_loss/perplexity = 5.63942337/281.3004761 secs/batch = 0.5576s, grad.norm=0.62144434\n",
      "  4852: 3 [  355/ 1499], train_loss/perplexity = 5.43121815/228.4273376 secs/batch = 0.5481s, grad.norm=0.55713493\n",
      "  4857: 3 [  360/ 1499], train_loss/perplexity = 5.53984213/254.6378021 secs/batch = 0.5381s, grad.norm=0.58684260\n",
      "  4862: 3 [  365/ 1499], train_loss/perplexity = 5.40216255/221.8857422 secs/batch = 0.5418s, grad.norm=0.49576452\n",
      "  4867: 3 [  370/ 1499], train_loss/perplexity = 5.46693277/236.7329712 secs/batch = 0.5383s, grad.norm=0.56083387\n",
      "  4872: 3 [  375/ 1499], train_loss/perplexity = 5.57897854/264.8009949 secs/batch = 0.5441s, grad.norm=0.53045303\n",
      "  4877: 3 [  380/ 1499], train_loss/perplexity = 5.75512600/315.8053284 secs/batch = 0.5469s, grad.norm=0.55688858\n",
      "  4882: 3 [  385/ 1499], train_loss/perplexity = 5.90082359/365.3382263 secs/batch = 0.5395s, grad.norm=0.50593191\n",
      "  4887: 3 [  390/ 1499], train_loss/perplexity = 5.96339464/388.9281616 secs/batch = 0.5430s, grad.norm=0.56090361\n",
      "  4892: 3 [  395/ 1499], train_loss/perplexity = 5.98615694/397.8825684 secs/batch = 0.5483s, grad.norm=0.53129333\n",
      "  4897: 3 [  400/ 1499], train_loss/perplexity = 5.86112118/351.1175842 secs/batch = 0.5473s, grad.norm=0.70004940\n",
      "  4902: 3 [  405/ 1499], train_loss/perplexity = 5.76845407/320.0426025 secs/batch = 0.5459s, grad.norm=0.46778947\n",
      "  4907: 3 [  410/ 1499], train_loss/perplexity = 5.30668211/201.6799622 secs/batch = 0.5325s, grad.norm=0.69694304\n",
      "  4912: 3 [  415/ 1499], train_loss/perplexity = 5.84419584/345.2248230 secs/batch = 0.5408s, grad.norm=0.51215214\n",
      "  4917: 3 [  420/ 1499], train_loss/perplexity = 5.54184008/255.1470642 secs/batch = 0.5399s, grad.norm=0.47232506\n",
      "  4922: 3 [  425/ 1499], train_loss/perplexity = 5.87242794/355.1101074 secs/batch = 0.5382s, grad.norm=0.49094877\n",
      "  4927: 3 [  430/ 1499], train_loss/perplexity = 5.58330297/265.9485779 secs/batch = 0.5329s, grad.norm=0.48489156\n",
      "  4932: 3 [  435/ 1499], train_loss/perplexity = 5.53763962/254.0775757 secs/batch = 0.5878s, grad.norm=0.46810594\n",
      "  4937: 3 [  440/ 1499], train_loss/perplexity = 5.62679768/277.7711792 secs/batch = 0.5405s, grad.norm=0.56308961\n",
      "  4942: 3 [  445/ 1499], train_loss/perplexity = 5.72802830/307.3626404 secs/batch = 0.5441s, grad.norm=0.51516259\n",
      "  4947: 3 [  450/ 1499], train_loss/perplexity = 5.65918779/286.9155273 secs/batch = 0.5415s, grad.norm=0.53125465\n",
      "  4952: 3 [  455/ 1499], train_loss/perplexity = 5.80496836/331.9447021 secs/batch = 0.5413s, grad.norm=0.46026886\n",
      "  4957: 3 [  460/ 1499], train_loss/perplexity = 5.76891804/320.1911011 secs/batch = 0.5959s, grad.norm=0.57507694\n",
      "  4962: 3 [  465/ 1499], train_loss/perplexity = 5.76464033/318.8243408 secs/batch = 0.5430s, grad.norm=0.61177456\n",
      "  4967: 3 [  470/ 1499], train_loss/perplexity = 5.82733965/339.4544067 secs/batch = 0.5463s, grad.norm=0.47968405\n",
      "  4972: 3 [  475/ 1499], train_loss/perplexity = 5.63322258/279.5615845 secs/batch = 0.5438s, grad.norm=0.51286316\n",
      "  4977: 3 [  480/ 1499], train_loss/perplexity = 5.75939846/317.1575012 secs/batch = 0.5424s, grad.norm=0.50668705\n",
      "  4982: 3 [  485/ 1499], train_loss/perplexity = 5.72825861/307.4334412 secs/batch = 0.5428s, grad.norm=0.48631239\n",
      "  4987: 3 [  490/ 1499], train_loss/perplexity = 5.62089586/276.1366577 secs/batch = 0.5458s, grad.norm=0.50803643\n",
      "  4992: 3 [  495/ 1499], train_loss/perplexity = 5.65831327/286.6647034 secs/batch = 0.5414s, grad.norm=0.56016713\n",
      "  4997: 3 [  500/ 1499], train_loss/perplexity = 5.73768616/310.3454895 secs/batch = 0.5434s, grad.norm=0.48287961\n",
      "  5002: 3 [  505/ 1499], train_loss/perplexity = 5.51749992/249.0117035 secs/batch = 0.5440s, grad.norm=0.62333244\n",
      "  5007: 3 [  510/ 1499], train_loss/perplexity = 5.87868738/357.3398743 secs/batch = 0.5500s, grad.norm=0.52035391\n",
      "  5012: 3 [  515/ 1499], train_loss/perplexity = 5.61230421/273.7743530 secs/batch = 0.5442s, grad.norm=0.66706026\n",
      "  5017: 3 [  520/ 1499], train_loss/perplexity = 5.82370806/338.2238770 secs/batch = 0.5414s, grad.norm=0.56390798\n",
      "  5022: 3 [  525/ 1499], train_loss/perplexity = 5.91369057/370.0693970 secs/batch = 0.5477s, grad.norm=0.56716275\n",
      "  5027: 3 [  530/ 1499], train_loss/perplexity = 5.68640614/294.8321228 secs/batch = 0.5429s, grad.norm=0.49570203\n",
      "  5032: 3 [  535/ 1499], train_loss/perplexity = 5.67684698/292.0272217 secs/batch = 0.5354s, grad.norm=0.50856400\n",
      "  5037: 3 [  540/ 1499], train_loss/perplexity = 5.53464222/253.3171387 secs/batch = 0.5411s, grad.norm=0.51333088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5042: 3 [  545/ 1499], train_loss/perplexity = 5.30772686/201.8907776 secs/batch = 0.5370s, grad.norm=0.55924791\n",
      "  5047: 3 [  550/ 1499], train_loss/perplexity = 5.75231600/314.9191589 secs/batch = 0.5431s, grad.norm=0.56208599\n",
      "  5052: 3 [  555/ 1499], train_loss/perplexity = 5.54132462/255.0155792 secs/batch = 0.5413s, grad.norm=0.51101333\n",
      "  5057: 3 [  560/ 1499], train_loss/perplexity = 5.93056488/376.3670654 secs/batch = 0.5470s, grad.norm=0.52870917\n",
      "  5062: 3 [  565/ 1499], train_loss/perplexity = 5.78970432/326.9163513 secs/batch = 0.5393s, grad.norm=0.50941259\n",
      "  5067: 3 [  570/ 1499], train_loss/perplexity = 5.75434971/315.5602722 secs/batch = 0.5432s, grad.norm=0.48101228\n",
      "  5072: 3 [  575/ 1499], train_loss/perplexity = 5.77391195/321.7941284 secs/batch = 0.5389s, grad.norm=0.50528854\n",
      "  5077: 3 [  580/ 1499], train_loss/perplexity = 5.55038595/257.3368530 secs/batch = 0.5426s, grad.norm=0.66021574\n",
      "  5082: 3 [  585/ 1499], train_loss/perplexity = 5.44460630/231.5061188 secs/batch = 0.5387s, grad.norm=0.49454716\n",
      "  5087: 3 [  590/ 1499], train_loss/perplexity = 5.66043282/287.2729492 secs/batch = 0.5375s, grad.norm=0.56404561\n",
      "  5092: 3 [  595/ 1499], train_loss/perplexity = 5.60562372/271.9515076 secs/batch = 0.5350s, grad.norm=0.47614950\n",
      "  5097: 3 [  600/ 1499], train_loss/perplexity = 5.45429039/233.7589417 secs/batch = 0.5451s, grad.norm=0.54408187\n",
      "  5102: 3 [  605/ 1499], train_loss/perplexity = 5.46996260/237.4513092 secs/batch = 0.5409s, grad.norm=0.50125962\n",
      "  5107: 3 [  610/ 1499], train_loss/perplexity = 5.72102976/305.2190552 secs/batch = 0.5459s, grad.norm=0.50572675\n",
      "  5112: 3 [  615/ 1499], train_loss/perplexity = 5.74135685/311.4867554 secs/batch = 0.5495s, grad.norm=0.46888542\n",
      "  5117: 3 [  620/ 1499], train_loss/perplexity = 5.57683802/264.2347717 secs/batch = 0.5440s, grad.norm=0.52136588\n",
      "  5122: 3 [  625/ 1499], train_loss/perplexity = 5.59020615/267.7908325 secs/batch = 0.5422s, grad.norm=0.54548019\n",
      "  5127: 3 [  630/ 1499], train_loss/perplexity = 5.69264889/296.6784363 secs/batch = 0.5417s, grad.norm=0.52574152\n",
      "  5132: 3 [  635/ 1499], train_loss/perplexity = 5.48720074/241.5800171 secs/batch = 0.5379s, grad.norm=0.67036712\n",
      "  5137: 3 [  640/ 1499], train_loss/perplexity = 5.73534441/309.6195984 secs/batch = 0.5366s, grad.norm=0.54864615\n",
      "  5142: 3 [  645/ 1499], train_loss/perplexity = 5.49059534/242.4014740 secs/batch = 0.5434s, grad.norm=0.80909634\n",
      "  5147: 3 [  650/ 1499], train_loss/perplexity = 5.42579842/227.1926727 secs/batch = 0.5430s, grad.norm=0.59563893\n",
      "  5152: 3 [  655/ 1499], train_loss/perplexity = 5.75456381/315.6278381 secs/batch = 0.5415s, grad.norm=0.47735581\n",
      "  5157: 3 [  660/ 1499], train_loss/perplexity = 5.91111469/369.1173706 secs/batch = 0.5497s, grad.norm=0.58360755\n",
      "  5162: 3 [  665/ 1499], train_loss/perplexity = 5.84355211/345.0026550 secs/batch = 0.5353s, grad.norm=0.59925032\n",
      "  5167: 3 [  670/ 1499], train_loss/perplexity = 5.83477831/341.9889221 secs/batch = 0.5438s, grad.norm=0.50897926\n",
      "  5172: 3 [  675/ 1499], train_loss/perplexity = 5.73521900/309.5807495 secs/batch = 0.5388s, grad.norm=0.52672660\n",
      "  5177: 3 [  680/ 1499], train_loss/perplexity = 5.70021009/298.9302063 secs/batch = 0.5405s, grad.norm=0.49955335\n",
      "  5182: 3 [  685/ 1499], train_loss/perplexity = 5.79779911/329.5733948 secs/batch = 0.5397s, grad.norm=0.47754478\n",
      "  5187: 3 [  690/ 1499], train_loss/perplexity = 5.72058153/305.0822754 secs/batch = 0.5416s, grad.norm=0.55118847\n",
      "  5192: 3 [  695/ 1499], train_loss/perplexity = 5.63823462/280.9662781 secs/batch = 0.5392s, grad.norm=0.56602478\n",
      "  5197: 3 [  700/ 1499], train_loss/perplexity = 5.85307598/348.3041077 secs/batch = 0.5371s, grad.norm=0.55915201\n",
      "  5202: 3 [  705/ 1499], train_loss/perplexity = 5.38980770/219.1612396 secs/batch = 0.5459s, grad.norm=0.54128683\n",
      "  5207: 3 [  710/ 1499], train_loss/perplexity = 5.65288401/285.1125488 secs/batch = 0.5440s, grad.norm=0.46699095\n",
      "  5212: 3 [  715/ 1499], train_loss/perplexity = 5.67018270/290.0875244 secs/batch = 0.5348s, grad.norm=0.52296519\n",
      "  5217: 3 [  720/ 1499], train_loss/perplexity = 5.72176170/305.4425354 secs/batch = 0.5416s, grad.norm=0.53436983\n",
      "  5222: 3 [  725/ 1499], train_loss/perplexity = 5.56291199/260.5805359 secs/batch = 0.5428s, grad.norm=0.60907412\n",
      "  5227: 3 [  730/ 1499], train_loss/perplexity = 5.72892475/307.6383057 secs/batch = 0.5855s, grad.norm=0.63890737\n",
      "  5232: 3 [  735/ 1499], train_loss/perplexity = 5.52736759/251.4810333 secs/batch = 0.5429s, grad.norm=0.51960576\n",
      "  5237: 3 [  740/ 1499], train_loss/perplexity = 5.51293612/247.8778534 secs/batch = 0.5460s, grad.norm=0.49316904\n",
      "  5242: 3 [  745/ 1499], train_loss/perplexity = 5.36959505/214.7758789 secs/batch = 0.5362s, grad.norm=0.53347123\n",
      "  5247: 3 [  750/ 1499], train_loss/perplexity = 5.64687490/283.4044189 secs/batch = 0.5414s, grad.norm=0.51979101\n",
      "  5252: 3 [  755/ 1499], train_loss/perplexity = 5.47931433/239.6823120 secs/batch = 0.5460s, grad.norm=0.63608670\n",
      "  5257: 3 [  760/ 1499], train_loss/perplexity = 5.13157272/169.2831421 secs/batch = 0.5487s, grad.norm=0.63938659\n",
      "  5262: 3 [  765/ 1499], train_loss/perplexity = 5.49632215/243.7936401 secs/batch = 0.5446s, grad.norm=0.60815495\n",
      "  5267: 3 [  770/ 1499], train_loss/perplexity = 5.62386084/276.9566040 secs/batch = 0.5409s, grad.norm=0.55827826\n",
      "  5272: 3 [  775/ 1499], train_loss/perplexity = 5.90879822/368.2633057 secs/batch = 0.5417s, grad.norm=0.55813891\n",
      "  5277: 3 [  780/ 1499], train_loss/perplexity = 5.42498446/227.0078125 secs/batch = 0.5449s, grad.norm=0.50218409\n",
      "  5282: 3 [  785/ 1499], train_loss/perplexity = 5.57860041/264.7008667 secs/batch = 0.5404s, grad.norm=0.53438079\n",
      "  5287: 3 [  790/ 1499], train_loss/perplexity = 5.19615221/180.5760803 secs/batch = 0.5426s, grad.norm=0.55111319\n",
      "  5292: 3 [  795/ 1499], train_loss/perplexity = 5.69443178/297.2078552 secs/batch = 0.5395s, grad.norm=0.56232852\n",
      "  5297: 3 [  800/ 1499], train_loss/perplexity = 5.53908110/254.4440765 secs/batch = 0.5387s, grad.norm=0.58450240\n",
      "  5302: 3 [  805/ 1499], train_loss/perplexity = 5.58711910/266.9654236 secs/batch = 0.5519s, grad.norm=0.52928454\n",
      "  5307: 3 [  810/ 1499], train_loss/perplexity = 5.52804852/251.6523285 secs/batch = 0.5437s, grad.norm=0.48987943\n",
      "  5312: 3 [  815/ 1499], train_loss/perplexity = 5.67691088/292.0458679 secs/batch = 0.5410s, grad.norm=0.51645964\n",
      "  5317: 3 [  820/ 1499], train_loss/perplexity = 5.23449659/187.6346283 secs/batch = 0.5367s, grad.norm=0.50188708\n",
      "  5322: 3 [  825/ 1499], train_loss/perplexity = 5.41628981/225.0426178 secs/batch = 0.5461s, grad.norm=0.66996479\n",
      "  5327: 3 [  830/ 1499], train_loss/perplexity = 5.65952396/287.0119934 secs/batch = 0.5408s, grad.norm=0.59365135\n",
      "  5332: 3 [  835/ 1499], train_loss/perplexity = 5.64029360/281.5453796 secs/batch = 0.5483s, grad.norm=0.61159402\n",
      "  5337: 3 [  840/ 1499], train_loss/perplexity = 5.54458141/255.8474579 secs/batch = 0.5366s, grad.norm=0.54753846\n",
      "  5342: 3 [  845/ 1499], train_loss/perplexity = 5.45802593/234.6337891 secs/batch = 0.5370s, grad.norm=0.66009635\n",
      "  5347: 3 [  850/ 1499], train_loss/perplexity = 5.55171347/257.6787109 secs/batch = 0.5394s, grad.norm=0.60262507\n",
      "  5352: 3 [  855/ 1499], train_loss/perplexity = 5.66822910/289.5213623 secs/batch = 0.5386s, grad.norm=0.52959162\n",
      "  5357: 3 [  860/ 1499], train_loss/perplexity = 5.17805576/177.3376923 secs/batch = 0.5369s, grad.norm=0.63034940\n",
      "  5362: 3 [  865/ 1499], train_loss/perplexity = 5.50815725/246.6961060 secs/batch = 0.5416s, grad.norm=0.51997513\n",
      "  5367: 3 [  870/ 1499], train_loss/perplexity = 5.49200726/242.7439728 secs/batch = 0.5364s, grad.norm=0.53551149\n",
      "  5372: 3 [  875/ 1499], train_loss/perplexity = 5.51569939/248.5637512 secs/batch = 0.5357s, grad.norm=0.51149476\n",
      "  5377: 3 [  880/ 1499], train_loss/perplexity = 5.43242836/228.7039490 secs/batch = 0.5451s, grad.norm=0.51443428\n",
      "  5382: 3 [  885/ 1499], train_loss/perplexity = 5.36536884/213.8701019 secs/batch = 0.5761s, grad.norm=0.62876439\n",
      "  5387: 3 [  890/ 1499], train_loss/perplexity = 5.65750790/286.4339294 secs/batch = 0.5452s, grad.norm=0.53448528\n",
      "  5392: 3 [  895/ 1499], train_loss/perplexity = 5.68950558/295.7473755 secs/batch = 0.5402s, grad.norm=0.50195438\n",
      "  5397: 3 [  900/ 1499], train_loss/perplexity = 5.45343924/233.5600586 secs/batch = 0.5403s, grad.norm=0.53052783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5402: 3 [  905/ 1499], train_loss/perplexity = 5.56944132/262.2875366 secs/batch = 0.5392s, grad.norm=0.58234221\n",
      "  5407: 3 [  910/ 1499], train_loss/perplexity = 5.72072554/305.1262207 secs/batch = 0.5473s, grad.norm=0.57953745\n",
      "  5412: 3 [  915/ 1499], train_loss/perplexity = 5.48584890/241.2536621 secs/batch = 0.5523s, grad.norm=0.59361750\n",
      "  5417: 3 [  920/ 1499], train_loss/perplexity = 5.19138479/179.7172546 secs/batch = 0.5415s, grad.norm=0.46387133\n",
      "  5422: 3 [  925/ 1499], train_loss/perplexity = 5.62708712/277.8515930 secs/batch = 0.5383s, grad.norm=0.65862906\n",
      "  5427: 3 [  930/ 1499], train_loss/perplexity = 5.56869030/262.0906067 secs/batch = 0.5473s, grad.norm=0.57407534\n",
      "  5432: 3 [  935/ 1499], train_loss/perplexity = 5.66202450/287.7305603 secs/batch = 0.5414s, grad.norm=0.52726960\n",
      "  5437: 3 [  940/ 1499], train_loss/perplexity = 5.28040934/196.4502716 secs/batch = 0.5382s, grad.norm=0.62236583\n",
      "  5442: 3 [  945/ 1499], train_loss/perplexity = 5.65296650/285.1360779 secs/batch = 0.5410s, grad.norm=0.52058882\n",
      "  5447: 3 [  950/ 1499], train_loss/perplexity = 5.56676197/261.5856934 secs/batch = 0.5469s, grad.norm=0.50892413\n",
      "  5452: 3 [  955/ 1499], train_loss/perplexity = 5.71200800/302.4778442 secs/batch = 0.5498s, grad.norm=0.48246646\n",
      "  5457: 3 [  960/ 1499], train_loss/perplexity = 5.45592308/234.1408997 secs/batch = 0.5466s, grad.norm=0.53849906\n",
      "  5462: 3 [  965/ 1499], train_loss/perplexity = 5.55439615/258.3709106 secs/batch = 0.5407s, grad.norm=0.70467055\n",
      "  5467: 3 [  970/ 1499], train_loss/perplexity = 5.77685928/322.7439575 secs/batch = 0.5402s, grad.norm=0.50380707\n",
      "  5472: 3 [  975/ 1499], train_loss/perplexity = 5.55975866/259.7601318 secs/batch = 0.5408s, grad.norm=0.54694128\n",
      "  5477: 3 [  980/ 1499], train_loss/perplexity = 5.62896776/278.3746338 secs/batch = 0.5413s, grad.norm=0.62610292\n",
      "  5482: 3 [  985/ 1499], train_loss/perplexity = 5.33875895/208.2540894 secs/batch = 0.5428s, grad.norm=0.50052315\n",
      "  5487: 3 [  990/ 1499], train_loss/perplexity = 5.48510265/241.0736847 secs/batch = 0.5392s, grad.norm=0.49243358\n",
      "  5492: 3 [  995/ 1499], train_loss/perplexity = 5.75991917/317.3226624 secs/batch = 0.5393s, grad.norm=0.59229243\n",
      "  5497: 3 [ 1000/ 1499], train_loss/perplexity = 5.42783356/227.6555023 secs/batch = 0.5769s, grad.norm=0.49957874\n",
      "  5502: 3 [ 1005/ 1499], train_loss/perplexity = 5.82017040/337.0294800 secs/batch = 0.5342s, grad.norm=0.54406041\n",
      "  5507: 3 [ 1010/ 1499], train_loss/perplexity = 5.70748949/301.1141663 secs/batch = 0.5533s, grad.norm=0.57764763\n",
      "  5512: 3 [ 1015/ 1499], train_loss/perplexity = 5.43952513/230.3327789 secs/batch = 0.5389s, grad.norm=0.50335336\n",
      "  5517: 3 [ 1020/ 1499], train_loss/perplexity = 5.56287718/260.5714722 secs/batch = 0.5408s, grad.norm=0.59105217\n",
      "  5522: 3 [ 1025/ 1499], train_loss/perplexity = 5.61499643/274.5123901 secs/batch = 0.5389s, grad.norm=0.68251199\n",
      "  5527: 3 [ 1030/ 1499], train_loss/perplexity = 5.71631384/303.7830505 secs/batch = 0.5409s, grad.norm=0.56451494\n",
      "  5532: 3 [ 1035/ 1499], train_loss/perplexity = 5.95155191/384.3493652 secs/batch = 0.5406s, grad.norm=0.57802230\n",
      "  5537: 3 [ 1040/ 1499], train_loss/perplexity = 5.37759638/216.5012665 secs/batch = 0.5415s, grad.norm=0.49928468\n",
      "  5542: 3 [ 1045/ 1499], train_loss/perplexity = 5.50873947/246.8397827 secs/batch = 0.5368s, grad.norm=0.51650381\n",
      "  5547: 3 [ 1050/ 1499], train_loss/perplexity = 5.19584417/180.5204620 secs/batch = 0.5435s, grad.norm=0.51458544\n",
      "  5552: 3 [ 1055/ 1499], train_loss/perplexity = 5.59849596/270.0199890 secs/batch = 0.5343s, grad.norm=0.61633331\n",
      "  5557: 3 [ 1060/ 1499], train_loss/perplexity = 5.76320076/318.3657227 secs/batch = 0.5426s, grad.norm=0.56936407\n",
      "  5562: 3 [ 1065/ 1499], train_loss/perplexity = 5.33691883/207.8712311 secs/batch = 0.5390s, grad.norm=0.55322891\n",
      "  5567: 3 [ 1070/ 1499], train_loss/perplexity = 5.78114891/324.1313782 secs/batch = 0.5372s, grad.norm=0.52494931\n",
      "  5572: 3 [ 1075/ 1499], train_loss/perplexity = 5.66015530/287.1932373 secs/batch = 0.5412s, grad.norm=0.57356465\n",
      "  5577: 3 [ 1080/ 1499], train_loss/perplexity = 5.84664869/346.0726318 secs/batch = 0.5419s, grad.norm=0.53630179\n",
      "  5582: 3 [ 1085/ 1499], train_loss/perplexity = 5.93878984/379.4754333 secs/batch = 0.5378s, grad.norm=0.53449416\n",
      "  5587: 3 [ 1090/ 1499], train_loss/perplexity = 5.49773455/244.1382141 secs/batch = 0.5388s, grad.norm=0.50963181\n",
      "  5592: 3 [ 1095/ 1499], train_loss/perplexity = 5.65194273/284.8442993 secs/batch = 0.5409s, grad.norm=0.48659292\n",
      "  5597: 3 [ 1100/ 1499], train_loss/perplexity = 5.72338581/305.9390259 secs/batch = 0.5413s, grad.norm=0.72809875\n",
      "  5602: 3 [ 1105/ 1499], train_loss/perplexity = 5.42948961/228.0328369 secs/batch = 0.5688s, grad.norm=0.52530599\n",
      "  5607: 3 [ 1110/ 1499], train_loss/perplexity = 5.61549330/274.6488342 secs/batch = 0.5482s, grad.norm=0.52142972\n",
      "  5612: 3 [ 1115/ 1499], train_loss/perplexity = 5.24003029/188.6758118 secs/batch = 0.5399s, grad.norm=0.55584133\n",
      "  5617: 3 [ 1120/ 1499], train_loss/perplexity = 5.45095778/232.9812012 secs/batch = 0.5402s, grad.norm=0.51809776\n",
      "  5622: 3 [ 1125/ 1499], train_loss/perplexity = 5.42459774/226.9200439 secs/batch = 0.5396s, grad.norm=0.53059661\n",
      "  5627: 3 [ 1130/ 1499], train_loss/perplexity = 5.22653484/186.1466522 secs/batch = 0.5469s, grad.norm=0.47845149\n",
      "  5632: 3 [ 1135/ 1499], train_loss/perplexity = 5.83067226/340.5875549 secs/batch = 0.5458s, grad.norm=0.48607072\n",
      "  5637: 3 [ 1140/ 1499], train_loss/perplexity = 5.71678400/303.9259338 secs/batch = 0.5418s, grad.norm=0.61493754\n",
      "  5642: 3 [ 1145/ 1499], train_loss/perplexity = 5.75410509/315.4830933 secs/batch = 0.5417s, grad.norm=0.54053962\n",
      "  5647: 3 [ 1150/ 1499], train_loss/perplexity = 5.68071222/293.1581421 secs/batch = 0.5399s, grad.norm=0.50791967\n",
      "  5652: 3 [ 1155/ 1499], train_loss/perplexity = 5.37542057/216.0307159 secs/batch = 0.5391s, grad.norm=0.54876715\n",
      "  5657: 3 [ 1160/ 1499], train_loss/perplexity = 5.54550648/256.0842590 secs/batch = 0.5456s, grad.norm=0.51614404\n",
      "  5662: 3 [ 1165/ 1499], train_loss/perplexity = 5.48205662/240.3404846 secs/batch = 0.5469s, grad.norm=0.59098828\n",
      "  5667: 3 [ 1170/ 1499], train_loss/perplexity = 5.72174406/305.4371643 secs/batch = 0.5440s, grad.norm=0.56707734\n",
      "  5672: 3 [ 1175/ 1499], train_loss/perplexity = 5.12265921/167.7809448 secs/batch = 0.5438s, grad.norm=0.60604030\n",
      "  5677: 3 [ 1180/ 1499], train_loss/perplexity = 5.41378117/224.4787750 secs/batch = 0.5475s, grad.norm=0.58640325\n",
      "  5682: 3 [ 1185/ 1499], train_loss/perplexity = 5.35184240/210.9966736 secs/batch = 0.5371s, grad.norm=0.61154056\n",
      "  5687: 3 [ 1190/ 1499], train_loss/perplexity = 5.57752037/264.4151306 secs/batch = 0.5331s, grad.norm=0.55871028\n",
      "  5692: 3 [ 1195/ 1499], train_loss/perplexity = 5.62399244/276.9930725 secs/batch = 0.5415s, grad.norm=0.56754827\n",
      "  5697: 3 [ 1200/ 1499], train_loss/perplexity = 5.49875546/244.3875885 secs/batch = 0.5357s, grad.norm=0.68462759\n",
      "  5702: 3 [ 1205/ 1499], train_loss/perplexity = 5.43284988/228.8003693 secs/batch = 0.5375s, grad.norm=0.62269950\n",
      "  5707: 3 [ 1210/ 1499], train_loss/perplexity = 5.39960527/221.3190460 secs/batch = 0.5422s, grad.norm=0.58615297\n",
      "  5712: 3 [ 1215/ 1499], train_loss/perplexity = 5.08522940/161.6170044 secs/batch = 0.5450s, grad.norm=0.58603758\n",
      "  5717: 3 [ 1220/ 1499], train_loss/perplexity = 5.48404980/240.8200073 secs/batch = 0.5389s, grad.norm=0.56519300\n",
      "  5722: 3 [ 1225/ 1499], train_loss/perplexity = 4.95992613/142.5832672 secs/batch = 0.5420s, grad.norm=0.68806249\n",
      "  5727: 3 [ 1230/ 1499], train_loss/perplexity = 5.50497198/245.9115601 secs/batch = 0.5361s, grad.norm=0.55845094\n",
      "  5732: 3 [ 1235/ 1499], train_loss/perplexity = 5.32447863/205.3013000 secs/batch = 0.5364s, grad.norm=0.63071799\n",
      "  5737: 3 [ 1240/ 1499], train_loss/perplexity = 5.38492966/218.0947571 secs/batch = 0.5442s, grad.norm=0.56442332\n",
      "  5742: 3 [ 1245/ 1499], train_loss/perplexity = 5.65553904/285.8705444 secs/batch = 0.5369s, grad.norm=0.50909817\n",
      "  5747: 3 [ 1250/ 1499], train_loss/perplexity = 5.88538551/359.7414246 secs/batch = 0.5428s, grad.norm=0.57751697\n",
      "  5752: 3 [ 1255/ 1499], train_loss/perplexity = 5.46918440/237.2666016 secs/batch = 0.5403s, grad.norm=0.54519761\n",
      "  5757: 3 [ 1260/ 1499], train_loss/perplexity = 5.53287983/252.8710938 secs/batch = 0.5437s, grad.norm=0.50159240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5762: 3 [ 1265/ 1499], train_loss/perplexity = 5.59605455/269.3615417 secs/batch = 0.5438s, grad.norm=0.48422015\n",
      "  5767: 3 [ 1270/ 1499], train_loss/perplexity = 5.57837152/264.6402893 secs/batch = 0.5966s, grad.norm=0.51380485\n",
      "  5772: 3 [ 1275/ 1499], train_loss/perplexity = 5.59074068/267.9339905 secs/batch = 0.5399s, grad.norm=0.58012235\n",
      "  5777: 3 [ 1280/ 1499], train_loss/perplexity = 5.22716475/186.2639465 secs/batch = 0.5368s, grad.norm=0.58040404\n",
      "  5782: 3 [ 1285/ 1499], train_loss/perplexity = 5.67457438/291.3642883 secs/batch = 0.5413s, grad.norm=0.60756665\n",
      "  5787: 3 [ 1290/ 1499], train_loss/perplexity = 5.61714792/275.1036377 secs/batch = 0.5387s, grad.norm=0.58219230\n",
      "  5792: 3 [ 1295/ 1499], train_loss/perplexity = 5.49642706/243.8192291 secs/batch = 0.5413s, grad.norm=0.51811647\n",
      "  5797: 3 [ 1300/ 1499], train_loss/perplexity = 5.68850279/295.4509277 secs/batch = 0.5385s, grad.norm=0.49984932\n",
      "  5802: 3 [ 1305/ 1499], train_loss/perplexity = 5.56954527/262.3147888 secs/batch = 0.5382s, grad.norm=0.59630805\n",
      "  5807: 3 [ 1310/ 1499], train_loss/perplexity = 5.64448261/282.7272339 secs/batch = 0.5457s, grad.norm=0.50513268\n",
      "  5812: 3 [ 1315/ 1499], train_loss/perplexity = 5.50042057/244.7948608 secs/batch = 0.5430s, grad.norm=0.69359660\n",
      "  5817: 3 [ 1320/ 1499], train_loss/perplexity = 5.19298601/180.0052490 secs/batch = 0.5434s, grad.norm=0.55668175\n",
      "  5822: 3 [ 1325/ 1499], train_loss/perplexity = 5.55327892/258.0823975 secs/batch = 0.5493s, grad.norm=0.54161888\n",
      "  5827: 3 [ 1330/ 1499], train_loss/perplexity = 5.69126654/296.2686157 secs/batch = 0.5377s, grad.norm=0.64126396\n",
      "  5832: 3 [ 1335/ 1499], train_loss/perplexity = 5.50494289/245.9044189 secs/batch = 0.5368s, grad.norm=0.55928552\n",
      "  5837: 3 [ 1340/ 1499], train_loss/perplexity = 4.96684933/143.5738220 secs/batch = 0.5407s, grad.norm=0.56869924\n",
      "  5842: 3 [ 1345/ 1499], train_loss/perplexity = 5.16903067/175.7444000 secs/batch = 0.5482s, grad.norm=0.66126108\n",
      "  5847: 3 [ 1350/ 1499], train_loss/perplexity = 5.26882219/194.1871185 secs/batch = 0.5439s, grad.norm=0.50971216\n",
      "  5852: 3 [ 1355/ 1499], train_loss/perplexity = 5.07398939/159.8106079 secs/batch = 0.5391s, grad.norm=0.68192774\n",
      "  5857: 3 [ 1360/ 1499], train_loss/perplexity = 5.13356352/169.6204834 secs/batch = 0.5466s, grad.norm=0.51464742\n",
      "  5862: 3 [ 1365/ 1499], train_loss/perplexity = 5.04599667/155.3991089 secs/batch = 0.5482s, grad.norm=0.52625948\n",
      "  5867: 3 [ 1370/ 1499], train_loss/perplexity = 5.01537991/150.7133789 secs/batch = 0.5425s, grad.norm=0.52419317\n",
      "  5872: 3 [ 1375/ 1499], train_loss/perplexity = 5.09824514/163.7343292 secs/batch = 0.5514s, grad.norm=0.65914863\n",
      "  5877: 3 [ 1380/ 1499], train_loss/perplexity = 5.53606892/253.6788025 secs/batch = 0.5432s, grad.norm=0.59510392\n",
      "  5882: 3 [ 1385/ 1499], train_loss/perplexity = 5.29935455/200.2075500 secs/batch = 0.5448s, grad.norm=0.52551323\n",
      "  5887: 3 [ 1390/ 1499], train_loss/perplexity = 5.33517313/207.5086670 secs/batch = 0.5449s, grad.norm=0.49870360\n",
      "  5892: 3 [ 1395/ 1499], train_loss/perplexity = 5.50408411/245.6933289 secs/batch = 0.5422s, grad.norm=0.48887309\n",
      "  5897: 3 [ 1400/ 1499], train_loss/perplexity = 5.52265739/250.2993011 secs/batch = 0.5432s, grad.norm=0.57325035\n",
      "  5902: 3 [ 1405/ 1499], train_loss/perplexity = 5.36182547/213.1136169 secs/batch = 0.5414s, grad.norm=0.52183163\n",
      "  5907: 3 [ 1410/ 1499], train_loss/perplexity = 5.61089182/273.3879395 secs/batch = 0.5445s, grad.norm=0.50791925\n",
      "  5912: 3 [ 1415/ 1499], train_loss/perplexity = 5.50720644/246.4616547 secs/batch = 0.5535s, grad.norm=0.55946594\n",
      "  5917: 3 [ 1420/ 1499], train_loss/perplexity = 5.49408293/243.2483521 secs/batch = 0.5433s, grad.norm=0.50835973\n",
      "  5922: 3 [ 1425/ 1499], train_loss/perplexity = 5.39794111/220.9510345 secs/batch = 0.5457s, grad.norm=0.60588568\n",
      "  5927: 3 [ 1430/ 1499], train_loss/perplexity = 5.60422468/271.5712891 secs/batch = 0.5362s, grad.norm=0.52459383\n",
      "  5932: 3 [ 1435/ 1499], train_loss/perplexity = 5.28330231/197.0194244 secs/batch = 0.5422s, grad.norm=0.56223297\n",
      "  5937: 3 [ 1440/ 1499], train_loss/perplexity = 5.00197935/148.7072144 secs/batch = 0.5411s, grad.norm=0.51436239\n",
      "  5942: 3 [ 1445/ 1499], train_loss/perplexity = 5.44563437/231.7442474 secs/batch = 0.5369s, grad.norm=0.61688989\n",
      "  5947: 3 [ 1450/ 1499], train_loss/perplexity = 5.50738430/246.5054932 secs/batch = 0.5419s, grad.norm=0.61654305\n",
      "  5952: 3 [ 1455/ 1499], train_loss/perplexity = 5.78301287/324.7361145 secs/batch = 0.5464s, grad.norm=0.53499854\n",
      "  5957: 3 [ 1460/ 1499], train_loss/perplexity = 5.76950073/320.3777466 secs/batch = 0.5530s, grad.norm=0.49560508\n",
      "  5962: 3 [ 1465/ 1499], train_loss/perplexity = 5.90678310/367.5219727 secs/batch = 0.5377s, grad.norm=0.51659501\n",
      "  5967: 3 [ 1470/ 1499], train_loss/perplexity = 5.57138300/262.7973022 secs/batch = 0.5415s, grad.norm=0.59964716\n",
      "  5972: 3 [ 1475/ 1499], train_loss/perplexity = 5.59959602/270.3171692 secs/batch = 0.5337s, grad.norm=0.50458288\n",
      "  5977: 3 [ 1480/ 1499], train_loss/perplexity = 5.59690952/269.5919495 secs/batch = 0.5424s, grad.norm=0.50908387\n",
      "  5982: 3 [ 1485/ 1499], train_loss/perplexity = 5.36680079/214.1765747 secs/batch = 0.5435s, grad.norm=0.65072000\n",
      "  5987: 3 [ 1490/ 1499], train_loss/perplexity = 5.44047260/230.5511169 secs/batch = 0.5407s, grad.norm=0.51260817\n",
      "  5992: 3 [ 1495/ 1499], train_loss/perplexity = 5.74737549/313.3671570 secs/batch = 0.5388s, grad.norm=0.55304027\n",
      "Epoch training time: 815.8495335578918\n",
      "Saved char model cv/epoch003_5.5638.model\n",
      "  6001: 4 [    5/ 1499], train_loss/perplexity = 5.63415623/279.8227234 secs/batch = 0.6153s, grad.norm=0.54224586\n",
      "  6006: 4 [   10/ 1499], train_loss/perplexity = 5.62969398/278.5768433 secs/batch = 0.6451s, grad.norm=0.53400046\n",
      "  6011: 4 [   15/ 1499], train_loss/perplexity = 5.49131584/242.5761871 secs/batch = 0.5686s, grad.norm=0.72756034\n",
      "  6016: 4 [   20/ 1499], train_loss/perplexity = 5.36019516/212.7664642 secs/batch = 0.5405s, grad.norm=0.49679118\n",
      "  6021: 4 [   25/ 1499], train_loss/perplexity = 5.77708721/322.8175354 secs/batch = 0.5419s, grad.norm=0.56842190\n",
      "  6026: 4 [   30/ 1499], train_loss/perplexity = 5.70085764/299.1238403 secs/batch = 0.5409s, grad.norm=0.59952074\n",
      "  6031: 4 [   35/ 1499], train_loss/perplexity = 5.54838324/256.8219910 secs/batch = 0.5729s, grad.norm=0.51538563\n",
      "  6036: 4 [   40/ 1499], train_loss/perplexity = 5.54300642/255.4448242 secs/batch = 0.5605s, grad.norm=0.51672941\n",
      "  6041: 4 [   45/ 1499], train_loss/perplexity = 5.52350521/250.5115967 secs/batch = 0.5625s, grad.norm=0.55909425\n",
      "  6046: 4 [   50/ 1499], train_loss/perplexity = 5.44219160/230.9477692 secs/batch = 0.5510s, grad.norm=0.52502894\n",
      "  6051: 4 [   55/ 1499], train_loss/perplexity = 5.38427877/217.9528503 secs/batch = 0.5505s, grad.norm=0.61251271\n",
      "  6056: 4 [   60/ 1499], train_loss/perplexity = 5.32437992/205.2810364 secs/batch = 0.5513s, grad.norm=0.53135574\n",
      "  6061: 4 [   65/ 1499], train_loss/perplexity = 5.37950182/216.9141846 secs/batch = 0.5441s, grad.norm=0.61276019\n",
      "  6066: 4 [   70/ 1499], train_loss/perplexity = 5.34815454/210.2199860 secs/batch = 0.5547s, grad.norm=0.63031423\n",
      "  6071: 4 [   75/ 1499], train_loss/perplexity = 5.13873148/170.4993439 secs/batch = 0.5519s, grad.norm=0.55024922\n",
      "  6076: 4 [   80/ 1499], train_loss/perplexity = 5.28943682/198.2317505 secs/batch = 0.5534s, grad.norm=0.50798792\n",
      "  6081: 4 [   85/ 1499], train_loss/perplexity = 5.21752882/184.4777374 secs/batch = 0.5575s, grad.norm=0.59512043\n",
      "  6086: 4 [   90/ 1499], train_loss/perplexity = 5.44738436/232.1501465 secs/batch = 0.5476s, grad.norm=0.61358780\n",
      "  6091: 4 [   95/ 1499], train_loss/perplexity = 5.19800711/180.9113464 secs/batch = 0.5533s, grad.norm=0.51568687\n",
      "  6096: 4 [  100/ 1499], train_loss/perplexity = 5.25930071/192.3469391 secs/batch = 0.5589s, grad.norm=0.49912837\n",
      "  6101: 4 [  105/ 1499], train_loss/perplexity = 5.19640636/180.6219788 secs/batch = 0.5503s, grad.norm=0.56702739\n",
      "  6106: 4 [  110/ 1499], train_loss/perplexity = 5.20786524/182.7036133 secs/batch = 0.5539s, grad.norm=0.56531775\n",
      "  6111: 4 [  115/ 1499], train_loss/perplexity = 5.42521477/227.0601044 secs/batch = 0.5601s, grad.norm=0.48129606\n",
      "  6116: 4 [  120/ 1499], train_loss/perplexity = 5.26957750/194.3338470 secs/batch = 0.5491s, grad.norm=0.55711544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6121: 4 [  125/ 1499], train_loss/perplexity = 5.50282669/245.3845825 secs/batch = 0.5560s, grad.norm=0.68629920\n",
      "  6126: 4 [  130/ 1499], train_loss/perplexity = 5.42908764/227.9411926 secs/batch = 0.5634s, grad.norm=0.53032947\n",
      "  6131: 4 [  135/ 1499], train_loss/perplexity = 5.43758011/229.8852081 secs/batch = 0.5509s, grad.norm=0.57254535\n",
      "  6136: 4 [  140/ 1499], train_loss/perplexity = 5.44690466/232.0388184 secs/batch = 0.5502s, grad.norm=0.60254824\n",
      "  6141: 4 [  145/ 1499], train_loss/perplexity = 5.18459749/178.5015869 secs/batch = 0.5537s, grad.norm=0.55803508\n",
      "  6146: 4 [  150/ 1499], train_loss/perplexity = 5.41601610/224.9810333 secs/batch = 0.5496s, grad.norm=0.55128151\n",
      "  6151: 4 [  155/ 1499], train_loss/perplexity = 5.54189444/255.1609344 secs/batch = 0.5480s, grad.norm=0.52777064\n",
      "  6156: 4 [  160/ 1499], train_loss/perplexity = 5.82739353/339.4726868 secs/batch = 0.6064s, grad.norm=0.57841212\n",
      "  6161: 4 [  165/ 1499], train_loss/perplexity = 5.26996422/194.4090118 secs/batch = 0.5525s, grad.norm=0.53231978\n",
      "  6166: 4 [  170/ 1499], train_loss/perplexity = 5.67019129/290.0900269 secs/batch = 0.5797s, grad.norm=0.55410159\n",
      "  6171: 4 [  175/ 1499], train_loss/perplexity = 5.41260958/224.2159271 secs/batch = 0.5664s, grad.norm=0.52698100\n",
      "  6176: 4 [  180/ 1499], train_loss/perplexity = 5.62693882/277.8103943 secs/batch = 0.5462s, grad.norm=0.60515720\n",
      "  6181: 4 [  185/ 1499], train_loss/perplexity = 5.37081718/215.0385284 secs/batch = 0.5842s, grad.norm=0.53645766\n",
      "  6186: 4 [  190/ 1499], train_loss/perplexity = 5.60730553/272.4092407 secs/batch = 0.7070s, grad.norm=0.62013197\n",
      "  6191: 4 [  195/ 1499], train_loss/perplexity = 5.67971230/292.8651733 secs/batch = 0.5526s, grad.norm=0.52736294\n",
      "  6196: 4 [  200/ 1499], train_loss/perplexity = 5.60297203/271.2313232 secs/batch = 0.5730s, grad.norm=0.56043047\n",
      "  6201: 4 [  205/ 1499], train_loss/perplexity = 5.49735165/244.0447540 secs/batch = 0.5515s, grad.norm=0.55084687\n",
      "  6206: 4 [  210/ 1499], train_loss/perplexity = 5.29857540/200.0516205 secs/batch = 0.5757s, grad.norm=0.53006786\n",
      "  6211: 4 [  215/ 1499], train_loss/perplexity = 5.51559305/248.5373230 secs/batch = 0.5870s, grad.norm=0.50953859\n",
      "  6216: 4 [  220/ 1499], train_loss/perplexity = 5.23576164/187.8721466 secs/batch = 0.5433s, grad.norm=0.47243357\n",
      "  6221: 4 [  225/ 1499], train_loss/perplexity = 5.45953417/234.9879303 secs/batch = 0.5861s, grad.norm=0.51290971\n",
      "  6226: 4 [  230/ 1499], train_loss/perplexity = 5.50085878/244.9021606 secs/batch = 0.5696s, grad.norm=0.49905920\n",
      "  6231: 4 [  235/ 1499], train_loss/perplexity = 5.52330112/250.4604797 secs/batch = 0.5360s, grad.norm=0.56056398\n",
      "  6236: 4 [  240/ 1499], train_loss/perplexity = 5.62566090/277.4555969 secs/batch = 0.5626s, grad.norm=0.53276920\n",
      "  6241: 4 [  245/ 1499], train_loss/perplexity = 5.33325243/207.1104889 secs/batch = 0.5508s, grad.norm=0.62619060\n",
      "  6246: 4 [  250/ 1499], train_loss/perplexity = 5.41314125/224.3351746 secs/batch = 0.5445s, grad.norm=0.62312299\n",
      "  6251: 4 [  255/ 1499], train_loss/perplexity = 5.20090008/181.4354706 secs/batch = 0.5546s, grad.norm=0.50155914\n",
      "  6256: 4 [  260/ 1499], train_loss/perplexity = 5.56452417/261.0009766 secs/batch = 0.5541s, grad.norm=0.53498399\n",
      "  6261: 4 [  265/ 1499], train_loss/perplexity = 5.48486137/241.0155334 secs/batch = 0.6483s, grad.norm=0.58261675\n",
      "  6266: 4 [  270/ 1499], train_loss/perplexity = 5.54104996/254.9455414 secs/batch = 0.6066s, grad.norm=0.51980311\n",
      "  6271: 4 [  275/ 1499], train_loss/perplexity = 5.15796518/173.8104248 secs/batch = 0.6778s, grad.norm=0.66197026\n",
      "  6276: 4 [  280/ 1499], train_loss/perplexity = 5.37011671/214.8879395 secs/batch = 0.6112s, grad.norm=0.57742494\n",
      "  6281: 4 [  285/ 1499], train_loss/perplexity = 5.77216148/321.2313232 secs/batch = 0.6949s, grad.norm=0.58759218\n",
      "  6286: 4 [  290/ 1499], train_loss/perplexity = 5.69226885/296.5657349 secs/batch = 0.5972s, grad.norm=0.51805198\n",
      "  6291: 4 [  295/ 1499], train_loss/perplexity = 5.46050692/235.2166290 secs/batch = 0.6296s, grad.norm=0.53409052\n",
      "  6296: 4 [  300/ 1499], train_loss/perplexity = 5.31384706/203.1301880 secs/batch = 0.5955s, grad.norm=0.53826064\n",
      "  6301: 4 [  305/ 1499], train_loss/perplexity = 5.59798241/269.8813477 secs/batch = 0.5738s, grad.norm=0.55981249\n",
      "  6306: 4 [  310/ 1499], train_loss/perplexity = 5.66707611/289.1877441 secs/batch = 0.5896s, grad.norm=0.59094036\n",
      "  6311: 4 [  315/ 1499], train_loss/perplexity = 5.60798120/272.5933838 secs/batch = 0.5806s, grad.norm=0.51763105\n",
      "  6316: 4 [  320/ 1499], train_loss/perplexity = 5.53287220/252.8691559 secs/batch = 0.5955s, grad.norm=0.52826923\n",
      "  6321: 4 [  325/ 1499], train_loss/perplexity = 5.43954611/230.3376160 secs/batch = 0.7224s, grad.norm=0.53144187\n",
      "  6326: 4 [  330/ 1499], train_loss/perplexity = 5.40736961/223.0441284 secs/batch = 0.7628s, grad.norm=0.68400615\n",
      "  6331: 4 [  335/ 1499], train_loss/perplexity = 5.52210712/250.1616058 secs/batch = 0.5670s, grad.norm=0.58701742\n",
      "  6336: 4 [  340/ 1499], train_loss/perplexity = 5.19595718/180.5408783 secs/batch = 0.6214s, grad.norm=0.51836109\n",
      "  6341: 4 [  345/ 1499], train_loss/perplexity = 5.15831137/173.8706055 secs/batch = 0.6961s, grad.norm=0.49025732\n",
      "  6346: 4 [  350/ 1499], train_loss/perplexity = 5.42958260/228.0540314 secs/batch = 0.6022s, grad.norm=0.70517904\n",
      "  6351: 4 [  355/ 1499], train_loss/perplexity = 5.22148895/185.2097473 secs/batch = 0.6378s, grad.norm=0.59424001\n",
      "  6356: 4 [  360/ 1499], train_loss/perplexity = 5.33021021/206.4813690 secs/batch = 0.6468s, grad.norm=0.58241558\n",
      "  6361: 4 [  365/ 1499], train_loss/perplexity = 5.20390368/181.9812469 secs/batch = 0.6754s, grad.norm=0.55930078\n",
      "  6366: 4 [  370/ 1499], train_loss/perplexity = 5.22422218/185.7166595 secs/batch = 0.6392s, grad.norm=0.54903001\n",
      "  6371: 4 [  375/ 1499], train_loss/perplexity = 5.36000395/212.7257843 secs/batch = 0.5878s, grad.norm=0.52782559\n",
      "  6376: 4 [  380/ 1499], train_loss/perplexity = 5.56819201/261.9600525 secs/batch = 0.6167s, grad.norm=0.58041292\n",
      "  6381: 4 [  385/ 1499], train_loss/perplexity = 5.69779921/298.2103882 secs/batch = 0.6132s, grad.norm=0.55209059\n",
      "  6386: 4 [  390/ 1499], train_loss/perplexity = 5.76537037/319.0571899 secs/batch = 0.5780s, grad.norm=0.56521875\n",
      "  6391: 4 [  395/ 1499], train_loss/perplexity = 5.77058601/320.7256165 secs/batch = 0.6031s, grad.norm=0.51941812\n",
      "  6396: 4 [  400/ 1499], train_loss/perplexity = 5.67703009/292.0806885 secs/batch = 0.6052s, grad.norm=0.88121474\n",
      "  6401: 4 [  405/ 1499], train_loss/perplexity = 5.57836103/264.6375122 secs/batch = 0.5720s, grad.norm=0.54117483\n",
      "  6406: 4 [  410/ 1499], train_loss/perplexity = 5.09717369/163.5589905 secs/batch = 0.5883s, grad.norm=0.68882704\n",
      "  6411: 4 [  415/ 1499], train_loss/perplexity = 5.66882229/289.6931458 secs/batch = 0.5703s, grad.norm=0.55448169\n",
      "  6416: 4 [  420/ 1499], train_loss/perplexity = 5.31939793/204.2608643 secs/batch = 0.6013s, grad.norm=0.53361899\n",
      "  6421: 4 [  425/ 1499], train_loss/perplexity = 5.65279675/285.0876770 secs/batch = 0.6906s, grad.norm=0.49854764\n",
      "  6426: 4 [  430/ 1499], train_loss/perplexity = 5.37514448/215.9710693 secs/batch = 0.7277s, grad.norm=0.50705975\n",
      "  6431: 4 [  435/ 1499], train_loss/perplexity = 5.34401989/209.3526001 secs/batch = 0.6910s, grad.norm=0.52147698\n",
      "  6436: 4 [  440/ 1499], train_loss/perplexity = 5.41699886/225.2022400 secs/batch = 0.5861s, grad.norm=0.63281131\n",
      "  6441: 4 [  445/ 1499], train_loss/perplexity = 5.51394939/248.1291504 secs/batch = 0.6063s, grad.norm=0.54007775\n",
      "  6446: 4 [  450/ 1499], train_loss/perplexity = 5.46992588/237.4425964 secs/batch = 0.6988s, grad.norm=0.55506963\n",
      "  6451: 4 [  455/ 1499], train_loss/perplexity = 5.60793972/272.5820618 secs/batch = 0.6791s, grad.norm=0.48966208\n",
      "  6456: 4 [  460/ 1499], train_loss/perplexity = 5.51515770/248.4291534 secs/batch = 0.7384s, grad.norm=0.52983922\n",
      "  6461: 4 [  465/ 1499], train_loss/perplexity = 5.52898121/251.8871613 secs/batch = 0.9045s, grad.norm=0.59317088\n",
      "  6466: 4 [  470/ 1499], train_loss/perplexity = 5.63020754/278.7199707 secs/batch = 0.5688s, grad.norm=0.64216536\n",
      "  6471: 4 [  475/ 1499], train_loss/perplexity = 5.37572718/216.0969543 secs/batch = 0.5904s, grad.norm=0.55657703\n",
      "  6476: 4 [  480/ 1499], train_loss/perplexity = 5.50746346/246.5250092 secs/batch = 0.5856s, grad.norm=0.51796371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6481: 4 [  485/ 1499], train_loss/perplexity = 5.49381876/243.1840973 secs/batch = 0.5818s, grad.norm=0.52666485\n",
      "  6486: 4 [  490/ 1499], train_loss/perplexity = 5.39480829/220.2599182 secs/batch = 0.5658s, grad.norm=0.54433674\n",
      "  6491: 4 [  495/ 1499], train_loss/perplexity = 5.41773701/225.3685303 secs/batch = 0.6280s, grad.norm=0.68186414\n",
      "  6496: 4 [  500/ 1499], train_loss/perplexity = 5.53594065/253.6462708 secs/batch = 0.5908s, grad.norm=0.57979721\n",
      "  6501: 4 [  505/ 1499], train_loss/perplexity = 5.29227018/198.7942200 secs/batch = 0.5667s, grad.norm=0.57505751\n",
      "  6506: 4 [  510/ 1499], train_loss/perplexity = 5.70099688/299.1654968 secs/batch = 0.6268s, grad.norm=0.60979825\n",
      "  6511: 4 [  515/ 1499], train_loss/perplexity = 5.34397078/209.3423157 secs/batch = 0.5877s, grad.norm=0.62862748\n",
      "  6516: 4 [  520/ 1499], train_loss/perplexity = 5.57517147/263.7947998 secs/batch = 0.5889s, grad.norm=0.62149096\n",
      "  6521: 4 [  525/ 1499], train_loss/perplexity = 5.72657871/306.9174194 secs/batch = 0.6834s, grad.norm=0.59621882\n",
      "  6526: 4 [  530/ 1499], train_loss/perplexity = 5.46811628/237.0133057 secs/batch = 0.7286s, grad.norm=0.57367182\n",
      "  6531: 4 [  535/ 1499], train_loss/perplexity = 5.45230722/233.2958069 secs/batch = 0.5572s, grad.norm=0.54481757\n",
      "  6536: 4 [  540/ 1499], train_loss/perplexity = 5.34951544/210.5062714 secs/batch = 0.6372s, grad.norm=0.55013996\n",
      "  6541: 4 [  545/ 1499], train_loss/perplexity = 5.05710506/157.1349640 secs/batch = 0.5512s, grad.norm=0.56380779\n",
      "  6546: 4 [  550/ 1499], train_loss/perplexity = 5.55439329/258.3701477 secs/batch = 0.5568s, grad.norm=0.60642338\n",
      "  6551: 4 [  555/ 1499], train_loss/perplexity = 5.34391546/209.3307343 secs/batch = 0.6105s, grad.norm=0.52907658\n",
      "  6556: 4 [  560/ 1499], train_loss/perplexity = 5.70987606/301.8336487 secs/batch = 0.5706s, grad.norm=0.57003069\n",
      "  6561: 4 [  565/ 1499], train_loss/perplexity = 5.62436342/277.0958252 secs/batch = 0.5616s, grad.norm=0.60406089\n",
      "  6566: 4 [  570/ 1499], train_loss/perplexity = 5.58674192/266.8647461 secs/batch = 0.5452s, grad.norm=0.53332257\n",
      "  6571: 4 [  575/ 1499], train_loss/perplexity = 5.60855103/272.7487488 secs/batch = 0.6048s, grad.norm=0.59342968\n",
      "  6576: 4 [  580/ 1499], train_loss/perplexity = 5.38343477/217.7689819 secs/batch = 0.5891s, grad.norm=0.69119251\n",
      "  6581: 4 [  585/ 1499], train_loss/perplexity = 5.26389456/193.2325897 secs/batch = 0.5681s, grad.norm=0.56947392\n",
      "  6586: 4 [  590/ 1499], train_loss/perplexity = 5.44332218/231.2090302 secs/batch = 0.5478s, grad.norm=0.58300990\n",
      "  6591: 4 [  595/ 1499], train_loss/perplexity = 5.46325493/235.8638916 secs/batch = 0.5583s, grad.norm=0.57061195\n",
      "  6596: 4 [  600/ 1499], train_loss/perplexity = 5.25783634/192.0654755 secs/batch = 0.5489s, grad.norm=0.56367534\n",
      "  6601: 4 [  605/ 1499], train_loss/perplexity = 5.27539301/195.4672852 secs/batch = 0.5614s, grad.norm=0.58418322\n",
      "  6606: 4 [  610/ 1499], train_loss/perplexity = 5.55015469/257.2773438 secs/batch = 0.5518s, grad.norm=0.56725019\n",
      "  6611: 4 [  615/ 1499], train_loss/perplexity = 5.54974270/257.1713867 secs/batch = 0.5446s, grad.norm=0.53951216\n",
      "  6616: 4 [  620/ 1499], train_loss/perplexity = 5.37508774/215.9588165 secs/batch = 0.5437s, grad.norm=0.53044063\n",
      "  6621: 4 [  625/ 1499], train_loss/perplexity = 5.36187553/213.1242981 secs/batch = 0.5436s, grad.norm=0.54661536\n",
      "  6626: 4 [  630/ 1499], train_loss/perplexity = 5.47394943/238.3998871 secs/batch = 0.5517s, grad.norm=0.57707232\n",
      "  6631: 4 [  635/ 1499], train_loss/perplexity = 5.21953297/184.8478394 secs/batch = 0.5594s, grad.norm=0.64820826\n",
      "  6636: 4 [  640/ 1499], train_loss/perplexity = 5.53429317/253.2287292 secs/batch = 0.5633s, grad.norm=0.66300678\n",
      "  6641: 4 [  645/ 1499], train_loss/perplexity = 5.21341276/183.7199860 secs/batch = 0.5552s, grad.norm=0.57036096\n",
      "  6646: 4 [  650/ 1499], train_loss/perplexity = 5.18630457/178.8065643 secs/batch = 0.5760s, grad.norm=0.64228427\n",
      "  6651: 4 [  655/ 1499], train_loss/perplexity = 5.57799101/264.5396118 secs/batch = 0.5721s, grad.norm=0.52173555\n",
      "  6656: 4 [  660/ 1499], train_loss/perplexity = 5.68998337/295.8887024 secs/batch = 0.5917s, grad.norm=0.57748342\n",
      "  6661: 4 [  665/ 1499], train_loss/perplexity = 5.58362865/266.0352173 secs/batch = 0.5838s, grad.norm=0.61015761\n",
      "  6666: 4 [  670/ 1499], train_loss/perplexity = 5.67854977/292.5249023 secs/batch = 0.6470s, grad.norm=0.53028899\n",
      "  6671: 4 [  675/ 1499], train_loss/perplexity = 5.55623722/258.8470154 secs/batch = 0.6327s, grad.norm=0.55489337\n",
      "  6676: 4 [  680/ 1499], train_loss/perplexity = 5.49950361/244.5704956 secs/batch = 0.5902s, grad.norm=0.52389437\n",
      "  6681: 4 [  685/ 1499], train_loss/perplexity = 5.61116076/273.4614868 secs/batch = 0.5926s, grad.norm=0.56775612\n",
      "  6686: 4 [  690/ 1499], train_loss/perplexity = 5.52832747/251.7225494 secs/batch = 0.5524s, grad.norm=0.61450166\n",
      "  6691: 4 [  695/ 1499], train_loss/perplexity = 5.44303370/231.1423340 secs/batch = 0.5488s, grad.norm=0.51344591\n",
      "  6696: 4 [  700/ 1499], train_loss/perplexity = 5.65718794/286.3422852 secs/batch = 0.5706s, grad.norm=0.59204507\n",
      "  6701: 4 [  705/ 1499], train_loss/perplexity = 5.18452549/178.4887390 secs/batch = 0.5564s, grad.norm=0.60357773\n",
      "  6706: 4 [  710/ 1499], train_loss/perplexity = 5.47675037/239.0685577 secs/batch = 0.5491s, grad.norm=0.54978466\n",
      "  6711: 4 [  715/ 1499], train_loss/perplexity = 5.46929979/237.2939758 secs/batch = 0.6037s, grad.norm=0.59478134\n",
      "  6716: 4 [  720/ 1499], train_loss/perplexity = 5.51120043/247.4479980 secs/batch = 0.5587s, grad.norm=0.53041136\n",
      "  6721: 4 [  725/ 1499], train_loss/perplexity = 5.33189011/206.8285370 secs/batch = 0.5448s, grad.norm=0.61670685\n",
      "  6726: 4 [  730/ 1499], train_loss/perplexity = 5.49172449/242.6753387 secs/batch = 0.6525s, grad.norm=0.54752928\n",
      "  6731: 4 [  735/ 1499], train_loss/perplexity = 5.35361147/211.3702850 secs/batch = 0.5726s, grad.norm=0.69227642\n",
      "  6736: 4 [  740/ 1499], train_loss/perplexity = 5.31341410/203.0422516 secs/batch = 0.5574s, grad.norm=0.56334239\n",
      "  6741: 4 [  745/ 1499], train_loss/perplexity = 5.17644739/177.0526886 secs/batch = 0.5517s, grad.norm=0.57155591\n",
      "  6746: 4 [  750/ 1499], train_loss/perplexity = 5.49282455/242.9424438 secs/batch = 0.5956s, grad.norm=0.60698932\n",
      "  6751: 4 [  755/ 1499], train_loss/perplexity = 5.26966524/194.3508911 secs/batch = 0.5460s, grad.norm=0.64115345\n",
      "  6756: 4 [  760/ 1499], train_loss/perplexity = 4.87637281/131.1540833 secs/batch = 0.5562s, grad.norm=0.60030907\n",
      "  6761: 4 [  765/ 1499], train_loss/perplexity = 5.30446672/201.2336578 secs/batch = 0.5628s, grad.norm=0.62224907\n",
      "  6766: 4 [  770/ 1499], train_loss/perplexity = 5.40230894/221.9182281 secs/batch = 0.5701s, grad.norm=0.52035904\n",
      "  6771: 4 [  775/ 1499], train_loss/perplexity = 5.72729588/307.1376038 secs/batch = 0.6013s, grad.norm=0.56395727\n",
      "  6776: 4 [  780/ 1499], train_loss/perplexity = 5.25280714/191.1019592 secs/batch = 0.5606s, grad.norm=0.49939483\n",
      "  6781: 4 [  785/ 1499], train_loss/perplexity = 5.44231701/230.9767456 secs/batch = 0.5767s, grad.norm=0.56774533\n",
      "  6786: 4 [  790/ 1499], train_loss/perplexity = 5.01536942/150.7118073 secs/batch = 0.6766s, grad.norm=0.61206174\n",
      "  6791: 4 [  795/ 1499], train_loss/perplexity = 5.48106050/240.1011963 secs/batch = 0.6528s, grad.norm=0.57226431\n",
      "  6796: 4 [  800/ 1499], train_loss/perplexity = 5.34693336/209.9634247 secs/batch = 0.6780s, grad.norm=0.56331980\n",
      "  6801: 4 [  805/ 1499], train_loss/perplexity = 5.36645460/214.1024475 secs/batch = 0.6885s, grad.norm=0.52022558\n",
      "  6806: 4 [  810/ 1499], train_loss/perplexity = 5.31010294/202.3710632 secs/batch = 0.5585s, grad.norm=0.51365417\n",
      "  6811: 4 [  815/ 1499], train_loss/perplexity = 5.50267696/245.3478394 secs/batch = 0.5514s, grad.norm=0.55808783\n",
      "  6816: 4 [  820/ 1499], train_loss/perplexity = 5.10845041/165.4138336 secs/batch = 0.5644s, grad.norm=0.67644554\n",
      "  6821: 4 [  825/ 1499], train_loss/perplexity = 5.21019554/183.1298676 secs/batch = 0.6482s, grad.norm=0.63321185\n",
      "  6826: 4 [  830/ 1499], train_loss/perplexity = 5.40265131/221.9942169 secs/batch = 0.5505s, grad.norm=0.60882205\n",
      "  6831: 4 [  835/ 1499], train_loss/perplexity = 5.41620064/225.0225525 secs/batch = 0.5885s, grad.norm=0.57719672\n",
      "  6836: 4 [  840/ 1499], train_loss/perplexity = 5.36291027/213.3449402 secs/batch = 0.6730s, grad.norm=0.55273521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6841: 4 [  845/ 1499], train_loss/perplexity = 5.17289448/176.4247589 secs/batch = 0.6541s, grad.norm=0.52743155\n",
      "  6846: 4 [  850/ 1499], train_loss/perplexity = 5.33295441/207.0487823 secs/batch = 0.6616s, grad.norm=0.56747961\n",
      "  6851: 4 [  855/ 1499], train_loss/perplexity = 5.46770906/236.9168091 secs/batch = 0.6154s, grad.norm=0.51794189\n",
      "  6856: 4 [  860/ 1499], train_loss/perplexity = 4.97529173/144.7910614 secs/batch = 0.5888s, grad.norm=0.70668685\n",
      "  6861: 4 [  865/ 1499], train_loss/perplexity = 5.31681633/203.7342224 secs/batch = 0.6006s, grad.norm=0.57561839\n",
      "  6866: 4 [  870/ 1499], train_loss/perplexity = 5.35294199/211.2288208 secs/batch = 0.5485s, grad.norm=0.65257198\n",
      "  6871: 4 [  875/ 1499], train_loss/perplexity = 5.32999516/206.4369812 secs/batch = 0.5516s, grad.norm=0.56289172\n",
      "  6876: 4 [  880/ 1499], train_loss/perplexity = 5.27586365/195.5592957 secs/batch = 0.5449s, grad.norm=0.53724498\n",
      "  6881: 4 [  885/ 1499], train_loss/perplexity = 5.17949200/177.5925751 secs/batch = 0.5626s, grad.norm=0.65426087\n",
      "  6886: 4 [  890/ 1499], train_loss/perplexity = 5.48665762/241.4488373 secs/batch = 0.5524s, grad.norm=0.54842603\n",
      "  6891: 4 [  895/ 1499], train_loss/perplexity = 5.51815939/249.1759796 secs/batch = 0.5800s, grad.norm=0.52648515\n",
      "  6896: 4 [  900/ 1499], train_loss/perplexity = 5.19690466/180.7120056 secs/batch = 0.6688s, grad.norm=0.51428378\n",
      "  6901: 4 [  905/ 1499], train_loss/perplexity = 5.30527687/201.3967590 secs/batch = 0.5849s, grad.norm=0.52072090\n",
      "  6906: 4 [  910/ 1499], train_loss/perplexity = 5.55596399/258.7763062 secs/batch = 0.5533s, grad.norm=0.61752099\n",
      "  6911: 4 [  915/ 1499], train_loss/perplexity = 5.31019306/202.3892975 secs/batch = 0.6769s, grad.norm=0.63797897\n",
      "  6916: 4 [  920/ 1499], train_loss/perplexity = 4.99786139/148.0960999 secs/batch = 0.6595s, grad.norm=0.49843177\n",
      "  6921: 4 [  925/ 1499], train_loss/perplexity = 5.42321253/226.6059265 secs/batch = 0.5737s, grad.norm=0.67176592\n",
      "  6926: 4 [  930/ 1499], train_loss/perplexity = 5.38086653/217.2104187 secs/batch = 0.5769s, grad.norm=0.62873429\n",
      "  6931: 4 [  935/ 1499], train_loss/perplexity = 5.44472837/231.5343781 secs/batch = 0.5516s, grad.norm=0.52100581\n",
      "  6936: 4 [  940/ 1499], train_loss/perplexity = 5.09428883/163.0878143 secs/batch = 0.5616s, grad.norm=0.65188807\n",
      "  6941: 4 [  945/ 1499], train_loss/perplexity = 5.48931837/242.0921326 secs/batch = 0.6560s, grad.norm=0.56648695\n",
      "  6946: 4 [  950/ 1499], train_loss/perplexity = 5.38739061/218.6321411 secs/batch = 0.6421s, grad.norm=0.54921365\n",
      "  6951: 4 [  955/ 1499], train_loss/perplexity = 5.52045107/249.7476654 secs/batch = 0.5515s, grad.norm=0.51708847\n",
      "  6956: 4 [  960/ 1499], train_loss/perplexity = 5.27315855/195.0310059 secs/batch = 0.6662s, grad.norm=0.57637030\n",
      "  6961: 4 [  965/ 1499], train_loss/perplexity = 5.33767414/208.0283051 secs/batch = 0.5869s, grad.norm=0.67951071\n",
      "  6966: 4 [  970/ 1499], train_loss/perplexity = 5.54515791/255.9949951 secs/batch = 0.5478s, grad.norm=0.54075909\n",
      "  6971: 4 [  975/ 1499], train_loss/perplexity = 5.35509920/211.6849823 secs/batch = 0.5521s, grad.norm=0.57197571\n",
      "  6976: 4 [  980/ 1499], train_loss/perplexity = 5.48709393/241.5542145 secs/batch = 0.6171s, grad.norm=0.61742598\n",
      "  6981: 4 [  985/ 1499], train_loss/perplexity = 5.15741205/173.7143097 secs/batch = 0.5703s, grad.norm=0.61404860\n",
      "  6986: 4 [  990/ 1499], train_loss/perplexity = 5.28135490/196.6361237 secs/batch = 0.5532s, grad.norm=0.52443260\n",
      "  6991: 4 [  995/ 1499], train_loss/perplexity = 5.61586285/274.7503357 secs/batch = 0.5421s, grad.norm=0.55643541\n",
      "  6996: 4 [ 1000/ 1499], train_loss/perplexity = 5.26127148/192.7263794 secs/batch = 0.5591s, grad.norm=0.57691723\n",
      "  7001: 4 [ 1005/ 1499], train_loss/perplexity = 5.67738962/292.1857300 secs/batch = 0.5627s, grad.norm=0.51654339\n",
      "  7006: 4 [ 1010/ 1499], train_loss/perplexity = 5.50642109/246.2681732 secs/batch = 0.5508s, grad.norm=0.56866789\n",
      "  7011: 4 [ 1015/ 1499], train_loss/perplexity = 5.32206964/204.8073273 secs/batch = 0.5457s, grad.norm=0.57315743\n",
      "  7016: 4 [ 1020/ 1499], train_loss/perplexity = 5.40269947/222.0048981 secs/batch = 0.5422s, grad.norm=0.62950861\n",
      "  7021: 4 [ 1025/ 1499], train_loss/perplexity = 5.39945841/221.2865295 secs/batch = 0.5524s, grad.norm=0.53797692\n",
      "  7026: 4 [ 1030/ 1499], train_loss/perplexity = 5.54852247/256.8577576 secs/batch = 0.5412s, grad.norm=0.57816386\n",
      "  7031: 4 [ 1035/ 1499], train_loss/perplexity = 5.80955267/333.4699097 secs/batch = 0.5490s, grad.norm=0.71031427\n",
      "  7036: 4 [ 1040/ 1499], train_loss/perplexity = 5.16874504/175.6942139 secs/batch = 0.5505s, grad.norm=0.49608567\n",
      "  7041: 4 [ 1045/ 1499], train_loss/perplexity = 5.30608606/201.5597839 secs/batch = 0.5437s, grad.norm=0.56861228\n",
      "  7046: 4 [ 1050/ 1499], train_loss/perplexity = 5.02269888/151.8204956 secs/batch = 0.5504s, grad.norm=0.52710652\n",
      "  7051: 4 [ 1055/ 1499], train_loss/perplexity = 5.35409069/211.4716034 secs/batch = 0.5521s, grad.norm=0.53759807\n",
      "  7056: 4 [ 1060/ 1499], train_loss/perplexity = 5.60572863/271.9800110 secs/batch = 0.5554s, grad.norm=0.57194442\n",
      "  7061: 4 [ 1065/ 1499], train_loss/perplexity = 5.15484428/173.2688293 secs/batch = 0.5886s, grad.norm=0.58570278\n",
      "  7066: 4 [ 1070/ 1499], train_loss/perplexity = 5.59960651/270.3200073 secs/batch = 0.5944s, grad.norm=0.58887208\n",
      "  7071: 4 [ 1075/ 1499], train_loss/perplexity = 5.46057463/235.2325592 secs/batch = 0.5596s, grad.norm=0.60880893\n",
      "  7076: 4 [ 1080/ 1499], train_loss/perplexity = 5.67849922/292.5101013 secs/batch = 0.5574s, grad.norm=0.58817953\n",
      "  7081: 4 [ 1085/ 1499], train_loss/perplexity = 5.76179075/317.9171448 secs/batch = 0.5621s, grad.norm=0.61802953\n",
      "  7086: 4 [ 1090/ 1499], train_loss/perplexity = 5.31869984/204.1183167 secs/batch = 0.5758s, grad.norm=0.59508663\n",
      "  7091: 4 [ 1095/ 1499], train_loss/perplexity = 5.50098658/244.9334564 secs/batch = 0.5445s, grad.norm=0.53908986\n",
      "  7096: 4 [ 1100/ 1499], train_loss/perplexity = 5.49198580/242.7387543 secs/batch = 0.5470s, grad.norm=0.60718727\n",
      "  7101: 4 [ 1105/ 1499], train_loss/perplexity = 5.26087999/192.6509552 secs/batch = 0.5608s, grad.norm=0.56813401\n",
      "  7106: 4 [ 1110/ 1499], train_loss/perplexity = 5.43487978/229.2652893 secs/batch = 0.5569s, grad.norm=0.56239450\n",
      "  7111: 4 [ 1115/ 1499], train_loss/perplexity = 5.07648706/160.2102509 secs/batch = 0.5479s, grad.norm=0.57903653\n",
      "  7116: 4 [ 1120/ 1499], train_loss/perplexity = 5.28218699/196.7998047 secs/batch = 0.5462s, grad.norm=0.57131487\n",
      "  7121: 4 [ 1125/ 1499], train_loss/perplexity = 5.28731823/197.8122253 secs/batch = 0.5405s, grad.norm=0.68785149\n",
      "  7126: 4 [ 1130/ 1499], train_loss/perplexity = 5.07885170/160.5895386 secs/batch = 0.7017s, grad.norm=0.58858740\n",
      "  7131: 4 [ 1135/ 1499], train_loss/perplexity = 5.68025827/293.0251160 secs/batch = 0.5765s, grad.norm=0.51113701\n",
      "  7136: 4 [ 1140/ 1499], train_loss/perplexity = 5.49296188/242.9758148 secs/batch = 0.5444s, grad.norm=0.58119369\n",
      "  7141: 4 [ 1145/ 1499], train_loss/perplexity = 5.61511946/274.5461731 secs/batch = 0.6627s, grad.norm=0.58738375\n",
      "  7146: 4 [ 1150/ 1499], train_loss/perplexity = 5.53357649/253.0473175 secs/batch = 0.6750s, grad.norm=0.58889109\n",
      "  7151: 4 [ 1155/ 1499], train_loss/perplexity = 5.23339987/187.4289551 secs/batch = 0.5454s, grad.norm=0.59337616\n",
      "  7156: 4 [ 1160/ 1499], train_loss/perplexity = 5.36548519/213.8949890 secs/batch = 0.5427s, grad.norm=0.51370424\n",
      "  7161: 4 [ 1165/ 1499], train_loss/perplexity = 5.37959385/216.9341431 secs/batch = 0.5475s, grad.norm=0.58853459\n",
      "  7166: 4 [ 1170/ 1499], train_loss/perplexity = 5.51393223/248.1248932 secs/batch = 0.6650s, grad.norm=0.58927846\n",
      "  7171: 4 [ 1175/ 1499], train_loss/perplexity = 4.88095522/131.7564545 secs/batch = 0.6596s, grad.norm=0.55824018\n",
      "  7176: 4 [ 1180/ 1499], train_loss/perplexity = 5.23066425/186.9169159 secs/batch = 0.6596s, grad.norm=0.67258745\n",
      "  7181: 4 [ 1185/ 1499], train_loss/perplexity = 5.15033388/172.4890747 secs/batch = 0.5671s, grad.norm=0.69082206\n",
      "  7186: 4 [ 1190/ 1499], train_loss/perplexity = 5.40001297/221.4092865 secs/batch = 0.5937s, grad.norm=0.67695487\n",
      "  7191: 4 [ 1195/ 1499], train_loss/perplexity = 5.38282967/217.6372528 secs/batch = 0.6839s, grad.norm=0.56516320\n",
      "  7196: 4 [ 1200/ 1499], train_loss/perplexity = 5.28709221/197.7675171 secs/batch = 0.6324s, grad.norm=0.59879446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7201: 4 [ 1205/ 1499], train_loss/perplexity = 5.20504045/182.1882477 secs/batch = 0.6485s, grad.norm=0.58447534\n",
      "  7206: 4 [ 1210/ 1499], train_loss/perplexity = 5.19378090/180.1483917 secs/batch = 0.6046s, grad.norm=0.60093921\n",
      "  7211: 4 [ 1215/ 1499], train_loss/perplexity = 4.89040661/133.0076447 secs/batch = 0.7271s, grad.norm=0.57102960\n",
      "  7216: 4 [ 1220/ 1499], train_loss/perplexity = 5.25797176/192.0914917 secs/batch = 0.7437s, grad.norm=0.58375788\n",
      "  7221: 4 [ 1225/ 1499], train_loss/perplexity = 4.72044182/112.2178192 secs/batch = 0.7546s, grad.norm=0.62778449\n",
      "  7226: 4 [ 1230/ 1499], train_loss/perplexity = 5.25314665/191.1668549 secs/batch = 0.6183s, grad.norm=0.55674744\n",
      "  7231: 4 [ 1235/ 1499], train_loss/perplexity = 5.15487623/173.2743530 secs/batch = 0.7260s, grad.norm=0.78874940\n",
      "  7236: 4 [ 1240/ 1499], train_loss/perplexity = 5.16701126/175.3898621 secs/batch = 0.7036s, grad.norm=0.55907875\n",
      "  7241: 4 [ 1245/ 1499], train_loss/perplexity = 5.45855427/234.7577820 secs/batch = 0.7079s, grad.norm=0.54564476\n",
      "  7246: 4 [ 1250/ 1499], train_loss/perplexity = 5.64796972/283.7148743 secs/batch = 0.7253s, grad.norm=0.58469903\n",
      "  7251: 4 [ 1255/ 1499], train_loss/perplexity = 5.24089479/188.8390045 secs/batch = 0.7529s, grad.norm=0.59122282\n",
      "  7256: 4 [ 1260/ 1499], train_loss/perplexity = 5.38234472/217.5317230 secs/batch = 0.6917s, grad.norm=0.55436647\n",
      "  7261: 4 [ 1265/ 1499], train_loss/perplexity = 5.42055130/226.0036774 secs/batch = 0.5884s, grad.norm=0.54674321\n",
      "  7266: 4 [ 1270/ 1499], train_loss/perplexity = 5.36786699/214.4050446 secs/batch = 0.6070s, grad.norm=0.54556018\n",
      "  7271: 4 [ 1275/ 1499], train_loss/perplexity = 5.40347624/222.1774139 secs/batch = 0.6128s, grad.norm=0.55346459\n",
      "  7276: 4 [ 1280/ 1499], train_loss/perplexity = 5.03792572/154.1499329 secs/batch = 0.5684s, grad.norm=0.55684578\n",
      "  7281: 4 [ 1285/ 1499], train_loss/perplexity = 5.44721889/232.1117401 secs/batch = 0.6523s, grad.norm=0.53728664\n",
      "  7286: 4 [ 1290/ 1499], train_loss/perplexity = 5.37304831/215.5188293 secs/batch = 0.6757s, grad.norm=0.57877624\n",
      "  7291: 4 [ 1295/ 1499], train_loss/perplexity = 5.31409502/203.1805573 secs/batch = 0.6206s, grad.norm=0.55232012\n",
      "  7296: 4 [ 1300/ 1499], train_loss/perplexity = 5.47937202/239.6961365 secs/batch = 0.7341s, grad.norm=0.54572862\n",
      "  7301: 4 [ 1305/ 1499], train_loss/perplexity = 5.33777189/208.0486450 secs/batch = 0.7664s, grad.norm=0.56674433\n",
      "  7306: 4 [ 1310/ 1499], train_loss/perplexity = 5.46312141/235.8324127 secs/batch = 0.6219s, grad.norm=0.54185104\n",
      "  7311: 4 [ 1315/ 1499], train_loss/perplexity = 5.34256315/209.0478516 secs/batch = 0.6006s, grad.norm=0.73868036\n",
      "  7316: 4 [ 1320/ 1499], train_loss/perplexity = 5.04874516/155.8267975 secs/batch = 0.6388s, grad.norm=0.57263249\n",
      "  7321: 4 [ 1325/ 1499], train_loss/perplexity = 5.35187721/211.0040283 secs/batch = 0.7225s, grad.norm=0.54323047\n",
      "  7326: 4 [ 1330/ 1499], train_loss/perplexity = 5.50347328/245.5432892 secs/batch = 0.5782s, grad.norm=0.58448058\n",
      "  7331: 4 [ 1335/ 1499], train_loss/perplexity = 5.32052279/204.4907532 secs/batch = 1.1712s, grad.norm=0.54371971\n",
      "  7336: 4 [ 1340/ 1499], train_loss/perplexity = 4.76886320/117.7852707 secs/batch = 0.5763s, grad.norm=0.61326993\n",
      "  7341: 4 [ 1345/ 1499], train_loss/perplexity = 4.92305851/137.4222717 secs/batch = 0.5715s, grad.norm=0.56917089\n",
      "  7346: 4 [ 1350/ 1499], train_loss/perplexity = 5.11579609/166.6333771 secs/batch = 0.5621s, grad.norm=0.60375810\n",
      "  7351: 4 [ 1355/ 1499], train_loss/perplexity = 4.83359051/125.6613388 secs/batch = 0.5887s, grad.norm=0.61611676\n",
      "  7356: 4 [ 1360/ 1499], train_loss/perplexity = 4.95994329/142.5857086 secs/batch = 0.5629s, grad.norm=0.52700227\n",
      "  7361: 4 [ 1365/ 1499], train_loss/perplexity = 4.84757423/127.4309006 secs/batch = 0.6366s, grad.norm=0.69993979\n",
      "  7366: 4 [ 1370/ 1499], train_loss/perplexity = 4.84499121/127.1021652 secs/batch = 0.5543s, grad.norm=0.56598759\n",
      "  7371: 4 [ 1375/ 1499], train_loss/perplexity = 4.89279413/133.3255768 secs/batch = 0.5602s, grad.norm=0.61874741\n",
      "  7376: 4 [ 1380/ 1499], train_loss/perplexity = 5.34984922/210.5765381 secs/batch = 0.5855s, grad.norm=0.58531308\n",
      "  7381: 4 [ 1385/ 1499], train_loss/perplexity = 5.09079981/162.5197906 secs/batch = 0.5636s, grad.norm=0.56918687\n",
      "  7386: 4 [ 1390/ 1499], train_loss/perplexity = 5.17385340/176.5940094 secs/batch = 0.5565s, grad.norm=0.55824941\n",
      "  7391: 4 [ 1395/ 1499], train_loss/perplexity = 5.36226130/213.2065277 secs/batch = 0.5407s, grad.norm=0.55560863\n",
      "  7396: 4 [ 1400/ 1499], train_loss/perplexity = 5.37359476/215.6366425 secs/batch = 0.5652s, grad.norm=0.60863835\n",
      "  7401: 4 [ 1405/ 1499], train_loss/perplexity = 5.18491602/178.5584564 secs/batch = 0.5507s, grad.norm=0.63522178\n",
      "  7406: 4 [ 1410/ 1499], train_loss/perplexity = 5.43628025/229.5865936 secs/batch = 0.5976s, grad.norm=0.53132272\n",
      "  7411: 4 [ 1415/ 1499], train_loss/perplexity = 5.33337307/207.1354828 secs/batch = 0.5680s, grad.norm=0.58073264\n",
      "  7416: 4 [ 1420/ 1499], train_loss/perplexity = 5.32520962/205.4514160 secs/batch = 0.5516s, grad.norm=0.54103416\n",
      "  7421: 4 [ 1425/ 1499], train_loss/perplexity = 5.22204018/185.3118744 secs/batch = 0.5486s, grad.norm=0.61643624\n",
      "  7426: 4 [ 1430/ 1499], train_loss/perplexity = 5.41602707/224.9835052 secs/batch = 0.5586s, grad.norm=0.63074487\n",
      "  7431: 4 [ 1435/ 1499], train_loss/perplexity = 5.10666037/165.1179962 secs/batch = 0.5556s, grad.norm=0.57371932\n",
      "  7436: 4 [ 1440/ 1499], train_loss/perplexity = 4.86800432/130.0610962 secs/batch = 0.5561s, grad.norm=0.63503015\n",
      "  7441: 4 [ 1445/ 1499], train_loss/perplexity = 5.25540590/191.5992432 secs/batch = 0.5554s, grad.norm=0.55488646\n",
      "  7446: 4 [ 1450/ 1499], train_loss/perplexity = 5.30059433/200.4559174 secs/batch = 0.5532s, grad.norm=0.57399976\n",
      "  7451: 4 [ 1455/ 1499], train_loss/perplexity = 5.62550640/277.4127197 secs/batch = 0.5530s, grad.norm=0.57213473\n",
      "  7456: 4 [ 1460/ 1499], train_loss/perplexity = 5.58225393/265.6697388 secs/batch = 0.5438s, grad.norm=0.55201828\n",
      "  7461: 4 [ 1465/ 1499], train_loss/perplexity = 5.72485399/306.3885193 secs/batch = 0.5566s, grad.norm=0.53823596\n",
      "  7466: 4 [ 1470/ 1499], train_loss/perplexity = 5.36954403/214.7649231 secs/batch = 0.5896s, grad.norm=0.59988558\n",
      "  7471: 4 [ 1475/ 1499], train_loss/perplexity = 5.45693588/234.3781586 secs/batch = 0.6473s, grad.norm=0.54950637\n",
      "  7476: 4 [ 1480/ 1499], train_loss/perplexity = 5.43338585/228.9230347 secs/batch = 0.5695s, grad.norm=0.54177779\n",
      "  7481: 4 [ 1485/ 1499], train_loss/perplexity = 5.20264244/181.7518768 secs/batch = 0.5566s, grad.norm=0.62806457\n",
      "  7486: 4 [ 1490/ 1499], train_loss/perplexity = 5.31299543/202.9572601 secs/batch = 0.5844s, grad.norm=0.56419200\n",
      "  7491: 4 [ 1495/ 1499], train_loss/perplexity = 5.56617641/261.4325867 secs/batch = 0.5448s, grad.norm=0.52806580\n",
      "Epoch training time: 892.5484125614166\n",
      "Saved char model cv/epoch004_5.4050.model\n",
      "  7500: 5 [    5/ 1499], train_loss/perplexity = 5.45106554/233.0063171 secs/batch = 0.5510s, grad.norm=0.57821786\n",
      "  7505: 5 [   10/ 1499], train_loss/perplexity = 5.45913935/234.8951721 secs/batch = 0.5483s, grad.norm=0.58759743\n",
      "  7510: 5 [   15/ 1499], train_loss/perplexity = 5.32955647/206.3464355 secs/batch = 0.5467s, grad.norm=0.62803578\n",
      "  7515: 5 [   20/ 1499], train_loss/perplexity = 5.17330360/176.4969482 secs/batch = 0.5463s, grad.norm=0.52521604\n",
      "  7520: 5 [   25/ 1499], train_loss/perplexity = 5.59529734/269.1576538 secs/batch = 0.5797s, grad.norm=0.59508181\n",
      "  7525: 5 [   30/ 1499], train_loss/perplexity = 5.56971073/262.3581848 secs/batch = 0.5696s, grad.norm=0.67684603\n",
      "  7530: 5 [   35/ 1499], train_loss/perplexity = 5.37681437/216.3320160 secs/batch = 0.5954s, grad.norm=0.54527837\n",
      "  7535: 5 [   40/ 1499], train_loss/perplexity = 5.37723446/216.4229279 secs/batch = 0.5518s, grad.norm=0.51928627\n",
      "  7540: 5 [   45/ 1499], train_loss/perplexity = 5.34658861/209.8910522 secs/batch = 0.5646s, grad.norm=0.54335284\n",
      "  7545: 5 [   50/ 1499], train_loss/perplexity = 5.30327559/200.9941101 secs/batch = 0.5458s, grad.norm=0.56735456\n",
      "  7550: 5 [   55/ 1499], train_loss/perplexity = 5.16485357/175.0118256 secs/batch = 0.5935s, grad.norm=0.61730570\n",
      "  7555: 5 [   60/ 1499], train_loss/perplexity = 5.09349823/162.9589386 secs/batch = 0.5643s, grad.norm=0.52122492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7560: 5 [   65/ 1499], train_loss/perplexity = 5.16917419/175.7696228 secs/batch = 0.6083s, grad.norm=0.54049182\n",
      "  7565: 5 [   70/ 1499], train_loss/perplexity = 5.20835829/182.7937164 secs/batch = 0.5532s, grad.norm=0.66360283\n",
      "  7570: 5 [   75/ 1499], train_loss/perplexity = 4.97969246/145.4296417 secs/batch = 0.5440s, grad.norm=0.61811376\n",
      "  7575: 5 [   80/ 1499], train_loss/perplexity = 5.11683369/166.8063660 secs/batch = 0.5405s, grad.norm=0.59562320\n",
      "  7580: 5 [   85/ 1499], train_loss/perplexity = 5.01706600/150.9677124 secs/batch = 0.5510s, grad.norm=0.56060117\n",
      "  7585: 5 [   90/ 1499], train_loss/perplexity = 5.28261518/196.8840942 secs/batch = 0.5491s, grad.norm=0.67781186\n",
      "  7590: 5 [   95/ 1499], train_loss/perplexity = 5.01928854/151.3036194 secs/batch = 0.6378s, grad.norm=0.56018102\n",
      "  7595: 5 [  100/ 1499], train_loss/perplexity = 5.08514977/161.6041412 secs/batch = 0.5857s, grad.norm=0.53754461\n",
      "  7600: 5 [  105/ 1499], train_loss/perplexity = 5.01036835/149.9599609 secs/batch = 0.5867s, grad.norm=0.59222782\n",
      "  7605: 5 [  110/ 1499], train_loss/perplexity = 5.03795719/154.1547852 secs/batch = 0.5461s, grad.norm=0.59199369\n",
      "  7610: 5 [  115/ 1499], train_loss/perplexity = 5.23757982/188.2140350 secs/batch = 0.5776s, grad.norm=0.52852952\n",
      "  7615: 5 [  120/ 1499], train_loss/perplexity = 5.11692667/166.8218842 secs/batch = 0.5485s, grad.norm=0.62463635\n",
      "  7620: 5 [  125/ 1499], train_loss/perplexity = 5.26897573/194.2169342 secs/batch = 0.5492s, grad.norm=0.59276527\n",
      "  7625: 5 [  130/ 1499], train_loss/perplexity = 5.25048590/190.6588898 secs/batch = 0.5436s, grad.norm=0.60513633\n",
      "  7630: 5 [  135/ 1499], train_loss/perplexity = 5.26072598/192.6212769 secs/batch = 0.6570s, grad.norm=0.56022745\n",
      "  7635: 5 [  140/ 1499], train_loss/perplexity = 5.28223753/196.8097534 secs/batch = 0.5855s, grad.norm=0.63189453\n",
      "  7640: 5 [  145/ 1499], train_loss/perplexity = 5.07391882/159.7993317 secs/batch = 0.5821s, grad.norm=0.65374249\n",
      "  7645: 5 [  150/ 1499], train_loss/perplexity = 5.26018906/192.5178833 secs/batch = 0.5463s, grad.norm=0.58892405\n",
      "  7650: 5 [  155/ 1499], train_loss/perplexity = 5.37278080/215.4611969 secs/batch = 0.5380s, grad.norm=0.55380827\n",
      "  7655: 5 [  160/ 1499], train_loss/perplexity = 5.68373680/294.0461731 secs/batch = 0.6626s, grad.norm=0.58137697\n",
      "  7660: 5 [  165/ 1499], train_loss/perplexity = 5.15531301/173.3500519 secs/batch = 0.5791s, grad.norm=0.52762973\n",
      "  7665: 5 [  170/ 1499], train_loss/perplexity = 5.54745913/256.5847778 secs/batch = 0.6220s, grad.norm=0.64331967\n",
      "  7670: 5 [  175/ 1499], train_loss/perplexity = 5.27144480/194.6970520 secs/batch = 0.6366s, grad.norm=0.57648408\n",
      "  7675: 5 [  180/ 1499], train_loss/perplexity = 5.45137167/233.0776520 secs/batch = 0.5559s, grad.norm=0.55386740\n",
      "  7680: 5 [  185/ 1499], train_loss/perplexity = 5.29260969/198.8617096 secs/batch = 0.5828s, grad.norm=0.67983997\n",
      "  7685: 5 [  190/ 1499], train_loss/perplexity = 5.42664385/227.3848267 secs/batch = 0.5689s, grad.norm=0.55297637\n",
      "  7690: 5 [  195/ 1499], train_loss/perplexity = 5.51961040/249.5377960 secs/batch = 0.5792s, grad.norm=0.56224984\n",
      "  7695: 5 [  200/ 1499], train_loss/perplexity = 5.44385195/231.3315430 secs/batch = 0.5617s, grad.norm=0.57056022\n",
      "  7700: 5 [  205/ 1499], train_loss/perplexity = 5.33225346/206.9037018 secs/batch = 0.5529s, grad.norm=0.55492586\n",
      "  7705: 5 [  210/ 1499], train_loss/perplexity = 5.14238739/171.1238251 secs/batch = 0.5446s, grad.norm=0.52307463\n",
      "  7710: 5 [  215/ 1499], train_loss/perplexity = 5.37472248/215.8799591 secs/batch = 0.5480s, grad.norm=0.58261842\n",
      "  7715: 5 [  220/ 1499], train_loss/perplexity = 5.11039162/165.7352448 secs/batch = 0.5469s, grad.norm=0.57747382\n",
      "  7720: 5 [  225/ 1499], train_loss/perplexity = 5.32591391/205.5961761 secs/batch = 0.5485s, grad.norm=0.56682843\n",
      "  7725: 5 [  230/ 1499], train_loss/perplexity = 5.37809515/216.6092682 secs/batch = 0.6005s, grad.norm=0.59068006\n",
      "  7730: 5 [  235/ 1499], train_loss/perplexity = 5.37702990/216.3786469 secs/batch = 0.5574s, grad.norm=0.57342970\n",
      "  7735: 5 [  240/ 1499], train_loss/perplexity = 5.47804260/239.3776855 secs/batch = 0.5528s, grad.norm=0.54621691\n",
      "  7740: 5 [  245/ 1499], train_loss/perplexity = 5.20742655/182.6234741 secs/batch = 0.5593s, grad.norm=0.74809426\n",
      "  7745: 5 [  250/ 1499], train_loss/perplexity = 5.20197535/181.6306763 secs/batch = 0.7137s, grad.norm=0.55540979\n",
      "  7750: 5 [  255/ 1499], train_loss/perplexity = 5.03416491/153.5712891 secs/batch = 0.5891s, grad.norm=0.57067382\n",
      "  7755: 5 [  260/ 1499], train_loss/perplexity = 5.40372181/222.2319794 secs/batch = 0.6336s, grad.norm=0.61171561\n",
      "  7760: 5 [  265/ 1499], train_loss/perplexity = 5.30281401/200.9013519 secs/batch = 0.7413s, grad.norm=0.61699986\n",
      "  7765: 5 [  270/ 1499], train_loss/perplexity = 5.37037182/214.9427795 secs/batch = 0.6350s, grad.norm=0.52363813\n",
      "  7770: 5 [  275/ 1499], train_loss/perplexity = 5.02926779/152.8210754 secs/batch = 0.5743s, grad.norm=0.66348040\n",
      "  7775: 5 [  280/ 1499], train_loss/perplexity = 5.21147299/183.3639526 secs/batch = 0.5628s, grad.norm=0.57657558\n",
      "  7780: 5 [  285/ 1499], train_loss/perplexity = 5.57794142/264.5264893 secs/batch = 0.5771s, grad.norm=0.55195236\n",
      "  7785: 5 [  290/ 1499], train_loss/perplexity = 5.55427027/258.3383789 secs/batch = 0.5421s, grad.norm=0.55904907\n",
      "  7790: 5 [  295/ 1499], train_loss/perplexity = 5.32116604/204.6223450 secs/batch = 0.5947s, grad.norm=0.54535323\n",
      "  7795: 5 [  300/ 1499], train_loss/perplexity = 5.20264912/181.7530823 secs/batch = 0.6534s, grad.norm=0.56340730\n",
      "  7800: 5 [  305/ 1499], train_loss/perplexity = 5.43492842/229.2764282 secs/batch = 0.5400s, grad.norm=0.59782523\n",
      "  7805: 5 [  310/ 1499], train_loss/perplexity = 5.51786089/249.1016083 secs/batch = 0.5396s, grad.norm=0.61258280\n",
      "  7810: 5 [  315/ 1499], train_loss/perplexity = 5.44085979/230.6403961 secs/batch = 0.5737s, grad.norm=0.55732501\n",
      "  7815: 5 [  320/ 1499], train_loss/perplexity = 5.36078072/212.8910828 secs/batch = 0.5542s, grad.norm=0.57291716\n",
      "  7820: 5 [  325/ 1499], train_loss/perplexity = 5.28582191/197.5164642 secs/batch = 0.5511s, grad.norm=0.55029410\n",
      "  7825: 5 [  330/ 1499], train_loss/perplexity = 5.24652624/189.9054413 secs/batch = 0.5411s, grad.norm=0.60917538\n",
      "  7830: 5 [  335/ 1499], train_loss/perplexity = 5.35861444/212.4304047 secs/batch = 0.5511s, grad.norm=0.60619873\n",
      "  7835: 5 [  340/ 1499], train_loss/perplexity = 5.04984617/155.9984589 secs/batch = 0.5473s, grad.norm=0.63478982\n",
      "  7840: 5 [  345/ 1499], train_loss/perplexity = 5.00521278/149.1888275 secs/batch = 0.5428s, grad.norm=0.53316700\n",
      "  7845: 5 [  350/ 1499], train_loss/perplexity = 5.16564560/175.1504974 secs/batch = 0.5446s, grad.norm=0.59800035\n",
      "  7850: 5 [  355/ 1499], train_loss/perplexity = 5.05022049/156.0568695 secs/batch = 0.5439s, grad.norm=0.63285518\n",
      "  7855: 5 [  360/ 1499], train_loss/perplexity = 5.16224289/174.5555267 secs/batch = 0.5392s, grad.norm=0.60653710\n",
      "  7860: 5 [  365/ 1499], train_loss/perplexity = 5.03543568/153.7665710 secs/batch = 0.5409s, grad.norm=0.55331147\n",
      "  7865: 5 [  370/ 1499], train_loss/perplexity = 5.01072073/150.0128174 secs/batch = 0.5471s, grad.norm=0.54957265\n",
      "  7870: 5 [  375/ 1499], train_loss/perplexity = 5.20721531/182.5848999 secs/batch = 0.5482s, grad.norm=0.57896262\n",
      "  7875: 5 [  380/ 1499], train_loss/perplexity = 5.38353348/217.7904816 secs/batch = 0.5495s, grad.norm=0.59655511\n",
      "  7880: 5 [  385/ 1499], train_loss/perplexity = 5.52390671/250.6121979 secs/batch = 0.5488s, grad.norm=0.56785983\n",
      "  7885: 5 [  390/ 1499], train_loss/perplexity = 5.64873457/283.9319458 secs/batch = 0.6074s, grad.norm=0.63720220\n",
      "  7890: 5 [  395/ 1499], train_loss/perplexity = 5.61372995/274.1649475 secs/batch = 0.6321s, grad.norm=0.60636991\n",
      "  7895: 5 [  400/ 1499], train_loss/perplexity = 5.49326611/243.0497437 secs/batch = 0.5424s, grad.norm=0.71836960\n",
      "  7900: 5 [  405/ 1499], train_loss/perplexity = 5.41077423/223.8047943 secs/batch = 0.5992s, grad.norm=0.53101259\n",
      "  7905: 5 [  410/ 1499], train_loss/perplexity = 4.87595129/131.0988007 secs/batch = 0.6484s, grad.norm=0.66449630\n",
      "  7910: 5 [  415/ 1499], train_loss/perplexity = 5.49660349/243.8622437 secs/batch = 0.5769s, grad.norm=0.57467818\n",
      "  7915: 5 [  420/ 1499], train_loss/perplexity = 5.12917423/168.8776093 secs/batch = 0.5514s, grad.norm=0.55991310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7920: 5 [  425/ 1499], train_loss/perplexity = 5.45195150/233.2128448 secs/batch = 0.5899s, grad.norm=0.57070601\n",
      "  7925: 5 [  430/ 1499], train_loss/perplexity = 5.18288183/178.1956024 secs/batch = 0.5759s, grad.norm=0.54750091\n",
      "  7930: 5 [  435/ 1499], train_loss/perplexity = 5.15087986/172.5832672 secs/batch = 0.5518s, grad.norm=0.56366748\n",
      "  7935: 5 [  440/ 1499], train_loss/perplexity = 5.22981310/186.7578888 secs/batch = 0.5496s, grad.norm=0.63666821\n",
      "  7940: 5 [  445/ 1499], train_loss/perplexity = 5.40272665/222.0109406 secs/batch = 0.5392s, grad.norm=0.60793978\n",
      "  7945: 5 [  450/ 1499], train_loss/perplexity = 5.31609774/203.5878754 secs/batch = 0.5690s, grad.norm=0.54176784\n",
      "  7950: 5 [  455/ 1499], train_loss/perplexity = 5.41862249/225.5681915 secs/batch = 0.6553s, grad.norm=0.52115911\n",
      "  7955: 5 [  460/ 1499], train_loss/perplexity = 5.36180162/213.1085358 secs/batch = 0.5708s, grad.norm=0.55702490\n",
      "  7960: 5 [  465/ 1499], train_loss/perplexity = 5.35455704/211.5702362 secs/batch = 0.5677s, grad.norm=0.55655193\n",
      "  7965: 5 [  470/ 1499], train_loss/perplexity = 5.43336010/228.9171448 secs/batch = 0.5384s, grad.norm=0.59878391\n",
      "  7970: 5 [  475/ 1499], train_loss/perplexity = 5.20400333/181.9993896 secs/batch = 0.5414s, grad.norm=0.58621061\n",
      "  7975: 5 [  480/ 1499], train_loss/perplexity = 5.34701061/209.9796448 secs/batch = 0.5468s, grad.norm=0.58624464\n",
      "  7980: 5 [  485/ 1499], train_loss/perplexity = 5.31601858/203.5717621 secs/batch = 0.5597s, grad.norm=0.59248620\n",
      "  7985: 5 [  490/ 1499], train_loss/perplexity = 5.24944735/190.4609833 secs/batch = 0.5415s, grad.norm=0.54099333\n",
      "  7990: 5 [  495/ 1499], train_loss/perplexity = 5.21812439/184.5876465 secs/batch = 0.5488s, grad.norm=0.60647315\n",
      "  7995: 5 [  500/ 1499], train_loss/perplexity = 5.36366844/213.5067444 secs/batch = 0.5410s, grad.norm=0.63223857\n",
      "  8000: 5 [  505/ 1499], train_loss/perplexity = 5.17568541/176.9178314 secs/batch = 0.5396s, grad.norm=0.62551528\n",
      "  8005: 5 [  510/ 1499], train_loss/perplexity = 5.54336977/255.5376587 secs/batch = 0.5427s, grad.norm=0.63557166\n",
      "  8010: 5 [  515/ 1499], train_loss/perplexity = 5.14783049/172.0578003 secs/batch = 0.5376s, grad.norm=0.68544149\n",
      "  8015: 5 [  520/ 1499], train_loss/perplexity = 5.37246084/215.3922577 secs/batch = 0.5486s, grad.norm=0.57505739\n",
      "  8020: 5 [  525/ 1499], train_loss/perplexity = 5.56902981/262.1796265 secs/batch = 0.5487s, grad.norm=0.70061851\n",
      "  8025: 5 [  530/ 1499], train_loss/perplexity = 5.27931690/196.2357788 secs/batch = 0.5572s, grad.norm=0.58264381\n",
      "  8030: 5 [  535/ 1499], train_loss/perplexity = 5.30135775/200.6090088 secs/batch = 0.5436s, grad.norm=0.54103845\n",
      "  8035: 5 [  540/ 1499], train_loss/perplexity = 5.18474102/178.5272064 secs/batch = 0.5388s, grad.norm=0.57514095\n",
      "  8040: 5 [  545/ 1499], train_loss/perplexity = 4.93931818/139.6749878 secs/batch = 0.5487s, grad.norm=0.63660300\n",
      "  8045: 5 [  550/ 1499], train_loss/perplexity = 5.42768002/227.6205597 secs/batch = 0.5383s, grad.norm=0.60071635\n",
      "  8050: 5 [  555/ 1499], train_loss/perplexity = 5.16953421/175.8329163 secs/batch = 0.5356s, grad.norm=0.57136160\n",
      "  8055: 5 [  560/ 1499], train_loss/perplexity = 5.54344702/255.5573883 secs/batch = 0.5421s, grad.norm=0.55691737\n",
      "  8060: 5 [  565/ 1499], train_loss/perplexity = 5.45445204/233.7967224 secs/batch = 0.5405s, grad.norm=0.56361616\n",
      "  8065: 5 [  570/ 1499], train_loss/perplexity = 5.43350267/228.9497833 secs/batch = 0.5395s, grad.norm=0.55181682\n",
      "  8070: 5 [  575/ 1499], train_loss/perplexity = 5.48433971/240.8898315 secs/batch = 0.5451s, grad.norm=0.58076769\n",
      "  8075: 5 [  580/ 1499], train_loss/perplexity = 5.18568897/178.6965179 secs/batch = 0.5391s, grad.norm=0.67488885\n",
      "  8080: 5 [  585/ 1499], train_loss/perplexity = 5.10495281/164.8362885 secs/batch = 0.5453s, grad.norm=0.58543032\n",
      "  8085: 5 [  590/ 1499], train_loss/perplexity = 5.28195190/196.7535400 secs/batch = 0.6431s, grad.norm=0.67112648\n",
      "  8090: 5 [  595/ 1499], train_loss/perplexity = 5.29693127/199.7229767 secs/batch = 0.6196s, grad.norm=0.57799768\n",
      "  8095: 5 [  600/ 1499], train_loss/perplexity = 5.08664465/161.8459015 secs/batch = 0.6042s, grad.norm=0.58141494\n",
      "  8100: 5 [  605/ 1499], train_loss/perplexity = 5.12597084/168.3374939 secs/batch = 0.6138s, grad.norm=0.58201057\n",
      "  8105: 5 [  610/ 1499], train_loss/perplexity = 5.39401484/220.0852203 secs/batch = 0.5965s, grad.norm=0.56750816\n",
      "  8110: 5 [  615/ 1499], train_loss/perplexity = 5.39526415/220.3603516 secs/batch = 0.6017s, grad.norm=0.55721337\n",
      "  8115: 5 [  620/ 1499], train_loss/perplexity = 5.22523499/185.9048462 secs/batch = 0.5895s, grad.norm=0.56416684\n",
      "  8120: 5 [  625/ 1499], train_loss/perplexity = 5.18957996/179.3931885 secs/batch = 0.5767s, grad.norm=0.54412371\n",
      "  8125: 5 [  630/ 1499], train_loss/perplexity = 5.34808445/210.2052460 secs/batch = 0.5559s, grad.norm=0.62951159\n",
      "  8130: 5 [  635/ 1499], train_loss/perplexity = 5.04504776/155.2517090 secs/batch = 0.5990s, grad.norm=0.58353496\n",
      "  8135: 5 [  640/ 1499], train_loss/perplexity = 5.31437397/203.2372437 secs/batch = 0.5843s, grad.norm=0.65825868\n",
      "  8140: 5 [  645/ 1499], train_loss/perplexity = 5.05502129/156.8078613 secs/batch = 0.5541s, grad.norm=0.61860502\n",
      "  8145: 5 [  650/ 1499], train_loss/perplexity = 5.02047062/151.4825745 secs/batch = 0.5694s, grad.norm=0.73371297\n",
      "  8150: 5 [  655/ 1499], train_loss/perplexity = 5.38591290/218.3093109 secs/batch = 0.5669s, grad.norm=0.55756015\n",
      "  8155: 5 [  660/ 1499], train_loss/perplexity = 5.52655983/251.2779846 secs/batch = 0.5584s, grad.norm=0.61669898\n",
      "  8160: 5 [  665/ 1499], train_loss/perplexity = 5.41619730/225.0218048 secs/batch = 0.5472s, grad.norm=0.59942377\n",
      "  8165: 5 [  670/ 1499], train_loss/perplexity = 5.55063915/257.4020081 secs/batch = 0.5544s, grad.norm=0.56733078\n",
      "  8170: 5 [  675/ 1499], train_loss/perplexity = 5.37046528/214.9628601 secs/batch = 0.5672s, grad.norm=0.55723286\n",
      "  8175: 5 [  680/ 1499], train_loss/perplexity = 5.32731962/205.8853760 secs/batch = 0.5529s, grad.norm=0.56773311\n",
      "  8180: 5 [  685/ 1499], train_loss/perplexity = 5.49066734/242.4189301 secs/batch = 0.5678s, grad.norm=0.67589837\n",
      "  8185: 5 [  690/ 1499], train_loss/perplexity = 5.34077406/208.6741791 secs/batch = 0.5443s, grad.norm=0.56414783\n",
      "  8190: 5 [  695/ 1499], train_loss/perplexity = 5.35406780/211.4667511 secs/batch = 0.5491s, grad.norm=0.56746334\n",
      "  8195: 5 [  700/ 1499], train_loss/perplexity = 5.49111319/242.5270386 secs/batch = 0.7245s, grad.norm=0.57144648\n",
      "  8200: 5 [  705/ 1499], train_loss/perplexity = 5.01679897/150.9274139 secs/batch = 0.6887s, grad.norm=0.60220426\n",
      "  8205: 5 [  710/ 1499], train_loss/perplexity = 5.33095312/206.6348267 secs/batch = 0.6376s, grad.norm=0.56399173\n",
      "  8210: 5 [  715/ 1499], train_loss/perplexity = 5.32731247/205.8839111 secs/batch = 0.5890s, grad.norm=0.62082201\n",
      "  8215: 5 [  720/ 1499], train_loss/perplexity = 5.34704161/209.9861603 secs/batch = 0.6543s, grad.norm=0.57173645\n",
      "  8220: 5 [  725/ 1499], train_loss/perplexity = 5.16195107/174.5045929 secs/batch = 0.7045s, grad.norm=0.60359091\n",
      "  8225: 5 [  730/ 1499], train_loss/perplexity = 5.31073093/202.4981842 secs/batch = 0.7097s, grad.norm=0.59177369\n",
      "  8230: 5 [  735/ 1499], train_loss/perplexity = 5.13928699/170.5940857 secs/batch = 1.0778s, grad.norm=0.57982862\n",
      "  8235: 5 [  740/ 1499], train_loss/perplexity = 5.15776825/173.7761993 secs/batch = 0.9430s, grad.norm=0.65289956\n",
      "  8240: 5 [  745/ 1499], train_loss/perplexity = 5.00232172/148.7581329 secs/batch = 0.5949s, grad.norm=0.55271453\n",
      "  8245: 5 [  750/ 1499], train_loss/perplexity = 5.28460932/197.2770996 secs/batch = 0.5557s, grad.norm=0.59944373\n",
      "  8250: 5 [  755/ 1499], train_loss/perplexity = 5.08883476/162.2007446 secs/batch = 0.5767s, grad.norm=0.62277132\n",
      "  8255: 5 [  760/ 1499], train_loss/perplexity = 4.72582674/112.8237381 secs/batch = 0.5616s, grad.norm=0.62206995\n",
      "  8260: 5 [  765/ 1499], train_loss/perplexity = 5.13305187/169.5337219 secs/batch = 0.5516s, grad.norm=0.59700626\n",
      "  8265: 5 [  770/ 1499], train_loss/perplexity = 5.27125549/194.6602020 secs/batch = 0.5984s, grad.norm=0.60360914\n",
      "  8270: 5 [  775/ 1499], train_loss/perplexity = 5.58213758/265.6388245 secs/batch = 0.6407s, grad.norm=0.61323828\n",
      "  8275: 5 [  780/ 1499], train_loss/perplexity = 5.12313271/167.8603973 secs/batch = 0.5983s, grad.norm=0.52463514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8280: 5 [  785/ 1499], train_loss/perplexity = 5.28046656/196.4615173 secs/batch = 0.5767s, grad.norm=0.61310154\n",
      "  8285: 5 [  790/ 1499], train_loss/perplexity = 4.87359858/130.7907257 secs/batch = 0.6191s, grad.norm=0.58071774\n",
      "  8290: 5 [  795/ 1499], train_loss/perplexity = 5.29258728/198.8572540 secs/batch = 0.5438s, grad.norm=0.58094692\n",
      "  8295: 5 [  800/ 1499], train_loss/perplexity = 5.23855305/188.3973083 secs/batch = 0.5729s, grad.norm=0.63492548\n",
      "  8300: 5 [  805/ 1499], train_loss/perplexity = 5.21053171/183.1914368 secs/batch = 0.5560s, grad.norm=0.54385179\n",
      "  8305: 5 [  810/ 1499], train_loss/perplexity = 5.13895798/170.5379639 secs/batch = 0.5421s, grad.norm=0.54641324\n",
      "  8310: 5 [  815/ 1499], train_loss/perplexity = 5.37043047/214.9553833 secs/batch = 0.5615s, grad.norm=0.60760051\n",
      "  8315: 5 [  820/ 1499], train_loss/perplexity = 4.93614006/139.2317810 secs/batch = 0.5557s, grad.norm=0.61794251\n",
      "  8320: 5 [  825/ 1499], train_loss/perplexity = 5.04757309/155.6442719 secs/batch = 0.6472s, grad.norm=0.63056040\n",
      "  8325: 5 [  830/ 1499], train_loss/perplexity = 5.25366735/191.2664185 secs/batch = 0.5370s, grad.norm=0.57093054\n",
      "  8330: 5 [  835/ 1499], train_loss/perplexity = 5.24912739/190.4000549 secs/batch = 0.6256s, grad.norm=0.61472094\n",
      "  8335: 5 [  840/ 1499], train_loss/perplexity = 5.19085836/179.6226654 secs/batch = 0.5644s, grad.norm=0.67292649\n",
      "  8340: 5 [  845/ 1499], train_loss/perplexity = 5.01913023/151.2796631 secs/batch = 0.5506s, grad.norm=0.55589491\n",
      "  8345: 5 [  850/ 1499], train_loss/perplexity = 5.19082928/179.6174469 secs/batch = 0.5394s, grad.norm=0.64286578\n",
      "  8350: 5 [  855/ 1499], train_loss/perplexity = 5.31318140/202.9950104 secs/batch = 0.5404s, grad.norm=0.58740485\n",
      "  8355: 5 [  860/ 1499], train_loss/perplexity = 4.74780846/115.3312531 secs/batch = 0.5444s, grad.norm=0.61824185\n",
      "  8360: 5 [  865/ 1499], train_loss/perplexity = 5.11992216/167.3223419 secs/batch = 0.5436s, grad.norm=0.68753999\n",
      "  8365: 5 [  870/ 1499], train_loss/perplexity = 5.17423344/176.6611328 secs/batch = 0.5424s, grad.norm=0.60463518\n",
      "  8370: 5 [  875/ 1499], train_loss/perplexity = 5.17708588/177.1657715 secs/batch = 0.5353s, grad.norm=0.54351443\n",
      "  8375: 5 [  880/ 1499], train_loss/perplexity = 5.12703800/168.5172272 secs/batch = 0.5466s, grad.norm=0.59247446\n",
      "  8380: 5 [  885/ 1499], train_loss/perplexity = 5.01623869/150.8428650 secs/batch = 0.6199s, grad.norm=0.63995737\n",
      "  8385: 5 [  890/ 1499], train_loss/perplexity = 5.35889292/212.4895782 secs/batch = 0.5510s, grad.norm=0.55303019\n",
      "  8390: 5 [  895/ 1499], train_loss/perplexity = 5.38332033/217.7440643 secs/batch = 0.5658s, grad.norm=0.56886041\n",
      "  8395: 5 [  900/ 1499], train_loss/perplexity = 5.03147125/153.1581879 secs/batch = 0.5784s, grad.norm=0.53562498\n",
      "  8400: 5 [  905/ 1499], train_loss/perplexity = 5.22999477/186.7918243 secs/batch = 0.5356s, grad.norm=0.57685256\n",
      "  8405: 5 [  910/ 1499], train_loss/perplexity = 5.40296030/222.0628204 secs/batch = 0.5352s, grad.norm=0.71419132\n",
      "  8410: 5 [  915/ 1499], train_loss/perplexity = 5.15388107/173.1020050 secs/batch = 0.5447s, grad.norm=0.62963533\n",
      "  8415: 5 [  920/ 1499], train_loss/perplexity = 4.86541939/129.7253265 secs/batch = 0.5467s, grad.norm=0.54083365\n",
      "  8420: 5 [  925/ 1499], train_loss/perplexity = 5.21279669/183.6068268 secs/batch = 0.6179s, grad.norm=0.67736423\n",
      "  8425: 5 [  930/ 1499], train_loss/perplexity = 5.21270418/183.5898438 secs/batch = 0.5853s, grad.norm=0.64524245\n",
      "  8430: 5 [  935/ 1499], train_loss/perplexity = 5.30648470/201.6401520 secs/batch = 0.6380s, grad.norm=0.55635357\n",
      "  8435: 5 [  940/ 1499], train_loss/perplexity = 4.91343784/136.1065216 secs/batch = 0.5893s, grad.norm=0.61847144\n",
      "  8440: 5 [  945/ 1499], train_loss/perplexity = 5.37310410/215.5308533 secs/batch = 0.6748s, grad.norm=0.58069700\n",
      "  8445: 5 [  950/ 1499], train_loss/perplexity = 5.28871298/198.0883179 secs/batch = 0.5923s, grad.norm=0.58723783\n",
      "  8450: 5 [  955/ 1499], train_loss/perplexity = 5.40705729/222.9744720 secs/batch = 0.5481s, grad.norm=0.54924065\n",
      "  8455: 5 [  960/ 1499], train_loss/perplexity = 5.10583687/164.9820862 secs/batch = 0.5694s, grad.norm=0.59026921\n",
      "  8460: 5 [  965/ 1499], train_loss/perplexity = 5.15702581/173.6472321 secs/batch = 0.5478s, grad.norm=0.54295534\n",
      "  8465: 5 [  970/ 1499], train_loss/perplexity = 5.42958117/228.0537109 secs/batch = 0.5426s, grad.norm=0.60458392\n",
      "  8470: 5 [  975/ 1499], train_loss/perplexity = 5.18626213/178.7989807 secs/batch = 0.5433s, grad.norm=0.63230938\n",
      "  8475: 5 [  980/ 1499], train_loss/perplexity = 5.32071018/204.5290833 secs/batch = 0.5701s, grad.norm=0.70426100\n",
      "  8480: 5 [  985/ 1499], train_loss/perplexity = 4.96571493/143.4110413 secs/batch = 0.5527s, grad.norm=0.58687252\n",
      "  8485: 5 [  990/ 1499], train_loss/perplexity = 5.15274906/172.9061737 secs/batch = 0.6466s, grad.norm=0.57615161\n",
      "  8490: 5 [  995/ 1499], train_loss/perplexity = 5.46327734/235.8691864 secs/batch = 0.5588s, grad.norm=0.61214876\n",
      "  8495: 5 [ 1000/ 1499], train_loss/perplexity = 5.18789005/179.0902863 secs/batch = 0.5521s, grad.norm=0.58641803\n",
      "  8500: 5 [ 1005/ 1499], train_loss/perplexity = 5.53785944/254.1334229 secs/batch = 0.5488s, grad.norm=0.55522370\n",
      "  8505: 5 [ 1010/ 1499], train_loss/perplexity = 5.36265373/213.2902069 secs/batch = 0.5365s, grad.norm=0.62540132\n",
      "  8510: 5 [ 1015/ 1499], train_loss/perplexity = 5.18035173/177.7453156 secs/batch = 0.5425s, grad.norm=0.58391535\n",
      "  8515: 5 [ 1020/ 1499], train_loss/perplexity = 5.25002193/190.5704498 secs/batch = 0.5602s, grad.norm=0.64956486\n",
      "  8520: 5 [ 1025/ 1499], train_loss/perplexity = 5.28095961/196.5584106 secs/batch = 0.5473s, grad.norm=0.64165330\n",
      "  8525: 5 [ 1030/ 1499], train_loss/perplexity = 5.41307354/224.3199921 secs/batch = 0.6133s, grad.norm=0.59196234\n",
      "  8530: 5 [ 1035/ 1499], train_loss/perplexity = 5.67959404/292.8305359 secs/batch = 0.6347s, grad.norm=0.74526322\n",
      "  8535: 5 [ 1040/ 1499], train_loss/perplexity = 5.05288410/156.4730988 secs/batch = 0.5588s, grad.norm=0.52555543\n",
      "  8540: 5 [ 1045/ 1499], train_loss/perplexity = 5.17955971/177.6045990 secs/batch = 0.5410s, grad.norm=0.58717537\n",
      "  8545: 5 [ 1050/ 1499], train_loss/perplexity = 4.85879898/128.8693390 secs/batch = 0.5413s, grad.norm=0.58977491\n",
      "  8550: 5 [ 1055/ 1499], train_loss/perplexity = 5.23019171/186.8286133 secs/batch = 0.5624s, grad.norm=0.56646717\n",
      "  8555: 5 [ 1060/ 1499], train_loss/perplexity = 5.42816114/227.7301025 secs/batch = 0.5430s, grad.norm=0.56466180\n",
      "  8560: 5 [ 1065/ 1499], train_loss/perplexity = 4.98855734/146.7245941 secs/batch = 0.5460s, grad.norm=0.63826895\n",
      "  8565: 5 [ 1070/ 1499], train_loss/perplexity = 5.46838856/237.0778503 secs/batch = 0.5851s, grad.norm=0.61946213\n",
      "  8570: 5 [ 1075/ 1499], train_loss/perplexity = 5.30834150/202.0149078 secs/batch = 0.5529s, grad.norm=0.57631201\n",
      "  8575: 5 [ 1080/ 1499], train_loss/perplexity = 5.51855373/249.2742615 secs/batch = 0.5572s, grad.norm=0.56644422\n",
      "  8580: 5 [ 1085/ 1499], train_loss/perplexity = 5.59528828/269.1552429 secs/batch = 0.6139s, grad.norm=0.56713355\n",
      "  8585: 5 [ 1090/ 1499], train_loss/perplexity = 5.16088200/174.3181305 secs/batch = 0.5655s, grad.norm=0.60690486\n",
      "  8590: 5 [ 1095/ 1499], train_loss/perplexity = 5.36104584/212.9475403 secs/batch = 0.5530s, grad.norm=0.56549037\n",
      "  8595: 5 [ 1100/ 1499], train_loss/perplexity = 5.33481121/207.4335785 secs/batch = 0.6255s, grad.norm=0.60011953\n",
      "  8600: 5 [ 1105/ 1499], train_loss/perplexity = 5.10841799/165.4084625 secs/batch = 0.6875s, grad.norm=0.61536515\n",
      "  8605: 5 [ 1110/ 1499], train_loss/perplexity = 5.28281927/196.9242706 secs/batch = 0.5583s, grad.norm=0.55178529\n",
      "  8610: 5 [ 1115/ 1499], train_loss/perplexity = 4.91476965/136.2879181 secs/batch = 0.5485s, grad.norm=0.56561047\n",
      "  8615: 5 [ 1120/ 1499], train_loss/perplexity = 5.16463184/174.9730225 secs/batch = 0.6906s, grad.norm=0.61076039\n",
      "  8620: 5 [ 1125/ 1499], train_loss/perplexity = 5.09862232/163.7960968 secs/batch = 0.5996s, grad.norm=0.62905467\n",
      "  8625: 5 [ 1130/ 1499], train_loss/perplexity = 4.89453936/133.5584717 secs/batch = 0.7054s, grad.norm=0.55210930\n",
      "  8630: 5 [ 1135/ 1499], train_loss/perplexity = 5.54152584/255.0668945 secs/batch = 0.6287s, grad.norm=0.53631246\n",
      "  8635: 5 [ 1140/ 1499], train_loss/perplexity = 5.37162399/215.2120819 secs/batch = 1.0744s, grad.norm=0.63694441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8640: 5 [ 1145/ 1499], train_loss/perplexity = 5.46407795/236.0580902 secs/batch = 0.9179s, grad.norm=0.57858449\n",
      "  8645: 5 [ 1150/ 1499], train_loss/perplexity = 5.41338396/224.3896332 secs/batch = 1.1841s, grad.norm=0.57032448\n",
      "  8650: 5 [ 1155/ 1499], train_loss/perplexity = 5.08205032/161.1040344 secs/batch = 0.7009s, grad.norm=0.59319836\n",
      "  8655: 5 [ 1160/ 1499], train_loss/perplexity = 5.24466276/189.5518799 secs/batch = 0.6917s, grad.norm=0.58018506\n",
      "  8660: 5 [ 1165/ 1499], train_loss/perplexity = 5.22147608/185.2073669 secs/batch = 0.6863s, grad.norm=0.59872693\n",
      "  8665: 5 [ 1170/ 1499], train_loss/perplexity = 5.35309982/211.2621613 secs/batch = 0.5852s, grad.norm=0.56863374\n",
      "  8670: 5 [ 1175/ 1499], train_loss/perplexity = 4.71758318/111.8974915 secs/batch = 0.6430s, grad.norm=0.59707254\n",
      "  8675: 5 [ 1180/ 1499], train_loss/perplexity = 5.05129766/156.2250671 secs/batch = 0.5563s, grad.norm=0.66196239\n",
      "  8680: 5 [ 1185/ 1499], train_loss/perplexity = 4.96881151/143.8558197 secs/batch = 0.5658s, grad.norm=0.65299165\n",
      "  8685: 5 [ 1190/ 1499], train_loss/perplexity = 5.25947523/192.3805084 secs/batch = 0.5430s, grad.norm=0.72623938\n",
      "  8690: 5 [ 1195/ 1499], train_loss/perplexity = 5.21933985/184.8121338 secs/batch = 0.5654s, grad.norm=0.57724875\n",
      "  8695: 5 [ 1200/ 1499], train_loss/perplexity = 5.13373375/169.6493683 secs/batch = 0.5593s, grad.norm=0.62240249\n",
      "  8700: 5 [ 1205/ 1499], train_loss/perplexity = 5.03644943/153.9225311 secs/batch = 0.5681s, grad.norm=0.58279318\n",
      "  8705: 5 [ 1210/ 1499], train_loss/perplexity = 4.98675680/146.4606476 secs/batch = 0.5677s, grad.norm=0.60827190\n",
      "  8710: 5 [ 1215/ 1499], train_loss/perplexity = 4.74414253/114.9092331 secs/batch = 0.5395s, grad.norm=0.63859129\n",
      "  8715: 5 [ 1220/ 1499], train_loss/perplexity = 5.11210680/166.0197601 secs/batch = 0.5420s, grad.norm=0.58022237\n",
      "  8720: 5 [ 1225/ 1499], train_loss/perplexity = 4.51960278/91.7991257 secs/batch = 0.5409s, grad.norm=0.56949985\n",
      "  8725: 5 [ 1230/ 1499], train_loss/perplexity = 5.10672188/165.1281586 secs/batch = 0.6429s, grad.norm=0.58324248\n",
      "  8730: 5 [ 1235/ 1499], train_loss/perplexity = 4.90456152/134.9037476 secs/batch = 0.5610s, grad.norm=0.70214391\n",
      "  8735: 5 [ 1240/ 1499], train_loss/perplexity = 5.00530958/149.2032623 secs/batch = 0.6366s, grad.norm=0.59121585\n",
      "  8740: 5 [ 1245/ 1499], train_loss/perplexity = 5.31811094/203.9981537 secs/batch = 0.5799s, grad.norm=0.57567763\n",
      "  8745: 5 [ 1250/ 1499], train_loss/perplexity = 5.47158432/237.8367004 secs/batch = 0.5672s, grad.norm=0.60682029\n",
      "  8750: 5 [ 1255/ 1499], train_loss/perplexity = 5.11550665/166.5851593 secs/batch = 0.6041s, grad.norm=0.59166598\n",
      "  8755: 5 [ 1260/ 1499], train_loss/perplexity = 5.22553825/185.9612427 secs/batch = 0.5428s, grad.norm=0.58559507\n",
      "  8760: 5 [ 1265/ 1499], train_loss/perplexity = 5.23670721/188.0498657 secs/batch = 0.6829s, grad.norm=0.60383028\n",
      "  8765: 5 [ 1270/ 1499], train_loss/perplexity = 5.21271610/183.5920410 secs/batch = 0.6322s, grad.norm=0.55874890\n",
      "  8770: 5 [ 1275/ 1499], train_loss/perplexity = 5.24929953/190.4328308 secs/batch = 0.5679s, grad.norm=0.58986592\n",
      "  8775: 5 [ 1280/ 1499], train_loss/perplexity = 4.85988474/129.0093384 secs/batch = 0.5707s, grad.norm=0.58183849\n",
      "  8780: 5 [ 1285/ 1499], train_loss/perplexity = 5.29516935/199.3713837 secs/batch = 0.5440s, grad.norm=0.55694729\n",
      "  8785: 5 [ 1290/ 1499], train_loss/perplexity = 5.19365215/180.1251984 secs/batch = 0.5396s, grad.norm=0.58337313\n",
      "  8790: 5 [ 1295/ 1499], train_loss/perplexity = 5.20181942/181.6023560 secs/batch = 0.5388s, grad.norm=0.60901839\n",
      "  8795: 5 [ 1300/ 1499], train_loss/perplexity = 5.32551241/205.5136414 secs/batch = 0.5413s, grad.norm=0.57962799\n",
      "  8800: 5 [ 1305/ 1499], train_loss/perplexity = 5.22979927/186.7553101 secs/batch = 0.5427s, grad.norm=0.59794575\n",
      "  8805: 5 [ 1310/ 1499], train_loss/perplexity = 5.32957411/206.3500671 secs/batch = 0.5513s, grad.norm=0.61202550\n",
      "  8810: 5 [ 1315/ 1499], train_loss/perplexity = 5.16741371/175.4604645 secs/batch = 0.5771s, grad.norm=0.71689689\n",
      "  8815: 5 [ 1320/ 1499], train_loss/perplexity = 4.84582281/127.2079086 secs/batch = 0.5865s, grad.norm=0.56938857\n",
      "  8820: 5 [ 1325/ 1499], train_loss/perplexity = 5.23228884/187.2208252 secs/batch = 0.5761s, grad.norm=0.57228982\n",
      "  8825: 5 [ 1330/ 1499], train_loss/perplexity = 5.32918406/206.2696075 secs/batch = 0.5686s, grad.norm=0.54813999\n",
      "  8830: 5 [ 1335/ 1499], train_loss/perplexity = 5.16831112/175.6179962 secs/batch = 0.5597s, grad.norm=0.54753697\n",
      "  8835: 5 [ 1340/ 1499], train_loss/perplexity = 4.64752388/104.3263397 secs/batch = 0.5788s, grad.norm=0.58548391\n",
      "  8840: 5 [ 1345/ 1499], train_loss/perplexity = 4.81080294/122.8302002 secs/batch = 0.5440s, grad.norm=0.63068050\n",
      "  8845: 5 [ 1350/ 1499], train_loss/perplexity = 4.97586441/144.8740082 secs/batch = 0.5504s, grad.norm=0.59849250\n",
      "  8850: 5 [ 1355/ 1499], train_loss/perplexity = 4.67348337/107.0700607 secs/batch = 0.5409s, grad.norm=0.60670710\n",
      "  8855: 5 [ 1360/ 1499], train_loss/perplexity = 4.86674213/129.8970337 secs/batch = 0.5611s, grad.norm=0.55148572\n",
      "  8860: 5 [ 1365/ 1499], train_loss/perplexity = 4.65387344/104.9908752 secs/batch = 0.5430s, grad.norm=0.59309864\n",
      "  8865: 5 [ 1370/ 1499], train_loss/perplexity = 4.68371248/108.1709137 secs/batch = 0.5475s, grad.norm=0.58352339\n",
      "  8870: 5 [ 1375/ 1499], train_loss/perplexity = 4.73162985/113.4803696 secs/batch = 0.5448s, grad.norm=0.59495878\n",
      "  8875: 5 [ 1380/ 1499], train_loss/perplexity = 5.21392059/183.8133087 secs/batch = 0.5438s, grad.norm=0.60782778\n",
      "  8880: 5 [ 1385/ 1499], train_loss/perplexity = 4.93028164/138.4184875 secs/batch = 0.7266s, grad.norm=0.58672047\n",
      "  8885: 5 [ 1390/ 1499], train_loss/perplexity = 5.04633570/155.4517975 secs/batch = 0.6836s, grad.norm=0.58943295\n",
      "  8890: 5 [ 1395/ 1499], train_loss/perplexity = 5.22152281/185.2160187 secs/batch = 0.6006s, grad.norm=0.58435500\n",
      "  8895: 5 [ 1400/ 1499], train_loss/perplexity = 5.22733498/186.2956543 secs/batch = 0.5358s, grad.norm=0.58720678\n",
      "  8900: 5 [ 1405/ 1499], train_loss/perplexity = 5.08171511/161.0500336 secs/batch = 0.6079s, grad.norm=0.65040511\n",
      "  8905: 5 [ 1410/ 1499], train_loss/perplexity = 5.24523115/189.6596527 secs/batch = 0.5687s, grad.norm=0.55667359\n",
      "  8910: 5 [ 1415/ 1499], train_loss/perplexity = 5.17426586/176.6668701 secs/batch = 0.6405s, grad.norm=0.57787818\n",
      "  8915: 5 [ 1420/ 1499], train_loss/perplexity = 5.16570616/175.1611023 secs/batch = 0.6515s, grad.norm=0.56476843\n",
      "  8920: 5 [ 1425/ 1499], train_loss/perplexity = 5.09911776/163.8772583 secs/batch = 0.6467s, grad.norm=0.62034887\n",
      "  8925: 5 [ 1430/ 1499], train_loss/perplexity = 5.27137613/194.6836853 secs/batch = 0.5586s, grad.norm=0.60130131\n",
      "  8930: 5 [ 1435/ 1499], train_loss/perplexity = 4.96287584/143.0044708 secs/batch = 0.5713s, grad.norm=0.54394817\n",
      "  8935: 5 [ 1440/ 1499], train_loss/perplexity = 4.71432495/111.5334930 secs/batch = 0.5785s, grad.norm=0.59099883\n",
      "  8940: 5 [ 1445/ 1499], train_loss/perplexity = 5.13202000/169.3588715 secs/batch = 0.5986s, grad.norm=0.62749177\n",
      "  8945: 5 [ 1450/ 1499], train_loss/perplexity = 5.21923828/184.7933655 secs/batch = 0.5522s, grad.norm=0.61745781\n",
      "  8950: 5 [ 1455/ 1499], train_loss/perplexity = 5.48389053/240.7816620 secs/batch = 0.5440s, grad.norm=0.57787341\n",
      "  8955: 5 [ 1460/ 1499], train_loss/perplexity = 5.46202135/235.5731201 secs/batch = 0.5689s, grad.norm=0.58916074\n",
      "  8960: 5 [ 1465/ 1499], train_loss/perplexity = 5.55454683/258.4098206 secs/batch = 0.6128s, grad.norm=0.58063585\n",
      "  8965: 5 [ 1470/ 1499], train_loss/perplexity = 5.24865818/190.3107300 secs/batch = 0.5489s, grad.norm=0.62878114\n",
      "  8970: 5 [ 1475/ 1499], train_loss/perplexity = 5.33867168/208.2359161 secs/batch = 0.5440s, grad.norm=0.62507433\n",
      "  8975: 5 [ 1480/ 1499], train_loss/perplexity = 5.33193493/206.8377991 secs/batch = 0.5458s, grad.norm=0.58693266\n",
      "  8980: 5 [ 1485/ 1499], train_loss/perplexity = 5.07520771/160.0054169 secs/batch = 0.5497s, grad.norm=0.67859423\n",
      "  8985: 5 [ 1490/ 1499], train_loss/perplexity = 5.16009188/174.1804504 secs/batch = 0.5456s, grad.norm=0.57185215\n",
      "  8990: 5 [ 1495/ 1499], train_loss/perplexity = 5.46162987/235.4809113 secs/batch = 0.5423s, grad.norm=0.57605410\n",
      "Epoch training time: 877.4170427322388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved char model cv/epoch005_5.2778.model\n",
      "  8999: 6 [    5/ 1499], train_loss/perplexity = 5.31407213/203.1759033 secs/batch = 0.5506s, grad.norm=0.62635422\n",
      "  9004: 6 [   10/ 1499], train_loss/perplexity = 5.29303789/198.9468842 secs/batch = 0.5532s, grad.norm=0.56602955\n",
      "  9009: 6 [   15/ 1499], train_loss/perplexity = 5.14961147/172.3645020 secs/batch = 0.5592s, grad.norm=0.60268825\n",
      "  9014: 6 [   20/ 1499], train_loss/perplexity = 5.01764011/151.0544128 secs/batch = 0.5551s, grad.norm=0.53625184\n",
      "  9019: 6 [   25/ 1499], train_loss/perplexity = 5.50810671/246.6836395 secs/batch = 0.5433s, grad.norm=0.63892788\n",
      "  9024: 6 [   30/ 1499], train_loss/perplexity = 5.38045931/217.1219788 secs/batch = 0.5640s, grad.norm=0.66007745\n",
      "  9029: 6 [   35/ 1499], train_loss/perplexity = 5.25917482/192.3227234 secs/batch = 0.5775s, grad.norm=0.57757258\n",
      "  9034: 6 [   40/ 1499], train_loss/perplexity = 5.31674862/203.7204285 secs/batch = 0.5462s, grad.norm=0.56841326\n",
      "  9039: 6 [   45/ 1499], train_loss/perplexity = 5.22378540/185.6355591 secs/batch = 0.5564s, grad.norm=0.60275918\n",
      "  9044: 6 [   50/ 1499], train_loss/perplexity = 5.17998695/177.6804962 secs/batch = 0.5966s, grad.norm=0.62177044\n",
      "  9049: 6 [   55/ 1499], train_loss/perplexity = 5.02993822/152.9235687 secs/batch = 0.5531s, grad.norm=0.59881693\n",
      "  9054: 6 [   60/ 1499], train_loss/perplexity = 4.96318913/143.0492706 secs/batch = 0.5806s, grad.norm=0.56078976\n",
      "  9059: 6 [   65/ 1499], train_loss/perplexity = 5.01419640/150.5351105 secs/batch = 0.5586s, grad.norm=0.55071604\n",
      "  9064: 6 [   70/ 1499], train_loss/perplexity = 5.03591728/153.8406372 secs/batch = 0.5712s, grad.norm=0.63166910\n",
      "  9069: 6 [   75/ 1499], train_loss/perplexity = 4.84861612/127.5637360 secs/batch = 0.6313s, grad.norm=0.61830837\n",
      "  9074: 6 [   80/ 1499], train_loss/perplexity = 4.92755127/138.0410767 secs/batch = 0.5728s, grad.norm=0.58328956\n",
      "  9079: 6 [   85/ 1499], train_loss/perplexity = 4.85051680/127.8064194 secs/batch = 0.5989s, grad.norm=0.55750459\n",
      "  9084: 6 [   90/ 1499], train_loss/perplexity = 5.13434458/169.7530212 secs/batch = 0.6147s, grad.norm=0.70796633\n",
      "  9089: 6 [   95/ 1499], train_loss/perplexity = 4.81285095/123.0820160 secs/batch = 0.5545s, grad.norm=0.58429617\n",
      "  9094: 6 [  100/ 1499], train_loss/perplexity = 4.94950199/141.1046753 secs/batch = 0.5688s, grad.norm=0.58708799\n",
      "  9099: 6 [  105/ 1499], train_loss/perplexity = 4.82740974/124.8870544 secs/batch = 0.6630s, grad.norm=0.63240784\n",
      "  9104: 6 [  110/ 1499], train_loss/perplexity = 4.89272738/133.3166809 secs/batch = 0.5985s, grad.norm=0.57844311\n",
      "  9109: 6 [  115/ 1499], train_loss/perplexity = 5.07285118/159.6288147 secs/batch = 0.5813s, grad.norm=0.56361830\n",
      "  9114: 6 [  120/ 1499], train_loss/perplexity = 4.91353750/136.1200867 secs/batch = 0.6237s, grad.norm=0.58049333\n",
      "  9119: 6 [  125/ 1499], train_loss/perplexity = 5.09988260/164.0026550 secs/batch = 0.6502s, grad.norm=0.64672101\n",
      "  9124: 6 [  130/ 1499], train_loss/perplexity = 5.16812181/175.5847473 secs/batch = 0.5995s, grad.norm=0.61593986\n",
      "  9129: 6 [  135/ 1499], train_loss/perplexity = 5.16533613/175.0962982 secs/batch = 0.7329s, grad.norm=0.57481676\n",
      "  9134: 6 [  140/ 1499], train_loss/perplexity = 5.15900040/173.9904480 secs/batch = 0.7169s, grad.norm=0.61888236\n",
      "  9139: 6 [  145/ 1499], train_loss/perplexity = 4.87272167/130.6760864 secs/batch = 0.6391s, grad.norm=0.61136264\n",
      "  9144: 6 [  150/ 1499], train_loss/perplexity = 5.13266230/169.4676971 secs/batch = 0.5936s, grad.norm=0.60851538\n",
      "  9149: 6 [  155/ 1499], train_loss/perplexity = 5.24765682/190.1202545 secs/batch = 0.6234s, grad.norm=0.57671368\n",
      "  9154: 6 [  160/ 1499], train_loss/perplexity = 5.51963711/249.5444641 secs/batch = 0.5794s, grad.norm=0.59842598\n",
      "  9159: 6 [  165/ 1499], train_loss/perplexity = 5.04399872/155.0889282 secs/batch = 0.6115s, grad.norm=0.59448475\n",
      "  9164: 6 [  170/ 1499], train_loss/perplexity = 5.43830585/230.0521088 secs/batch = 0.6692s, grad.norm=0.73405558\n",
      "  9169: 6 [  175/ 1499], train_loss/perplexity = 5.14399576/171.3992767 secs/batch = 0.7400s, grad.norm=0.58766901\n",
      "  9174: 6 [  180/ 1499], train_loss/perplexity = 5.29592848/199.5227966 secs/batch = 0.6865s, grad.norm=0.55428046\n",
      "  9179: 6 [  185/ 1499], train_loss/perplexity = 5.14220762/171.0930634 secs/batch = 0.6936s, grad.norm=0.62960225\n",
      "  9184: 6 [  190/ 1499], train_loss/perplexity = 5.28459883/197.2750244 secs/batch = 0.5996s, grad.norm=0.61323708\n",
      "  9189: 6 [  195/ 1499], train_loss/perplexity = 5.38623190/218.3789520 secs/batch = 0.5782s, grad.norm=0.59692538\n",
      "  9194: 6 [  200/ 1499], train_loss/perplexity = 5.28482723/197.3200836 secs/batch = 0.6140s, grad.norm=0.60516274\n",
      "  9199: 6 [  205/ 1499], train_loss/perplexity = 5.21661139/184.3085785 secs/batch = 0.5952s, grad.norm=0.61532110\n",
      "  9204: 6 [  210/ 1499], train_loss/perplexity = 5.05190611/156.3201447 secs/batch = 0.5686s, grad.norm=0.55835432\n",
      "  9209: 6 [  215/ 1499], train_loss/perplexity = 5.25408936/191.3471527 secs/batch = 0.5997s, grad.norm=0.57475346\n",
      "  9214: 6 [  220/ 1499], train_loss/perplexity = 4.97825003/145.2200317 secs/batch = 0.6008s, grad.norm=0.53837341\n",
      "  9219: 6 [  225/ 1499], train_loss/perplexity = 5.23574734/187.8694611 secs/batch = 0.5633s, grad.norm=0.55211502\n",
      "  9224: 6 [  230/ 1499], train_loss/perplexity = 5.25800371/192.0976257 secs/batch = 0.6090s, grad.norm=0.60512793\n",
      "  9229: 6 [  235/ 1499], train_loss/perplexity = 5.23722935/188.1480865 secs/batch = 0.6402s, grad.norm=0.60054886\n",
      "  9234: 6 [  240/ 1499], train_loss/perplexity = 5.33942699/208.3932648 secs/batch = 0.5993s, grad.norm=0.62737459\n",
      "  9239: 6 [  245/ 1499], train_loss/perplexity = 5.03488159/153.6813965 secs/batch = 0.5623s, grad.norm=0.65486383\n",
      "  9244: 6 [  250/ 1499], train_loss/perplexity = 5.12374210/167.9627228 secs/batch = 0.5797s, grad.norm=0.64399904\n",
      "  9249: 6 [  255/ 1499], train_loss/perplexity = 4.92275524/137.3806152 secs/batch = 0.5813s, grad.norm=0.59265715\n",
      "  9254: 6 [  260/ 1499], train_loss/perplexity = 5.27719784/195.8203888 secs/batch = 0.5433s, grad.norm=0.56067538\n",
      "  9259: 6 [  265/ 1499], train_loss/perplexity = 5.21707106/184.3933105 secs/batch = 0.5469s, grad.norm=0.63285506\n",
      "  9264: 6 [  270/ 1499], train_loss/perplexity = 5.23882198/188.4479828 secs/batch = 0.5504s, grad.norm=0.55484962\n",
      "  9269: 6 [  275/ 1499], train_loss/perplexity = 4.84127617/126.6308517 secs/batch = 0.5478s, grad.norm=0.57612365\n",
      "  9274: 6 [  280/ 1499], train_loss/perplexity = 5.12708235/168.5247040 secs/batch = 0.5451s, grad.norm=0.61130166\n",
      "  9279: 6 [  285/ 1499], train_loss/perplexity = 5.45699120/234.3911285 secs/batch = 0.5514s, grad.norm=0.56726545\n",
      "  9284: 6 [  290/ 1499], train_loss/perplexity = 5.39735603/220.8217926 secs/batch = 0.5674s, grad.norm=0.59636027\n",
      "  9289: 6 [  295/ 1499], train_loss/perplexity = 5.24277544/189.1944733 secs/batch = 0.5510s, grad.norm=0.60609752\n",
      "  9294: 6 [  300/ 1499], train_loss/perplexity = 5.07971859/160.7288208 secs/batch = 0.5490s, grad.norm=0.58372802\n",
      "  9299: 6 [  305/ 1499], train_loss/perplexity = 5.29035187/198.4132233 secs/batch = 0.5441s, grad.norm=0.61084396\n",
      "  9304: 6 [  310/ 1499], train_loss/perplexity = 5.38591528/218.3098297 secs/batch = 0.5443s, grad.norm=0.60481405\n",
      "  9309: 6 [  315/ 1499], train_loss/perplexity = 5.34268856/209.0740662 secs/batch = 0.5582s, grad.norm=0.64601743\n",
      "  9314: 6 [  320/ 1499], train_loss/perplexity = 5.22328472/185.5426331 secs/batch = 0.5488s, grad.norm=0.56067765\n",
      "  9319: 6 [  325/ 1499], train_loss/perplexity = 5.10792685/165.3272552 secs/batch = 0.5484s, grad.norm=0.57835054\n",
      "  9324: 6 [  330/ 1499], train_loss/perplexity = 5.08431816/161.4698029 secs/batch = 0.5428s, grad.norm=0.57063049\n",
      "  9329: 6 [  335/ 1499], train_loss/perplexity = 5.20237875/181.7039490 secs/batch = 0.5506s, grad.norm=0.76552296\n",
      "  9334: 6 [  340/ 1499], train_loss/perplexity = 4.88772202/132.6510468 secs/batch = 0.5526s, grad.norm=0.61207157\n",
      "  9339: 6 [  345/ 1499], train_loss/perplexity = 4.87534523/131.0193787 secs/batch = 0.5437s, grad.norm=0.56031144\n",
      "  9344: 6 [  350/ 1499], train_loss/perplexity = 5.04765797/155.6574860 secs/batch = 0.5680s, grad.norm=0.66745818\n",
      "  9349: 6 [  355/ 1499], train_loss/perplexity = 4.91088247/135.7591705 secs/batch = 0.6485s, grad.norm=0.66624016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9354: 6 [  360/ 1499], train_loss/perplexity = 5.02555895/152.2553406 secs/batch = 0.6530s, grad.norm=0.59763527\n",
      "  9359: 6 [  365/ 1499], train_loss/perplexity = 4.90241385/134.6143188 secs/batch = 0.5995s, grad.norm=0.58407044\n",
      "  9364: 6 [  370/ 1499], train_loss/perplexity = 4.83271646/125.5515518 secs/batch = 0.6121s, grad.norm=0.55696309\n",
      "  9369: 6 [  375/ 1499], train_loss/perplexity = 5.08446169/161.4929810 secs/batch = 0.5455s, grad.norm=0.58723730\n",
      "  9374: 6 [  380/ 1499], train_loss/perplexity = 5.22969913/186.7366180 secs/batch = 0.5430s, grad.norm=0.59261096\n",
      "  9379: 6 [  385/ 1499], train_loss/perplexity = 5.42299318/226.5562286 secs/batch = 0.5418s, grad.norm=0.57980698\n",
      "  9384: 6 [  390/ 1499], train_loss/perplexity = 5.47517395/238.6919861 secs/batch = 0.5437s, grad.norm=0.60198343\n",
      "  9389: 6 [  395/ 1499], train_loss/perplexity = 5.49546099/243.5837860 secs/batch = 0.5507s, grad.norm=0.59051448\n",
      "  9394: 6 [  400/ 1499], train_loss/perplexity = 5.32988262/206.4137421 secs/batch = 0.5364s, grad.norm=0.67273861\n",
      "  9399: 6 [  405/ 1499], train_loss/perplexity = 5.31740189/203.8535614 secs/batch = 0.5350s, grad.norm=0.59979659\n",
      "  9404: 6 [  410/ 1499], train_loss/perplexity = 4.76666212/117.5262985 secs/batch = 0.6012s, grad.norm=0.68511862\n",
      "  9409: 6 [  415/ 1499], train_loss/perplexity = 5.37227869/215.3530273 secs/batch = 0.6221s, grad.norm=0.60934049\n",
      "  9414: 6 [  420/ 1499], train_loss/perplexity = 4.97704554/145.0452118 secs/batch = 0.6287s, grad.norm=0.59423989\n",
      "  9419: 6 [  425/ 1499], train_loss/perplexity = 5.32188702/204.7699280 secs/batch = 0.5949s, grad.norm=0.61101019\n",
      "  9424: 6 [  430/ 1499], train_loss/perplexity = 5.04322052/154.9682922 secs/batch = 0.5453s, grad.norm=0.57015949\n",
      "  9429: 6 [  435/ 1499], train_loss/perplexity = 5.00343704/148.9241333 secs/batch = 0.5926s, grad.norm=0.59394181\n",
      "  9434: 6 [  440/ 1499], train_loss/perplexity = 5.03872061/154.2725067 secs/batch = 0.6226s, grad.norm=0.61352319\n",
      "  9439: 6 [  445/ 1499], train_loss/perplexity = 5.25907946/192.3043823 secs/batch = 0.5712s, grad.norm=0.58707649\n",
      "  9444: 6 [  450/ 1499], train_loss/perplexity = 5.20837688/182.7971191 secs/batch = 0.5397s, grad.norm=0.56552058\n",
      "  9449: 6 [  455/ 1499], train_loss/perplexity = 5.31050396/202.4522247 secs/batch = 0.5570s, grad.norm=0.57789093\n",
      "  9454: 6 [  460/ 1499], train_loss/perplexity = 5.23252916/187.2658234 secs/batch = 0.5569s, grad.norm=0.60967994\n",
      "  9459: 6 [  465/ 1499], train_loss/perplexity = 5.21681213/184.3455811 secs/batch = 0.5502s, grad.norm=0.58236605\n",
      "  9464: 6 [  470/ 1499], train_loss/perplexity = 5.31007004/202.3643951 secs/batch = 0.6214s, grad.norm=0.60380971\n",
      "  9469: 6 [  475/ 1499], train_loss/perplexity = 5.04235744/154.8345947 secs/batch = 0.5621s, grad.norm=0.61001372\n",
      "  9474: 6 [  480/ 1499], train_loss/perplexity = 5.20553541/182.2784424 secs/batch = 0.6444s, grad.norm=0.59410065\n",
      "  9479: 6 [  485/ 1499], train_loss/perplexity = 5.13580370/170.0009003 secs/batch = 0.6498s, grad.norm=0.59812891\n",
      "  9484: 6 [  490/ 1499], train_loss/perplexity = 5.08359289/161.3527374 secs/batch = 0.6337s, grad.norm=0.55855602\n",
      "  9489: 6 [  495/ 1499], train_loss/perplexity = 5.09573603/163.3240051 secs/batch = 0.5873s, grad.norm=0.64888072\n",
      "  9494: 6 [  500/ 1499], train_loss/perplexity = 5.21737766/184.4498596 secs/batch = 0.5439s, grad.norm=0.61599100\n",
      "  9499: 6 [  505/ 1499], train_loss/perplexity = 5.00945520/149.8230896 secs/batch = 0.5434s, grad.norm=0.64747137\n",
      "  9504: 6 [  510/ 1499], train_loss/perplexity = 5.42147636/226.2128448 secs/batch = 0.5485s, grad.norm=0.66469485\n",
      "  9509: 6 [  515/ 1499], train_loss/perplexity = 4.90204191/134.5642700 secs/batch = 0.5451s, grad.norm=0.59333247\n",
      "  9514: 6 [  520/ 1499], train_loss/perplexity = 5.20579720/182.3261719 secs/batch = 0.5465s, grad.norm=0.61366057\n",
      "  9519: 6 [  525/ 1499], train_loss/perplexity = 5.36492825/213.7758942 secs/batch = 0.6043s, grad.norm=0.66518587\n",
      "  9524: 6 [  530/ 1499], train_loss/perplexity = 5.13124561/169.2277832 secs/batch = 0.5552s, grad.norm=0.63267463\n",
      "  9529: 6 [  535/ 1499], train_loss/perplexity = 5.11947060/167.2468109 secs/batch = 0.5474s, grad.norm=0.56481320\n",
      "  9534: 6 [  540/ 1499], train_loss/perplexity = 5.05671310/157.0733795 secs/batch = 0.5474s, grad.norm=0.62989718\n",
      "  9539: 6 [  545/ 1499], train_loss/perplexity = 4.82209539/124.2251205 secs/batch = 0.5440s, grad.norm=0.58049244\n",
      "  9544: 6 [  550/ 1499], train_loss/perplexity = 5.30259228/200.8568115 secs/batch = 0.5590s, grad.norm=0.64280081\n",
      "  9549: 6 [  555/ 1499], train_loss/perplexity = 5.03082085/153.0585938 secs/batch = 0.5520s, grad.norm=0.57830638\n",
      "  9554: 6 [  560/ 1499], train_loss/perplexity = 5.42277575/226.5069733 secs/batch = 0.5483s, grad.norm=0.57904333\n",
      "  9559: 6 [  565/ 1499], train_loss/perplexity = 5.37895298/216.7951660 secs/batch = 0.5433s, grad.norm=0.64090830\n",
      "  9564: 6 [  570/ 1499], train_loss/perplexity = 5.30099010/200.5352631 secs/batch = 0.5425s, grad.norm=0.56096679\n",
      "  9569: 6 [  575/ 1499], train_loss/perplexity = 5.39155388/219.5442657 secs/batch = 0.5421s, grad.norm=0.58043718\n",
      "  9574: 6 [  580/ 1499], train_loss/perplexity = 5.08568096/161.6900024 secs/batch = 0.5440s, grad.norm=0.67850196\n",
      "  9579: 6 [  585/ 1499], train_loss/perplexity = 4.97237635/144.3695526 secs/batch = 0.5429s, grad.norm=0.56596041\n",
      "  9584: 6 [  590/ 1499], train_loss/perplexity = 5.09226370/162.7578735 secs/batch = 0.5463s, grad.norm=0.60908127\n",
      "  9589: 6 [  595/ 1499], train_loss/perplexity = 5.16036558/174.2281342 secs/batch = 0.5496s, grad.norm=0.58005041\n",
      "  9594: 6 [  600/ 1499], train_loss/perplexity = 4.98064327/145.5679932 secs/batch = 0.5433s, grad.norm=0.59591168\n",
      "  9599: 6 [  605/ 1499], train_loss/perplexity = 4.96392250/143.1542206 secs/batch = 0.5478s, grad.norm=0.58432615\n",
      "  9604: 6 [  610/ 1499], train_loss/perplexity = 5.26098251/192.6707001 secs/batch = 0.5549s, grad.norm=0.59511167\n",
      "  9609: 6 [  615/ 1499], train_loss/perplexity = 5.29417515/199.1732635 secs/batch = 0.5523s, grad.norm=0.60232776\n",
      "  9614: 6 [  620/ 1499], train_loss/perplexity = 5.15614796/173.4948578 secs/batch = 0.5523s, grad.norm=0.69743627\n",
      "  9619: 6 [  625/ 1499], train_loss/perplexity = 5.02931118/152.8277130 secs/batch = 0.5516s, grad.norm=0.56368852\n",
      "  9624: 6 [  630/ 1499], train_loss/perplexity = 5.23571730/187.8638153 secs/batch = 0.5593s, grad.norm=0.63648391\n",
      "  9629: 6 [  635/ 1499], train_loss/perplexity = 4.87250662/130.6479950 secs/batch = 0.5533s, grad.norm=0.68155932\n",
      "  9634: 6 [  640/ 1499], train_loss/perplexity = 5.20619440/182.3986053 secs/batch = 0.5554s, grad.norm=0.61465776\n",
      "  9639: 6 [  645/ 1499], train_loss/perplexity = 4.94763613/140.8416443 secs/batch = 0.5473s, grad.norm=0.62786561\n",
      "  9644: 6 [  650/ 1499], train_loss/perplexity = 4.86279440/129.3852539 secs/batch = 0.5417s, grad.norm=0.70691395\n",
      "  9649: 6 [  655/ 1499], train_loss/perplexity = 5.27400255/195.1956787 secs/batch = 0.5474s, grad.norm=0.60872942\n",
      "  9654: 6 [  660/ 1499], train_loss/perplexity = 5.36190653/213.1309052 secs/batch = 0.5504s, grad.norm=0.61697799\n",
      "  9659: 6 [  665/ 1499], train_loss/perplexity = 5.27272797/194.9470520 secs/batch = 0.5508s, grad.norm=0.60141492\n",
      "  9664: 6 [  670/ 1499], train_loss/perplexity = 5.44964647/232.6758881 secs/batch = 0.5531s, grad.norm=0.60520250\n",
      "  9669: 6 [  675/ 1499], train_loss/perplexity = 5.27847719/196.0710754 secs/batch = 0.5445s, grad.norm=0.59487629\n",
      "  9674: 6 [  680/ 1499], train_loss/perplexity = 5.23748112/188.1954651 secs/batch = 0.5443s, grad.norm=0.62080681\n",
      "  9679: 6 [  685/ 1499], train_loss/perplexity = 5.33840132/208.1796265 secs/batch = 0.5488s, grad.norm=0.56467450\n",
      "  9684: 6 [  690/ 1499], train_loss/perplexity = 5.19664669/180.6654053 secs/batch = 0.5503s, grad.norm=0.58380514\n",
      "  9689: 6 [  695/ 1499], train_loss/perplexity = 5.25961018/192.4064789 secs/batch = 0.5469s, grad.norm=0.59519506\n",
      "  9694: 6 [  700/ 1499], train_loss/perplexity = 5.37274361/215.4531708 secs/batch = 0.5438s, grad.norm=0.59621704\n",
      "  9699: 6 [  705/ 1499], train_loss/perplexity = 4.86284971/129.3924103 secs/batch = 0.5441s, grad.norm=0.58521533\n",
      "  9704: 6 [  710/ 1499], train_loss/perplexity = 5.21302509/183.6487732 secs/batch = 0.5429s, grad.norm=0.59596461\n",
      "  9709: 6 [  715/ 1499], train_loss/perplexity = 5.14894056/172.2489014 secs/batch = 0.5573s, grad.norm=0.60152298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9714: 6 [  720/ 1499], train_loss/perplexity = 5.19783974/180.8810730 secs/batch = 0.5545s, grad.norm=0.58000255\n",
      "  9719: 6 [  725/ 1499], train_loss/perplexity = 5.07695198/160.2847595 secs/batch = 0.5522s, grad.norm=0.67617553\n",
      "  9724: 6 [  730/ 1499], train_loss/perplexity = 5.18804884/179.1187286 secs/batch = 0.5518s, grad.norm=0.58100045\n",
      "  9729: 6 [  735/ 1499], train_loss/perplexity = 5.03678942/153.9748688 secs/batch = 0.5383s, grad.norm=0.63856554\n",
      "  9734: 6 [  740/ 1499], train_loss/perplexity = 5.02308512/151.8791504 secs/batch = 0.5469s, grad.norm=0.66486335\n",
      "  9739: 6 [  745/ 1499], train_loss/perplexity = 4.91188574/135.8954315 secs/batch = 0.5524s, grad.norm=0.60005075\n",
      "  9744: 6 [  750/ 1499], train_loss/perplexity = 5.15085030/172.5781708 secs/batch = 0.5434s, grad.norm=0.63332546\n",
      "  9749: 6 [  755/ 1499], train_loss/perplexity = 4.96064043/142.6851501 secs/batch = 0.5435s, grad.norm=0.65111512\n",
      "  9754: 6 [  760/ 1499], train_loss/perplexity = 4.58970451/98.4653320 secs/batch = 0.5448s, grad.norm=0.62926024\n",
      "  9759: 6 [  765/ 1499], train_loss/perplexity = 4.97052956/144.1031799 secs/batch = 0.5480s, grad.norm=0.60784286\n",
      "  9764: 6 [  770/ 1499], train_loss/perplexity = 5.11509132/166.5159912 secs/batch = 0.5483s, grad.norm=0.61015058\n",
      "  9769: 6 [  775/ 1499], train_loss/perplexity = 5.48070049/240.0147705 secs/batch = 0.5499s, grad.norm=0.66839570\n",
      "  9774: 6 [  780/ 1499], train_loss/perplexity = 5.01101208/150.0565338 secs/batch = 0.5421s, grad.norm=0.55539566\n",
      "  9779: 6 [  785/ 1499], train_loss/perplexity = 5.17831850/177.3842926 secs/batch = 0.5503s, grad.norm=0.59595340\n",
      "  9784: 6 [  790/ 1499], train_loss/perplexity = 4.74677992/115.2126923 secs/batch = 0.5423s, grad.norm=0.59561300\n",
      "  9789: 6 [  795/ 1499], train_loss/perplexity = 5.16328764/174.7379913 secs/batch = 0.5513s, grad.norm=0.58304167\n",
      "  9794: 6 [  800/ 1499], train_loss/perplexity = 5.11408424/166.3483734 secs/batch = 0.5442s, grad.norm=0.62450916\n",
      "  9799: 6 [  805/ 1499], train_loss/perplexity = 5.11384392/166.3084106 secs/batch = 0.5417s, grad.norm=0.59253311\n",
      "  9804: 6 [  810/ 1499], train_loss/perplexity = 5.03029013/152.9773865 secs/batch = 0.5418s, grad.norm=0.59881735\n",
      "  9809: 6 [  815/ 1499], train_loss/perplexity = 5.23703909/188.1122894 secs/batch = 0.5474s, grad.norm=0.60796291\n",
      "  9814: 6 [  820/ 1499], train_loss/perplexity = 4.76374292/117.1837158 secs/batch = 0.5456s, grad.norm=0.62548292\n",
      "  9819: 6 [  825/ 1499], train_loss/perplexity = 4.91163778/135.8617401 secs/batch = 0.5524s, grad.norm=0.66897643\n",
      "  9824: 6 [  830/ 1499], train_loss/perplexity = 5.10922766/165.5424500 secs/batch = 0.5492s, grad.norm=0.60066843\n",
      "  9829: 6 [  835/ 1499], train_loss/perplexity = 5.12385273/167.9813080 secs/batch = 0.5410s, grad.norm=0.59642380\n",
      "  9834: 6 [  840/ 1499], train_loss/perplexity = 5.04998302/156.0198212 secs/batch = 0.5433s, grad.norm=0.61521178\n",
      "  9839: 6 [  845/ 1499], train_loss/perplexity = 4.87858820/131.4449615 secs/batch = 0.5474s, grad.norm=0.55820829\n",
      "  9844: 6 [  850/ 1499], train_loss/perplexity = 5.02579403/152.2911377 secs/batch = 0.5472s, grad.norm=0.61198550\n",
      "  9849: 6 [  855/ 1499], train_loss/perplexity = 5.15403557/173.1287537 secs/batch = 0.5454s, grad.norm=0.59148145\n",
      "  9854: 6 [  860/ 1499], train_loss/perplexity = 4.63352680/102.8762512 secs/batch = 0.5436s, grad.norm=0.68727762\n",
      "  9859: 6 [  865/ 1499], train_loss/perplexity = 4.96789503/143.7240295 secs/batch = 0.5459s, grad.norm=0.61185694\n",
      "  9864: 6 [  870/ 1499], train_loss/perplexity = 5.00594425/149.2979889 secs/batch = 0.5430s, grad.norm=0.63077360\n",
      "  9869: 6 [  875/ 1499], train_loss/perplexity = 5.04840469/155.7737579 secs/batch = 0.5441s, grad.norm=0.57746613\n",
      "  9874: 6 [  880/ 1499], train_loss/perplexity = 5.06548119/158.4566650 secs/batch = 0.5494s, grad.norm=0.60708439\n",
      "  9879: 6 [  885/ 1499], train_loss/perplexity = 4.89645195/133.8141632 secs/batch = 0.5484s, grad.norm=0.66156244\n",
      "  9884: 6 [  890/ 1499], train_loss/perplexity = 5.23439503/187.6155701 secs/batch = 0.5490s, grad.norm=0.58291614\n",
      "  9889: 6 [  895/ 1499], train_loss/perplexity = 5.29964256/200.2652130 secs/batch = 0.5475s, grad.norm=0.62114066\n",
      "  9894: 6 [  900/ 1499], train_loss/perplexity = 4.92390776/137.5390320 secs/batch = 0.5471s, grad.norm=0.58297575\n",
      "  9899: 6 [  905/ 1499], train_loss/perplexity = 5.02418900/152.0468903 secs/batch = 0.5425s, grad.norm=0.55330217\n",
      "  9904: 6 [  910/ 1499], train_loss/perplexity = 5.30418730/201.1774445 secs/batch = 0.5430s, grad.norm=0.65675771\n",
      "  9909: 6 [  915/ 1499], train_loss/perplexity = 5.01971245/151.3677673 secs/batch = 0.5459s, grad.norm=0.63722581\n",
      "  9914: 6 [  920/ 1499], train_loss/perplexity = 4.72298670/112.5037689 secs/batch = 0.5478s, grad.norm=0.53717506\n",
      "  9919: 6 [  925/ 1499], train_loss/perplexity = 5.09186935/162.6937103 secs/batch = 0.5461s, grad.norm=0.69355679\n",
      "  9924: 6 [  930/ 1499], train_loss/perplexity = 5.10315371/164.5399933 secs/batch = 0.5473s, grad.norm=0.64644247\n",
      "  9929: 6 [  935/ 1499], train_loss/perplexity = 5.20006275/181.2836151 secs/batch = 0.5966s, grad.norm=0.59520447\n",
      "  9934: 6 [  940/ 1499], train_loss/perplexity = 4.81309462/123.1120148 secs/batch = 0.5885s, grad.norm=0.61238950\n",
      "  9939: 6 [  945/ 1499], train_loss/perplexity = 5.20183659/181.6054688 secs/batch = 0.5404s, grad.norm=0.60409629\n",
      "  9944: 6 [  950/ 1499], train_loss/perplexity = 5.19161272/179.7582245 secs/batch = 0.5417s, grad.norm=0.62867498\n",
      "  9949: 6 [  955/ 1499], train_loss/perplexity = 5.33405685/207.2771606 secs/batch = 0.5476s, grad.norm=0.58409411\n",
      "  9954: 6 [  960/ 1499], train_loss/perplexity = 5.01660490/150.8981171 secs/batch = 0.5410s, grad.norm=0.62716573\n",
      "  9959: 6 [  965/ 1499], train_loss/perplexity = 5.05296278/156.4854126 secs/batch = 0.5520s, grad.norm=0.58690119\n",
      "  9964: 6 [  970/ 1499], train_loss/perplexity = 5.27254152/194.9107056 secs/batch = 0.5498s, grad.norm=0.63325262\n",
      "  9969: 6 [  975/ 1499], train_loss/perplexity = 5.06402063/158.2254028 secs/batch = 0.5417s, grad.norm=0.66483790\n",
      "  9974: 6 [  980/ 1499], train_loss/perplexity = 5.17444420/176.6983795 secs/batch = 0.5466s, grad.norm=0.63999110\n",
      "  9979: 6 [  985/ 1499], train_loss/perplexity = 4.87724257/131.2682037 secs/batch = 0.5464s, grad.norm=0.60333908\n",
      "  9984: 6 [  990/ 1499], train_loss/perplexity = 5.01746941/151.0286255 secs/batch = 0.5548s, grad.norm=0.56185937\n",
      "  9989: 6 [  995/ 1499], train_loss/perplexity = 5.35221815/211.0759735 secs/batch = 0.5462s, grad.norm=0.61094528\n",
      "  9994: 6 [ 1000/ 1499], train_loss/perplexity = 5.07251501/159.5751495 secs/batch = 0.5442s, grad.norm=0.67053860\n",
      "  9999: 6 [ 1005/ 1499], train_loss/perplexity = 5.44686317/232.0291901 secs/batch = 0.5510s, grad.norm=0.57519215\n",
      " 10004: 6 [ 1010/ 1499], train_loss/perplexity = 5.26611757/193.6626129 secs/batch = 0.5452s, grad.norm=0.64473498\n",
      " 10009: 6 [ 1015/ 1499], train_loss/perplexity = 5.05459452/156.7409668 secs/batch = 0.5397s, grad.norm=0.56925118\n",
      " 10014: 6 [ 1020/ 1499], train_loss/perplexity = 5.13038158/169.0816193 secs/batch = 0.5444s, grad.norm=0.62443858\n",
      " 10019: 6 [ 1025/ 1499], train_loss/perplexity = 5.16150045/174.4259796 secs/batch = 0.5446s, grad.norm=0.69999284\n",
      " 10024: 6 [ 1030/ 1499], train_loss/perplexity = 5.29819107/199.9747467 secs/batch = 0.5443s, grad.norm=0.58645433\n",
      " 10029: 6 [ 1035/ 1499], train_loss/perplexity = 5.54932976/257.0652161 secs/batch = 0.5467s, grad.norm=0.73104727\n",
      " 10034: 6 [ 1040/ 1499], train_loss/perplexity = 4.90175295/134.5253906 secs/batch = 0.5589s, grad.norm=0.55425847\n",
      " 10039: 6 [ 1045/ 1499], train_loss/perplexity = 5.07489300/159.9550781 secs/batch = 0.5462s, grad.norm=0.60011446\n",
      " 10044: 6 [ 1050/ 1499], train_loss/perplexity = 4.75640869/116.3274078 secs/batch = 0.5473s, grad.norm=0.62119401\n",
      " 10049: 6 [ 1055/ 1499], train_loss/perplexity = 5.11993408/167.3243408 secs/batch = 0.5496s, grad.norm=0.56437939\n",
      " 10054: 6 [ 1060/ 1499], train_loss/perplexity = 5.32611132/205.6367645 secs/batch = 0.5467s, grad.norm=0.56653368\n",
      " 10059: 6 [ 1065/ 1499], train_loss/perplexity = 4.85966969/128.9815979 secs/batch = 0.5401s, grad.norm=0.58251899\n",
      " 10064: 6 [ 1070/ 1499], train_loss/perplexity = 5.34144640/208.8145142 secs/batch = 0.5472s, grad.norm=0.56491333\n",
      " 10069: 6 [ 1075/ 1499], train_loss/perplexity = 5.18027925/177.7324371 secs/batch = 0.5476s, grad.norm=0.58343303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10074: 6 [ 1080/ 1499], train_loss/perplexity = 5.42584801/227.2039337 secs/batch = 0.5469s, grad.norm=0.58974034\n",
      " 10079: 6 [ 1085/ 1499], train_loss/perplexity = 5.49880028/244.3985443 secs/batch = 0.5438s, grad.norm=0.60411149\n",
      " 10084: 6 [ 1090/ 1499], train_loss/perplexity = 5.06986570/159.1529541 secs/batch = 0.5458s, grad.norm=0.60757893\n",
      " 10089: 6 [ 1095/ 1499], train_loss/perplexity = 5.24184752/189.0189972 secs/batch = 0.5499s, grad.norm=0.58796698\n",
      " 10094: 6 [ 1100/ 1499], train_loss/perplexity = 5.23473549/187.6794586 secs/batch = 0.5497s, grad.norm=0.59616333\n",
      " 10099: 6 [ 1105/ 1499], train_loss/perplexity = 5.01344776/150.4224701 secs/batch = 0.5457s, grad.norm=0.66058028\n",
      " 10104: 6 [ 1110/ 1499], train_loss/perplexity = 5.23171520/187.1134644 secs/batch = 0.5905s, grad.norm=0.62006414\n",
      " 10109: 6 [ 1115/ 1499], train_loss/perplexity = 4.80377817/121.9703751 secs/batch = 0.5454s, grad.norm=0.58348894\n",
      " 10114: 6 [ 1120/ 1499], train_loss/perplexity = 5.03976011/154.4329681 secs/batch = 0.5405s, grad.norm=0.60352385\n",
      " 10119: 6 [ 1125/ 1499], train_loss/perplexity = 4.98225784/145.8032074 secs/batch = 0.5454s, grad.norm=0.62016541\n",
      " 10124: 6 [ 1130/ 1499], train_loss/perplexity = 4.78998375/120.2994156 secs/batch = 0.5418s, grad.norm=0.56265301\n",
      " 10129: 6 [ 1135/ 1499], train_loss/perplexity = 5.43597937/229.5175171 secs/batch = 0.5469s, grad.norm=0.58866477\n",
      " 10134: 6 [ 1140/ 1499], train_loss/perplexity = 5.25263786/191.0696259 secs/batch = 0.5432s, grad.norm=0.68275708\n",
      " 10139: 6 [ 1145/ 1499], train_loss/perplexity = 5.33879280/208.2611389 secs/batch = 0.5451s, grad.norm=0.62024248\n",
      " 10144: 6 [ 1150/ 1499], train_loss/perplexity = 5.29124737/198.5909882 secs/batch = 0.5493s, grad.norm=0.57962197\n",
      " 10149: 6 [ 1155/ 1499], train_loss/perplexity = 4.95992613/142.5832672 secs/batch = 0.5444s, grad.norm=0.61439210\n",
      " 10154: 6 [ 1160/ 1499], train_loss/perplexity = 5.15699482/173.6418457 secs/batch = 0.5410s, grad.norm=0.57034421\n",
      " 10159: 6 [ 1165/ 1499], train_loss/perplexity = 5.07762575/160.3927917 secs/batch = 0.5380s, grad.norm=0.60443175\n",
      " 10164: 6 [ 1170/ 1499], train_loss/perplexity = 5.19797421/180.9053955 secs/batch = 0.5407s, grad.norm=0.62285185\n",
      " 10169: 6 [ 1175/ 1499], train_loss/perplexity = 4.62147093/101.6434326 secs/batch = 0.5452s, grad.norm=0.69190705\n",
      " 10174: 6 [ 1180/ 1499], train_loss/perplexity = 4.90390253/134.8148804 secs/batch = 0.5425s, grad.norm=0.64406496\n",
      " 10179: 6 [ 1185/ 1499], train_loss/perplexity = 4.87973976/131.5964203 secs/batch = 0.5420s, grad.norm=0.60161638\n",
      " 10184: 6 [ 1190/ 1499], train_loss/perplexity = 5.08490515/161.5646210 secs/batch = 0.5542s, grad.norm=0.64042962\n",
      " 10189: 6 [ 1195/ 1499], train_loss/perplexity = 5.08789539/162.0484619 secs/batch = 0.5467s, grad.norm=0.60759097\n",
      " 10194: 6 [ 1200/ 1499], train_loss/perplexity = 4.95692253/142.1556396 secs/batch = 0.5380s, grad.norm=0.63848615\n",
      " 10199: 6 [ 1205/ 1499], train_loss/perplexity = 4.91643572/136.5151672 secs/batch = 0.5482s, grad.norm=0.59757245\n",
      " 10204: 6 [ 1210/ 1499], train_loss/perplexity = 4.85561895/128.4601746 secs/batch = 0.5439s, grad.norm=0.65848309\n",
      " 10209: 6 [ 1215/ 1499], train_loss/perplexity = 4.59735966/99.2219925 secs/batch = 0.5397s, grad.norm=0.58685720\n",
      " 10214: 6 [ 1220/ 1499], train_loss/perplexity = 4.99190140/147.2160797 secs/batch = 0.5432s, grad.norm=0.58469820\n",
      " 10219: 6 [ 1225/ 1499], train_loss/perplexity = 4.41345358/82.5540771 secs/batch = 0.5478s, grad.norm=0.64513856\n",
      " 10224: 6 [ 1230/ 1499], train_loss/perplexity = 4.94465733/140.4227295 secs/batch = 0.5403s, grad.norm=0.59946704\n",
      " 10229: 6 [ 1235/ 1499], train_loss/perplexity = 4.79722786/121.1740417 secs/batch = 0.5405s, grad.norm=0.76399589\n",
      " 10234: 6 [ 1240/ 1499], train_loss/perplexity = 4.87277079/130.6825104 secs/batch = 0.5446s, grad.norm=0.61020112\n",
      " 10239: 6 [ 1245/ 1499], train_loss/perplexity = 5.17857265/177.4293823 secs/batch = 0.5430s, grad.norm=0.59774441\n",
      " 10244: 6 [ 1250/ 1499], train_loss/perplexity = 5.32423878/205.2520599 secs/batch = 0.5452s, grad.norm=0.61390233\n",
      " 10249: 6 [ 1255/ 1499], train_loss/perplexity = 4.99684095/147.9450531 secs/batch = 0.5452s, grad.norm=0.62561744\n",
      " 10254: 6 [ 1260/ 1499], train_loss/perplexity = 5.11145496/165.9115753 secs/batch = 0.5504s, grad.norm=0.57815522\n",
      " 10259: 6 [ 1265/ 1499], train_loss/perplexity = 5.15423632/173.1635132 secs/batch = 0.5472s, grad.norm=0.61663699\n",
      " 10264: 6 [ 1270/ 1499], train_loss/perplexity = 5.07631683/160.1829834 secs/batch = 0.5449s, grad.norm=0.58447206\n",
      " 10269: 6 [ 1275/ 1499], train_loss/perplexity = 5.14673996/171.8702698 secs/batch = 0.5424s, grad.norm=0.59925407\n",
      " 10274: 6 [ 1280/ 1499], train_loss/perplexity = 4.73354912/113.6983795 secs/batch = 0.5452s, grad.norm=0.57103807\n",
      " 10279: 6 [ 1285/ 1499], train_loss/perplexity = 5.23667240/188.0433350 secs/batch = 0.5513s, grad.norm=0.59364152\n",
      " 10284: 6 [ 1290/ 1499], train_loss/perplexity = 5.04019785/154.5005798 secs/batch = 0.5478s, grad.norm=0.58521324\n",
      " 10289: 6 [ 1295/ 1499], train_loss/perplexity = 5.03701305/154.0093079 secs/batch = 0.5430s, grad.norm=0.62236220\n",
      " 10294: 6 [ 1300/ 1499], train_loss/perplexity = 5.16628838/175.2631226 secs/batch = 0.5438s, grad.norm=0.65217239\n",
      " 10299: 6 [ 1305/ 1499], train_loss/perplexity = 5.07460499/159.9090118 secs/batch = 0.5423s, grad.norm=0.57361841\n",
      " 10304: 6 [ 1310/ 1499], train_loss/perplexity = 5.22296095/185.4825745 secs/batch = 0.5505s, grad.norm=0.61528772\n",
      " 10309: 6 [ 1315/ 1499], train_loss/perplexity = 5.01336670/150.4102783 secs/batch = 0.5493s, grad.norm=0.60052699\n",
      " 10314: 6 [ 1320/ 1499], train_loss/perplexity = 4.74343586/114.8280563 secs/batch = 0.5389s, grad.norm=0.58489728\n",
      " 10319: 6 [ 1325/ 1499], train_loss/perplexity = 5.08064461/160.8777313 secs/batch = 0.5509s, grad.norm=0.58104199\n",
      " 10324: 6 [ 1330/ 1499], train_loss/perplexity = 5.21406269/183.8394318 secs/batch = 0.5952s, grad.norm=0.57008010\n",
      " 10329: 6 [ 1335/ 1499], train_loss/perplexity = 5.05883598/157.4071808 secs/batch = 0.5417s, grad.norm=0.61624491\n",
      " 10334: 6 [ 1340/ 1499], train_loss/perplexity = 4.52945185/92.7077255 secs/batch = 0.5411s, grad.norm=0.62280142\n",
      " 10339: 6 [ 1345/ 1499], train_loss/perplexity = 4.70181894/110.1473389 secs/batch = 0.5464s, grad.norm=0.65028346\n",
      " 10344: 6 [ 1350/ 1499], train_loss/perplexity = 4.88271666/131.9887390 secs/batch = 0.5447s, grad.norm=0.60284251\n",
      " 10349: 6 [ 1355/ 1499], train_loss/perplexity = 4.55176020/94.7991257 secs/batch = 0.5412s, grad.norm=0.63612086\n",
      " 10354: 6 [ 1360/ 1499], train_loss/perplexity = 4.73900080/114.3199158 secs/batch = 0.5441s, grad.norm=0.56987852\n",
      " 10359: 6 [ 1365/ 1499], train_loss/perplexity = 4.50666761/90.6193390 secs/batch = 0.5509s, grad.norm=0.60412955\n",
      " 10364: 6 [ 1370/ 1499], train_loss/perplexity = 4.54757929/94.4036102 secs/batch = 0.5465s, grad.norm=0.58541179\n",
      " 10369: 6 [ 1375/ 1499], train_loss/perplexity = 4.62057543/101.5524521 secs/batch = 0.5446s, grad.norm=0.61221260\n",
      " 10374: 6 [ 1380/ 1499], train_loss/perplexity = 5.11450052/166.4176331 secs/batch = 0.5475s, grad.norm=0.62153983\n",
      " 10379: 6 [ 1385/ 1499], train_loss/perplexity = 4.80799389/122.4856491 secs/batch = 0.5454s, grad.norm=0.63601428\n",
      " 10384: 6 [ 1390/ 1499], train_loss/perplexity = 4.89636421/133.8024139 secs/batch = 0.5425s, grad.norm=0.60741025\n",
      " 10389: 6 [ 1395/ 1499], train_loss/perplexity = 5.09982586/163.9933472 secs/batch = 0.5420s, grad.norm=0.59521365\n",
      " 10394: 6 [ 1400/ 1499], train_loss/perplexity = 5.13249016/169.4385223 secs/batch = 0.5451s, grad.norm=0.58696079\n",
      " 10399: 6 [ 1405/ 1499], train_loss/perplexity = 4.96532011/143.3544312 secs/batch = 0.5463s, grad.norm=0.60737348\n",
      " 10404: 6 [ 1410/ 1499], train_loss/perplexity = 5.10620451/165.0427399 secs/batch = 0.5472s, grad.norm=0.59967828\n",
      " 10409: 6 [ 1415/ 1499], train_loss/perplexity = 5.09527349/163.2484894 secs/batch = 0.5473s, grad.norm=0.64137268\n",
      " 10414: 6 [ 1420/ 1499], train_loss/perplexity = 5.04652739/155.4815979 secs/batch = 0.5573s, grad.norm=0.58286726\n",
      " 10419: 6 [ 1425/ 1499], train_loss/perplexity = 4.96532106/143.3545685 secs/batch = 0.5548s, grad.norm=0.60244268\n",
      " 10424: 6 [ 1430/ 1499], train_loss/perplexity = 5.18430471/178.4493408 secs/batch = 0.5404s, grad.norm=0.64412868\n",
      " 10429: 6 [ 1435/ 1499], train_loss/perplexity = 4.85376072/128.2216949 secs/batch = 0.5467s, grad.norm=0.57818782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10434: 6 [ 1440/ 1499], train_loss/perplexity = 4.57235956/96.7721786 secs/batch = 0.5427s, grad.norm=0.57906282\n",
      " 10439: 6 [ 1445/ 1499], train_loss/perplexity = 5.00587559/149.2877350 secs/batch = 0.5448s, grad.norm=0.60875076\n",
      " 10444: 6 [ 1450/ 1499], train_loss/perplexity = 5.13612747/170.0559387 secs/batch = 0.5407s, grad.norm=0.64320487\n",
      " 10449: 6 [ 1455/ 1499], train_loss/perplexity = 5.39197254/219.6362000 secs/batch = 0.5389s, grad.norm=0.60496610\n",
      " 10454: 6 [ 1460/ 1499], train_loss/perplexity = 5.30281878/200.9023132 secs/batch = 0.5452s, grad.norm=0.59959638\n",
      " 10459: 6 [ 1465/ 1499], train_loss/perplexity = 5.44595575/231.8187408 secs/batch = 0.5443s, grad.norm=0.62366259\n",
      " 10464: 6 [ 1470/ 1499], train_loss/perplexity = 5.16052151/174.2553101 secs/batch = 0.5463s, grad.norm=0.61410260\n",
      " 10469: 6 [ 1475/ 1499], train_loss/perplexity = 5.23841095/188.3705292 secs/batch = 0.5512s, grad.norm=0.60116512\n",
      " 10474: 6 [ 1480/ 1499], train_loss/perplexity = 5.17946434/177.5876617 secs/batch = 0.5470s, grad.norm=0.61470658\n",
      " 10479: 6 [ 1485/ 1499], train_loss/perplexity = 4.90440416/134.8825226 secs/batch = 0.5362s, grad.norm=0.62242854\n",
      " 10484: 6 [ 1490/ 1499], train_loss/perplexity = 5.09154320/162.6406555 secs/batch = 0.5441s, grad.norm=0.59845537\n",
      " 10489: 6 [ 1495/ 1499], train_loss/perplexity = 5.35650539/211.9828491 secs/batch = 0.5425s, grad.norm=0.57752669\n",
      "Epoch training time: 842.7173111438751\n",
      "Saved char model cv/epoch006_5.1674.model\n",
      " 10498: 7 [    5/ 1499], train_loss/perplexity = 5.17690802/177.1342621 secs/batch = 0.5400s, grad.norm=0.58023071\n",
      " 10503: 7 [   10/ 1499], train_loss/perplexity = 5.17775726/177.2847595 secs/batch = 0.5438s, grad.norm=0.58103722\n",
      " 10508: 7 [   15/ 1499], train_loss/perplexity = 5.05256844/156.4237061 secs/batch = 0.5417s, grad.norm=0.61003959\n",
      " 10513: 7 [   20/ 1499], train_loss/perplexity = 4.90632915/135.1424103 secs/batch = 0.5467s, grad.norm=0.56039143\n",
      " 10518: 7 [   25/ 1499], train_loss/perplexity = 5.35968685/212.6583405 secs/batch = 0.5458s, grad.norm=0.63502538\n",
      " 10523: 7 [   30/ 1499], train_loss/perplexity = 5.26041555/192.5614929 secs/batch = 0.5561s, grad.norm=0.63736475\n",
      " 10528: 7 [   35/ 1499], train_loss/perplexity = 5.17270517/176.3913574 secs/batch = 0.5456s, grad.norm=0.59820336\n",
      " 10533: 7 [   40/ 1499], train_loss/perplexity = 5.21430302/183.8836060 secs/batch = 0.5481s, grad.norm=0.59818149\n",
      " 10538: 7 [   45/ 1499], train_loss/perplexity = 5.05866861/157.3808441 secs/batch = 0.5513s, grad.norm=0.59697455\n",
      " 10543: 7 [   50/ 1499], train_loss/perplexity = 5.05212831/156.3548889 secs/batch = 0.5476s, grad.norm=0.62399179\n",
      " 10548: 7 [   55/ 1499], train_loss/perplexity = 4.87490702/130.9619751 secs/batch = 0.5473s, grad.norm=0.61530423\n",
      " 10553: 7 [   60/ 1499], train_loss/perplexity = 4.85116482/127.8892746 secs/batch = 0.5418s, grad.norm=0.58270901\n",
      " 10558: 7 [   65/ 1499], train_loss/perplexity = 4.87227488/130.6177216 secs/batch = 0.5424s, grad.norm=0.59373194\n",
      " 10563: 7 [   70/ 1499], train_loss/perplexity = 4.89718485/133.9122620 secs/batch = 0.5476s, grad.norm=0.61491907\n",
      " 10568: 7 [   75/ 1499], train_loss/perplexity = 4.71664810/111.7929077 secs/batch = 0.5497s, grad.norm=0.61844951\n",
      " 10573: 7 [   80/ 1499], train_loss/perplexity = 4.83180380/125.4370193 secs/batch = 0.5397s, grad.norm=0.58447802\n",
      " 10578: 7 [   85/ 1499], train_loss/perplexity = 4.70387650/110.3742065 secs/batch = 0.5552s, grad.norm=0.60646588\n",
      " 10583: 7 [   90/ 1499], train_loss/perplexity = 4.97103548/144.1761017 secs/batch = 0.5510s, grad.norm=0.61542380\n",
      " 10588: 7 [   95/ 1499], train_loss/perplexity = 4.73327971/113.6677475 secs/batch = 0.5485s, grad.norm=0.63074696\n",
      " 10593: 7 [  100/ 1499], train_loss/perplexity = 4.81972694/123.9312439 secs/batch = 0.5886s, grad.norm=0.57977873\n",
      " 10598: 7 [  105/ 1499], train_loss/perplexity = 4.71041822/111.0986176 secs/batch = 0.5481s, grad.norm=0.65538681\n",
      " 10603: 7 [  110/ 1499], train_loss/perplexity = 4.78492451/119.6923294 secs/batch = 0.5473s, grad.norm=0.60426164\n",
      " 10608: 7 [  115/ 1499], train_loss/perplexity = 4.97790051/145.1692810 secs/batch = 0.5455s, grad.norm=0.60609603\n",
      " 10613: 7 [  120/ 1499], train_loss/perplexity = 4.80157566/121.7020264 secs/batch = 0.5465s, grad.norm=0.58257830\n",
      " 10618: 7 [  125/ 1499], train_loss/perplexity = 4.97028637/144.0681458 secs/batch = 0.5928s, grad.norm=0.62561083\n",
      " 10623: 7 [  130/ 1499], train_loss/perplexity = 5.03325796/153.4320679 secs/batch = 0.5513s, grad.norm=0.62265432\n",
      " 10628: 7 [  135/ 1499], train_loss/perplexity = 5.07133675/159.3872528 secs/batch = 0.5638s, grad.norm=0.62794900\n",
      " 10633: 7 [  140/ 1499], train_loss/perplexity = 5.09359741/162.9750977 secs/batch = 0.5484s, grad.norm=0.65775824\n",
      " 10638: 7 [  145/ 1499], train_loss/perplexity = 4.76684904/117.5482712 secs/batch = 0.5590s, grad.norm=0.65457296\n",
      " 10643: 7 [  150/ 1499], train_loss/perplexity = 4.96903133/143.8874359 secs/batch = 0.5533s, grad.norm=0.60736632\n",
      " 10648: 7 [  155/ 1499], train_loss/perplexity = 5.13181496/169.3241577 secs/batch = 0.5473s, grad.norm=0.58762640\n",
      " 10653: 7 [  160/ 1499], train_loss/perplexity = 5.39052057/219.3175201 secs/batch = 0.5642s, grad.norm=0.60988164\n",
      " 10658: 7 [  165/ 1499], train_loss/perplexity = 4.98881531/146.7624512 secs/batch = 0.5662s, grad.norm=0.66173559\n",
      " 10663: 7 [  170/ 1499], train_loss/perplexity = 5.27512360/195.4146271 secs/batch = 0.5997s, grad.norm=0.60927999\n",
      " 10668: 7 [  175/ 1499], train_loss/perplexity = 5.03619146/153.8828278 secs/batch = 0.5568s, grad.norm=0.59919292\n",
      " 10673: 7 [  180/ 1499], train_loss/perplexity = 5.22255516/185.4073181 secs/batch = 0.5584s, grad.norm=0.60736459\n",
      " 10678: 7 [  185/ 1499], train_loss/perplexity = 5.04320717/154.9662170 secs/batch = 0.6353s, grad.norm=0.62761444\n",
      " 10683: 7 [  190/ 1499], train_loss/perplexity = 5.14595270/171.7350159 secs/batch = 0.5741s, grad.norm=0.62810254\n",
      " 10688: 7 [  195/ 1499], train_loss/perplexity = 5.26316595/193.0918427 secs/batch = 0.5482s, grad.norm=0.62441319\n",
      " 10693: 7 [  200/ 1499], train_loss/perplexity = 5.18378830/178.3572083 secs/batch = 0.6348s, grad.norm=0.64653260\n",
      " 10698: 7 [  205/ 1499], train_loss/perplexity = 5.09272242/162.8325500 secs/batch = 0.5646s, grad.norm=0.58667713\n",
      " 10703: 7 [  210/ 1499], train_loss/perplexity = 4.95797014/142.3046417 secs/batch = 0.6192s, grad.norm=0.57037276\n",
      " 10708: 7 [  215/ 1499], train_loss/perplexity = 5.14256620/171.1544189 secs/batch = 0.5543s, grad.norm=0.59530044\n",
      " 10713: 7 [  220/ 1499], train_loss/perplexity = 4.87216187/130.6029510 secs/batch = 0.5799s, grad.norm=0.53694922\n",
      " 10718: 7 [  225/ 1499], train_loss/perplexity = 5.13244772/169.4313354 secs/batch = 0.6055s, grad.norm=0.57700402\n",
      " 10723: 7 [  230/ 1499], train_loss/perplexity = 5.19204950/179.8367462 secs/batch = 0.5489s, grad.norm=0.63511038\n",
      " 10728: 7 [  235/ 1499], train_loss/perplexity = 5.17348719/176.5293579 secs/batch = 0.5681s, grad.norm=0.59490597\n",
      " 10733: 7 [  240/ 1499], train_loss/perplexity = 5.19703579/180.7357025 secs/batch = 0.6067s, grad.norm=0.60493016\n",
      " 10738: 7 [  245/ 1499], train_loss/perplexity = 4.92470407/137.6486053 secs/batch = 0.6032s, grad.norm=0.72350293\n",
      " 10743: 7 [  250/ 1499], train_loss/perplexity = 4.98182869/145.7406464 secs/batch = 0.6114s, grad.norm=0.63770455\n",
      " 10748: 7 [  255/ 1499], train_loss/perplexity = 4.81238604/123.0248108 secs/batch = 0.6433s, grad.norm=0.59975779\n",
      " 10753: 7 [  260/ 1499], train_loss/perplexity = 5.19182873/179.7970581 secs/batch = 0.6678s, grad.norm=0.59659243\n",
      " 10758: 7 [  265/ 1499], train_loss/perplexity = 5.07088280/159.3149109 secs/batch = 0.5800s, grad.norm=0.61541367\n",
      " 10763: 7 [  270/ 1499], train_loss/perplexity = 5.14895201/172.2508850 secs/batch = 0.6267s, grad.norm=0.57573640\n",
      " 10768: 7 [  275/ 1499], train_loss/perplexity = 4.71809912/111.9552383 secs/batch = 0.6408s, grad.norm=0.59054917\n",
      " 10773: 7 [  280/ 1499], train_loss/perplexity = 5.06153345/157.8323517 secs/batch = 0.6408s, grad.norm=0.64307845\n",
      " 10778: 7 [  285/ 1499], train_loss/perplexity = 5.31387758/203.1363831 secs/batch = 0.6421s, grad.norm=0.58841008\n",
      " 10783: 7 [  290/ 1499], train_loss/perplexity = 5.29353285/199.0453796 secs/batch = 0.6477s, grad.norm=0.61736226\n",
      " 10788: 7 [  295/ 1499], train_loss/perplexity = 5.15962553/174.0992432 secs/batch = 0.5943s, grad.norm=0.62471622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10793: 7 [  300/ 1499], train_loss/perplexity = 4.95674372/142.1302185 secs/batch = 0.6709s, grad.norm=0.59676111\n",
      " 10798: 7 [  305/ 1499], train_loss/perplexity = 5.19120359/179.6846924 secs/batch = 0.6739s, grad.norm=0.63592470\n",
      " 10803: 7 [  310/ 1499], train_loss/perplexity = 5.26651907/193.7403870 secs/batch = 0.5531s, grad.norm=0.59272313\n",
      " 10808: 7 [  315/ 1499], train_loss/perplexity = 5.20410013/182.0170135 secs/batch = 0.6011s, grad.norm=0.64283067\n",
      " 10813: 7 [  320/ 1499], train_loss/perplexity = 5.10217619/164.3792419 secs/batch = 0.5859s, grad.norm=0.61057103\n",
      " 10818: 7 [  325/ 1499], train_loss/perplexity = 5.02556562/152.2563477 secs/batch = 0.5472s, grad.norm=0.59819192\n",
      " 10823: 7 [  330/ 1499], train_loss/perplexity = 4.95898581/142.4492493 secs/batch = 0.6303s, grad.norm=0.59067947\n",
      " 10828: 7 [  335/ 1499], train_loss/perplexity = 5.04265642/154.8809052 secs/batch = 0.6709s, grad.norm=0.63386381\n",
      " 10833: 7 [  340/ 1499], train_loss/perplexity = 4.77178383/118.1297760 secs/batch = 0.6379s, grad.norm=0.62543672\n",
      " 10838: 7 [  345/ 1499], train_loss/perplexity = 4.77620363/118.6530457 secs/batch = 0.6658s, grad.norm=0.62901038\n",
      " 10843: 7 [  350/ 1499], train_loss/perplexity = 4.91149712/135.8426361 secs/batch = 0.5863s, grad.norm=0.67737758\n",
      " 10848: 7 [  355/ 1499], train_loss/perplexity = 4.75381422/116.0259933 secs/batch = 0.7406s, grad.norm=0.65910190\n",
      " 10853: 7 [  360/ 1499], train_loss/perplexity = 4.86155033/129.2243805 secs/batch = 0.6686s, grad.norm=0.60567820\n",
      " 10858: 7 [  365/ 1499], train_loss/perplexity = 4.78440523/119.6301880 secs/batch = 0.6953s, grad.norm=0.60971248\n",
      " 10863: 7 [  370/ 1499], train_loss/perplexity = 4.72699261/112.9553528 secs/batch = 0.6335s, grad.norm=0.59946805\n",
      " 10868: 7 [  375/ 1499], train_loss/perplexity = 4.96777105/143.7062225 secs/batch = 0.6118s, grad.norm=0.58388394\n",
      " 10873: 7 [  380/ 1499], train_loss/perplexity = 5.16661930/175.3211212 secs/batch = 0.6827s, grad.norm=0.60971034\n",
      " 10878: 7 [  385/ 1499], train_loss/perplexity = 5.30848980/202.0448761 secs/batch = 0.6094s, grad.norm=0.59068596\n",
      " 10883: 7 [  390/ 1499], train_loss/perplexity = 5.36447382/213.6787720 secs/batch = 0.6287s, grad.norm=0.62755626\n",
      " 10888: 7 [  395/ 1499], train_loss/perplexity = 5.41089630/223.8321228 secs/batch = 0.5824s, grad.norm=0.70054436\n",
      " 10893: 7 [  400/ 1499], train_loss/perplexity = 5.18683243/178.9009705 secs/batch = 0.6709s, grad.norm=0.60129821\n",
      " 10898: 7 [  405/ 1499], train_loss/perplexity = 5.21887016/184.7253571 secs/batch = 0.6246s, grad.norm=0.58454603\n",
      " 10903: 7 [  410/ 1499], train_loss/perplexity = 4.62275362/101.7738953 secs/batch = 0.7120s, grad.norm=0.60600930\n",
      " 10908: 7 [  415/ 1499], train_loss/perplexity = 5.22901344/186.6086121 secs/batch = 0.7080s, grad.norm=0.58360255\n",
      " 10913: 7 [  420/ 1499], train_loss/perplexity = 4.88747025/132.6176605 secs/batch = 0.6562s, grad.norm=0.60853785\n",
      " 10918: 7 [  425/ 1499], train_loss/perplexity = 5.15183306/172.7478638 secs/batch = 0.6057s, grad.norm=0.61674863\n",
      " 10923: 7 [  430/ 1499], train_loss/perplexity = 4.97093964/144.1622772 secs/batch = 0.7128s, grad.norm=0.57645208\n",
      " 10928: 7 [  435/ 1499], train_loss/perplexity = 4.86667538/129.8883667 secs/batch = 0.6058s, grad.norm=0.61035985\n",
      " 10933: 7 [  440/ 1499], train_loss/perplexity = 4.87106085/130.4592438 secs/batch = 0.5949s, grad.norm=0.65680081\n",
      " 10938: 7 [  445/ 1499], train_loss/perplexity = 5.16566849/175.1545105 secs/batch = 0.6516s, grad.norm=0.64614028\n",
      " 10943: 7 [  450/ 1499], train_loss/perplexity = 5.12071657/167.4553223 secs/batch = 0.6220s, grad.norm=0.58506340\n",
      " 10948: 7 [  455/ 1499], train_loss/perplexity = 5.16805983/175.5738678 secs/batch = 0.6528s, grad.norm=0.59350520\n",
      " 10953: 7 [  460/ 1499], train_loss/perplexity = 5.10078382/164.1505280 secs/batch = 0.6095s, grad.norm=0.63261032\n",
      " 10958: 7 [  465/ 1499], train_loss/perplexity = 5.09538889/163.2673187 secs/batch = 0.6111s, grad.norm=0.62807345\n",
      " 10963: 7 [  470/ 1499], train_loss/perplexity = 5.22295570/185.4815979 secs/batch = 0.6632s, grad.norm=0.68165541\n",
      " 10968: 7 [  475/ 1499], train_loss/perplexity = 4.92981291/138.3536224 secs/batch = 0.6198s, grad.norm=0.60728431\n",
      " 10973: 7 [  480/ 1499], train_loss/perplexity = 5.07928514/160.6591644 secs/batch = 0.7381s, grad.norm=0.62221491\n",
      " 10978: 7 [  485/ 1499], train_loss/perplexity = 4.99777365/148.0830994 secs/batch = 0.5775s, grad.norm=0.59972614\n",
      " 10983: 7 [  490/ 1499], train_loss/perplexity = 5.03986597/154.4493103 secs/batch = 0.7261s, grad.norm=0.58791941\n",
      " 10988: 7 [  495/ 1499], train_loss/perplexity = 5.02538967/152.2295685 secs/batch = 0.6964s, grad.norm=0.63096762\n",
      " 10993: 7 [  500/ 1499], train_loss/perplexity = 5.06097698/157.7445526 secs/batch = 0.6358s, grad.norm=0.64058608\n",
      " 10998: 7 [  505/ 1499], train_loss/perplexity = 4.92111063/137.1548615 secs/batch = 0.6736s, grad.norm=0.63260418\n",
      " 11003: 7 [  510/ 1499], train_loss/perplexity = 5.28431320/197.2186890 secs/batch = 0.6190s, grad.norm=0.61981320\n",
      " 11008: 7 [  515/ 1499], train_loss/perplexity = 4.80705261/122.3704147 secs/batch = 0.6044s, grad.norm=0.63745564\n",
      " 11013: 7 [  520/ 1499], train_loss/perplexity = 5.08445787/161.4923706 secs/batch = 0.6207s, grad.norm=0.58701080\n",
      " 11018: 7 [  525/ 1499], train_loss/perplexity = 5.24995375/190.5574493 secs/batch = 0.6080s, grad.norm=0.62425554\n",
      " 11023: 7 [  530/ 1499], train_loss/perplexity = 5.00216675/148.7350769 secs/batch = 0.5929s, grad.norm=0.63084024\n",
      " 11028: 7 [  535/ 1499], train_loss/perplexity = 5.01656199/150.8916473 secs/batch = 0.5752s, grad.norm=0.60051477\n",
      " 11033: 7 [  540/ 1499], train_loss/perplexity = 4.92444849/137.6134186 secs/batch = 0.6069s, grad.norm=0.61808735\n",
      " 11038: 7 [  545/ 1499], train_loss/perplexity = 4.72053480/112.2282562 secs/batch = 0.6079s, grad.norm=0.59209031\n",
      " 11043: 7 [  550/ 1499], train_loss/perplexity = 5.19262791/179.9407959 secs/batch = 0.6205s, grad.norm=0.63911706\n",
      " 11048: 7 [  555/ 1499], train_loss/perplexity = 4.95099068/141.3148956 secs/batch = 0.5938s, grad.norm=0.59448075\n",
      " 11053: 7 [  560/ 1499], train_loss/perplexity = 5.28464556/197.2842407 secs/batch = 0.5765s, grad.norm=0.55880374\n",
      " 11058: 7 [  565/ 1499], train_loss/perplexity = 5.30369759/201.0789490 secs/batch = 1.0925s, grad.norm=0.62223017\n",
      " 11063: 7 [  570/ 1499], train_loss/perplexity = 5.20396852/181.9930573 secs/batch = 0.7073s, grad.norm=0.59104252\n",
      " 11068: 7 [  575/ 1499], train_loss/perplexity = 5.27155018/194.7175751 secs/batch = 0.5919s, grad.norm=0.60067123\n",
      " 11073: 7 [  580/ 1499], train_loss/perplexity = 5.01960802/151.3519592 secs/batch = 0.5779s, grad.norm=0.77143359\n",
      " 11078: 7 [  585/ 1499], train_loss/perplexity = 4.85585165/128.4900665 secs/batch = 0.5594s, grad.norm=0.59178615\n",
      " 11083: 7 [  590/ 1499], train_loss/perplexity = 5.02499485/152.1694794 secs/batch = 0.5410s, grad.norm=0.64174432\n",
      " 11088: 7 [  595/ 1499], train_loss/perplexity = 5.06562042/158.4787292 secs/batch = 0.5562s, grad.norm=0.57975161\n",
      " 11093: 7 [  600/ 1499], train_loss/perplexity = 4.89165163/133.1733398 secs/batch = 0.5438s, grad.norm=0.62294841\n",
      " 11098: 7 [  605/ 1499], train_loss/perplexity = 4.83482075/125.8160248 secs/batch = 0.5500s, grad.norm=0.63670331\n",
      " 11103: 7 [  610/ 1499], train_loss/perplexity = 5.15106821/172.6157837 secs/batch = 0.5544s, grad.norm=0.57927346\n",
      " 11108: 7 [  615/ 1499], train_loss/perplexity = 5.19928217/181.1421661 secs/batch = 0.5444s, grad.norm=0.61623955\n",
      " 11113: 7 [  620/ 1499], train_loss/perplexity = 4.94525671/140.5069122 secs/batch = 0.5958s, grad.norm=0.58551693\n",
      " 11118: 7 [  625/ 1499], train_loss/perplexity = 4.93180609/138.6296692 secs/batch = 0.6173s, grad.norm=0.60142756\n",
      " 11123: 7 [  630/ 1499], train_loss/perplexity = 5.06068945/157.6992035 secs/batch = 0.5752s, grad.norm=0.61635798\n",
      " 11128: 7 [  635/ 1499], train_loss/perplexity = 4.72709322/112.9667130 secs/batch = 0.5478s, grad.norm=0.61484492\n",
      " 11133: 7 [  640/ 1499], train_loss/perplexity = 5.07924557/160.6528015 secs/batch = 0.5975s, grad.norm=0.64106536\n",
      " 11138: 7 [  645/ 1499], train_loss/perplexity = 4.80779934/122.4618225 secs/batch = 0.5454s, grad.norm=0.65345693\n",
      " 11143: 7 [  650/ 1499], train_loss/perplexity = 4.72706556/112.9635925 secs/batch = 0.5500s, grad.norm=0.68671554\n",
      " 11148: 7 [  655/ 1499], train_loss/perplexity = 5.17042780/175.9901123 secs/batch = 0.6215s, grad.norm=0.61777890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11153: 7 [  660/ 1499], train_loss/perplexity = 5.22634935/186.1121368 secs/batch = 0.5473s, grad.norm=0.63705069\n",
      " 11158: 7 [  665/ 1499], train_loss/perplexity = 5.14410305/171.4176636 secs/batch = 0.5717s, grad.norm=0.61160791\n",
      " 11163: 7 [  670/ 1499], train_loss/perplexity = 5.38903332/218.9915924 secs/batch = 0.5487s, grad.norm=0.63376534\n",
      " 11168: 7 [  675/ 1499], train_loss/perplexity = 5.15065384/172.5442657 secs/batch = 0.5424s, grad.norm=0.63931370\n",
      " 11173: 7 [  680/ 1499], train_loss/perplexity = 5.10495424/164.8365326 secs/batch = 0.5588s, grad.norm=0.61546129\n",
      " 11178: 7 [  685/ 1499], train_loss/perplexity = 5.31336021/203.0313110 secs/batch = 0.5449s, grad.norm=0.63999617\n",
      " 11183: 7 [  690/ 1499], train_loss/perplexity = 5.08138943/160.9975891 secs/batch = 0.5548s, grad.norm=0.61619109\n",
      " 11188: 7 [  695/ 1499], train_loss/perplexity = 5.13985252/170.6905975 secs/batch = 0.5497s, grad.norm=0.61721265\n",
      " 11193: 7 [  700/ 1499], train_loss/perplexity = 5.29435873/199.2098389 secs/batch = 0.5451s, grad.norm=0.66620314\n",
      " 11198: 7 [  705/ 1499], train_loss/perplexity = 4.76398754/117.2123871 secs/batch = 0.5438s, grad.norm=0.56740242\n",
      " 11203: 7 [  710/ 1499], train_loss/perplexity = 5.09928608/163.9048462 secs/batch = 0.5594s, grad.norm=0.67930573\n",
      " 11208: 7 [  715/ 1499], train_loss/perplexity = 5.07436371/159.8704376 secs/batch = 0.5473s, grad.norm=0.59450936\n",
      " 11213: 7 [  720/ 1499], train_loss/perplexity = 5.08863068/162.1676483 secs/batch = 0.5459s, grad.norm=0.59907055\n",
      " 11218: 7 [  725/ 1499], train_loss/perplexity = 4.95121098/141.3460236 secs/batch = 0.5646s, grad.norm=0.66720688\n",
      " 11223: 7 [  730/ 1499], train_loss/perplexity = 5.07508850/159.9863434 secs/batch = 0.5454s, grad.norm=0.64964718\n",
      " 11228: 7 [  735/ 1499], train_loss/perplexity = 4.96403360/143.1701202 secs/batch = 0.5442s, grad.norm=0.65525931\n",
      " 11233: 7 [  740/ 1499], train_loss/perplexity = 4.93554878/139.1494904 secs/batch = 0.5485s, grad.norm=0.61081308\n",
      " 11238: 7 [  745/ 1499], train_loss/perplexity = 4.83586931/125.9480209 secs/batch = 0.6386s, grad.norm=0.62091911\n",
      " 11243: 7 [  750/ 1499], train_loss/perplexity = 5.06170464/157.8593750 secs/batch = 0.5857s, grad.norm=0.62396884\n",
      " 11248: 7 [  755/ 1499], train_loss/perplexity = 4.83213186/125.4781799 secs/batch = 0.6172s, grad.norm=0.67527515\n",
      " 11253: 7 [  760/ 1499], train_loss/perplexity = 4.49267006/89.3597260 secs/batch = 0.6378s, grad.norm=0.63091773\n",
      " 11258: 7 [  765/ 1499], train_loss/perplexity = 4.86625099/129.8332520 secs/batch = 0.5639s, grad.norm=0.62944442\n",
      " 11263: 7 [  770/ 1499], train_loss/perplexity = 4.99165058/147.1791534 secs/batch = 0.5510s, grad.norm=0.58483374\n",
      " 11268: 7 [  775/ 1499], train_loss/perplexity = 5.34887409/210.3713074 secs/batch = 0.5589s, grad.norm=0.64390379\n",
      " 11273: 7 [  780/ 1499], train_loss/perplexity = 4.93840742/139.5478363 secs/batch = 0.5426s, grad.norm=0.58115488\n",
      " 11278: 7 [  785/ 1499], train_loss/perplexity = 5.07496166/159.9660645 secs/batch = 0.5454s, grad.norm=0.62271720\n",
      " 11283: 7 [  790/ 1499], train_loss/perplexity = 4.65087605/104.6766434 secs/batch = 0.5714s, grad.norm=0.61402088\n",
      " 11288: 7 [  795/ 1499], train_loss/perplexity = 5.07536316/160.0303040 secs/batch = 0.5877s, grad.norm=0.60233033\n",
      " 11293: 7 [  800/ 1499], train_loss/perplexity = 5.03050137/153.0097046 secs/batch = 0.5464s, grad.norm=0.66959482\n",
      " 11298: 7 [  805/ 1499], train_loss/perplexity = 5.01601887/150.8097076 secs/batch = 0.5420s, grad.norm=0.62216228\n",
      " 11303: 7 [  810/ 1499], train_loss/perplexity = 4.91433477/136.2286530 secs/batch = 0.5681s, grad.norm=0.58507109\n",
      " 11308: 7 [  815/ 1499], train_loss/perplexity = 5.10660696/165.1091766 secs/batch = 0.5445s, grad.norm=0.61615396\n",
      " 11313: 7 [  820/ 1499], train_loss/perplexity = 4.69414377/109.3051758 secs/batch = 0.5470s, grad.norm=0.64926171\n",
      " 11318: 7 [  825/ 1499], train_loss/perplexity = 4.79829025/121.3028412 secs/batch = 0.5459s, grad.norm=0.61400962\n",
      " 11323: 7 [  830/ 1499], train_loss/perplexity = 5.04027557/154.5125885 secs/batch = 0.5438s, grad.norm=0.62331605\n",
      " 11328: 7 [  835/ 1499], train_loss/perplexity = 5.03187037/153.2193146 secs/batch = 0.5434s, grad.norm=0.61183947\n",
      " 11333: 7 [  840/ 1499], train_loss/perplexity = 4.95394659/141.7332306 secs/batch = 0.5492s, grad.norm=0.63308179\n",
      " 11338: 7 [  845/ 1499], train_loss/perplexity = 4.76340532/117.1441574 secs/batch = 0.5397s, grad.norm=0.57959759\n",
      " 11343: 7 [  850/ 1499], train_loss/perplexity = 4.88560820/132.3709412 secs/batch = 0.5434s, grad.norm=0.65067613\n",
      " 11348: 7 [  855/ 1499], train_loss/perplexity = 5.03732729/154.0577087 secs/batch = 0.5553s, grad.norm=0.65913290\n",
      " 11353: 7 [  860/ 1499], train_loss/perplexity = 4.52431345/92.2325821 secs/batch = 0.5429s, grad.norm=0.67820519\n",
      " 11358: 7 [  865/ 1499], train_loss/perplexity = 4.86502981/129.6748047 secs/batch = 0.6058s, grad.norm=0.62347358\n",
      " 11363: 7 [  870/ 1499], train_loss/perplexity = 4.87261868/130.6626282 secs/batch = 0.5748s, grad.norm=0.62222302\n",
      " 11368: 7 [  875/ 1499], train_loss/perplexity = 4.94729090/140.7930298 secs/batch = 0.5419s, grad.norm=0.58376789\n",
      " 11373: 7 [  880/ 1499], train_loss/perplexity = 4.98212528/145.7838898 secs/batch = 0.5607s, grad.norm=0.61767304\n",
      " 11378: 7 [  885/ 1499], train_loss/perplexity = 4.79389906/120.7713470 secs/batch = 0.5464s, grad.norm=0.64005166\n",
      " 11383: 7 [  890/ 1499], train_loss/perplexity = 5.09698725/163.5284882 secs/batch = 0.5502s, grad.norm=0.59415579\n",
      " 11388: 7 [  895/ 1499], train_loss/perplexity = 5.19925833/181.1378479 secs/batch = 0.5462s, grad.norm=0.61318523\n",
      " 11393: 7 [  900/ 1499], train_loss/perplexity = 4.83283758/125.5667648 secs/batch = 0.5461s, grad.norm=0.58766150\n",
      " 11398: 7 [  905/ 1499], train_loss/perplexity = 4.96598530/143.4498291 secs/batch = 0.5494s, grad.norm=0.59169400\n",
      " 11403: 7 [  910/ 1499], train_loss/perplexity = 5.16956329/175.8380280 secs/batch = 0.6036s, grad.norm=0.65644330\n",
      " 11408: 7 [  915/ 1499], train_loss/perplexity = 4.96790171/143.7249908 secs/batch = 0.6094s, grad.norm=0.68212563\n",
      " 11413: 7 [  920/ 1499], train_loss/perplexity = 4.67984247/107.7530975 secs/batch = 0.6462s, grad.norm=0.56593043\n",
      " 11418: 7 [  925/ 1499], train_loss/perplexity = 4.96561956/143.3973694 secs/batch = 0.6187s, grad.norm=0.66687340\n",
      " 11423: 7 [  930/ 1499], train_loss/perplexity = 5.00018215/148.4402008 secs/batch = 0.5883s, grad.norm=0.69399935\n",
      " 11428: 7 [  935/ 1499], train_loss/perplexity = 5.08863401/162.1681976 secs/batch = 0.5905s, grad.norm=0.62145466\n",
      " 11433: 7 [  940/ 1499], train_loss/perplexity = 4.69121313/108.9853134 secs/batch = 0.5978s, grad.norm=0.62005800\n",
      " 11438: 7 [  945/ 1499], train_loss/perplexity = 5.13749599/170.2888336 secs/batch = 0.6022s, grad.norm=0.61384124\n",
      " 11443: 7 [  950/ 1499], train_loss/perplexity = 5.07343102/159.7213898 secs/batch = 0.5840s, grad.norm=0.60875988\n",
      " 11448: 7 [  955/ 1499], train_loss/perplexity = 5.18996048/179.4614563 secs/batch = 0.5921s, grad.norm=0.59188217\n",
      " 11453: 7 [  960/ 1499], train_loss/perplexity = 4.92525053/137.7238464 secs/batch = 0.5984s, grad.norm=0.66852236\n",
      " 11458: 7 [  965/ 1499], train_loss/perplexity = 4.95398712/141.7389679 secs/batch = 0.5840s, grad.norm=0.56276739\n",
      " 11463: 7 [  970/ 1499], train_loss/perplexity = 5.20876980/182.8689575 secs/batch = 0.5916s, grad.norm=0.68944740\n",
      " 11468: 7 [  975/ 1499], train_loss/perplexity = 4.89389515/133.4724579 secs/batch = 0.5606s, grad.norm=0.67219609\n",
      " 11473: 7 [  980/ 1499], train_loss/perplexity = 5.08210087/161.1121826 secs/batch = 0.5467s, grad.norm=0.62201971\n",
      " 11478: 7 [  985/ 1499], train_loss/perplexity = 4.78215599/119.3614120 secs/batch = 0.5393s, grad.norm=0.63665134\n",
      " 11483: 7 [  990/ 1499], train_loss/perplexity = 4.92260981/137.3606262 secs/batch = 0.5648s, grad.norm=0.59539866\n",
      " 11488: 7 [  995/ 1499], train_loss/perplexity = 5.27448511/195.2899017 secs/batch = 0.5508s, grad.norm=0.63600183\n",
      " 11493: 7 [ 1000/ 1499], train_loss/perplexity = 4.97769976/145.1401367 secs/batch = 0.5384s, grad.norm=0.61746496\n",
      " 11498: 7 [ 1005/ 1499], train_loss/perplexity = 5.35656929/211.9963989 secs/batch = 0.5529s, grad.norm=0.59384358\n",
      " 11503: 7 [ 1010/ 1499], train_loss/perplexity = 5.13232756/169.4109802 secs/batch = 0.5492s, grad.norm=0.64053243\n",
      " 11508: 7 [ 1015/ 1499], train_loss/perplexity = 4.99048185/147.0072479 secs/batch = 0.5699s, grad.norm=0.57927138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11513: 7 [ 1020/ 1499], train_loss/perplexity = 5.03142071/153.1504364 secs/batch = 0.5550s, grad.norm=0.62837505\n",
      " 11518: 7 [ 1025/ 1499], train_loss/perplexity = 5.07008457/159.1877899 secs/batch = 0.5466s, grad.norm=0.64261228\n",
      " 11523: 7 [ 1030/ 1499], train_loss/perplexity = 5.19826460/180.9579315 secs/batch = 0.6546s, grad.norm=0.60721606\n",
      " 11528: 7 [ 1035/ 1499], train_loss/perplexity = 5.44098139/230.6684418 secs/batch = 0.6374s, grad.norm=0.65997064\n",
      " 11533: 7 [ 1040/ 1499], train_loss/perplexity = 4.82053757/124.0317459 secs/batch = 0.5792s, grad.norm=0.56484771\n",
      " 11538: 7 [ 1045/ 1499], train_loss/perplexity = 4.95422602/141.7728271 secs/batch = 0.5531s, grad.norm=0.58960003\n",
      " 11543: 7 [ 1050/ 1499], train_loss/perplexity = 4.66581488/106.2521286 secs/batch = 0.5664s, grad.norm=0.60510004\n",
      " 11548: 7 [ 1055/ 1499], train_loss/perplexity = 5.00380135/148.9784088 secs/batch = 0.5471s, grad.norm=0.60161120\n",
      " 11553: 7 [ 1060/ 1499], train_loss/perplexity = 5.21712875/184.4039612 secs/batch = 0.5453s, grad.norm=0.58896995\n",
      " 11558: 7 [ 1065/ 1499], train_loss/perplexity = 4.79458570/120.8543015 secs/batch = 0.5394s, grad.norm=0.65166831\n",
      " 11563: 7 [ 1070/ 1499], train_loss/perplexity = 5.22534847/185.9259491 secs/batch = 0.5617s, grad.norm=0.59752512\n",
      " 11568: 7 [ 1075/ 1499], train_loss/perplexity = 5.04167986/154.7297211 secs/batch = 0.6361s, grad.norm=0.59237289\n",
      " 11573: 7 [ 1080/ 1499], train_loss/perplexity = 5.30544758/201.4311371 secs/batch = 0.6512s, grad.norm=0.61404085\n",
      " 11578: 7 [ 1085/ 1499], train_loss/perplexity = 5.39849806/221.0741272 secs/batch = 0.5470s, grad.norm=0.62166905\n",
      " 11583: 7 [ 1090/ 1499], train_loss/perplexity = 4.98911572/146.8065491 secs/batch = 0.5898s, grad.norm=0.61501122\n",
      " 11588: 7 [ 1095/ 1499], train_loss/perplexity = 5.12578297/168.3058624 secs/batch = 0.5653s, grad.norm=0.60389858\n",
      " 11593: 7 [ 1100/ 1499], train_loss/perplexity = 5.13089800/169.1689606 secs/batch = 0.5711s, grad.norm=0.58718544\n",
      " 11598: 7 [ 1105/ 1499], train_loss/perplexity = 4.95552158/141.9566345 secs/batch = 0.6615s, grad.norm=0.69804192\n",
      " 11603: 7 [ 1110/ 1499], train_loss/perplexity = 5.10223913/164.3895874 secs/batch = 0.6462s, grad.norm=0.62236720\n",
      " 11608: 7 [ 1115/ 1499], train_loss/perplexity = 4.71458101/111.5620575 secs/batch = 0.6509s, grad.norm=0.60972363\n",
      " 11613: 7 [ 1120/ 1499], train_loss/perplexity = 4.97281361/144.4326935 secs/batch = 0.5409s, grad.norm=0.62730092\n",
      " 11618: 7 [ 1125/ 1499], train_loss/perplexity = 4.84629107/127.2674866 secs/batch = 0.5522s, grad.norm=0.61638004\n",
      " 11623: 7 [ 1130/ 1499], train_loss/perplexity = 4.70130634/110.0908966 secs/batch = 0.5712s, grad.norm=0.59960401\n",
      " 11628: 7 [ 1135/ 1499], train_loss/perplexity = 5.36191320/213.1323242 secs/batch = 0.5899s, grad.norm=0.60619020\n",
      " 11633: 7 [ 1140/ 1499], train_loss/perplexity = 5.15013409/172.4546051 secs/batch = 0.5918s, grad.norm=0.63516074\n",
      " 11638: 7 [ 1145/ 1499], train_loss/perplexity = 5.26569891/193.5815582 secs/batch = 0.5941s, grad.norm=0.60161716\n",
      " 11643: 7 [ 1150/ 1499], train_loss/perplexity = 5.21916723/184.7802429 secs/batch = 0.6588s, grad.norm=0.59396738\n",
      " 11648: 7 [ 1155/ 1499], train_loss/perplexity = 4.88154697/131.8344574 secs/batch = 0.6128s, grad.norm=0.66940421\n",
      " 11653: 7 [ 1160/ 1499], train_loss/perplexity = 5.07556820/160.0631104 secs/batch = 0.5579s, grad.norm=0.60614693\n",
      " 11658: 7 [ 1165/ 1499], train_loss/perplexity = 4.98081493/145.5929871 secs/batch = 0.5547s, grad.norm=0.63007748\n",
      " 11663: 7 [ 1170/ 1499], train_loss/perplexity = 5.10192871/164.3385620 secs/batch = 0.5561s, grad.norm=0.63319796\n",
      " 11668: 7 [ 1175/ 1499], train_loss/perplexity = 4.48366880/88.5589828 secs/batch = 0.5608s, grad.norm=0.66869146\n",
      " 11673: 7 [ 1180/ 1499], train_loss/perplexity = 4.75517273/116.1837234 secs/batch = 0.6271s, grad.norm=0.60548455\n",
      " 11678: 7 [ 1185/ 1499], train_loss/perplexity = 4.76454067/117.2772369 secs/batch = 0.5421s, grad.norm=0.67685121\n",
      " 11683: 7 [ 1190/ 1499], train_loss/perplexity = 4.98684311/146.4732971 secs/batch = 0.6125s, grad.norm=0.61481106\n",
      " 11688: 7 [ 1195/ 1499], train_loss/perplexity = 4.98467636/146.1562653 secs/batch = 0.5884s, grad.norm=0.66755831\n",
      " 11693: 7 [ 1200/ 1499], train_loss/perplexity = 4.84827852/127.5206757 secs/batch = 0.5443s, grad.norm=0.63166964\n",
      " 11698: 7 [ 1205/ 1499], train_loss/perplexity = 4.80513811/122.1363602 secs/batch = 0.5790s, grad.norm=0.62685621\n",
      " 11703: 7 [ 1210/ 1499], train_loss/perplexity = 4.78215313/119.3610764 secs/batch = 0.6211s, grad.norm=0.64395946\n",
      " 11708: 7 [ 1215/ 1499], train_loss/perplexity = 4.49563885/89.6254044 secs/batch = 0.5828s, grad.norm=0.60894763\n",
      " 11713: 7 [ 1220/ 1499], train_loss/perplexity = 4.87930298/131.5389404 secs/batch = 0.5536s, grad.norm=0.60631657\n",
      " 11718: 7 [ 1225/ 1499], train_loss/perplexity = 4.30561304/74.1146393 secs/batch = 0.5756s, grad.norm=0.62390608\n",
      " 11723: 7 [ 1230/ 1499], train_loss/perplexity = 4.82525969/124.6188278 secs/batch = 0.6121s, grad.norm=0.61274195\n",
      " 11728: 7 [ 1235/ 1499], train_loss/perplexity = 4.55891275/95.4796143 secs/batch = 0.5563s, grad.norm=0.68703151\n",
      " 11733: 7 [ 1240/ 1499], train_loss/perplexity = 4.79292345/120.6535797 secs/batch = 0.5438s, grad.norm=0.62574619\n",
      " 11738: 7 [ 1245/ 1499], train_loss/perplexity = 5.05785704/157.2531738 secs/batch = 0.5606s, grad.norm=0.62777531\n",
      " 11743: 7 [ 1250/ 1499], train_loss/perplexity = 5.17668247/177.0943146 secs/batch = 0.5601s, grad.norm=0.62991989\n",
      " 11748: 7 [ 1255/ 1499], train_loss/perplexity = 4.92904997/138.2481079 secs/batch = 0.5445s, grad.norm=0.62956917\n",
      " 11753: 7 [ 1260/ 1499], train_loss/perplexity = 5.01037264/149.9606018 secs/batch = 0.5388s, grad.norm=0.58389354\n",
      " 11758: 7 [ 1265/ 1499], train_loss/perplexity = 5.03882647/154.2888489 secs/batch = 0.5992s, grad.norm=0.60251856\n",
      " 11763: 7 [ 1270/ 1499], train_loss/perplexity = 4.99537134/147.7277985 secs/batch = 0.5706s, grad.norm=0.62563783\n",
      " 11768: 7 [ 1275/ 1499], train_loss/perplexity = 4.99622154/147.8534393 secs/batch = 0.5722s, grad.norm=0.61260539\n",
      " 11773: 7 [ 1280/ 1499], train_loss/perplexity = 4.65733576/105.3550186 secs/batch = 0.5647s, grad.norm=0.61404467\n",
      " 11778: 7 [ 1285/ 1499], train_loss/perplexity = 5.11167240/165.9476471 secs/batch = 0.5882s, grad.norm=0.61197490\n",
      " 11783: 7 [ 1290/ 1499], train_loss/perplexity = 4.95001888/141.1776276 secs/batch = 0.5776s, grad.norm=0.61310828\n",
      " 11788: 7 [ 1295/ 1499], train_loss/perplexity = 4.94784927/140.8716583 secs/batch = 0.5687s, grad.norm=0.64511478\n",
      " 11793: 7 [ 1300/ 1499], train_loss/perplexity = 5.09061193/162.4892578 secs/batch = 0.5556s, grad.norm=0.68198127\n",
      " 11798: 7 [ 1305/ 1499], train_loss/perplexity = 4.96322155/143.0539093 secs/batch = 0.5506s, grad.norm=0.61314642\n",
      " 11803: 7 [ 1310/ 1499], train_loss/perplexity = 5.10686541/165.1518555 secs/batch = 0.6376s, grad.norm=0.62971807\n",
      " 11808: 7 [ 1315/ 1499], train_loss/perplexity = 4.89427233/133.5228119 secs/batch = 0.5438s, grad.norm=0.64733487\n",
      " 11813: 7 [ 1320/ 1499], train_loss/perplexity = 4.64410019/103.9697723 secs/batch = 0.6354s, grad.norm=0.60582727\n",
      " 11818: 7 [ 1325/ 1499], train_loss/perplexity = 4.98031044/145.5195465 secs/batch = 0.5411s, grad.norm=0.63086128\n",
      " 11823: 7 [ 1330/ 1499], train_loss/perplexity = 5.11733150/166.8894348 secs/batch = 0.5355s, grad.norm=0.60062706\n",
      " 11828: 7 [ 1335/ 1499], train_loss/perplexity = 4.96422672/143.1977692 secs/batch = 0.5832s, grad.norm=0.60252273\n",
      " 11833: 7 [ 1340/ 1499], train_loss/perplexity = 4.39356136/80.9281235 secs/batch = 0.6310s, grad.norm=0.62161690\n",
      " 11838: 7 [ 1345/ 1499], train_loss/perplexity = 4.58269024/97.7770844 secs/batch = 0.6241s, grad.norm=0.62394112\n",
      " 11843: 7 [ 1350/ 1499], train_loss/perplexity = 4.76079130/116.8383408 secs/batch = 0.5421s, grad.norm=0.61060441\n",
      " 11848: 7 [ 1355/ 1499], train_loss/perplexity = 4.43984032/84.7614059 secs/batch = 0.5479s, grad.norm=0.61821777\n",
      " 11853: 7 [ 1360/ 1499], train_loss/perplexity = 4.66068220/105.7081680 secs/batch = 0.5473s, grad.norm=0.61514151\n",
      " 11858: 7 [ 1365/ 1499], train_loss/perplexity = 4.41113615/82.3629837 secs/batch = 0.5349s, grad.norm=0.63872772\n",
      " 11863: 7 [ 1370/ 1499], train_loss/perplexity = 4.43612719/84.4472580 secs/batch = 0.5376s, grad.norm=0.59219909\n",
      " 11868: 7 [ 1375/ 1499], train_loss/perplexity = 4.57214975/96.7518768 secs/batch = 0.6481s, grad.norm=0.64507639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11873: 7 [ 1380/ 1499], train_loss/perplexity = 5.00495720/149.1506958 secs/batch = 0.5493s, grad.norm=0.62445176\n",
      " 11878: 7 [ 1385/ 1499], train_loss/perplexity = 4.70584583/110.5917892 secs/batch = 0.5395s, grad.norm=0.60443699\n",
      " 11883: 7 [ 1390/ 1499], train_loss/perplexity = 4.77194595/118.1489334 secs/batch = 0.6277s, grad.norm=0.60163927\n",
      " 11888: 7 [ 1395/ 1499], train_loss/perplexity = 5.03710604/154.0236359 secs/batch = 0.5973s, grad.norm=0.63439995\n",
      " 11893: 7 [ 1400/ 1499], train_loss/perplexity = 5.00842381/149.6686401 secs/batch = 0.5417s, grad.norm=0.59474909\n",
      " 11898: 7 [ 1405/ 1499], train_loss/perplexity = 4.88059378/131.7088470 secs/batch = 0.6392s, grad.norm=0.60320175\n",
      " 11903: 7 [ 1410/ 1499], train_loss/perplexity = 5.00418901/149.0361633 secs/batch = 0.5519s, grad.norm=0.58126700\n",
      " 11908: 7 [ 1415/ 1499], train_loss/perplexity = 5.02147245/151.6344147 secs/batch = 0.5460s, grad.norm=0.62198687\n",
      " 11913: 7 [ 1420/ 1499], train_loss/perplexity = 4.96133137/142.7837677 secs/batch = 0.5738s, grad.norm=0.58895320\n",
      " 11918: 7 [ 1425/ 1499], train_loss/perplexity = 4.90319490/134.7195129 secs/batch = 0.5643s, grad.norm=0.63083857\n",
      " 11923: 7 [ 1430/ 1499], train_loss/perplexity = 5.07184124/159.4676819 secs/batch = 0.5425s, grad.norm=0.64328152\n",
      " 11928: 7 [ 1435/ 1499], train_loss/perplexity = 4.77411413/118.4053802 secs/batch = 0.5427s, grad.norm=0.58874345\n",
      " 11933: 7 [ 1440/ 1499], train_loss/perplexity = 4.46675730/87.0739136 secs/batch = 0.5439s, grad.norm=0.64370191\n",
      " 11938: 7 [ 1445/ 1499], train_loss/perplexity = 4.91360521/136.1293030 secs/batch = 0.5532s, grad.norm=0.61564702\n",
      " 11943: 7 [ 1450/ 1499], train_loss/perplexity = 5.02127171/151.6039734 secs/batch = 0.5420s, grad.norm=0.61265564\n",
      " 11948: 7 [ 1455/ 1499], train_loss/perplexity = 5.28521395/197.3964081 secs/batch = 0.5407s, grad.norm=0.66689795\n",
      " 11953: 7 [ 1460/ 1499], train_loss/perplexity = 5.20998240/183.0908356 secs/batch = 0.5395s, grad.norm=0.60555983\n",
      " 11958: 7 [ 1465/ 1499], train_loss/perplexity = 5.34191799/208.9130249 secs/batch = 0.5391s, grad.norm=0.65365291\n",
      " 11963: 7 [ 1470/ 1499], train_loss/perplexity = 5.05668736/157.0693359 secs/batch = 0.5382s, grad.norm=0.61428201\n",
      " 11968: 7 [ 1475/ 1499], train_loss/perplexity = 5.17855024/177.4253998 secs/batch = 0.5407s, grad.norm=0.61472732\n",
      " 11973: 7 [ 1480/ 1499], train_loss/perplexity = 5.08448505/161.4967499 secs/batch = 0.5440s, grad.norm=0.59312558\n",
      " 11978: 7 [ 1485/ 1499], train_loss/perplexity = 4.81448078/123.2827835 secs/batch = 0.6422s, grad.norm=0.63844949\n",
      " 11983: 7 [ 1490/ 1499], train_loss/perplexity = 4.99218988/147.2585449 secs/batch = 0.6052s, grad.norm=0.60663748\n",
      " 11988: 7 [ 1495/ 1499], train_loss/perplexity = 5.29777098/199.8907471 secs/batch = 0.5428s, grad.norm=0.59592801\n",
      "Epoch training time: 876.8510468006134\n",
      "Saved char model cv/epoch007_5.0740.model\n",
      " 11997: 8 [    5/ 1499], train_loss/perplexity = 5.08956480/162.3191986 secs/batch = 0.6117s, grad.norm=0.61187792\n",
      " 12002: 8 [   10/ 1499], train_loss/perplexity = 5.07293940/159.6428986 secs/batch = 0.5911s, grad.norm=0.61257017\n",
      " 12007: 8 [   15/ 1499], train_loss/perplexity = 5.00114107/148.5826111 secs/batch = 0.5762s, grad.norm=0.64225191\n",
      " 12012: 8 [   20/ 1499], train_loss/perplexity = 4.82161140/124.1650085 secs/batch = 0.5540s, grad.norm=0.58439380\n",
      " 12017: 8 [   25/ 1499], train_loss/perplexity = 5.24177313/189.0049286 secs/batch = 0.6410s, grad.norm=0.63121539\n",
      " 12022: 8 [   30/ 1499], train_loss/perplexity = 5.18687773/178.9090729 secs/batch = 0.5825s, grad.norm=0.65466577\n",
      " 12027: 8 [   35/ 1499], train_loss/perplexity = 5.04634905/155.4538727 secs/batch = 0.6413s, grad.norm=0.65025645\n",
      " 12032: 8 [   40/ 1499], train_loss/perplexity = 5.11354923/166.2593994 secs/batch = 0.6447s, grad.norm=0.62981498\n",
      " 12037: 8 [   45/ 1499], train_loss/perplexity = 4.97259855/144.4016418 secs/batch = 0.6256s, grad.norm=0.64203990\n",
      " 12042: 8 [   50/ 1499], train_loss/perplexity = 5.01422691/150.5397186 secs/batch = 0.6354s, grad.norm=0.63175941\n",
      " 12047: 8 [   55/ 1499], train_loss/perplexity = 4.80646563/122.2986069 secs/batch = 0.5717s, grad.norm=0.63992023\n",
      " 12052: 8 [   60/ 1499], train_loss/perplexity = 4.73771477/114.1729889 secs/batch = 0.5727s, grad.norm=0.59618384\n",
      " 12057: 8 [   65/ 1499], train_loss/perplexity = 4.76731348/117.6028748 secs/batch = 0.5432s, grad.norm=0.60759634\n",
      " 12062: 8 [   70/ 1499], train_loss/perplexity = 4.82174540/124.1816483 secs/batch = 0.5768s, grad.norm=0.63702416\n",
      " 12067: 8 [   75/ 1499], train_loss/perplexity = 4.65897083/105.5274200 secs/batch = 0.5443s, grad.norm=0.62673712\n",
      " 12072: 8 [   80/ 1499], train_loss/perplexity = 4.71557140/111.6725998 secs/batch = 0.5612s, grad.norm=0.60904068\n",
      " 12077: 8 [   85/ 1499], train_loss/perplexity = 4.58103228/97.6151047 secs/batch = 0.5391s, grad.norm=0.59708834\n",
      " 12082: 8 [   90/ 1499], train_loss/perplexity = 4.84252167/126.7886658 secs/batch = 0.5438s, grad.norm=0.63125664\n",
      " 12087: 8 [   95/ 1499], train_loss/perplexity = 4.60722876/100.2060699 secs/batch = 0.5469s, grad.norm=0.62683558\n",
      " 12092: 8 [  100/ 1499], train_loss/perplexity = 4.68933582/108.7809067 secs/batch = 0.5994s, grad.norm=0.61071146\n",
      " 12097: 8 [  105/ 1499], train_loss/perplexity = 4.61949539/101.4428329 secs/batch = 0.5518s, grad.norm=0.62259567\n",
      " 12102: 8 [  110/ 1499], train_loss/perplexity = 4.71183252/111.2558517 secs/batch = 0.5347s, grad.norm=0.62019610\n",
      " 12107: 8 [  115/ 1499], train_loss/perplexity = 4.85416031/128.2729340 secs/batch = 0.5451s, grad.norm=0.59726822\n",
      " 12112: 8 [  120/ 1499], train_loss/perplexity = 4.73904848/114.3253708 secs/batch = 0.5560s, grad.norm=0.59686375\n",
      " 12117: 8 [  125/ 1499], train_loss/perplexity = 4.87838125/131.4177551 secs/batch = 0.5430s, grad.norm=0.65875530\n",
      " 12122: 8 [  130/ 1499], train_loss/perplexity = 4.92714310/137.9847412 secs/batch = 0.5477s, grad.norm=0.64631224\n",
      " 12127: 8 [  135/ 1499], train_loss/perplexity = 4.99555826/147.7554016 secs/batch = 0.5528s, grad.norm=0.61808282\n",
      " 12132: 8 [  140/ 1499], train_loss/perplexity = 5.00766659/149.5553589 secs/batch = 0.5703s, grad.norm=0.65705711\n",
      " 12137: 8 [  145/ 1499], train_loss/perplexity = 4.64355230/103.9128189 secs/batch = 0.5419s, grad.norm=0.65905297\n",
      " 12142: 8 [  150/ 1499], train_loss/perplexity = 4.88742971/132.6122894 secs/batch = 0.5469s, grad.norm=0.63536084\n",
      " 12147: 8 [  155/ 1499], train_loss/perplexity = 5.05112839/156.1986237 secs/batch = 0.5534s, grad.norm=0.62105167\n",
      " 12152: 8 [  160/ 1499], train_loss/perplexity = 5.32597542/205.6088104 secs/batch = 0.5464s, grad.norm=0.67026854\n",
      " 12157: 8 [  165/ 1499], train_loss/perplexity = 4.84783888/127.4646225 secs/batch = 0.5449s, grad.norm=0.62551606\n",
      " 12162: 8 [  170/ 1499], train_loss/perplexity = 5.19867420/181.0320740 secs/batch = 0.5469s, grad.norm=0.60408556\n",
      " 12167: 8 [  175/ 1499], train_loss/perplexity = 4.97533512/144.7973480 secs/batch = 0.5444s, grad.norm=0.61878717\n",
      " 12172: 8 [  180/ 1499], train_loss/perplexity = 5.10917187/165.5332184 secs/batch = 0.5581s, grad.norm=0.60193729\n",
      " 12177: 8 [  185/ 1499], train_loss/perplexity = 4.94297838/140.1871643 secs/batch = 0.5493s, grad.norm=0.62833083\n",
      " 12182: 8 [  190/ 1499], train_loss/perplexity = 5.00535345/149.2098083 secs/batch = 0.5458s, grad.norm=0.60215926\n",
      " 12187: 8 [  195/ 1499], train_loss/perplexity = 5.20648241/182.4511414 secs/batch = 0.6200s, grad.norm=0.61252654\n",
      " 12192: 8 [  200/ 1499], train_loss/perplexity = 5.09812307/163.7143402 secs/batch = 0.5924s, grad.norm=0.62385887\n",
      " 12197: 8 [  205/ 1499], train_loss/perplexity = 5.00671291/149.4127960 secs/batch = 0.6057s, grad.norm=0.64300758\n",
      " 12202: 8 [  210/ 1499], train_loss/perplexity = 4.85631323/128.5493927 secs/batch = 0.5905s, grad.norm=0.57960397\n",
      " 12207: 8 [  215/ 1499], train_loss/perplexity = 5.09527683/163.2490234 secs/batch = 0.6027s, grad.norm=0.60822523\n",
      " 12212: 8 [  220/ 1499], train_loss/perplexity = 4.79307842/120.6722794 secs/batch = 0.5882s, grad.norm=0.55736405\n",
      " 12217: 8 [  225/ 1499], train_loss/perplexity = 5.03732443/154.0572662 secs/batch = 0.5792s, grad.norm=0.58397299\n",
      " 12222: 8 [  230/ 1499], train_loss/perplexity = 5.11023951/165.7100372 secs/batch = 0.5906s, grad.norm=0.63135350\n",
      " 12227: 8 [  235/ 1499], train_loss/perplexity = 5.11939049/167.2334137 secs/batch = 0.5835s, grad.norm=0.60552084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12232: 8 [  240/ 1499], train_loss/perplexity = 5.11267281/166.1137543 secs/batch = 0.5867s, grad.norm=0.62998271\n",
      " 12237: 8 [  245/ 1499], train_loss/perplexity = 4.84862709/127.5651321 secs/batch = 0.5864s, grad.norm=0.72018582\n",
      " 12242: 8 [  250/ 1499], train_loss/perplexity = 4.90516615/134.9853363 secs/batch = 0.6181s, grad.norm=0.61420226\n",
      " 12247: 8 [  255/ 1499], train_loss/perplexity = 4.74858618/115.4209824 secs/batch = 0.7553s, grad.norm=0.59966576\n",
      " 12252: 8 [  260/ 1499], train_loss/perplexity = 5.07277536/159.6167145 secs/batch = 0.6342s, grad.norm=0.61880845\n",
      " 12257: 8 [  265/ 1499], train_loss/perplexity = 5.00903463/149.7600861 secs/batch = 0.5730s, grad.norm=0.61445653\n",
      " 12262: 8 [  270/ 1499], train_loss/perplexity = 5.04055119/154.5551758 secs/batch = 0.5627s, grad.norm=0.58182931\n",
      " 12267: 8 [  275/ 1499], train_loss/perplexity = 4.67309189/107.0281525 secs/batch = 0.5650s, grad.norm=0.59027326\n",
      " 12272: 8 [  280/ 1499], train_loss/perplexity = 4.99122620/147.1167145 secs/batch = 0.6187s, grad.norm=0.61622602\n",
      " 12277: 8 [  285/ 1499], train_loss/perplexity = 5.23130465/187.0366669 secs/batch = 0.5827s, grad.norm=0.62091458\n",
      " 12282: 8 [  290/ 1499], train_loss/perplexity = 5.18481159/178.5398102 secs/batch = 0.5797s, grad.norm=0.64027852\n",
      " 12287: 8 [  295/ 1499], train_loss/perplexity = 5.08134174/160.9899139 secs/batch = 0.6554s, grad.norm=0.63078159\n",
      " 12292: 8 [  300/ 1499], train_loss/perplexity = 4.90861750/135.4520264 secs/batch = 0.5519s, grad.norm=0.63218844\n",
      " 12297: 8 [  305/ 1499], train_loss/perplexity = 5.09517193/163.2319031 secs/batch = 0.6312s, grad.norm=0.61495471\n",
      " 12302: 8 [  310/ 1499], train_loss/perplexity = 5.17472982/176.7488556 secs/batch = 0.7037s, grad.norm=0.63247102\n",
      " 12307: 8 [  315/ 1499], train_loss/perplexity = 5.10846949/165.4169922 secs/batch = 0.5912s, grad.norm=0.67294973\n",
      " 12312: 8 [  320/ 1499], train_loss/perplexity = 4.98555517/146.2847595 secs/batch = 0.5511s, grad.norm=0.59483743\n",
      " 12317: 8 [  325/ 1499], train_loss/perplexity = 4.96615314/143.4739075 secs/batch = 0.5815s, grad.norm=0.62966782\n",
      " 12322: 8 [  330/ 1499], train_loss/perplexity = 4.85277271/128.0950623 secs/batch = 0.6189s, grad.norm=0.60346925\n",
      " 12327: 8 [  335/ 1499], train_loss/perplexity = 4.92532778/137.7344818 secs/batch = 0.5537s, grad.norm=0.63552606\n",
      " 12332: 8 [  340/ 1499], train_loss/perplexity = 4.68054104/107.8283997 secs/batch = 0.5775s, grad.norm=0.64051300\n",
      " 12337: 8 [  345/ 1499], train_loss/perplexity = 4.64847040/104.4251328 secs/batch = 0.5645s, grad.norm=0.59402108\n",
      " 12342: 8 [  350/ 1499], train_loss/perplexity = 4.77294922/118.2675247 secs/batch = 0.6187s, grad.norm=0.60909945\n",
      " 12347: 8 [  355/ 1499], train_loss/perplexity = 4.60253572/99.7369003 secs/batch = 0.6394s, grad.norm=0.66909766\n",
      " 12352: 8 [  360/ 1499], train_loss/perplexity = 4.79547691/120.9620590 secs/batch = 0.5883s, grad.norm=0.63248914\n",
      " 12357: 8 [  365/ 1499], train_loss/perplexity = 4.71437693/111.5392914 secs/batch = 0.5565s, grad.norm=0.60633314\n",
      " 12362: 8 [  370/ 1499], train_loss/perplexity = 4.61900806/101.3934097 secs/batch = 0.5343s, grad.norm=0.62451065\n",
      " 12367: 8 [  375/ 1499], train_loss/perplexity = 4.85058212/127.8147736 secs/batch = 0.5929s, grad.norm=0.61212766\n",
      " 12372: 8 [  380/ 1499], train_loss/perplexity = 5.07175970/159.4546661 secs/batch = 0.5389s, grad.norm=0.65465146\n",
      " 12377: 8 [  385/ 1499], train_loss/perplexity = 5.22482586/185.8288116 secs/batch = 0.5386s, grad.norm=0.59961927\n",
      " 12382: 8 [  390/ 1499], train_loss/perplexity = 5.22073030/185.0692902 secs/batch = 0.5382s, grad.norm=0.63073146\n",
      " 12387: 8 [  395/ 1499], train_loss/perplexity = 5.30762959/201.8711395 secs/batch = 0.5965s, grad.norm=0.62373388\n",
      " 12392: 8 [  400/ 1499], train_loss/perplexity = 5.08564520/161.6842194 secs/batch = 0.6246s, grad.norm=0.68189651\n",
      " 12397: 8 [  405/ 1499], train_loss/perplexity = 5.10777760/165.3025818 secs/batch = 0.5907s, grad.norm=0.62160987\n",
      " 12402: 8 [  410/ 1499], train_loss/perplexity = 4.53871584/93.5705643 secs/batch = 0.6165s, grad.norm=0.65489823\n",
      " 12407: 8 [  415/ 1499], train_loss/perplexity = 5.13347244/169.6050415 secs/batch = 0.5816s, grad.norm=0.59512192\n",
      " 12412: 8 [  420/ 1499], train_loss/perplexity = 4.83167171/125.4204559 secs/batch = 0.5932s, grad.norm=0.61859173\n",
      " 12417: 8 [  425/ 1499], train_loss/perplexity = 5.05952168/157.5151520 secs/batch = 0.5575s, grad.norm=0.63325799\n",
      " 12422: 8 [  430/ 1499], train_loss/perplexity = 4.87772989/131.3321838 secs/batch = 0.5391s, grad.norm=0.59639180\n",
      " 12427: 8 [  435/ 1499], train_loss/perplexity = 4.79570961/120.9902039 secs/batch = 0.5975s, grad.norm=0.60493731\n",
      " 12432: 8 [  440/ 1499], train_loss/perplexity = 4.78906393/120.1888123 secs/batch = 0.5898s, grad.norm=0.65171021\n",
      " 12437: 8 [  445/ 1499], train_loss/perplexity = 5.05304670/156.4985504 secs/batch = 0.5586s, grad.norm=0.62295967\n",
      " 12442: 8 [  450/ 1499], train_loss/perplexity = 5.03084230/153.0618896 secs/batch = 0.6256s, grad.norm=0.59829122\n",
      " 12447: 8 [  455/ 1499], train_loss/perplexity = 5.04672909/155.5129700 secs/batch = 0.6230s, grad.norm=0.61699647\n",
      " 12452: 8 [  460/ 1499], train_loss/perplexity = 4.97973394/145.4356842 secs/batch = 0.6656s, grad.norm=0.62930363\n",
      " 12457: 8 [  465/ 1499], train_loss/perplexity = 4.96951103/143.9564819 secs/batch = 0.6091s, grad.norm=0.64587051\n",
      " 12462: 8 [  470/ 1499], train_loss/perplexity = 5.07713699/160.3144226 secs/batch = 0.5483s, grad.norm=0.62773532\n",
      " 12467: 8 [  475/ 1499], train_loss/perplexity = 4.82969141/125.1723251 secs/batch = 0.5814s, grad.norm=0.62047291\n",
      " 12472: 8 [  480/ 1499], train_loss/perplexity = 5.00401068/149.0095978 secs/batch = 0.5412s, grad.norm=0.62446088\n",
      " 12477: 8 [  485/ 1499], train_loss/perplexity = 4.85281801/128.1008759 secs/batch = 0.5422s, grad.norm=0.60089123\n",
      " 12482: 8 [  490/ 1499], train_loss/perplexity = 4.94143248/139.9706116 secs/batch = 0.5385s, grad.norm=0.65138853\n",
      " 12487: 8 [  495/ 1499], train_loss/perplexity = 4.90434408/134.8744202 secs/batch = 0.5410s, grad.norm=0.62850994\n",
      " 12492: 8 [  500/ 1499], train_loss/perplexity = 4.95402145/141.7438354 secs/batch = 0.5415s, grad.norm=0.62740058\n",
      " 12497: 8 [  505/ 1499], train_loss/perplexity = 4.84539175/127.1530838 secs/batch = 0.5401s, grad.norm=0.65460682\n",
      " 12502: 8 [  510/ 1499], train_loss/perplexity = 5.23331547/187.4131317 secs/batch = 0.5442s, grad.norm=0.73668885\n",
      " 12507: 8 [  515/ 1499], train_loss/perplexity = 4.63440037/102.9661560 secs/batch = 0.5408s, grad.norm=0.61302817\n",
      " 12512: 8 [  520/ 1499], train_loss/perplexity = 5.00397015/149.0035553 secs/batch = 0.5829s, grad.norm=0.60907173\n",
      " 12517: 8 [  525/ 1499], train_loss/perplexity = 5.11647463/166.7464905 secs/batch = 0.5401s, grad.norm=0.63016659\n",
      " 12522: 8 [  530/ 1499], train_loss/perplexity = 4.93275785/138.7616730 secs/batch = 0.5978s, grad.norm=0.69885200\n",
      " 12527: 8 [  535/ 1499], train_loss/perplexity = 4.92288876/137.3989563 secs/batch = 0.5855s, grad.norm=0.60726184\n",
      " 12532: 8 [  540/ 1499], train_loss/perplexity = 4.82837009/125.0070419 secs/batch = 0.5880s, grad.norm=0.63117224\n",
      " 12537: 8 [  545/ 1499], train_loss/perplexity = 4.67684746/107.4308548 secs/batch = 0.6575s, grad.norm=0.62292439\n",
      " 12542: 8 [  550/ 1499], train_loss/perplexity = 5.10295725/164.5076752 secs/batch = 0.6423s, grad.norm=0.65157437\n",
      " 12547: 8 [  555/ 1499], train_loss/perplexity = 4.88599634/132.4223328 secs/batch = 0.6374s, grad.norm=0.61844057\n",
      " 12552: 8 [  560/ 1499], train_loss/perplexity = 5.21684551/184.3517303 secs/batch = 0.6138s, grad.norm=0.60807520\n",
      " 12557: 8 [  565/ 1499], train_loss/perplexity = 5.22578239/186.0066376 secs/batch = 0.5856s, grad.norm=0.61794055\n",
      " 12562: 8 [  570/ 1499], train_loss/perplexity = 5.11233091/166.0569611 secs/batch = 0.5498s, grad.norm=0.58416700\n",
      " 12567: 8 [  575/ 1499], train_loss/perplexity = 5.19470406/180.3147736 secs/batch = 0.6206s, grad.norm=0.60739273\n",
      " 12572: 8 [  580/ 1499], train_loss/perplexity = 4.85360050/128.2011414 secs/batch = 0.5943s, grad.norm=0.63307285\n",
      " 12577: 8 [  585/ 1499], train_loss/perplexity = 4.83048439/125.2716293 secs/batch = 0.5789s, grad.norm=0.61709040\n",
      " 12582: 8 [  590/ 1499], train_loss/perplexity = 4.90417194/134.8511963 secs/batch = 0.6231s, grad.norm=0.65684158\n",
      " 12587: 8 [  595/ 1499], train_loss/perplexity = 5.01220131/150.2350922 secs/batch = 0.5734s, grad.norm=0.69039899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12592: 8 [  600/ 1499], train_loss/perplexity = 4.78116560/119.2432556 secs/batch = 0.5526s, grad.norm=0.59841639\n",
      " 12597: 8 [  605/ 1499], train_loss/perplexity = 4.74377298/114.8667755 secs/batch = 0.5390s, grad.norm=0.63734090\n",
      " 12602: 8 [  610/ 1499], train_loss/perplexity = 5.08840847/162.1316223 secs/batch = 0.5456s, grad.norm=0.61183989\n",
      " 12607: 8 [  615/ 1499], train_loss/perplexity = 5.10532856/164.8982391 secs/batch = 0.5487s, grad.norm=0.60894477\n",
      " 12612: 8 [  620/ 1499], train_loss/perplexity = 4.89002562/132.9569855 secs/batch = 0.5571s, grad.norm=0.62209654\n",
      " 12617: 8 [  625/ 1499], train_loss/perplexity = 4.83921194/126.3697281 secs/batch = 0.5483s, grad.norm=0.62604856\n",
      " 12622: 8 [  630/ 1499], train_loss/perplexity = 5.01626587/150.8469696 secs/batch = 0.5455s, grad.norm=0.66772181\n",
      " 12627: 8 [  635/ 1499], train_loss/perplexity = 4.65718842/105.3394928 secs/batch = 0.5547s, grad.norm=0.67427576\n",
      " 12632: 8 [  640/ 1499], train_loss/perplexity = 5.01264906/150.3023682 secs/batch = 0.5651s, grad.norm=0.66038430\n",
      " 12637: 8 [  645/ 1499], train_loss/perplexity = 4.76820755/117.7080688 secs/batch = 0.5530s, grad.norm=0.63686562\n",
      " 12642: 8 [  650/ 1499], train_loss/perplexity = 4.53784609/93.4892120 secs/batch = 0.5480s, grad.norm=0.63281745\n",
      " 12647: 8 [  655/ 1499], train_loss/perplexity = 5.04932356/155.9169617 secs/batch = 0.5589s, grad.norm=0.60417891\n",
      " 12652: 8 [  660/ 1499], train_loss/perplexity = 5.13970995/170.6662598 secs/batch = 0.5507s, grad.norm=0.65006655\n",
      " 12657: 8 [  665/ 1499], train_loss/perplexity = 5.02521944/152.2036591 secs/batch = 0.6327s, grad.norm=0.61507159\n",
      " 12662: 8 [  670/ 1499], train_loss/perplexity = 5.27138901/194.6861877 secs/batch = 0.5464s, grad.norm=0.65621883\n",
      " 12667: 8 [  675/ 1499], train_loss/perplexity = 5.10377216/164.6417999 secs/batch = 0.5559s, grad.norm=0.65622258\n",
      " 12672: 8 [  680/ 1499], train_loss/perplexity = 5.01542807/150.7206421 secs/batch = 0.6059s, grad.norm=0.64595616\n",
      " 12677: 8 [  685/ 1499], train_loss/perplexity = 5.21864462/184.6837006 secs/batch = 0.5441s, grad.norm=0.65682775\n",
      " 12682: 8 [  690/ 1499], train_loss/perplexity = 4.98653603/146.4283142 secs/batch = 0.5469s, grad.norm=0.63867927\n",
      " 12687: 8 [  695/ 1499], train_loss/perplexity = 5.05492067/156.7920990 secs/batch = 0.5431s, grad.norm=0.63335621\n",
      " 12692: 8 [  700/ 1499], train_loss/perplexity = 5.14740801/171.9851227 secs/batch = 0.5503s, grad.norm=0.61831260\n",
      " 12697: 8 [  705/ 1499], train_loss/perplexity = 4.67794371/107.5486908 secs/batch = 0.5444s, grad.norm=0.58606964\n",
      " 12702: 8 [  710/ 1499], train_loss/perplexity = 5.03045082/153.0019684 secs/batch = 0.5732s, grad.norm=0.63964683\n",
      " 12707: 8 [  715/ 1499], train_loss/perplexity = 4.99007940/146.9480896 secs/batch = 0.5481s, grad.norm=0.62558025\n",
      " 12712: 8 [  720/ 1499], train_loss/perplexity = 5.01460409/150.5964966 secs/batch = 0.5745s, grad.norm=0.62591046\n",
      " 12717: 8 [  725/ 1499], train_loss/perplexity = 4.83369637/125.6746445 secs/batch = 0.6792s, grad.norm=0.65317154\n",
      " 12722: 8 [  730/ 1499], train_loss/perplexity = 4.97078800/144.1404266 secs/batch = 0.6009s, grad.norm=0.63516277\n",
      " 12727: 8 [  735/ 1499], train_loss/perplexity = 4.85360098/128.2012024 secs/batch = 0.6919s, grad.norm=0.63057554\n",
      " 12732: 8 [  740/ 1499], train_loss/perplexity = 4.80702496/122.3670273 secs/batch = 0.6924s, grad.norm=0.65246773\n",
      " 12737: 8 [  745/ 1499], train_loss/perplexity = 4.75994968/116.7400513 secs/batch = 0.5841s, grad.norm=0.61746645\n",
      " 12742: 8 [  750/ 1499], train_loss/perplexity = 4.94517565/140.4955292 secs/batch = 0.5852s, grad.norm=0.62203097\n",
      " 12747: 8 [  755/ 1499], train_loss/perplexity = 4.77307081/118.2819061 secs/batch = 0.5437s, grad.norm=0.67146802\n",
      " 12752: 8 [  760/ 1499], train_loss/perplexity = 4.39014387/80.6520233 secs/batch = 0.6110s, grad.norm=0.66229695\n",
      " 12757: 8 [  765/ 1499], train_loss/perplexity = 4.77945805/119.0398178 secs/batch = 0.5760s, grad.norm=0.65846568\n",
      " 12762: 8 [  770/ 1499], train_loss/perplexity = 4.93326473/138.8320160 secs/batch = 0.5377s, grad.norm=0.60246158\n",
      " 12767: 8 [  775/ 1499], train_loss/perplexity = 5.25580835/191.6763611 secs/batch = 0.6389s, grad.norm=0.66313219\n",
      " 12772: 8 [  780/ 1499], train_loss/perplexity = 4.86505938/129.6786346 secs/batch = 0.6101s, grad.norm=0.56538928\n",
      " 12777: 8 [  785/ 1499], train_loss/perplexity = 4.98852015/146.7191467 secs/batch = 0.6434s, grad.norm=0.61985648\n",
      " 12782: 8 [  790/ 1499], train_loss/perplexity = 4.57970142/97.4852829 secs/batch = 0.5842s, grad.norm=0.61766571\n",
      " 12787: 8 [  795/ 1499], train_loss/perplexity = 4.95164776/141.4077759 secs/batch = 0.5775s, grad.norm=0.59986240\n",
      " 12792: 8 [  800/ 1499], train_loss/perplexity = 4.92865419/138.1934052 secs/batch = 0.5469s, grad.norm=0.65234709\n",
      " 12797: 8 [  805/ 1499], train_loss/perplexity = 4.93089533/138.5034637 secs/batch = 0.5996s, grad.norm=0.62033641\n",
      " 12802: 8 [  810/ 1499], train_loss/perplexity = 4.83346319/125.6453400 secs/batch = 0.5443s, grad.norm=0.58348209\n",
      " 12807: 8 [  815/ 1499], train_loss/perplexity = 5.07201147/159.4948273 secs/batch = 0.5615s, grad.norm=0.62859112\n",
      " 12812: 8 [  820/ 1499], train_loss/perplexity = 4.60942888/100.4267807 secs/batch = 0.5854s, grad.norm=0.61410946\n",
      " 12817: 8 [  825/ 1499], train_loss/perplexity = 4.70518064/110.5182495 secs/batch = 0.5642s, grad.norm=0.63964933\n",
      " 12822: 8 [  830/ 1499], train_loss/perplexity = 4.93606853/139.2218323 secs/batch = 0.5395s, grad.norm=0.62310344\n",
      " 12827: 8 [  835/ 1499], train_loss/perplexity = 4.94782782/140.8686371 secs/batch = 0.5680s, grad.norm=0.62762272\n",
      " 12832: 8 [  840/ 1499], train_loss/perplexity = 4.83072138/125.3013153 secs/batch = 0.5813s, grad.norm=0.63294023\n",
      " 12837: 8 [  845/ 1499], train_loss/perplexity = 4.68284035/108.0766144 secs/batch = 0.5556s, grad.norm=0.61938798\n",
      " 12842: 8 [  850/ 1499], train_loss/perplexity = 4.77806425/118.8740158 secs/batch = 0.5840s, grad.norm=0.62738150\n",
      " 12847: 8 [  855/ 1499], train_loss/perplexity = 4.94655085/140.6888733 secs/batch = 0.5532s, grad.norm=0.66535866\n",
      " 12852: 8 [  860/ 1499], train_loss/perplexity = 4.42144156/83.2161560 secs/batch = 0.5749s, grad.norm=0.65454251\n",
      " 12857: 8 [  865/ 1499], train_loss/perplexity = 4.75500393/116.1641083 secs/batch = 0.5589s, grad.norm=0.62029749\n",
      " 12862: 8 [  870/ 1499], train_loss/perplexity = 4.80643129/122.2944031 secs/batch = 0.5624s, grad.norm=0.68493265\n",
      " 12867: 8 [  875/ 1499], train_loss/perplexity = 4.86484480/129.6508179 secs/batch = 0.5553s, grad.norm=0.61524719\n",
      " 12872: 8 [  880/ 1499], train_loss/perplexity = 4.88187790/131.8780823 secs/batch = 0.5528s, grad.norm=0.60088605\n",
      " 12877: 8 [  885/ 1499], train_loss/perplexity = 4.71311760/111.3989182 secs/batch = 0.7184s, grad.norm=0.64182109\n",
      " 12882: 8 [  890/ 1499], train_loss/perplexity = 5.01918554/151.2880402 secs/batch = 0.5859s, grad.norm=0.61751050\n",
      " 12887: 8 [  895/ 1499], train_loss/perplexity = 5.11180687/165.9699707 secs/batch = 0.6522s, grad.norm=0.62296087\n",
      " 12892: 8 [  900/ 1499], train_loss/perplexity = 4.71462059/111.5664749 secs/batch = 0.5856s, grad.norm=0.58283705\n",
      " 12897: 8 [  905/ 1499], train_loss/perplexity = 4.86543322/129.7271271 secs/batch = 0.6602s, grad.norm=0.60767180\n",
      " 12902: 8 [  910/ 1499], train_loss/perplexity = 5.06031227/157.6397400 secs/batch = 0.5864s, grad.norm=0.62671620\n",
      " 12907: 8 [  915/ 1499], train_loss/perplexity = 4.89687395/133.8706360 secs/batch = 0.5404s, grad.norm=0.68679565\n",
      " 12912: 8 [  920/ 1499], train_loss/perplexity = 4.60570860/100.0538559 secs/batch = 0.6181s, grad.norm=0.57183474\n",
      " 12917: 8 [  925/ 1499], train_loss/perplexity = 4.84318542/126.8728561 secs/batch = 0.6148s, grad.norm=0.63525486\n",
      " 12922: 8 [  930/ 1499], train_loss/perplexity = 4.90152216/134.4943390 secs/batch = 0.5402s, grad.norm=0.65462846\n",
      " 12927: 8 [  935/ 1499], train_loss/perplexity = 5.03718662/154.0360413 secs/batch = 0.5767s, grad.norm=0.63661945\n",
      " 12932: 8 [  940/ 1499], train_loss/perplexity = 4.59639454/99.1262741 secs/batch = 0.5485s, grad.norm=0.65841258\n",
      " 12937: 8 [  945/ 1499], train_loss/perplexity = 5.03124666/153.1237946 secs/batch = 0.5480s, grad.norm=0.60298651\n",
      " 12942: 8 [  950/ 1499], train_loss/perplexity = 4.99277782/147.3451538 secs/batch = 0.5552s, grad.norm=0.64151567\n",
      " 12947: 8 [  955/ 1499], train_loss/perplexity = 5.15478659/173.2588196 secs/batch = 0.5455s, grad.norm=0.64735705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12952: 8 [  960/ 1499], train_loss/perplexity = 4.84892321/127.6029129 secs/batch = 0.5418s, grad.norm=0.64602643\n",
      " 12957: 8 [  965/ 1499], train_loss/perplexity = 4.87381887/130.8195496 secs/batch = 0.6015s, grad.norm=0.62999624\n",
      " 12962: 8 [  970/ 1499], train_loss/perplexity = 5.12058163/167.4327240 secs/batch = 0.5892s, grad.norm=0.70906788\n",
      " 12967: 8 [  975/ 1499], train_loss/perplexity = 4.77651596/118.6901093 secs/batch = 0.5840s, grad.norm=0.63882500\n",
      " 12972: 8 [  980/ 1499], train_loss/perplexity = 4.96374655/143.1290283 secs/batch = 0.5806s, grad.norm=0.59894627\n",
      " 12977: 8 [  985/ 1499], train_loss/perplexity = 4.69333076/109.2163467 secs/batch = 0.5789s, grad.norm=0.65410972\n",
      " 12982: 8 [  990/ 1499], train_loss/perplexity = 4.82820177/124.9860077 secs/batch = 0.5860s, grad.norm=0.59659171\n",
      " 12987: 8 [  995/ 1499], train_loss/perplexity = 5.17659569/177.0789490 secs/batch = 0.6744s, grad.norm=0.63902086\n",
      " 12992: 8 [ 1000/ 1499], train_loss/perplexity = 4.89375925/133.4543152 secs/batch = 0.6058s, grad.norm=0.63475603\n",
      " 12997: 8 [ 1005/ 1499], train_loss/perplexity = 5.28332233/197.0233612 secs/batch = 0.6338s, grad.norm=0.62415326\n",
      " 13002: 8 [ 1010/ 1499], train_loss/perplexity = 5.07177114/159.4564972 secs/batch = 0.5787s, grad.norm=0.66400051\n",
      " 13007: 8 [ 1015/ 1499], train_loss/perplexity = 4.90892220/135.4933014 secs/batch = 0.5415s, grad.norm=0.62353098\n",
      " 13012: 8 [ 1020/ 1499], train_loss/perplexity = 4.97030973/144.0715027 secs/batch = 0.5572s, grad.norm=0.63187659\n",
      " 13017: 8 [ 1025/ 1499], train_loss/perplexity = 4.94048595/139.8381805 secs/batch = 0.5406s, grad.norm=0.61803007\n",
      " 13022: 8 [ 1030/ 1499], train_loss/perplexity = 5.13228035/169.4029694 secs/batch = 0.5792s, grad.norm=0.61696512\n",
      " 13027: 8 [ 1035/ 1499], train_loss/perplexity = 5.33460951/207.3917542 secs/batch = 0.5346s, grad.norm=0.63446873\n",
      " 13032: 8 [ 1040/ 1499], train_loss/perplexity = 4.71384621/111.4801102 secs/batch = 0.5707s, grad.norm=0.58283162\n",
      " 13037: 8 [ 1045/ 1499], train_loss/perplexity = 4.84480095/127.0779877 secs/batch = 0.5415s, grad.norm=0.59057492\n",
      " 13042: 8 [ 1050/ 1499], train_loss/perplexity = 4.54589272/94.2445221 secs/batch = 0.5522s, grad.norm=0.58721119\n",
      " 13047: 8 [ 1055/ 1499], train_loss/perplexity = 4.91401052/136.1844940 secs/batch = 0.5445s, grad.norm=0.61794531\n",
      " 13052: 8 [ 1060/ 1499], train_loss/perplexity = 5.16193581/174.5019379 secs/batch = 0.5467s, grad.norm=0.62462485\n",
      " 13057: 8 [ 1065/ 1499], train_loss/perplexity = 4.69583130/109.4897919 secs/batch = 0.6402s, grad.norm=0.59398669\n",
      " 13062: 8 [ 1070/ 1499], train_loss/perplexity = 5.12720633/168.5455933 secs/batch = 0.5418s, grad.norm=0.61443669\n",
      " 13067: 8 [ 1075/ 1499], train_loss/perplexity = 5.00597143/149.3020477 secs/batch = 0.5652s, grad.norm=0.61505234\n",
      " 13072: 8 [ 1080/ 1499], train_loss/perplexity = 5.17827654/177.3768463 secs/batch = 0.5754s, grad.norm=0.63921517\n",
      " 13077: 8 [ 1085/ 1499], train_loss/perplexity = 5.28630161/197.6112366 secs/batch = 0.6273s, grad.norm=0.62511337\n",
      " 13082: 8 [ 1090/ 1499], train_loss/perplexity = 4.87117290/130.4738617 secs/batch = 0.6085s, grad.norm=0.62433070\n",
      " 13087: 8 [ 1095/ 1499], train_loss/perplexity = 5.05749512/157.1962585 secs/batch = 0.5549s, grad.norm=0.62671417\n",
      " 13092: 8 [ 1100/ 1499], train_loss/perplexity = 5.08885098/162.2033844 secs/batch = 0.5455s, grad.norm=0.63293511\n",
      " 13097: 8 [ 1105/ 1499], train_loss/perplexity = 4.86614180/129.8190765 secs/batch = 0.5378s, grad.norm=0.65188456\n",
      " 13102: 8 [ 1110/ 1499], train_loss/perplexity = 4.99880743/148.2362671 secs/batch = 0.5398s, grad.norm=0.59701681\n",
      " 13107: 8 [ 1115/ 1499], train_loss/perplexity = 4.62454319/101.9561844 secs/batch = 0.5513s, grad.norm=0.59277642\n",
      " 13112: 8 [ 1120/ 1499], train_loss/perplexity = 4.87194633/130.5748138 secs/batch = 0.5738s, grad.norm=0.65453893\n",
      " 13117: 8 [ 1125/ 1499], train_loss/perplexity = 4.81245899/123.0337830 secs/batch = 0.5869s, grad.norm=0.63404721\n",
      " 13122: 8 [ 1130/ 1499], train_loss/perplexity = 4.64033699/103.5792465 secs/batch = 0.5733s, grad.norm=0.59043175\n",
      " 13127: 8 [ 1135/ 1499], train_loss/perplexity = 5.24823475/190.2301636 secs/batch = 0.5811s, grad.norm=0.58330274\n",
      " 13132: 8 [ 1140/ 1499], train_loss/perplexity = 5.03045321/153.0023346 secs/batch = 0.6321s, grad.norm=0.62919223\n",
      " 13137: 8 [ 1145/ 1499], train_loss/perplexity = 5.17253828/176.3619232 secs/batch = 0.6059s, grad.norm=0.64650482\n",
      " 13142: 8 [ 1150/ 1499], train_loss/perplexity = 5.16440773/174.9338226 secs/batch = 0.5830s, grad.norm=0.61443508\n",
      " 13147: 8 [ 1155/ 1499], train_loss/perplexity = 4.80705357/122.3705292 secs/batch = 0.6841s, grad.norm=0.65162981\n",
      " 13152: 8 [ 1160/ 1499], train_loss/perplexity = 4.95849323/142.3791046 secs/batch = 0.6279s, grad.norm=0.63623625\n",
      " 13157: 8 [ 1165/ 1499], train_loss/perplexity = 4.94075155/139.8753357 secs/batch = 0.5761s, grad.norm=0.65340561\n",
      " 13162: 8 [ 1170/ 1499], train_loss/perplexity = 5.04921865/155.9006042 secs/batch = 0.5635s, grad.norm=0.63950074\n",
      " 13167: 8 [ 1175/ 1499], train_loss/perplexity = 4.37373495/79.3394089 secs/batch = 0.5805s, grad.norm=0.67549127\n",
      " 13172: 8 [ 1180/ 1499], train_loss/perplexity = 4.67859459/107.6187210 secs/batch = 0.5854s, grad.norm=0.60941267\n",
      " 13177: 8 [ 1185/ 1499], train_loss/perplexity = 4.65464401/105.0718079 secs/batch = 0.5977s, grad.norm=0.61870867\n",
      " 13182: 8 [ 1190/ 1499], train_loss/perplexity = 4.92541838/137.7469635 secs/batch = 0.5877s, grad.norm=0.64921194\n",
      " 13187: 8 [ 1195/ 1499], train_loss/perplexity = 4.86716461/129.9519348 secs/batch = 0.6929s, grad.norm=0.62037510\n",
      " 13192: 8 [ 1200/ 1499], train_loss/perplexity = 4.72728872/112.9888000 secs/batch = 0.5759s, grad.norm=0.62994975\n",
      " 13197: 8 [ 1205/ 1499], train_loss/perplexity = 4.71209764/111.2853546 secs/batch = 0.5806s, grad.norm=0.63429278\n",
      " 13202: 8 [ 1210/ 1499], train_loss/perplexity = 4.66283369/105.9358444 secs/batch = 0.5787s, grad.norm=0.64490902\n",
      " 13207: 8 [ 1215/ 1499], train_loss/perplexity = 4.40583086/81.9271851 secs/batch = 0.6626s, grad.norm=0.64874625\n",
      " 13212: 8 [ 1220/ 1499], train_loss/perplexity = 4.80193090/121.7452698 secs/batch = 0.6366s, grad.norm=0.64039356\n",
      " 13217: 8 [ 1225/ 1499], train_loss/perplexity = 4.15362740/63.6645164 secs/batch = 0.6922s, grad.norm=0.61961079\n",
      " 13222: 8 [ 1230/ 1499], train_loss/perplexity = 4.70367146/110.3515778 secs/batch = 0.6358s, grad.norm=0.63003922\n",
      " 13227: 8 [ 1235/ 1499], train_loss/perplexity = 4.42275095/83.3251953 secs/batch = 1.1875s, grad.norm=0.67751682\n",
      " 13232: 8 [ 1240/ 1499], train_loss/perplexity = 4.64364767/103.9227295 secs/batch = 1.0132s, grad.norm=0.65490121\n",
      " 13237: 8 [ 1245/ 1499], train_loss/perplexity = 4.95045471/141.2391663 secs/batch = 0.6234s, grad.norm=0.63058484\n",
      " 13242: 8 [ 1250/ 1499], train_loss/perplexity = 5.07186365/159.4712524 secs/batch = 0.5463s, grad.norm=0.74661267\n",
      " 13247: 8 [ 1255/ 1499], train_loss/perplexity = 4.80239105/121.8013000 secs/batch = 0.5863s, grad.norm=0.61965883\n",
      " 13252: 8 [ 1260/ 1499], train_loss/perplexity = 4.94645500/140.6753845 secs/batch = 0.5557s, grad.norm=0.60764486\n",
      " 13257: 8 [ 1265/ 1499], train_loss/perplexity = 4.94260406/140.1346893 secs/batch = 0.5545s, grad.norm=0.64013731\n",
      " 13262: 8 [ 1270/ 1499], train_loss/perplexity = 4.95483255/141.8588562 secs/batch = 0.5526s, grad.norm=0.60655987\n",
      " 13267: 8 [ 1275/ 1499], train_loss/perplexity = 4.93663645/139.3009186 secs/batch = 0.5509s, grad.norm=0.64981425\n",
      " 13272: 8 [ 1280/ 1499], train_loss/perplexity = 4.55786228/95.3793640 secs/batch = 0.5949s, grad.norm=0.60389459\n",
      " 13277: 8 [ 1285/ 1499], train_loss/perplexity = 5.07176924/159.4561920 secs/batch = 0.6453s, grad.norm=0.65263152\n",
      " 13282: 8 [ 1290/ 1499], train_loss/perplexity = 4.84347439/126.9095230 secs/batch = 0.6394s, grad.norm=0.62717444\n",
      " 13287: 8 [ 1295/ 1499], train_loss/perplexity = 4.85733557/128.6808777 secs/batch = 0.6006s, grad.norm=0.64161670\n",
      " 13292: 8 [ 1300/ 1499], train_loss/perplexity = 4.94707680/140.7628784 secs/batch = 0.5880s, grad.norm=0.65226561\n",
      " 13297: 8 [ 1305/ 1499], train_loss/perplexity = 4.84277487/126.8207779 secs/batch = 0.5957s, grad.norm=0.62173474\n",
      " 13302: 8 [ 1310/ 1499], train_loss/perplexity = 5.02197409/151.7104950 secs/batch = 0.5933s, grad.norm=0.64650941\n",
      " 13307: 8 [ 1315/ 1499], train_loss/perplexity = 4.74490690/114.9971008 secs/batch = 0.5942s, grad.norm=0.60829675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13312: 8 [ 1320/ 1499], train_loss/perplexity = 4.54731035/94.3782196 secs/batch = 0.5977s, grad.norm=0.61563301\n",
      " 13317: 8 [ 1325/ 1499], train_loss/perplexity = 4.91209888/135.9244080 secs/batch = 0.6434s, grad.norm=0.62333131\n",
      " 13322: 8 [ 1330/ 1499], train_loss/perplexity = 5.06177664/157.8707428 secs/batch = 0.5720s, grad.norm=0.65150470\n",
      " 13327: 8 [ 1335/ 1499], train_loss/perplexity = 4.84289551/126.8360748 secs/batch = 0.5903s, grad.norm=0.61143804\n",
      " 13332: 8 [ 1340/ 1499], train_loss/perplexity = 4.32753801/75.7575455 secs/batch = 0.5718s, grad.norm=0.63493007\n",
      " 13337: 8 [ 1345/ 1499], train_loss/perplexity = 4.50455189/90.4278107 secs/batch = 1.0842s, grad.norm=0.64380777\n",
      " 13342: 8 [ 1350/ 1499], train_loss/perplexity = 4.69060326/108.9188690 secs/batch = 0.5455s, grad.norm=0.60318297\n",
      " 13347: 8 [ 1355/ 1499], train_loss/perplexity = 4.29185772/73.1021423 secs/batch = 0.5468s, grad.norm=0.63174069\n",
      " 13352: 8 [ 1360/ 1499], train_loss/perplexity = 4.59291744/98.7822037 secs/batch = 0.5467s, grad.norm=0.60349262\n",
      " 13357: 8 [ 1365/ 1499], train_loss/perplexity = 4.30755758/74.2588959 secs/batch = 0.5503s, grad.norm=0.62256753\n",
      " 13362: 8 [ 1370/ 1499], train_loss/perplexity = 4.36095524/78.3319244 secs/batch = 0.5455s, grad.norm=0.60183018\n",
      " 13367: 8 [ 1375/ 1499], train_loss/perplexity = 4.46974039/87.3340454 secs/batch = 0.5452s, grad.norm=0.66462129\n",
      " 13372: 8 [ 1380/ 1499], train_loss/perplexity = 4.92383766/137.5293884 secs/batch = 0.5506s, grad.norm=0.63950586\n",
      " 13377: 8 [ 1385/ 1499], train_loss/perplexity = 4.64387274/103.9461288 secs/batch = 0.5515s, grad.norm=0.62944412\n",
      " 13382: 8 [ 1390/ 1499], train_loss/perplexity = 4.70998144/111.0501022 secs/batch = 0.5378s, grad.norm=0.62699807\n",
      " 13387: 8 [ 1395/ 1499], train_loss/perplexity = 4.91636848/136.5059814 secs/batch = 0.5520s, grad.norm=0.62717164\n",
      " 13392: 8 [ 1400/ 1499], train_loss/perplexity = 4.95453310/141.8163757 secs/batch = 0.5515s, grad.norm=0.61413133\n",
      " 13397: 8 [ 1405/ 1499], train_loss/perplexity = 4.81672668/123.5599747 secs/batch = 0.5432s, grad.norm=0.63374192\n",
      " 13402: 8 [ 1410/ 1499], train_loss/perplexity = 4.93943453/139.6912384 secs/batch = 0.5415s, grad.norm=0.61161125\n",
      " 13407: 8 [ 1415/ 1499], train_loss/perplexity = 4.92848587/138.1701508 secs/batch = 0.5498s, grad.norm=0.62599361\n",
      " 13412: 8 [ 1420/ 1499], train_loss/perplexity = 4.87068319/130.4099884 secs/batch = 0.6157s, grad.norm=0.62236834\n",
      " 13417: 8 [ 1425/ 1499], train_loss/perplexity = 4.77104187/118.0421600 secs/batch = 0.5500s, grad.norm=0.64823252\n",
      " 13422: 8 [ 1430/ 1499], train_loss/perplexity = 4.94764471/140.8428497 secs/batch = 0.5583s, grad.norm=0.64793277\n",
      " 13427: 8 [ 1435/ 1499], train_loss/perplexity = 4.69672251/109.5874100 secs/batch = 0.6047s, grad.norm=0.61745292\n",
      " 13432: 8 [ 1440/ 1499], train_loss/perplexity = 4.36681843/78.7925491 secs/batch = 0.5521s, grad.norm=0.64671564\n",
      " 13437: 8 [ 1445/ 1499], train_loss/perplexity = 4.78770876/120.0260468 secs/batch = 0.5849s, grad.norm=0.62453920\n",
      " 13442: 8 [ 1450/ 1499], train_loss/perplexity = 4.95812654/142.3269043 secs/batch = 0.5467s, grad.norm=0.63053036\n",
      " 13447: 8 [ 1455/ 1499], train_loss/perplexity = 5.19844532/180.9906464 secs/batch = 0.5506s, grad.norm=0.64806402\n",
      " 13452: 8 [ 1460/ 1499], train_loss/perplexity = 5.12095594/167.4954071 secs/batch = 0.6073s, grad.norm=0.61533237\n",
      " 13457: 8 [ 1465/ 1499], train_loss/perplexity = 5.25973272/192.4300537 secs/batch = 0.7113s, grad.norm=0.63096380\n",
      " 13462: 8 [ 1470/ 1499], train_loss/perplexity = 4.95438099/141.7947998 secs/batch = 0.5860s, grad.norm=0.62105954\n",
      " 13467: 8 [ 1475/ 1499], train_loss/perplexity = 5.11329556/166.2172241 secs/batch = 0.6275s, grad.norm=0.65959710\n",
      " 13472: 8 [ 1480/ 1499], train_loss/perplexity = 5.00065422/148.5102844 secs/batch = 0.6042s, grad.norm=0.61785150\n",
      " 13477: 8 [ 1485/ 1499], train_loss/perplexity = 4.72455692/112.6805649 secs/batch = 0.6481s, grad.norm=0.63826346\n",
      " 13482: 8 [ 1490/ 1499], train_loss/perplexity = 4.89832115/134.0645142 secs/batch = 0.5905s, grad.norm=0.64602125\n",
      " 13487: 8 [ 1495/ 1499], train_loss/perplexity = 5.20480824/182.1459351 secs/batch = 0.6146s, grad.norm=0.64681828\n",
      "Epoch training time: 877.8147974014282\n",
      "Saved char model cv/epoch008_4.9944.model\n",
      " 13496: 9 [    5/ 1499], train_loss/perplexity = 4.98807526/146.6538849 secs/batch = 0.5936s, grad.norm=0.62514126\n",
      " 13501: 9 [   10/ 1499], train_loss/perplexity = 5.00042582/148.4763641 secs/batch = 0.6118s, grad.norm=0.63979530\n",
      " 13506: 9 [   15/ 1499], train_loss/perplexity = 4.88077307/131.7324677 secs/batch = 0.6747s, grad.norm=0.62940115\n",
      " 13511: 9 [   20/ 1499], train_loss/perplexity = 4.70926523/110.9705887 secs/batch = 0.6633s, grad.norm=0.57937998\n",
      " 13516: 9 [   25/ 1499], train_loss/perplexity = 5.13564491/169.9739075 secs/batch = 0.5974s, grad.norm=0.65469337\n",
      " 13521: 9 [   30/ 1499], train_loss/perplexity = 5.09522772/163.2410126 secs/batch = 0.6559s, grad.norm=0.66862226\n",
      " 13526: 9 [   35/ 1499], train_loss/perplexity = 4.97737074/145.0923920 secs/batch = 0.5749s, grad.norm=0.64735806\n",
      " 13531: 9 [   40/ 1499], train_loss/perplexity = 5.00196552/148.7051544 secs/batch = 0.5688s, grad.norm=0.64482719\n",
      " 13536: 9 [   45/ 1499], train_loss/perplexity = 4.89436960/133.5357971 secs/batch = 0.5595s, grad.norm=0.61850762\n",
      " 13541: 9 [   50/ 1499], train_loss/perplexity = 4.91337347/136.0977631 secs/batch = 1.2836s, grad.norm=0.63548827\n",
      " 13546: 9 [   55/ 1499], train_loss/perplexity = 4.70671320/110.6877518 secs/batch = 0.6910s, grad.norm=0.64939576\n",
      " 13551: 9 [   60/ 1499], train_loss/perplexity = 4.63923311/103.4649734 secs/batch = 0.5549s, grad.norm=0.61636567\n",
      " 13556: 9 [   65/ 1499], train_loss/perplexity = 4.71241713/111.3209152 secs/batch = 0.5695s, grad.norm=0.66372800\n",
      " 13561: 9 [   70/ 1499], train_loss/perplexity = 4.71776867/111.9182434 secs/batch = 0.6195s, grad.norm=0.63118517\n",
      " 13566: 9 [   75/ 1499], train_loss/perplexity = 4.59345055/98.8348770 secs/batch = 0.6084s, grad.norm=0.63707411\n",
      " 13571: 9 [   80/ 1499], train_loss/perplexity = 4.69360638/109.2464523 secs/batch = 0.5702s, grad.norm=0.62559700\n",
      " 13576: 9 [   85/ 1499], train_loss/perplexity = 4.49022007/89.1410599 secs/batch = 0.5426s, grad.norm=0.61997449\n",
      " 13581: 9 [   90/ 1499], train_loss/perplexity = 4.78893805/120.1736832 secs/batch = 0.5913s, grad.norm=0.64195114\n",
      " 13586: 9 [   95/ 1499], train_loss/perplexity = 4.50799656/90.7398453 secs/batch = 0.5532s, grad.norm=0.63492024\n",
      " 13591: 9 [  100/ 1499], train_loss/perplexity = 4.60383463/99.8665314 secs/batch = 0.5817s, grad.norm=0.61473393\n",
      " 13596: 9 [  105/ 1499], train_loss/perplexity = 4.50270653/90.2610931 secs/batch = 0.5613s, grad.norm=0.66044998\n",
      " 13601: 9 [  110/ 1499], train_loss/perplexity = 4.63682318/103.2159271 secs/batch = 0.6347s, grad.norm=0.65012413\n",
      " 13606: 9 [  115/ 1499], train_loss/perplexity = 4.79154873/120.4878235 secs/batch = 0.5599s, grad.norm=0.62163454\n",
      " 13611: 9 [  120/ 1499], train_loss/perplexity = 4.58415270/97.9201813 secs/batch = 0.5430s, grad.norm=0.59564322\n",
      " 13616: 9 [  125/ 1499], train_loss/perplexity = 4.77487040/118.4949570 secs/batch = 0.5531s, grad.norm=0.66643739\n",
      " 13621: 9 [  130/ 1499], train_loss/perplexity = 4.85827208/128.8014526 secs/batch = 0.6295s, grad.norm=0.65768969\n",
      " 13626: 9 [  135/ 1499], train_loss/perplexity = 4.88209009/131.9060669 secs/batch = 0.5872s, grad.norm=0.67724293\n",
      " 13631: 9 [  140/ 1499], train_loss/perplexity = 4.89518404/133.6446075 secs/batch = 0.6347s, grad.norm=0.65647215\n",
      " 13636: 9 [  145/ 1499], train_loss/perplexity = 4.58741093/98.2397537 secs/batch = 0.5606s, grad.norm=0.64763194\n",
      " 13641: 9 [  150/ 1499], train_loss/perplexity = 4.79519129/120.9275131 secs/batch = 0.5652s, grad.norm=0.63399488\n",
      " 13646: 9 [  155/ 1499], train_loss/perplexity = 4.94152641/139.9837646 secs/batch = 0.5584s, grad.norm=0.60101414\n",
      " 13651: 9 [  160/ 1499], train_loss/perplexity = 5.22688818/186.2124481 secs/batch = 0.5873s, grad.norm=0.63854790\n",
      " 13656: 9 [  165/ 1499], train_loss/perplexity = 4.82694435/124.8289413 secs/batch = 0.5636s, grad.norm=0.67907703\n",
      " 13661: 9 [  170/ 1499], train_loss/perplexity = 5.14054680/170.8091431 secs/batch = 0.5542s, grad.norm=0.64407742\n",
      " 13666: 9 [  175/ 1499], train_loss/perplexity = 4.84050846/126.5336761 secs/batch = 0.6010s, grad.norm=0.61774075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13671: 9 [  180/ 1499], train_loss/perplexity = 5.04420137/155.1203613 secs/batch = 0.5457s, grad.norm=0.64704633\n",
      " 13676: 9 [  185/ 1499], train_loss/perplexity = 4.87427473/130.8791962 secs/batch = 0.5538s, grad.norm=0.64361888\n",
      " 13681: 9 [  190/ 1499], train_loss/perplexity = 4.96886778/143.8639069 secs/batch = 0.6234s, grad.norm=0.63555056\n",
      " 13686: 9 [  195/ 1499], train_loss/perplexity = 5.12590551/168.3264923 secs/batch = 0.5865s, grad.norm=0.64932841\n",
      " 13691: 9 [  200/ 1499], train_loss/perplexity = 5.06811476/158.8745270 secs/batch = 0.5765s, grad.norm=0.65143234\n",
      " 13696: 9 [  205/ 1499], train_loss/perplexity = 4.95790339/142.2951508 secs/batch = 0.5474s, grad.norm=0.63576275\n",
      " 13701: 9 [  210/ 1499], train_loss/perplexity = 4.80340528/121.9249039 secs/batch = 0.5651s, grad.norm=0.61377412\n",
      " 13706: 9 [  215/ 1499], train_loss/perplexity = 5.02237654/151.7715607 secs/batch = 0.5488s, grad.norm=0.62258363\n",
      " 13711: 9 [  220/ 1499], train_loss/perplexity = 4.74233580/114.7018127 secs/batch = 0.5439s, grad.norm=0.56723368\n",
      " 13716: 9 [  225/ 1499], train_loss/perplexity = 4.94115686/139.9320374 secs/batch = 0.5574s, grad.norm=0.58823234\n",
      " 13721: 9 [  230/ 1499], train_loss/perplexity = 5.02836800/152.6836243 secs/batch = 0.5694s, grad.norm=0.62728101\n",
      " 13726: 9 [  235/ 1499], train_loss/perplexity = 5.07710218/160.3088379 secs/batch = 0.5664s, grad.norm=0.61076730\n",
      " 13731: 9 [  240/ 1499], train_loss/perplexity = 5.03902054/154.3187866 secs/batch = 0.6222s, grad.norm=0.63991606\n",
      " 13736: 9 [  245/ 1499], train_loss/perplexity = 4.78152037/119.2855682 secs/batch = 0.5964s, grad.norm=0.65464514\n",
      " 13741: 9 [  250/ 1499], train_loss/perplexity = 4.82213068/124.2294998 secs/batch = 0.5885s, grad.norm=0.63978732\n",
      " 13746: 9 [  255/ 1499], train_loss/perplexity = 4.69280481/109.1589203 secs/batch = 0.5468s, grad.norm=0.63390070\n",
      " 13751: 9 [  260/ 1499], train_loss/perplexity = 5.01082182/150.0279846 secs/batch = 0.5604s, grad.norm=0.61427438\n",
      " 13756: 9 [  265/ 1499], train_loss/perplexity = 4.95278406/141.5685577 secs/batch = 0.7002s, grad.norm=0.63930315\n",
      " 13761: 9 [  270/ 1499], train_loss/perplexity = 4.98724270/146.5318298 secs/batch = 0.7052s, grad.norm=0.61838418\n",
      " 13766: 9 [  275/ 1499], train_loss/perplexity = 4.58926678/98.4222412 secs/batch = 0.5933s, grad.norm=0.59715426\n",
      " 13771: 9 [  280/ 1499], train_loss/perplexity = 4.92178917/137.2479553 secs/batch = 0.6236s, grad.norm=0.63678920\n",
      " 13776: 9 [  285/ 1499], train_loss/perplexity = 5.15825176/173.8602448 secs/batch = 0.6496s, grad.norm=0.61230612\n",
      " 13781: 9 [  290/ 1499], train_loss/perplexity = 5.14360332/171.3320160 secs/batch = 0.5938s, grad.norm=0.63008195\n",
      " 13786: 9 [  295/ 1499], train_loss/perplexity = 4.98159122/145.7060547 secs/batch = 0.5683s, grad.norm=0.62877095\n",
      " 13791: 9 [  300/ 1499], train_loss/perplexity = 4.83516026/125.8587494 secs/batch = 0.6283s, grad.norm=0.60878593\n",
      " 13796: 9 [  305/ 1499], train_loss/perplexity = 5.04188013/154.7607117 secs/batch = 0.6067s, grad.norm=0.62687880\n",
      " 13801: 9 [  310/ 1499], train_loss/perplexity = 5.07441187/159.8781281 secs/batch = 0.5397s, grad.norm=0.62626642\n",
      " 13806: 9 [  315/ 1499], train_loss/perplexity = 5.00423527/149.0430603 secs/batch = 0.6393s, grad.norm=0.66408414\n",
      " 13811: 9 [  320/ 1499], train_loss/perplexity = 4.93478441/139.0431671 secs/batch = 0.5382s, grad.norm=0.64595515\n",
      " 13816: 9 [  325/ 1499], train_loss/perplexity = 4.85616922/128.5308838 secs/batch = 0.5414s, grad.norm=0.62757879\n",
      " 13821: 9 [  330/ 1499], train_loss/perplexity = 4.77276182/118.2453613 secs/batch = 0.5451s, grad.norm=0.60755289\n",
      " 13826: 9 [  335/ 1499], train_loss/perplexity = 4.85716486/128.6589203 secs/batch = 0.5929s, grad.norm=0.67329490\n",
      " 13831: 9 [  340/ 1499], train_loss/perplexity = 4.59327221/98.8172531 secs/batch = 0.5445s, grad.norm=0.65939796\n",
      " 13836: 9 [  345/ 1499], train_loss/perplexity = 4.58864117/98.3606873 secs/batch = 0.6098s, grad.norm=0.60108620\n",
      " 13841: 9 [  350/ 1499], train_loss/perplexity = 4.67886496/107.6478195 secs/batch = 0.5706s, grad.norm=0.63657457\n",
      " 13846: 9 [  355/ 1499], train_loss/perplexity = 4.49994946/90.0125809 secs/batch = 0.6325s, grad.norm=0.66237706\n",
      " 13851: 9 [  360/ 1499], train_loss/perplexity = 4.70341349/110.3231201 secs/batch = 0.5919s, grad.norm=0.62625605\n",
      " 13856: 9 [  365/ 1499], train_loss/perplexity = 4.62864494/102.3752441 secs/batch = 0.5833s, grad.norm=0.59224319\n",
      " 13861: 9 [  370/ 1499], train_loss/perplexity = 4.54087305/93.7726364 secs/batch = 0.5445s, grad.norm=0.61919087\n",
      " 13866: 9 [  375/ 1499], train_loss/perplexity = 4.81300926/123.1015091 secs/batch = 0.5881s, grad.norm=0.57967901\n",
      " 13871: 9 [  380/ 1499], train_loss/perplexity = 5.02025795/151.4503632 secs/batch = 0.5768s, grad.norm=0.64116222\n",
      " 13876: 9 [  385/ 1499], train_loss/perplexity = 5.11988401/167.3159637 secs/batch = 0.5454s, grad.norm=0.63707840\n",
      " 13881: 9 [  390/ 1499], train_loss/perplexity = 5.13366079/169.6369934 secs/batch = 0.6010s, grad.norm=0.62032109\n",
      " 13886: 9 [  395/ 1499], train_loss/perplexity = 5.21309566/183.6617279 secs/batch = 0.6627s, grad.norm=0.62963653\n",
      " 13891: 9 [  400/ 1499], train_loss/perplexity = 4.98397398/146.0536499 secs/batch = 0.6292s, grad.norm=0.66978729\n",
      " 13896: 9 [  405/ 1499], train_loss/perplexity = 5.08200073/161.0960388 secs/batch = 0.5930s, grad.norm=0.66082633\n",
      " 13901: 9 [  410/ 1499], train_loss/perplexity = 4.46159267/86.6253662 secs/batch = 0.5994s, grad.norm=0.65126139\n",
      " 13906: 9 [  415/ 1499], train_loss/perplexity = 5.07241392/159.5590210 secs/batch = 0.5724s, grad.norm=0.62432092\n",
      " 13911: 9 [  420/ 1499], train_loss/perplexity = 4.70603991/110.6132507 secs/batch = 0.5869s, grad.norm=0.59827757\n",
      " 13916: 9 [  425/ 1499], train_loss/perplexity = 4.97802734/145.1876984 secs/batch = 0.5465s, grad.norm=0.63159281\n",
      " 13921: 9 [  430/ 1499], train_loss/perplexity = 4.81919670/123.8655472 secs/batch = 0.5400s, grad.norm=0.61558878\n",
      " 13926: 9 [  435/ 1499], train_loss/perplexity = 4.69280434/109.1588669 secs/batch = 0.5444s, grad.norm=0.64433783\n",
      " 13931: 9 [  440/ 1499], train_loss/perplexity = 4.71016121/111.0700607 secs/batch = 0.5522s, grad.norm=0.70138001\n",
      " 13936: 9 [  445/ 1499], train_loss/perplexity = 4.98531199/146.2491913 secs/batch = 0.5448s, grad.norm=0.66574663\n",
      " 13941: 9 [  450/ 1499], train_loss/perplexity = 4.98217106/145.7905579 secs/batch = 0.5533s, grad.norm=0.60480887\n",
      " 13946: 9 [  455/ 1499], train_loss/perplexity = 4.98414946/146.0792694 secs/batch = 0.5408s, grad.norm=0.63747489\n",
      " 13951: 9 [  460/ 1499], train_loss/perplexity = 4.88247681/131.9570923 secs/batch = 0.5444s, grad.norm=0.64299250\n",
      " 13956: 9 [  465/ 1499], train_loss/perplexity = 4.89281225/133.3280029 secs/batch = 0.6130s, grad.norm=0.66817302\n",
      " 13961: 9 [  470/ 1499], train_loss/perplexity = 5.00193739/148.7009735 secs/batch = 0.5434s, grad.norm=0.64146721\n",
      " 13966: 9 [  475/ 1499], train_loss/perplexity = 4.73148632/113.4640808 secs/batch = 0.5534s, grad.norm=0.61094058\n",
      " 13971: 9 [  480/ 1499], train_loss/perplexity = 4.94414473/140.3507538 secs/batch = 0.5521s, grad.norm=0.62266117\n",
      " 13976: 9 [  485/ 1499], train_loss/perplexity = 4.79760885/121.2202148 secs/batch = 0.5597s, grad.norm=0.67573917\n",
      " 13981: 9 [  490/ 1499], train_loss/perplexity = 4.85578108/128.4810028 secs/batch = 0.5567s, grad.norm=0.66122293\n",
      " 13986: 9 [  495/ 1499], train_loss/perplexity = 4.84512901/127.1196823 secs/batch = 0.5553s, grad.norm=0.61211377\n",
      " 13991: 9 [  500/ 1499], train_loss/perplexity = 4.90657043/135.1750336 secs/batch = 0.5606s, grad.norm=0.63623327\n",
      " 13996: 9 [  505/ 1499], train_loss/perplexity = 4.75183582/115.7966690 secs/batch = 0.5494s, grad.norm=0.64770627\n",
      " 14001: 9 [  510/ 1499], train_loss/perplexity = 5.12655973/168.4366608 secs/batch = 0.6299s, grad.norm=0.63821071\n",
      " 14006: 9 [  515/ 1499], train_loss/perplexity = 4.55664635/95.2634659 secs/batch = 0.5507s, grad.norm=0.62867498\n",
      " 14011: 9 [  520/ 1499], train_loss/perplexity = 4.88954878/132.8936005 secs/batch = 0.5647s, grad.norm=0.63130844\n",
      " 14016: 9 [  525/ 1499], train_loss/perplexity = 5.03911734/154.3337250 secs/batch = 0.5469s, grad.norm=0.66152525\n",
      " 14021: 9 [  530/ 1499], train_loss/perplexity = 4.83439255/125.7621689 secs/batch = 0.5408s, grad.norm=0.63475728\n",
      " 14026: 9 [  535/ 1499], train_loss/perplexity = 4.79973602/121.4783478 secs/batch = 0.5736s, grad.norm=0.65409267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14031: 9 [  540/ 1499], train_loss/perplexity = 4.76116371/116.8818665 secs/batch = 0.6153s, grad.norm=0.64626020\n",
      " 14036: 9 [  545/ 1499], train_loss/perplexity = 4.60927677/100.4115067 secs/batch = 0.6308s, grad.norm=0.63292873\n",
      " 14041: 9 [  550/ 1499], train_loss/perplexity = 5.06042814/157.6580048 secs/batch = 0.6459s, grad.norm=0.67764020\n",
      " 14046: 9 [  555/ 1499], train_loss/perplexity = 4.83798122/126.2142944 secs/batch = 0.6338s, grad.norm=0.64376473\n",
      " 14051: 9 [  560/ 1499], train_loss/perplexity = 5.14847517/172.1687622 secs/batch = 0.6210s, grad.norm=0.61732525\n",
      " 14056: 9 [  565/ 1499], train_loss/perplexity = 5.14686537/171.8918304 secs/batch = 0.7058s, grad.norm=0.65208060\n",
      " 14061: 9 [  570/ 1499], train_loss/perplexity = 5.05190611/156.3201447 secs/batch = 0.6231s, grad.norm=0.60726982\n",
      " 14066: 9 [  575/ 1499], train_loss/perplexity = 5.12521744/168.2107086 secs/batch = 0.6479s, grad.norm=0.60213804\n",
      " 14071: 9 [  580/ 1499], train_loss/perplexity = 4.79536629/120.9486771 secs/batch = 0.7150s, grad.norm=0.65860432\n",
      " 14076: 9 [  585/ 1499], train_loss/perplexity = 4.69774866/109.6999207 secs/batch = 0.6634s, grad.norm=0.59634471\n",
      " 14081: 9 [  590/ 1499], train_loss/perplexity = 4.83288383/125.5725708 secs/batch = 1.1030s, grad.norm=0.65512186\n",
      " 14086: 9 [  595/ 1499], train_loss/perplexity = 4.90515757/134.9841766 secs/batch = 0.6060s, grad.norm=0.60206711\n",
      " 14091: 9 [  600/ 1499], train_loss/perplexity = 4.73970222/114.4001312 secs/batch = 0.6099s, grad.norm=0.63299143\n",
      " 14096: 9 [  605/ 1499], train_loss/perplexity = 4.64254093/103.8077850 secs/batch = 0.6194s, grad.norm=0.59643811\n",
      " 14101: 9 [  610/ 1499], train_loss/perplexity = 4.98736477/146.5497284 secs/batch = 0.6433s, grad.norm=0.62798202\n",
      " 14106: 9 [  615/ 1499], train_loss/perplexity = 5.04149199/154.7006531 secs/batch = 0.5854s, grad.norm=0.63614899\n",
      " 14111: 9 [  620/ 1499], train_loss/perplexity = 4.80834579/122.5287628 secs/batch = 0.6343s, grad.norm=0.64107168\n",
      " 14116: 9 [  625/ 1499], train_loss/perplexity = 4.73735571/114.1320038 secs/batch = 0.6191s, grad.norm=0.62043667\n",
      " 14121: 9 [  630/ 1499], train_loss/perplexity = 4.89694452/133.8800812 secs/batch = 0.6361s, grad.norm=0.65540546\n",
      " 14126: 9 [  635/ 1499], train_loss/perplexity = 4.56156635/95.7333145 secs/batch = 0.6412s, grad.norm=0.65323806\n",
      " 14131: 9 [  640/ 1499], train_loss/perplexity = 4.92775774/138.0695801 secs/batch = 0.6685s, grad.norm=0.66560495\n",
      " 14136: 9 [  645/ 1499], train_loss/perplexity = 4.69195604/109.0663071 secs/batch = 0.6683s, grad.norm=0.65156829\n",
      " 14141: 9 [  650/ 1499], train_loss/perplexity = 4.42532778/83.5401840 secs/batch = 1.2401s, grad.norm=0.66220236\n",
      " 14146: 9 [  655/ 1499], train_loss/perplexity = 5.00002337/148.4166260 secs/batch = 0.7035s, grad.norm=0.62061799\n",
      " 14151: 9 [  660/ 1499], train_loss/perplexity = 5.03665400/153.9540253 secs/batch = 0.5352s, grad.norm=0.66886210\n",
      " 14156: 9 [  665/ 1499], train_loss/perplexity = 4.94122410/139.9414520 secs/batch = 0.5432s, grad.norm=0.63713932\n",
      " 14161: 9 [  670/ 1499], train_loss/perplexity = 5.17129374/176.1425781 secs/batch = 0.6363s, grad.norm=0.63012218\n",
      " 14166: 9 [  675/ 1499], train_loss/perplexity = 4.98627520/146.3901367 secs/batch = 0.5645s, grad.norm=0.64510930\n",
      " 14171: 9 [  680/ 1499], train_loss/perplexity = 4.91947889/136.9312439 secs/batch = 0.5776s, grad.norm=0.63622642\n",
      " 14176: 9 [  685/ 1499], train_loss/perplexity = 5.15882301/173.9595795 secs/batch = 0.6054s, grad.norm=0.65041554\n",
      " 14181: 9 [  690/ 1499], train_loss/perplexity = 4.91822767/136.7600098 secs/batch = 0.6307s, grad.norm=0.63720995\n",
      " 14186: 9 [  695/ 1499], train_loss/perplexity = 4.98585367/146.3284454 secs/batch = 0.6074s, grad.norm=0.63860899\n",
      " 14191: 9 [  700/ 1499], train_loss/perplexity = 5.11167049/165.9473419 secs/batch = 0.5839s, grad.norm=0.64534014\n",
      " 14196: 9 [  705/ 1499], train_loss/perplexity = 4.60431576/99.9145966 secs/batch = 0.5471s, grad.norm=0.60067755\n",
      " 14201: 9 [  710/ 1499], train_loss/perplexity = 4.91514301/136.3388062 secs/batch = 0.5484s, grad.norm=0.63091177\n",
      " 14206: 9 [  715/ 1499], train_loss/perplexity = 4.88705873/132.5630951 secs/batch = 0.5585s, grad.norm=0.63985449\n",
      " 14211: 9 [  720/ 1499], train_loss/perplexity = 4.94417286/140.3547058 secs/batch = 0.5649s, grad.norm=0.64192456\n",
      " 14216: 9 [  725/ 1499], train_loss/perplexity = 4.72233629/112.4306183 secs/batch = 0.5845s, grad.norm=0.63173699\n",
      " 14221: 9 [  730/ 1499], train_loss/perplexity = 4.87609339/131.1174316 secs/batch = 0.6143s, grad.norm=0.64223963\n",
      " 14226: 9 [  735/ 1499], train_loss/perplexity = 4.74292278/114.7691574 secs/batch = 0.6524s, grad.norm=0.65267879\n",
      " 14231: 9 [  740/ 1499], train_loss/perplexity = 4.75680876/116.3739548 secs/batch = 0.5872s, grad.norm=0.68420672\n",
      " 14236: 9 [  745/ 1499], train_loss/perplexity = 4.64784050/104.3593750 secs/batch = 0.5787s, grad.norm=0.61671036\n",
      " 14241: 9 [  750/ 1499], train_loss/perplexity = 4.86379337/129.5145721 secs/batch = 0.5794s, grad.norm=0.62396497\n",
      " 14246: 9 [  755/ 1499], train_loss/perplexity = 4.68312073/108.1069183 secs/batch = 0.5758s, grad.norm=0.69139636\n",
      " 14251: 9 [  760/ 1499], train_loss/perplexity = 4.26683807/71.2958450 secs/batch = 0.6395s, grad.norm=0.63698995\n",
      " 14256: 9 [  765/ 1499], train_loss/perplexity = 4.72065401/112.2416382 secs/batch = 1.1612s, grad.norm=0.64261478\n",
      " 14261: 9 [  770/ 1499], train_loss/perplexity = 4.79888296/121.3747635 secs/batch = 0.5946s, grad.norm=0.62206620\n",
      " 14266: 9 [  775/ 1499], train_loss/perplexity = 5.14632177/171.7984161 secs/batch = 0.5742s, grad.norm=0.64638364\n",
      " 14271: 9 [  780/ 1499], train_loss/perplexity = 4.77565765/118.5882797 secs/batch = 0.5698s, grad.norm=0.59554714\n",
      " 14276: 9 [  785/ 1499], train_loss/perplexity = 4.92938805/138.2948608 secs/batch = 0.5464s, grad.norm=0.68024647\n",
      " 14281: 9 [  790/ 1499], train_loss/perplexity = 4.50970364/90.8948746 secs/batch = 0.5426s, grad.norm=0.61579770\n",
      " 14286: 9 [  795/ 1499], train_loss/perplexity = 4.86275339/129.3799438 secs/batch = 0.5640s, grad.norm=0.62888443\n",
      " 14291: 9 [  800/ 1499], train_loss/perplexity = 4.90317440/134.7167511 secs/batch = 0.5460s, grad.norm=0.64754993\n",
      " 14296: 9 [  805/ 1499], train_loss/perplexity = 4.81894493/123.8343658 secs/batch = 0.5569s, grad.norm=0.61321348\n",
      " 14301: 9 [  810/ 1499], train_loss/perplexity = 4.78736544/119.9848480 secs/batch = 0.6041s, grad.norm=0.61131889\n",
      " 14306: 9 [  815/ 1499], train_loss/perplexity = 4.98472357/146.1631622 secs/batch = 0.5994s, grad.norm=0.64421427\n",
      " 14311: 9 [  820/ 1499], train_loss/perplexity = 4.55494213/95.1012497 secs/batch = 0.5501s, grad.norm=0.65328592\n",
      " 14316: 9 [  825/ 1499], train_loss/perplexity = 4.57534361/97.0613861 secs/batch = 0.5917s, grad.norm=0.63338375\n",
      " 14321: 9 [  830/ 1499], train_loss/perplexity = 4.86169243/129.2427521 secs/batch = 0.5492s, grad.norm=0.65338910\n",
      " 14326: 9 [  835/ 1499], train_loss/perplexity = 4.87642527/131.1609650 secs/batch = 0.7021s, grad.norm=0.63820970\n",
      " 14331: 9 [  840/ 1499], train_loss/perplexity = 4.75770569/116.4783783 secs/batch = 0.6634s, grad.norm=0.66115069\n",
      " 14336: 9 [  845/ 1499], train_loss/perplexity = 4.64597988/104.1653824 secs/batch = 0.6369s, grad.norm=0.62022495\n",
      " 14341: 9 [  850/ 1499], train_loss/perplexity = 4.67839003/107.5967026 secs/batch = 0.6100s, grad.norm=0.63800734\n",
      " 14346: 9 [  855/ 1499], train_loss/perplexity = 4.81739521/123.6426086 secs/batch = 0.6209s, grad.norm=0.65232778\n",
      " 14351: 9 [  860/ 1499], train_loss/perplexity = 4.32245636/75.3735428 secs/batch = 0.6297s, grad.norm=0.64166683\n",
      " 14356: 9 [  865/ 1499], train_loss/perplexity = 4.70065546/110.0192642 secs/batch = 0.6411s, grad.norm=0.63350725\n",
      " 14361: 9 [  870/ 1499], train_loss/perplexity = 4.75511217/116.1766815 secs/batch = 0.7044s, grad.norm=0.69741493\n",
      " 14366: 9 [  875/ 1499], train_loss/perplexity = 4.79124069/120.4507217 secs/batch = 0.5907s, grad.norm=0.63043916\n",
      " 14371: 9 [  880/ 1499], train_loss/perplexity = 4.79156733/120.4900665 secs/batch = 0.6045s, grad.norm=0.64532262\n",
      " 14376: 9 [  885/ 1499], train_loss/perplexity = 4.64751625/104.3255463 secs/batch = 0.6070s, grad.norm=0.67621213\n",
      " 14381: 9 [  890/ 1499], train_loss/perplexity = 4.92019749/137.0296783 secs/batch = 0.5786s, grad.norm=0.63127017\n",
      " 14386: 9 [  895/ 1499], train_loss/perplexity = 4.99678898/147.9373627 secs/batch = 0.6090s, grad.norm=0.60305274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14391: 9 [  900/ 1499], train_loss/perplexity = 4.64513111/104.0770111 secs/batch = 0.6603s, grad.norm=0.57509750\n",
      " 14396: 9 [  905/ 1499], train_loss/perplexity = 4.80580807/122.2182083 secs/batch = 0.5976s, grad.norm=0.60679024\n",
      " 14401: 9 [  910/ 1499], train_loss/perplexity = 4.97587109/144.8749695 secs/batch = 0.6771s, grad.norm=0.65896565\n",
      " 14406: 9 [  915/ 1499], train_loss/perplexity = 4.84910440/127.6260376 secs/batch = 0.6919s, grad.norm=0.70013601\n",
      " 14411: 9 [  920/ 1499], train_loss/perplexity = 4.54213476/93.8910217 secs/batch = 0.6294s, grad.norm=0.58493280\n",
      " 14416: 9 [  925/ 1499], train_loss/perplexity = 4.76387405/117.1990814 secs/batch = 0.6143s, grad.norm=0.61755979\n",
      " 14421: 9 [  930/ 1499], train_loss/perplexity = 4.84400415/126.9767685 secs/batch = 1.0978s, grad.norm=0.65597075\n",
      " 14426: 9 [  935/ 1499], train_loss/perplexity = 4.92137527/137.1911621 secs/batch = 0.6020s, grad.norm=0.63614130\n",
      " 14431: 9 [  940/ 1499], train_loss/perplexity = 4.53020668/92.7777328 secs/batch = 0.5954s, grad.norm=0.65421999\n",
      " 14436: 9 [  945/ 1499], train_loss/perplexity = 4.95049763/141.2452393 secs/batch = 0.5527s, grad.norm=0.63604313\n",
      " 14441: 9 [  950/ 1499], train_loss/perplexity = 4.90869474/135.4624786 secs/batch = 0.6535s, grad.norm=0.64919937\n",
      " 14446: 9 [  955/ 1499], train_loss/perplexity = 5.10262823/164.4535675 secs/batch = 0.6458s, grad.norm=0.64395541\n",
      " 14451: 9 [  960/ 1499], train_loss/perplexity = 4.75953913/116.6921310 secs/batch = 0.5496s, grad.norm=0.62857574\n",
      " 14456: 9 [  965/ 1499], train_loss/perplexity = 4.78768301/120.0229568 secs/batch = 0.5808s, grad.norm=0.62618399\n",
      " 14461: 9 [  970/ 1499], train_loss/perplexity = 5.01750135/151.0334473 secs/batch = 0.5540s, grad.norm=0.71895057\n",
      " 14466: 9 [  975/ 1499], train_loss/perplexity = 4.69978476/109.9235077 secs/batch = 0.5927s, grad.norm=0.65814465\n",
      " 14471: 9 [  980/ 1499], train_loss/perplexity = 4.89655352/133.8277435 secs/batch = 0.5907s, grad.norm=0.64088112\n",
      " 14476: 9 [  985/ 1499], train_loss/perplexity = 4.60206938/99.6903992 secs/batch = 0.5407s, grad.norm=0.65748709\n",
      " 14481: 9 [  990/ 1499], train_loss/perplexity = 4.71635103/111.7596970 secs/batch = 0.5842s, grad.norm=0.60393524\n",
      " 14486: 9 [  995/ 1499], train_loss/perplexity = 5.10108805/164.2004700 secs/batch = 0.5890s, grad.norm=0.65698022\n",
      " 14491: 9 [ 1000/ 1499], train_loss/perplexity = 4.79993534/121.5025635 secs/batch = 0.5963s, grad.norm=0.61388302\n",
      " 14496: 9 [ 1005/ 1499], train_loss/perplexity = 5.22042990/185.0137024 secs/batch = 0.5957s, grad.norm=0.63087207\n",
      " 14501: 9 [ 1010/ 1499], train_loss/perplexity = 4.96232319/142.9254608 secs/batch = 0.5571s, grad.norm=0.64906973\n",
      " 14506: 9 [ 1015/ 1499], train_loss/perplexity = 4.82154512/124.1567841 secs/batch = 0.6260s, grad.norm=0.60197085\n",
      " 14511: 9 [ 1020/ 1499], train_loss/perplexity = 4.88485861/132.2717590 secs/batch = 0.5637s, grad.norm=0.64743090\n",
      " 14516: 9 [ 1025/ 1499], train_loss/perplexity = 4.83576345/125.9346924 secs/batch = 0.5999s, grad.norm=0.61239398\n",
      " 14521: 9 [ 1030/ 1499], train_loss/perplexity = 5.03848505/154.2361755 secs/batch = 0.6493s, grad.norm=0.63335454\n",
      " 14526: 9 [ 1035/ 1499], train_loss/perplexity = 5.27729082/195.8385925 secs/batch = 0.6451s, grad.norm=0.64804828\n",
      " 14531: 9 [ 1040/ 1499], train_loss/perplexity = 4.58851099/98.3478775 secs/batch = 0.5725s, grad.norm=0.59690619\n",
      " 14536: 9 [ 1045/ 1499], train_loss/perplexity = 4.79400587/120.7842484 secs/batch = 0.5515s, grad.norm=0.59817302\n",
      " 14541: 9 [ 1050/ 1499], train_loss/perplexity = 4.52293205/92.1052628 secs/batch = 0.5667s, grad.norm=0.61399037\n",
      " 14546: 9 [ 1055/ 1499], train_loss/perplexity = 4.82487345/124.5707016 secs/batch = 0.6124s, grad.norm=0.60712022\n",
      " 14551: 9 [ 1060/ 1499], train_loss/perplexity = 5.08253050/161.1814117 secs/batch = 0.5752s, grad.norm=0.63196290\n",
      " 14556: 9 [ 1065/ 1499], train_loss/perplexity = 4.63562059/103.0918732 secs/batch = 0.5714s, grad.norm=0.63303638\n",
      " 14561: 9 [ 1070/ 1499], train_loss/perplexity = 5.02763510/152.5717621 secs/batch = 0.5571s, grad.norm=0.63031530\n",
      " 14566: 9 [ 1075/ 1499], train_loss/perplexity = 4.93340111/138.8509521 secs/batch = 0.5561s, grad.norm=0.63452291\n",
      " 14571: 9 [ 1080/ 1499], train_loss/perplexity = 5.13504601/169.8721313 secs/batch = 0.5740s, grad.norm=0.65742469\n",
      " 14576: 9 [ 1085/ 1499], train_loss/perplexity = 5.22000790/184.9356384 secs/batch = 0.5790s, grad.norm=0.63158506\n",
      " 14581: 9 [ 1090/ 1499], train_loss/perplexity = 4.85138988/127.9180603 secs/batch = 0.5521s, grad.norm=0.61108273\n",
      " 14586: 9 [ 1095/ 1499], train_loss/perplexity = 4.99820995/148.1477356 secs/batch = 0.5516s, grad.norm=0.62294477\n",
      " 14591: 9 [ 1100/ 1499], train_loss/perplexity = 5.01006699/149.9147797 secs/batch = 0.5697s, grad.norm=0.64121336\n",
      " 14596: 9 [ 1105/ 1499], train_loss/perplexity = 4.76553965/117.3944550 secs/batch = 0.5442s, grad.norm=0.66354460\n",
      " 14601: 9 [ 1110/ 1499], train_loss/perplexity = 4.92690134/137.9513855 secs/batch = 0.5468s, grad.norm=0.61859781\n",
      " 14606: 9 [ 1115/ 1499], train_loss/perplexity = 4.55442572/95.0521545 secs/batch = 0.5553s, grad.norm=0.62693232\n",
      " 14611: 9 [ 1120/ 1499], train_loss/perplexity = 4.72690678/112.9456558 secs/batch = 0.5482s, grad.norm=0.62509114\n",
      " 14616: 9 [ 1125/ 1499], train_loss/perplexity = 4.73390770/113.7391510 secs/batch = 0.6698s, grad.norm=0.65727878\n",
      " 14621: 9 [ 1130/ 1499], train_loss/perplexity = 4.56399441/95.9660416 secs/batch = 0.6003s, grad.norm=0.62968427\n",
      " 14626: 9 [ 1135/ 1499], train_loss/perplexity = 5.19410944/180.2075806 secs/batch = 0.5598s, grad.norm=0.62499523\n",
      " 14631: 9 [ 1140/ 1499], train_loss/perplexity = 4.94622421/140.6429291 secs/batch = 0.5622s, grad.norm=0.64850801\n",
      " 14636: 9 [ 1145/ 1499], train_loss/perplexity = 5.06644392/158.6092987 secs/batch = 0.6320s, grad.norm=0.64363885\n",
      " 14641: 9 [ 1150/ 1499], train_loss/perplexity = 5.08668900/161.8530731 secs/batch = 0.5921s, grad.norm=0.62591583\n",
      " 14646: 9 [ 1155/ 1499], train_loss/perplexity = 4.72908211/113.1916199 secs/batch = 0.6359s, grad.norm=0.63380152\n",
      " 14651: 9 [ 1160/ 1499], train_loss/perplexity = 4.88701820/132.5577240 secs/batch = 0.7117s, grad.norm=0.61992955\n",
      " 14656: 9 [ 1165/ 1499], train_loss/perplexity = 4.88378859/132.1303101 secs/batch = 0.5896s, grad.norm=0.62487835\n",
      " 14661: 9 [ 1170/ 1499], train_loss/perplexity = 4.95633507/142.0721588 secs/batch = 0.6119s, grad.norm=0.64917493\n",
      " 14666: 9 [ 1175/ 1499], train_loss/perplexity = 4.29693556/73.4742889 secs/batch = 0.5923s, grad.norm=0.66793722\n",
      " 14671: 9 [ 1180/ 1499], train_loss/perplexity = 4.58568192/98.0700378 secs/batch = 1.2375s, grad.norm=0.64048278\n",
      " 14676: 9 [ 1185/ 1499], train_loss/perplexity = 4.56509161/96.0713959 secs/batch = 0.6127s, grad.norm=0.61710399\n",
      " 14681: 9 [ 1190/ 1499], train_loss/perplexity = 4.79242849/120.5938721 secs/batch = 0.5900s, grad.norm=0.65147084\n",
      " 14686: 9 [ 1195/ 1499], train_loss/perplexity = 4.76813984/117.7000961 secs/batch = 0.5984s, grad.norm=0.65243423\n",
      " 14691: 9 [ 1200/ 1499], train_loss/perplexity = 4.61189222/100.6744690 secs/batch = 0.5862s, grad.norm=0.63678718\n",
      " 14696: 9 [ 1205/ 1499], train_loss/perplexity = 4.60028696/99.5128708 secs/batch = 0.6084s, grad.norm=0.64512163\n",
      " 14701: 9 [ 1210/ 1499], train_loss/perplexity = 4.58878374/98.3747101 secs/batch = 0.5404s, grad.norm=0.66590464\n",
      " 14706: 9 [ 1215/ 1499], train_loss/perplexity = 4.35215044/77.6452560 secs/batch = 0.5914s, grad.norm=0.65065503\n",
      " 14711: 9 [ 1220/ 1499], train_loss/perplexity = 4.72085238/112.2639008 secs/batch = 0.6173s, grad.norm=0.65440530\n",
      " 14716: 9 [ 1225/ 1499], train_loss/perplexity = 4.08735609/59.5821533 secs/batch = 0.5696s, grad.norm=0.62926328\n",
      " 14721: 9 [ 1230/ 1499], train_loss/perplexity = 4.61279726/100.7656250 secs/batch = 0.5448s, grad.norm=0.66321659\n",
      " 14726: 9 [ 1235/ 1499], train_loss/perplexity = 4.36455774/78.6146240 secs/batch = 0.5950s, grad.norm=0.66074049\n",
      " 14731: 9 [ 1240/ 1499], train_loss/perplexity = 4.61366892/100.8534927 secs/batch = 0.5757s, grad.norm=0.66812909\n",
      " 14736: 9 [ 1245/ 1499], train_loss/perplexity = 4.83446455/125.7712173 secs/batch = 0.6445s, grad.norm=0.63768750\n",
      " 14741: 9 [ 1250/ 1499], train_loss/perplexity = 4.96941233/143.9422760 secs/batch = 0.6097s, grad.norm=0.64348537\n",
      " 14746: 9 [ 1255/ 1499], train_loss/perplexity = 4.74207401/114.6717834 secs/batch = 0.5476s, grad.norm=0.63417882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14751: 9 [ 1260/ 1499], train_loss/perplexity = 4.85047007/127.8004532 secs/batch = 0.5699s, grad.norm=0.64646280\n",
      " 14756: 9 [ 1265/ 1499], train_loss/perplexity = 4.88884640/132.8002930 secs/batch = 0.6450s, grad.norm=0.64731294\n",
      " 14761: 9 [ 1270/ 1499], train_loss/perplexity = 4.83886576/126.3259888 secs/batch = 0.6747s, grad.norm=0.62575245\n",
      " 14766: 9 [ 1275/ 1499], train_loss/perplexity = 4.82626534/124.7442093 secs/batch = 0.5872s, grad.norm=0.63883382\n",
      " 14771: 9 [ 1280/ 1499], train_loss/perplexity = 4.49070454/89.1842575 secs/batch = 0.6483s, grad.norm=0.61034226\n",
      " 14776: 9 [ 1285/ 1499], train_loss/perplexity = 5.03062391/153.0284576 secs/batch = 0.6164s, grad.norm=0.66634321\n",
      " 14781: 9 [ 1290/ 1499], train_loss/perplexity = 4.73037291/113.3378220 secs/batch = 1.0736s, grad.norm=0.63523763\n",
      " 14786: 9 [ 1295/ 1499], train_loss/perplexity = 4.77402163/118.3944244 secs/batch = 0.6140s, grad.norm=0.68821311\n",
      " 14791: 9 [ 1300/ 1499], train_loss/perplexity = 4.85412788/128.2687836 secs/batch = 0.5768s, grad.norm=0.68491358\n",
      " 14796: 9 [ 1305/ 1499], train_loss/perplexity = 4.75761080/116.4673309 secs/batch = 0.5870s, grad.norm=0.61621845\n",
      " 14801: 9 [ 1310/ 1499], train_loss/perplexity = 4.94986105/141.1553497 secs/batch = 0.5875s, grad.norm=0.69610620\n",
      " 14806: 9 [ 1315/ 1499], train_loss/perplexity = 4.74553156/115.0689545 secs/batch = 0.6270s, grad.norm=0.63241512\n",
      " 14811: 9 [ 1320/ 1499], train_loss/perplexity = 4.48545933/88.7176895 secs/batch = 1.1396s, grad.norm=0.60843492\n",
      " 14816: 9 [ 1325/ 1499], train_loss/perplexity = 4.84596634/127.2261658 secs/batch = 0.5840s, grad.norm=0.64521700\n",
      " 14821: 9 [ 1330/ 1499], train_loss/perplexity = 4.96171379/142.8383789 secs/batch = 0.5408s, grad.norm=0.63839203\n",
      " 14826: 9 [ 1335/ 1499], train_loss/perplexity = 4.77816629/118.8861465 secs/batch = 0.5359s, grad.norm=0.62934548\n",
      " 14831: 9 [ 1340/ 1499], train_loss/perplexity = 4.28237152/72.4119644 secs/batch = 0.5391s, grad.norm=0.65287083\n",
      " 14836: 9 [ 1345/ 1499], train_loss/perplexity = 4.40745878/82.0606613 secs/batch = 0.5422s, grad.norm=0.63013512\n",
      " 14841: 9 [ 1350/ 1499], train_loss/perplexity = 4.60440493/99.9235001 secs/batch = 0.5450s, grad.norm=0.65850306\n",
      " 14846: 9 [ 1355/ 1499], train_loss/perplexity = 4.22321129/68.2523117 secs/batch = 0.5417s, grad.norm=0.64622039\n",
      " 14851: 9 [ 1360/ 1499], train_loss/perplexity = 4.55775261/95.3689041 secs/batch = 0.5948s, grad.norm=0.62771451\n",
      " 14856: 9 [ 1365/ 1499], train_loss/perplexity = 4.19773912/66.5357285 secs/batch = 0.5401s, grad.norm=0.63468808\n",
      " 14861: 9 [ 1370/ 1499], train_loss/perplexity = 4.31434059/74.7643051 secs/batch = 0.5449s, grad.norm=0.64423043\n",
      " 14866: 9 [ 1375/ 1499], train_loss/perplexity = 4.36330891/78.5165100 secs/batch = 0.5355s, grad.norm=0.65296835\n",
      " 14871: 9 [ 1380/ 1499], train_loss/perplexity = 4.84174585/126.6903381 secs/batch = 0.5400s, grad.norm=0.66557854\n",
      " 14876: 9 [ 1385/ 1499], train_loss/perplexity = 4.55580759/95.1835938 secs/batch = 0.5396s, grad.norm=0.61477530\n",
      " 14881: 9 [ 1390/ 1499], train_loss/perplexity = 4.61853743/101.3456955 secs/batch = 0.5982s, grad.norm=0.65897274\n",
      " 14886: 9 [ 1395/ 1499], train_loss/perplexity = 4.82661676/124.7880554 secs/batch = 0.5484s, grad.norm=0.63513517\n",
      " 14891: 9 [ 1400/ 1499], train_loss/perplexity = 4.85609913/128.5218811 secs/batch = 0.5470s, grad.norm=0.63552701\n",
      " 14896: 9 [ 1405/ 1499], train_loss/perplexity = 4.76695681/117.5609360 secs/batch = 0.5670s, grad.norm=0.64353377\n",
      " 14901: 9 [ 1410/ 1499], train_loss/perplexity = 4.83609629/125.9766159 secs/batch = 0.5493s, grad.norm=0.63056147\n",
      " 14906: 9 [ 1415/ 1499], train_loss/perplexity = 4.84829950/127.5233536 secs/batch = 0.6360s, grad.norm=0.62340862\n",
      " 14911: 9 [ 1420/ 1499], train_loss/perplexity = 4.79830122/121.3041763 secs/batch = 0.6304s, grad.norm=0.63664669\n",
      " 14916: 9 [ 1425/ 1499], train_loss/perplexity = 4.72805405/113.0753098 secs/batch = 0.6285s, grad.norm=0.66770917\n",
      " 14921: 9 [ 1430/ 1499], train_loss/perplexity = 4.87596846/131.1010590 secs/batch = 0.5822s, grad.norm=0.67099267\n",
      " 14926: 9 [ 1435/ 1499], train_loss/perplexity = 4.62000847/101.4948883 secs/batch = 0.5523s, grad.norm=0.62346482\n",
      " 14931: 9 [ 1440/ 1499], train_loss/perplexity = 4.26754999/71.3466187 secs/batch = 0.5724s, grad.norm=0.63575596\n",
      " 14936: 9 [ 1445/ 1499], train_loss/perplexity = 4.71658659/111.7860260 secs/batch = 0.5697s, grad.norm=0.64543235\n",
      " 14941: 9 [ 1450/ 1499], train_loss/perplexity = 4.88090086/131.7492981 secs/batch = 0.5783s, grad.norm=0.59873164\n",
      " 14946: 9 [ 1455/ 1499], train_loss/perplexity = 5.11320877/166.2028046 secs/batch = 0.5758s, grad.norm=0.63688189\n",
      " 14951: 9 [ 1460/ 1499], train_loss/perplexity = 5.03375578/153.5084686 secs/batch = 0.5636s, grad.norm=0.63766849\n",
      " 14956: 9 [ 1465/ 1499], train_loss/perplexity = 5.15861750/173.9238434 secs/batch = 0.5482s, grad.norm=0.63980448\n",
      " 14961: 9 [ 1470/ 1499], train_loss/perplexity = 4.87817478/131.3906250 secs/batch = 0.5486s, grad.norm=0.62018257\n",
      " 14966: 9 [ 1475/ 1499], train_loss/perplexity = 5.04693031/155.5442657 secs/batch = 0.5429s, grad.norm=0.67665875\n",
      " 14971: 9 [ 1480/ 1499], train_loss/perplexity = 4.92109156/137.1522369 secs/batch = 0.5478s, grad.norm=0.63875955\n",
      " 14976: 9 [ 1485/ 1499], train_loss/perplexity = 4.66560555/106.2298965 secs/batch = 0.5463s, grad.norm=0.62684590\n",
      " 14981: 9 [ 1490/ 1499], train_loss/perplexity = 4.81113243/122.8706818 secs/batch = 0.5445s, grad.norm=0.62623775\n",
      " 14986: 9 [ 1495/ 1499], train_loss/perplexity = 5.16205120/174.5220642 secs/batch = 0.5455s, grad.norm=0.64844841\n",
      "Epoch training time: 906.7940418720245\n",
      "Saved char model cv/epoch009_4.9193.model\n",
      " 14995: 10 [    5/ 1499], train_loss/perplexity = 4.91698074/136.5895844 secs/batch = 0.5877s, grad.norm=0.63947362\n",
      " 15000: 10 [   10/ 1499], train_loss/perplexity = 4.91159201/135.8555298 secs/batch = 0.5875s, grad.norm=0.63502175\n",
      " 15005: 10 [   15/ 1499], train_loss/perplexity = 4.81140423/122.9040833 secs/batch = 0.5987s, grad.norm=0.63753974\n",
      " 15010: 10 [   20/ 1499], train_loss/perplexity = 4.63581753/103.1121826 secs/batch = 0.5707s, grad.norm=0.59551340\n",
      " 15015: 10 [   25/ 1499], train_loss/perplexity = 5.07711792/160.3113556 secs/batch = 0.5809s, grad.norm=0.67407954\n",
      " 15020: 10 [   30/ 1499], train_loss/perplexity = 4.99773836/148.0778809 secs/batch = 0.5397s, grad.norm=0.65779614\n",
      " 15025: 10 [   35/ 1499], train_loss/perplexity = 4.91231394/135.9536438 secs/batch = 0.6332s, grad.norm=0.65097821\n",
      " 15030: 10 [   40/ 1499], train_loss/perplexity = 4.92497396/137.6857605 secs/batch = 0.5881s, grad.norm=0.62202597\n",
      " 15035: 10 [   45/ 1499], train_loss/perplexity = 4.77917051/119.0055923 secs/batch = 0.5951s, grad.norm=0.62271661\n",
      " 15040: 10 [   50/ 1499], train_loss/perplexity = 4.84830046/127.5234756 secs/batch = 0.6562s, grad.norm=0.65995777\n",
      " 15045: 10 [   55/ 1499], train_loss/perplexity = 4.59844875/99.3301086 secs/batch = 0.5874s, grad.norm=0.72625905\n",
      " 15050: 10 [   60/ 1499], train_loss/perplexity = 4.55065680/94.6945801 secs/batch = 0.5513s, grad.norm=0.61046118\n",
      " 15055: 10 [   65/ 1499], train_loss/perplexity = 4.62022924/101.5173035 secs/batch = 0.6527s, grad.norm=0.66262627\n",
      " 15060: 10 [   70/ 1499], train_loss/perplexity = 4.61393023/100.8798523 secs/batch = 0.5651s, grad.norm=0.65360779\n",
      " 15065: 10 [   75/ 1499], train_loss/perplexity = 4.48368645/88.5605469 secs/batch = 0.5738s, grad.norm=0.63610810\n",
      " 15070: 10 [   80/ 1499], train_loss/perplexity = 4.60381556/99.8646317 secs/batch = 0.5480s, grad.norm=0.66595888\n",
      " 15075: 10 [   85/ 1499], train_loss/perplexity = 4.35064411/77.5283813 secs/batch = 0.5334s, grad.norm=0.62587744\n",
      " 15080: 10 [   90/ 1499], train_loss/perplexity = 4.71064806/111.1241531 secs/batch = 0.5564s, grad.norm=0.65079969\n",
      " 15085: 10 [   95/ 1499], train_loss/perplexity = 4.43186331/84.0879517 secs/batch = 0.5629s, grad.norm=0.62680000\n",
      " 15090: 10 [  100/ 1499], train_loss/perplexity = 4.55372334/94.9854126 secs/batch = 0.5930s, grad.norm=0.63882810\n",
      " 15095: 10 [  105/ 1499], train_loss/perplexity = 4.41994524/83.0917358 secs/batch = 0.6230s, grad.norm=0.65660626\n",
      " 15100: 10 [  110/ 1499], train_loss/perplexity = 4.50836086/90.7729034 secs/batch = 0.5546s, grad.norm=0.64821750\n",
      " 15105: 10 [  115/ 1499], train_loss/perplexity = 4.70523787/110.5245743 secs/batch = 0.5578s, grad.norm=0.63731724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15110: 10 [  120/ 1499], train_loss/perplexity = 4.55523777/95.1293716 secs/batch = 0.5556s, grad.norm=0.61588126\n",
      " 15115: 10 [  125/ 1499], train_loss/perplexity = 4.66785336/106.4689484 secs/batch = 0.5508s, grad.norm=0.66774142\n",
      " 15120: 10 [  130/ 1499], train_loss/perplexity = 4.80306721/121.8836899 secs/batch = 0.6186s, grad.norm=0.67160994\n",
      " 15125: 10 [  135/ 1499], train_loss/perplexity = 4.85398006/128.2498169 secs/batch = 0.6224s, grad.norm=0.70908475\n",
      " 15130: 10 [  140/ 1499], train_loss/perplexity = 4.86500502/129.6715851 secs/batch = 0.5875s, grad.norm=0.65894848\n",
      " 15135: 10 [  145/ 1499], train_loss/perplexity = 4.51426077/91.3100433 secs/batch = 0.5584s, grad.norm=0.67035639\n",
      " 15140: 10 [  150/ 1499], train_loss/perplexity = 4.70496702/110.4946442 secs/batch = 0.5820s, grad.norm=0.66384786\n",
      " 15145: 10 [  155/ 1499], train_loss/perplexity = 4.87296438/130.7078094 secs/batch = 0.5429s, grad.norm=0.62957329\n",
      " 15150: 10 [  160/ 1499], train_loss/perplexity = 5.10725212/165.2157440 secs/batch = 0.5467s, grad.norm=0.65494871\n",
      " 15155: 10 [  165/ 1499], train_loss/perplexity = 4.78360891/119.5349655 secs/batch = 0.5617s, grad.norm=0.67604262\n",
      " 15160: 10 [  170/ 1499], train_loss/perplexity = 5.06192017/157.8934021 secs/batch = 0.6219s, grad.norm=0.65736413\n",
      " 15165: 10 [  175/ 1499], train_loss/perplexity = 4.77864027/118.9425125 secs/batch = 0.5759s, grad.norm=0.64378893\n",
      " 15170: 10 [  180/ 1499], train_loss/perplexity = 4.99448538/147.5969696 secs/batch = 0.5710s, grad.norm=0.64776039\n",
      " 15175: 10 [  185/ 1499], train_loss/perplexity = 4.79602671/121.0285797 secs/batch = 0.6002s, grad.norm=0.64850277\n",
      " 15180: 10 [  190/ 1499], train_loss/perplexity = 4.85191727/127.9855347 secs/batch = 0.6000s, grad.norm=0.62072486\n",
      " 15185: 10 [  195/ 1499], train_loss/perplexity = 5.05197430/156.3308105 secs/batch = 0.5921s, grad.norm=0.65682507\n",
      " 15190: 10 [  200/ 1499], train_loss/perplexity = 4.95980978/142.5666809 secs/batch = 0.5806s, grad.norm=0.69395489\n",
      " 15195: 10 [  205/ 1499], train_loss/perplexity = 4.88291502/132.0149231 secs/batch = 0.5552s, grad.norm=0.63425481\n",
      " 15200: 10 [  210/ 1499], train_loss/perplexity = 4.75167322/115.7778473 secs/batch = 0.5455s, grad.norm=0.60660559\n",
      " 15205: 10 [  215/ 1499], train_loss/perplexity = 4.94472361/140.4320374 secs/batch = 0.6067s, grad.norm=0.64354628\n",
      " 15210: 10 [  220/ 1499], train_loss/perplexity = 4.68411493/108.2144547 secs/batch = 0.6577s, grad.norm=0.58638585\n",
      " 15215: 10 [  225/ 1499], train_loss/perplexity = 4.89469194/133.5788574 secs/batch = 0.6209s, grad.norm=0.61925149\n",
      " 15220: 10 [  230/ 1499], train_loss/perplexity = 4.93955231/139.7076874 secs/batch = 0.6759s, grad.norm=0.66606045\n",
      " 15225: 10 [  235/ 1499], train_loss/perplexity = 4.97041941/144.0873108 secs/batch = 0.6252s, grad.norm=0.63258404\n",
      " 15230: 10 [  240/ 1499], train_loss/perplexity = 4.95398426/141.7385712 secs/batch = 0.5948s, grad.norm=0.63923776\n"
     ]
    }
   ],
   "source": [
    "lstm_char_cnn.Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "2019-08-27 14:24:43 Step: 10 Training loss: 0.09034851486794651 accuracy: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6da89ead7dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainSentiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenti_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainSentiModel\u001b[0;34m(sess, saver, train_model, senti_reader, train_batch, test_batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mret_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \u001b[0msum_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainSentiModel(sess, saver, senti_train_graph, sentiReader, FLAGS.batch_size, FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: said..hes\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: a\"cleric\n",
      "Unknown char: \n",
      "Word: ministre\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \"\n",
      "Word: been\"justifying\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: okevery\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: therefoolish\n",
      "Unknown char: \n",
      "Word: wellposted\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: deplorableright\n",
      "Unknown char: \n",
      "Word: nownow\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: apologizefor\n",
      "Unknown char: \"\n",
      "Word: t\"...back\n",
      "Unknown char: \n",
      "Word: dernires\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: dfendaient\n",
      "Unknown char: \n",
      "Word: if\n",
      "Unknown char: \n",
      "Word: zufllig\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \"\n",
      "Word: god\"...if\n",
      "Unknown char: \"\n",
      "Word: shameful\"for\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: =\n",
      "Word: that=point\n",
      "Unknown char: \n",
      "Word: artw\n",
      "Unknown char: \n",
      "Word: vlgame\n",
      "Unknown char: \n",
      "Word: uuyor\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: whoevers\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: perdn\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: gjr\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: lrt\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: disposicin\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: free\"-nsw\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: thats\n",
      "2019-08-27 12:25:59 Step: 10 Training loss: 32.675646018981936 accuracy: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2499f5393235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainRDMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainRDMModel\u001b[0;34m(sess, mm, t_acc, t_steps, new_data_len)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mret_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msum_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mt_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainRDMModel(sess, rdm_train_graph, 0.7, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in RL the begining\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: said..hes\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: a\"cleric\n",
      "Unknown char: \n",
      "Word: ministre\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \"\n",
      "Word: been\"justifying\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: okevery\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: therefoolish\n",
      "Unknown char: \n",
      "Word: wellposted\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: deplorableright\n",
      "Unknown char: \n",
      "Word: nownow\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: apologizefor\n",
      "Unknown char: \"\n",
      "Word: t\"...back\n",
      "Unknown char: \n",
      "Word: dernires\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: dfendaient\n",
      "Unknown char: \n",
      "Word: if\n",
      "Unknown char: \n",
      "Word: zufllig\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \"\n",
      "Word: god\"...if\n",
      "Unknown char: \"\n",
      "Word: shameful\"for\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: =\n",
      "Word: that=point\n",
      "Unknown char: \n",
      "Word: artw\n",
      "Unknown char: \n",
      "Word: vlgame\n",
      "Unknown char: \n",
      "Word: uuyor\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: whoevers\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: perdn\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: gjr\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: lrt\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: disposicin\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: free\"-nsw\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: that\"s\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: verstndnis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: congressmen:\"i\n",
      "Unknown char: \n",
      "Word: dantay\n",
      "Unknown char: \n",
      "Word: dantay\n",
      "Unknown char: \"\n",
      "guess again\"...\n",
      "Unknown char: \n",
      "guess again\"...\n",
      "Unknown char: \n",
      "guess again\"...\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \"\n",
      "Word: trust\"ourselves\n",
      "Unknown char: \n",
      "Word: familire\n",
      "Unknown char: \n",
      "Word: vre\n",
      "Unknown char: \n",
      "Word: paranod\n",
      "Unknown char: =\n",
      "Word: black=sunni\n",
      "Unknown char: =\n",
      "Word: green=shia\n",
      "Unknown char: \"\n",
      "Word: just\"sick\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: samobjcy\n",
      "Unknown char: \n",
      "Word: jzykach\n",
      "Unknown char: \n",
      "Word: skadasz\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: erdoan\n",
      "Unknown char: \n",
      "Word: biiler\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thisit's\n",
      "Unknown char: \n",
      "Word: tasteless/vulgar/pettyinhuman\n",
      "Unknown char: \n",
      "Word: evchs\n",
      "Unknown char: \n",
      "Word: evchs\n",
      "Unknown char: \n",
      "Word: ministers\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: lger\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: a\"riot\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: attaqus\n",
      "Unknown char: \n",
      "Word: ncessaire\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: lches\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: autorits\n",
      "Unknown char: \n",
      "Word: d'enqute\n",
      "Unknown char: \n",
      "Word: fminin\n",
      "Unknown char: \n",
      "Word: rsistance\n",
      "Unknown char: \n",
      "Word: gnrale\n",
      "Unknown char: \n",
      "Word: gnrale\n",
      "Unknown char: \n",
      "Word: aperue\n",
      "Unknown char: \n",
      "Word: librt\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: franois\n",
      "Unknown char: \n",
      "Word: renvoye\n",
      "Unknown char: \n",
      "Word: zgnz\n",
      "Unknown char: \n",
      "Word: zgnz\n",
      "Unknown char: \n",
      "Word: belive\n",
      "Unknown char: \n",
      "Word: service\n",
      "Unknown char: \n",
      "Word: this\n",
      "Unknown char: \n",
      "Word: think\n",
      "Unknown char: \n",
      "Word: choque\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \"\n",
      "Word: hate\"defending\n",
      "Unknown char: \n",
      "Word: deberan\n",
      "Unknown char: \n",
      "Word: rservs\n",
      "Unknown char: \n",
      "Word: rservs\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \"\n",
      "Word: guest:\"all\n",
      "Unknown char: \"\n",
      "Word: host:\"great\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: terror..\"glorious\"past\n",
      "Unknown char: \"\n",
      "Word: terror..\"glorious\"past\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: cdric\n",
      "Unknown char: \n",
      "Word: erwhnt\n",
      "Unknown char: \n",
      "Word: hpital\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: themirror.amen\n",
      "Unknown char: \n",
      "Word: castros\n",
      "Unknown char: \n",
      "Word: cubas\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: omalley\n",
      "Unknown char: \n",
      "Word: carrment\n",
      "Unknown char: \n",
      "Word: dmesur\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dbloquer\n",
      "Unknown char: \n",
      "Word: prsence\n",
      "Unknown char: \n",
      "Word: d'opration\n",
      "Unknown char: \n",
      "Word: armes\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: opration\n",
      "Unknown char: \n",
      "Word: l'tat\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: intressante\n",
      "Unknown char: \n",
      "Word: prs\n",
      "Unknown char: \n",
      "Word: sret\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \"\n",
      "Word: care.\"this\n",
      "Unknown char: \n",
      "Word: aplicacin\n",
      "Unknown char: \n",
      "Word: shara\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: theyd\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: tho.but\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: bsta\n",
      "Unknown char: \n",
      "Word: tnker\n",
      "Unknown char: \n",
      "Word: ngonsin\n",
      "Unknown char: \n",
      "Word: lser\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: ltsaskompis\n",
      "Unknown char: \n",
      "Word: hger\n",
      "Unknown char: \n",
      "Word: bekrftar\n",
      "Unknown char: \n",
      "Word: vnsters\n",
      "Unknown char: \n",
      "Word: vrldbild\n",
      "Unknown char: \n",
      "Word: nstan\n",
      "Unknown char: \n",
      "Word: sgas\n",
      "Unknown char: \n",
      "Word: gra\n",
      "Unknown char: \n",
      "Word: ppekar\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: jsses\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: hromdagen\n",
      "Unknown char: \n",
      "Word: vl\n",
      "Unknown char: \n",
      "Word: mrdaren\n",
      "Unknown char: \n",
      "Word: hmnas\n",
      "Unknown char: \n",
      "Word: gr\n",
      "Unknown char: \n",
      "Word: terrordden\n",
      "Unknown char: \n",
      "Word: utfrs\n",
      "Unknown char: \n",
      "Word: vnta\n",
      "Unknown char: \n",
      "Word: bestmmer\n",
      "Unknown char: \n",
      "Word: hller\n",
      "Unknown char: \n",
      "Word: str\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: bsta\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: hller\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: mnskliga\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: lser\n",
      "Unknown char: \n",
      "Word: lsa\n",
      "Unknown char: \n",
      "Word: istllet\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: hrt\n",
      "Unknown char: \n",
      "Word: sjlvmordsbombare\n",
      "Unknown char: \n",
      "Word: mellanstern\n",
      "Unknown char: \n",
      "Word: religs\n",
      "Unknown char: \n",
      "Word: str\n",
      "Unknown char: \n",
      "Word: religs\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: ls\n",
      "Unknown char: \n",
      "Word: sga\n",
      "Unknown char: \n",
      "Word: frolmpning\n",
      "Unknown char: \n",
      "Word: frolmpning\n",
      "Unknown char: \n",
      "Word: dr\n",
      "Unknown char: \n",
      "Word: nr\n",
      "Unknown char: \n",
      "Word: avrttar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: =\n",
      "Word: massmedia=junkfood\n",
      "Unknown char: \n",
      "Word: grard\n",
      "Unknown char: \n",
      "Word: frenchfrog\n",
      "Unknown char: \n",
      "Word: dfendre\n",
      "Unknown char: \n",
      "Word: dmocratie\n",
      "Unknown char: \n",
      "Word: gefhrlich\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: ramnent\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \"\n",
      "Word: issues.\"i\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \"\n",
      "Word: unemplymt.\"they\"deserve\n",
      "Unknown char: \"\n",
      "Word: unemplymt.\"they\"deserve\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: i\"m\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: displayas\n",
      "Unknown char: \n",
      "Word: staffpublish\n",
      "Unknown char: \n",
      "Word: week1m\n",
      "Unknown char: \"\n",
      "Word: important\"...more\n",
      "Unknown char: \"\n",
      "Word: the\"religion\n",
      "Unknown char: \n",
      "Word: manqus\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: pnico\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: vergenza\n",
      "Unknown char: \n",
      "Word: debera\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: mdchen\n",
      "Unknown char: =\n",
      "Word: paris=israeli\n",
      "Unknown char: \n",
      "Word: iin\n",
      "Unknown char: \n",
      "Word: baskn\n",
      "Unknown char: \n",
      "Word: yapyor\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: commmoratif\n",
      "Unknown char: \n",
      "Word: bahrains\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \\\n",
      "Word: little\\no\n",
      "Unknown char: \n",
      "Word: pministerin\n",
      "Unknown char: \n",
      "Word: pministerin\n",
      "Unknown char: \n",
      "Word: pministeri\n",
      "Unknown char: \n",
      "Word: pministeri\n",
      "Unknown char: \n",
      "Word: sentn\n",
      "Unknown char: \n",
      "Word: sentn\n",
      "Unknown char: \n",
      "Word: tst\n",
      "Unknown char: \n",
      "Word: hpsis\n",
      "Unknown char: \"\n",
      "Word: and.get.a.job\"ignorance/complacency\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: pilots\n",
      "Unknown char: =\n",
      "Word: about=minority\n",
      "Unknown char: =\n",
      "Word: thing=depending\n",
      "Unknown char: \n",
      "Word: tus\n",
      "Unknown char: \n",
      "Word: proccs\n",
      "Unknown char: \"\n",
      "Word: worked.\"multiculturalism\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: arme\n",
      "Unknown char: \n",
      "Word: ame\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \"\n",
      "Word: translates\"by\n",
      "Unknown char: \n",
      "Word: mrdias\n",
      "Unknown char: \"\n",
      "Word: translates\"by\n",
      "Unknown char: \n",
      "Word: seal\n",
      "Unknown char: \n",
      "Word: poblacin\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: needy\"...such\n",
      "Unknown char: \n",
      "Word: hros\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: \n",
      "Word: esprons\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: extrmistes\n",
      "Unknown char: \n",
      "Word: speechit\n",
      "Unknown char: \n",
      "Word: speechit\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: rvulse\n",
      "Unknown char: \n",
      "Word: franaise\n",
      "Unknown char: \n",
      "Word: vnement\n",
      "Unknown char: \n",
      "Word: l-bas\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \n",
      "Word: palhaada\n",
      "Unknown char: \n",
      "Word: choque\n",
      "Unknown char: \n",
      "Word: skl\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: frsvarar\n",
      "Unknown char: \n",
      "Word: tnkte\n",
      "Unknown char: \n",
      "Word: islams\n",
      "Unknown char: \n",
      "Word: zgina\n",
      "Unknown char: \n",
      "Word: zgina\n",
      "Unknown char: \n",
      "Word: dzikuj\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: wyprbowanie\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: myli\n",
      "Unknown char: \n",
      "Word: sowo\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: stracia\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: obamawould\n",
      "Unknown char: \n",
      "Word: grda\n",
      "Unknown char: \n",
      "Word: saighdiir\n",
      "Unknown char: \n",
      "Word: difrocht\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: enseando\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: glen\n",
      "Unknown char: \n",
      "Word: balayn\n",
      "Unknown char: \n",
      "Word: balayn\n",
      "Unknown char: \n",
      "Word: grce\n",
      "Unknown char: \n",
      "Word: concrtement\n",
      "Unknown char: \n",
      "Word: ragissons\n",
      "Unknown char: \"\n",
      "Word: a\"dark\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: situao\n",
      "Unknown char: \n",
      "Word: situao\n",
      "Unknown char: \n",
      "Word: reporte\n",
      "Unknown char: \n",
      "Word: trouve\n",
      "Unknown char: \n",
      "Word: j'tais\n",
      "Unknown char: \n",
      "Word: colre\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: islamici\n",
      "Unknown char: \n",
      "Word: zych\n",
      "Unknown char: \n",
      "Word: zdjcie\n",
      "Unknown char: \n",
      "Word: wiatw\n",
      "Unknown char: \n",
      "Word: pomyslaam\n",
      "Unknown char: \n",
      "Word: d'andras\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: unharmedxo\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: espre\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: thanks\n",
      "Unknown char: \"\n",
      "Word: say\"he\n",
      "Unknown char: \n",
      "Word: arrtes\n",
      "Unknown char: \n",
      "Word: lislam\n",
      "Unknown char: \n",
      "Word: newsmedia\n",
      "Unknown char: \n",
      "Word: sme\n",
      "Unknown char: \n",
      "Word: rcolte\n",
      "Unknown char: \n",
      "Word: tempte\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: shithow\n",
      "Unknown char: =\n",
      "Word: brigade=terrorist's\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: dcroch\n",
      "Unknown char: \n",
      "Word: alterao\n",
      "Unknown char: \n",
      "Word: alterao\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: magnficas\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: atencin\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: espaol\n",
      "Unknown char: \n",
      "Word: ingls\n",
      "Unknown char: \n",
      "Word: paradjico\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: liberacin\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: fcil\n",
      "Unknown char: \n",
      "prayinginish....\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: itd\n",
      "Unknown char: \n",
      "Word: twtd\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \"\n",
      "Word: their\"nations\n",
      "Unknown char: =\n",
      "Word: piece==&gt\n",
      "Unknown char: =\n",
      "Word: piece==&gt\n",
      "Unknown char: \n",
      "Word: tus\n",
      "Unknown char: \n",
      "Word: cur\n",
      "Unknown char: \n",
      "Word: n'taient\n",
      "Unknown char: \n",
      "Word: arms\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: muulmanos\n",
      "Unknown char: \"\n",
      "Word: martyrs\".i\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: kytt\n",
      "Unknown char: \n",
      "Word: kytt\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: lhteest\n",
      "Unknown char: \n",
      "Word: elif\n",
      "Unknown char: \n",
      "Word: ltfen\n",
      "Unknown char: \n",
      "Word: takip\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: whove\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: kapya\n",
      "Unknown char: \n",
      "Word: kyor\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: hed\n",
      "Unknown char: \n",
      "Word: wheres\n",
      "Unknown char: \"\n",
      "Word: allah\"to\n",
      "Unknown char: \n",
      "Word: allah\"to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: spisil\n",
      "Unknown char: \n",
      "Word: spisil\n",
      "Unknown char: \n",
      "Word: fill\n",
      "Unknown char: \n",
      "Word: shuomh\n",
      "Unknown char: \n",
      "Word: inscurit\n",
      "Unknown char: \n",
      "Word: mds\n",
      "Unknown char: \n",
      "Word: astrix\n",
      "Unknown char: \n",
      "Word: rsistant\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: crtica\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: article\"how\n",
      "Unknown char: =\n",
      "Word: it=a\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: you're*....\"as\n",
      "Unknown char: \"\n",
      "Word: faith\"when\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: areos\n",
      "Unknown char: \n",
      "Word: arent\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: govt\n",
      "Unknown char: \n",
      "Word: norvgien\n",
      "Unknown char: \n",
      "Word: dgouts\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: allans\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: frres\n",
      "Unknown char: \n",
      "Word: arrter\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: c'est--dire\n",
      "Unknown char: \n",
      "Word: gzaltna\n",
      "Unknown char: \n",
      "Word: gzaltna\n",
      "Unknown char: \n",
      "Word: alnd\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: ento\n",
      "Unknown char: \n",
      "Word: padres\n",
      "Unknown char: \n",
      "Word: so\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \"\n",
      "Word: muslim...\"silent\n",
      "Unknown char: \"\n",
      "Word: majority\"...condemn\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: razn\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: stck\n",
      "Unknown char: \n",
      "Word: kse\n",
      "Unknown char: \n",
      "Word: peaceit's\n",
      "Unknown char: \n",
      "Word: peaceit's\n",
      "Unknown char: =\n",
      "Word: uber=opportunistic\n",
      "Unknown char: \"\n",
      "Word: khanjar.\"a\n",
      "Unknown char: \"\n",
      "Word: case\"send\n",
      "Unknown char: \n",
      "Word: tomorrows\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'htel\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: pases\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: prf\n",
      "Unknown char: \n",
      "Word: prsent\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: europens\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: chrtiens\n",
      "Unknown char: \n",
      "Word: rpublicains\n",
      "Unknown char: \n",
      "Word: rpondre\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: terrist\n",
      "Unknown char: \n",
      "Word: terrist\n",
      "Unknown char: \n",
      "Word: dnya\n",
      "Unknown char: \n",
      "Word: baritan\n",
      "Unknown char: \n",
      "Word: geilmez\n",
      "Unknown char: \n",
      "Word: geilmez\n",
      "Unknown char: \n",
      "Word: kalrm\n",
      "Unknown char: \n",
      "Word: yannda\n",
      "Unknown char: \n",
      "Word: dn\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: kyamam\n",
      "Unknown char: \n",
      "Word: biey\n",
      "Unknown char: \n",
      "Word: syleyemiyo\n",
      "Unknown char: \n",
      "Word: artk\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: anlyorum\n",
      "Unknown char: \n",
      "Word: konuuyorum\n",
      "Unknown char: \n",
      "Word: azndan\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: p-p\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: schieen\n",
      "Unknown char: \n",
      "Word: livebertragung\n",
      "Unknown char: \n",
      "Word: drfte\n",
      "Unknown char: \n",
      "Word: wre\n",
      "Unknown char: \n",
      "Word: tter\n",
      "Unknown char: \n",
      "Word: auszulschen\n",
      "Unknown char: \n",
      "Word: gbe\n",
      "Unknown char: \n",
      "Word: strenden\n",
      "Unknown char: \n",
      "Word: verrterischen\n",
      "Unknown char: \n",
      "Word: mrder\n",
      "Unknown char: \n",
      "Word: kmen\n",
      "Unknown char: \n",
      "Word: grber\n",
      "Unknown char: \n",
      "Word: flchtet\n",
      "Unknown char: \n",
      "Word: parittisch\n",
      "Unknown char: \n",
      "Word: australias\n",
      "Unknown char: \n",
      "Word: americas\n",
      "Unknown char: \n",
      "Word: n'tes\n",
      "Unknown char: \n",
      "Word: iin\n",
      "Unknown char: \n",
      "Word: basp\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \"\n",
      "Word: afraid\"-thousands\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: tribresararivelesgentfaitesapoursuirejurtise\n",
      "Unknown char: \n",
      "Word: confiana\n",
      "Unknown char: \n",
      "Word: ns\n",
      "Unknown char: \n",
      "Word: to\n",
      "Unknown char: \n",
      "Word: difcil\n",
      "Unknown char: \n",
      "Word: condenao\n",
      "Unknown char: \n",
      "Word: condenao\n",
      "Unknown char: \n",
      "Word: habrn\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: gnration\n",
      "Unknown char: \n",
      "Word: gnration\n",
      "Unknown char: \n",
      "Word: hro\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: yoll\n",
      "Unknown char: \"\n",
      "Word: euh...\"holletje\n",
      "Unknown char: \n",
      "Word: vrios\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: thisssrt\n",
      "Unknown char: \n",
      "Word: thisssrt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: powana\n",
      "Unknown char: \n",
      "Word: zwyky\n",
      "Unknown char: \n",
      "Word: moe\n",
      "Unknown char: \n",
      "Word: moe\n",
      "Unknown char: \n",
      "Word: oczywicie\n",
      "Unknown char: \n",
      "Word: rne\n",
      "Unknown char: \n",
      "Word: rne\n",
      "Unknown char: \n",
      "Word: rda\n",
      "Unknown char: \n",
      "Word: rda\n",
      "Unknown char: \n",
      "Word: mona\n",
      "Unknown char: \n",
      "Word: wpadem\n",
      "Unknown char: \n",
      "Word: sida\n",
      "Unknown char: \n",
      "Word: wite\n",
      "Unknown char: \n",
      "Word: sowa\n",
      "Unknown char: \n",
      "Word: ostrzeenie\n",
      "Unknown char: \n",
      "Word: lotw\n",
      "Unknown char: \n",
      "Word: przeszkadzay\n",
      "Unknown char: \n",
      "Word: pkinois(nous\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: chre\n",
      "Unknown char: \n",
      "Word: prvert\n",
      "Unknown char: \n",
      "Word: youve\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \"\n",
      "Word: cellphoneshutdown\"evansolomonreporting\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: ar\n",
      "Unknown char: \n",
      "Word: ar\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: weve\n",
      "Unknown char: =\n",
      "Word: lt;=&gt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: rightno\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: gods\n",
      "Unknown char: =\n",
      "Word: religion=violence\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: translationno\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: situacin\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: konuuyor\n",
      "Unknown char: \n",
      "Word: sanyor\n",
      "Unknown char: \n",
      "Word: islam\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: wrong\"~~mjs\n",
      "Unknown char: \n",
      "Word: seora\n",
      "Unknown char: \n",
      "Word: constncia\n",
      "Unknown char: \n",
      "Word: xenofbia\n",
      "Unknown char: \n",
      "Word: ents\n",
      "Unknown char: \n",
      "Word: sn\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: tradut\n",
      "Unknown char: \n",
      "Word: dbarras\n",
      "Unknown char: \n",
      "Word: xrbi\n",
      "Unknown char: \n",
      "Word: xrbi\n",
      "Unknown char: \"\n",
      "Word: heroes.\"he\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: indignacin\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "excellent\n",
      "Unknown char: \n",
      "butd: idea\n",
      "Unknown char: \n",
      "they: about\n",
      "Unknown char: \n",
      "Word: headlesschookmpersonatingghekowithnotail\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: bata\n",
      "Unknown char: \n",
      "Word: hepinizi\n",
      "Unknown char: \n",
      "Word: hepinizi\n",
      "Unknown char: \n",
      "Word: ak\n",
      "Unknown char: \n",
      "Word: ak\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: inileriniz\n",
      "Unknown char: \n",
      "Word: inileriniz\n",
      "Unknown char: \n",
      "Word: gvenli\n",
      "Unknown char: \n",
      "Word: wtend\n",
      "Unknown char: \n",
      "Word: hrt\n",
      "Unknown char: \n",
      "Word: storlengmlsch\n",
      "Unknown char: \n",
      "Word: mllerkrnerkleft\n",
      "Unknown char: \n",
      "Word: mllerkrnerkleft\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: guantnamo\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: methats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dmocratie\n",
      "Unknown char: \n",
      "Word: dcidment\n",
      "Unknown char: \n",
      "Word: dcidment\n",
      "Unknown char: \n",
      "Word: convictionjust\n",
      "Unknown char: \n",
      "Word: convictionjust\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dammartin-en-gol\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: arent\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: =\n",
      "Word: violations=dismissal\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: glen's\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: mnner\n",
      "Unknown char: \n",
      "Word: wet\n",
      "Unknown char: \n",
      "Word: bozuuna\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \"\n",
      "Word: were\"gardien\n",
      "Unknown char: \n",
      "Word: wyraam\n",
      "Unknown char: \n",
      "Word: najgbsze\n",
      "Unknown char: \n",
      "Word: najgbsze\n",
      "Unknown char: \n",
      "Word: wspczucie\n",
      "Unknown char: \n",
      "Word: wspczucie\n",
      "Unknown char: \n",
      "Word: bdziemy\n",
      "Unknown char: \n",
      "Word: l'poque\n",
      "Unknown char: \n",
      "Word: isral\n",
      "Unknown char: \n",
      "Word: doru\n",
      "Unknown char: \n",
      "Word: sylemi\n",
      "Unknown char: \n",
      "Word: castros\n",
      "Unknown char: \n",
      "Word: cubas\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: d'arrter\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: nause\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: jocks\"?for\n",
      "Unknown char: \n",
      "Word: corn\n",
      "Unknown char: \n",
      "Word: provocacin\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: gg\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: priphrique\n",
      "Unknown char: \n",
      "Word: priphrique\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: democrticas\n",
      "Unknown char: \n",
      "Word: expulsin\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: confirmation?\"ask\n",
      "Unknown char: =\n",
      "Word: sorry=proven\n",
      "Unknown char: \n",
      "Word: cmon\n",
      "Unknown char: \"\n",
      "Word: topic-\"are\n",
      "Unknown char: \n",
      "Word: sad\n",
      "Unknown char: \n",
      "Word: cherf\n",
      "Unknown char: \n",
      "Word: crmer\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \n",
      "Word: erdoan's\n",
      "Unknown char: \n",
      "Word: erdoan\n",
      "Unknown char: \n",
      "Word: trkiyenin\n",
      "Unknown char: \n",
      "Word: gvenilir\n",
      "Unknown char: \n",
      "Word: takipi\n",
      "Unknown char: \n",
      "Word: satn\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: provocacin\n",
      "Unknown char: \n",
      "Word: religin\n",
      "Unknown char: \n",
      "Word: sunni/shite\n",
      "Unknown char: \n",
      "Word: israeles\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: a3204u\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: engage\n",
      "Unknown char: \n",
      "Word: dcrit\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: occupe\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \"\n",
      "Word: allah\"...and\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: anti-zoroastrianthe\n",
      "Unknown char: =\n",
      "Word: report=legal\n",
      "Unknown char: \"\n",
      "Word: so\"..wen\n",
      "Unknown char: \"\n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: mnner\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: ideologas\n",
      "Unknown char: \n",
      "Word: polticos\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \"\n",
      "Word: wifi\"...i\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: hlas\n",
      "Unknown char: \n",
      "Word: converthero\n",
      "Unknown char: \n",
      "ifrd: ism\n",
      "Unknown char: \"\n",
      "Word: here\"..generic\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \"\n",
      "Word: memorial:\"pray\n",
      "Unknown char: \n",
      "ifrd: ism\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: prire\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: nis\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: irans\n",
      "Unknown char: \n",
      "Word: irans\n",
      "Unknown char: \n",
      "Word: paraso\n",
      "Unknown char: \n",
      "Word: childs\n",
      "Unknown char: \"\n",
      "Word: wasn\"t\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: =\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \n",
      "Word: mio\n",
      "Unknown char: \"\n",
      "Word: purposes\"is\n",
      "Unknown char: \n",
      "Word: schbig\n",
      "Unknown char: \n",
      "Word: gesprch\n",
      "Unknown char: \n",
      "Word: obrien\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: gldjande\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: matana\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: circulao\n",
      "Unknown char: \n",
      "Word: circulao\n",
      "Unknown char: \n",
      "Word: edio\n",
      "Unknown char: \n",
      "Word: edio\n",
      "Unknown char: \n",
      "Word: prxima\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: ill\n",
      "Unknown char: \"\n",
      "Word: dissidents/undesirables\"-run\n",
      "Unknown char: \"\n",
      "Word: date\"-the\n",
      "Unknown char: \n",
      "Word: ones\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: gouverns\n",
      "Unknown char: \n",
      "Word: gallrie\n",
      "Unknown char: \n",
      "Word: fua\n",
      "Unknown char: \n",
      "Word: rtje\n",
      "Unknown char: \n",
      "Word: neutralised\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \"\n",
      "Word: not\"our\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: brush\"-i\n",
      "Unknown char: \n",
      "Word: villefranche-sur-sane\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: hasnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wouldve\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: tt\n",
      "Unknown char: \n",
      "Word: hypothse\n",
      "Unknown char: \n",
      "Word: cre\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: \n",
      "Word: m'crire\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: l'tat\n",
      "Unknown char: \n",
      "Word: procderais\n",
      "Unknown char: \n",
      "Word: ct\n",
      "Unknown char: \n",
      "Word: d'coute\n",
      "Unknown char: \n",
      "Word: honntement\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: l'vnement\n",
      "Unknown char: \n",
      "Word: l'vnement\n",
      "Unknown char: \n",
      "Word: prire\n",
      "Unknown char: \n",
      "Word: avrer\n",
      "Unknown char: \n",
      "Word: pice\n",
      "Unknown char: \n",
      "Word: mne\n",
      "Unknown char: \n",
      "Word: qubec\n",
      "Unknown char: \n",
      "Word: rellement\n",
      "Unknown char: \n",
      "Word: ct\n",
      "Unknown char: \n",
      "Word: toi-mme\n",
      "Unknown char: \n",
      "Word: mdia\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: gre\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: contrle\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: crer\n",
      "Unknown char: \n",
      "Word: infonde\n",
      "Unknown char: \n",
      "Word: errones\n",
      "Unknown char: \n",
      "Word: communiques\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: \n",
      "Word: arrter\n",
      "Unknown char: \n",
      "Word: caractre\n",
      "Unknown char: \n",
      "Word: rponde\n",
      "Unknown char: \n",
      "Word: srement\n",
      "Unknown char: \n",
      "Word: t'inquite\n",
      "Unknown char: \n",
      "Word: plutt\n",
      "Unknown char: \n",
      "Word: boe\n",
      "Unknown char: \n",
      "Word: sauvaj\n",
      "Unknown char: \n",
      "Word: fantismo\n",
      "Unknown char: \n",
      "Word: brbaro\n",
      "Unknown char: \n",
      "Word: donnes\n",
      "Unknown char: \n",
      "Word: spcial\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: d'lite\n",
      "Unknown char: \n",
      "Word: diffrente\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: units\n",
      "Unknown char: \n",
      "Word: mriterait\n",
      "Unknown char: \n",
      "Word: rforme\n",
      "Unknown char: \n",
      "Word: dautres\n",
      "Unknown char: \n",
      "Word: units\n",
      "Unknown char: \n",
      "Word: dinterventions\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: prviens\n",
      "Unknown char: \n",
      "Word: dsinformation\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: exagrer\n",
      "Unknown char: \n",
      "Word: l'extrmisme\n",
      "Unknown char: \n",
      "Word: atpeaceful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: priorits\n",
      "Unknown char: \n",
      "Word: dplac\n",
      "Unknown char: \n",
      "Word: mankind.ll\n",
      "Unknown char: =\n",
      "Word: sharialaw=hamas=isis\n",
      "Unknown char: =\n",
      "Word: sharialaw=hamas=isis\n",
      "Unknown char: =\n",
      "Word: haram=hezbollah\n",
      "Unknown char: \"\n",
      "Word: bold\"?...trying\n",
      "Unknown char: =\n",
      "Word: lies=lies=lies\n",
      "Unknown char: =\n",
      "Word: lies=lies=lies\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: uss\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: uss\n",
      "Unknown char: =\n",
      "Word: suspects=more\n",
      "Unknown char: \n",
      "Word: tmraire\n",
      "Unknown char: \n",
      "Word: tmraire\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: frre\n",
      "Unknown char: \n",
      "Word: monte\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: polcia\n",
      "Unknown char: \n",
      "Word: austrlia\n",
      "Unknown char: \n",
      "Word: mantm\n",
      "Unknown char: \n",
      "Word: vrios\n",
      "Unknown char: \n",
      "Word: refns\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \"\n",
      "Word: w/\"reported\n",
      "Unknown char: \n",
      "Word: sper\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \"\n",
      "Word: world\"....great\n",
      "Unknown char: \n",
      "Word: exploses\n",
      "Unknown char: \n",
      "Word: downtowncrazy\n",
      "Unknown char: \"\n",
      "Word: it\"workplace\n",
      "Unknown char: \n",
      "Word: that's\n",
      "Unknown char: =\n",
      "Word: repeat=&gt;if\n",
      "Unknown char: =\n",
      "Word: repeat=&gt;if\n",
      "Unknown char: \n",
      "Word: terristlerin\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: =\n",
      "Word: terrorismo=mafie\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: yes....\"accidents\n",
      "Unknown char: \"\n",
      "Word: i\"m\n",
      "Unknown char: \n",
      "Word: terrorfico\n",
      "Unknown char: \n",
      "Word: frmmestad\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: matter.\"..lol\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: tte\n",
      "Unknown char: \n",
      "Word: mdiatique\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \\\n",
      "Word: nat\\f\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \"\n",
      "Word: islamist\"attacks\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: consistentmuch\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: hows\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: guessim\n",
      "Unknown char: \n",
      "Word: guessim\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: rats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: do\"it\n",
      "Unknown char: \n",
      "Word: respects\n",
      "Unknown char: \n",
      "Word: assasins\n",
      "Unknown char: \n",
      "Word: brler\n",
      "Unknown char: \n",
      "Word: dnoncer\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: kady\n",
      "Unknown char: \n",
      "Word: bdzie\n",
      "Unknown char: \n",
      "Word: prbowa\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: zaktkach\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: kady\n",
      "Unknown char: \n",
      "Word: bdzie\n",
      "Unknown char: \n",
      "Word: prbowa\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: zaktkach\n",
      "Unknown char: \n",
      "Word: cytujc\n",
      "Unknown char: \"\n",
      "Word: returned:\"but\n",
      "Unknown char: \n",
      "Word: lches\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: cdez\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \"\n",
      "Word: him\"...what\n",
      "Unknown char: \"\n",
      "Word: security\"...from\n",
      "Unknown char: \"\n",
      "Word: him\"...what\n",
      "Unknown char: \"\n",
      "Word: saying...\"if\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \"\n",
      "Word: condulence\"-jebag\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: journe\n",
      "Unknown char: \n",
      "Word: yeahi\n",
      "Unknown char: \n",
      "Word: got\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wren\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: caazo\n",
      "Unknown char: \n",
      "Word: attaqus\n",
      "Unknown char: \n",
      "Word: dmissionne\n",
      "Unknown char: \n",
      "Word: l'indpendantiste\n",
      "Unknown char: \n",
      "Word: rviser\n",
      "Unknown char: \n",
      "Word: amricaine\n",
      "Unknown char: \"\n",
      "Word: attack:paris\"ll\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \"\n",
      "Word: cop/\"security\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: theydidn't\n",
      "Unknown char: \n",
      "Word: dport\n",
      "Unknown char: \"\n",
      "Word: believe\"from\n",
      "Unknown char: \"\n",
      "Word: order\":usjrno\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: too\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \"\n",
      "Word: martyrs\"french\n",
      "Unknown char: \n",
      "Word: podan\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: pasamontaas\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: inmolndose.pero\n",
      "Unknown char: \n",
      "Word: pelcula\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: das\n",
      "Unknown char: \n",
      "Word: sultn\n",
      "Unknown char: \n",
      "Word: algn\n",
      "Unknown char: \n",
      "Word: partindose\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: dcd\n",
      "Unknown char: \n",
      "Word: dcd\n",
      "Unknown char: \n",
      "Word: leiil\n",
      "Unknown char: \n",
      "Word: plutt\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: artculo\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: fume\n",
      "Unknown char: \n",
      "Word: poasi\n",
      "Unknown char: \n",
      "Word: nesrena\n",
      "Unknown char: \n",
      "Word: sluaj\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: cuntas\n",
      "Unknown char: \n",
      "Word: pakistn\n",
      "Unknown char: \n",
      "Word: irn\n",
      "Unknown char: \n",
      "Word: hed\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: sera\n",
      "Unknown char: \n",
      "Word: dediimiz\n",
      "Unknown char: \n",
      "Word: ite\n",
      "Unknown char: \n",
      "Word: yazld\n",
      "Unknown char: \n",
      "Word: kardeim\n",
      "Unknown char: \n",
      "Word: kavgann\n",
      "Unknown char: \n",
      "Word: istanbul\n",
      "Unknown char: \n",
      "Word: kfr\n",
      "Unknown char: \n",
      "Word: kfr\n",
      "Unknown char: \n",
      "Word: eteine\n",
      "Unknown char: \n",
      "Word: ayaksn\n",
      "Unknown char: \n",
      "Word: gerek\n",
      "Unknown char: \n",
      "Word: nasl\n",
      "Unknown char: \n",
      "Word: yazk\n",
      "Unknown char: \n",
      "Word: yazk\n",
      "Unknown char: \n",
      "Word: karl\n",
      "Unknown char: \n",
      "Word: karl\n",
      "Unknown char: \n",
      "Word: satrlarnda\n",
      "Unknown char: \n",
      "Word: satrlarnda\n",
      "Unknown char: \n",
      "Word: trk\n",
      "Unknown char: \n",
      "Word: deil\n",
      "Unknown char: \n",
      "Word: hereyi\n",
      "Unknown char: \n",
      "Word: satyorlar\n",
      "Unknown char: \n",
      "Word: yz\n",
      "Unknown char: \n",
      "Word: trkl\n",
      "Unknown char: \n",
      "Word: hakknda\n",
      "Unknown char: \n",
      "Word: olmutur\n",
      "Unknown char: \n",
      "Word: manasyla\n",
      "Unknown char: \n",
      "Word: istediin\n",
      "Unknown char: \n",
      "Word: sakalndan\n",
      "Unknown char: \n",
      "Word: srkleneceksin\n",
      "Unknown char: \n",
      "Word: srkleneceksin\n",
      "Unknown char: \n",
      "Word: dnya'da\n",
      "Unknown char: \n",
      "Word: grelim\n",
      "Unknown char: \n",
      "Word: yoksunluu\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: hainlii\n",
      "Unknown char: \n",
      "Word: ahsn\n",
      "Unknown char: \n",
      "Word: tarafndan\n",
      "Unknown char: \n",
      "Word: istihbaratn\n",
      "Unknown char: \n",
      "Word: inaatna\n",
      "Unknown char: \n",
      "Word: inaatna\n",
      "Unknown char: \n",
      "Word: kat\n",
      "Unknown char: \n",
      "Word: anlayamadk\n",
      "Unknown char: \n",
      "Word: bulalm\n",
      "Unknown char: \n",
      "Word: yaptlar\n",
      "Unknown char: \n",
      "Word: baka\n",
      "Unknown char: \n",
      "Word: grmez\n",
      "Unknown char: \n",
      "Word: gzlerin\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: alrsan\n",
      "Unknown char: \n",
      "Word: ettiine\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: kiilerden(iki\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: laflarn\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: kfrlere\n",
      "Unknown char: \n",
      "Word: kfrlere\n",
      "Unknown char: \n",
      "Word: tenezzl\n",
      "Unknown char: \n",
      "Word: aklmda\n",
      "Unknown char: \n",
      "Word: kaldn\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: bakp\n",
      "Unknown char: \n",
      "Word: ahlaksz\n",
      "Unknown char: \n",
      "Word: gndermelerini\n",
      "Unknown char: \n",
      "Word: okyacaksn\n",
      "Unknown char: \n",
      "Word: bakp\n",
      "Unknown char: \n",
      "Word: temil\n",
      "Unknown char: \n",
      "Word: alamazsn.kastl\n",
      "Unknown char: \n",
      "Word: alamazsn.kastl\n",
      "Unknown char: \n",
      "Word: yapyorsun\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: grevini\n",
      "Unknown char: \n",
      "Word: yapyor\n",
      "Unknown char: \n",
      "Word: yedii\n",
      "Unknown char: \n",
      "Word: ca\n",
      "Unknown char: \n",
      "Word: nn\n",
      "Unknown char: \n",
      "Word: ahlaksz\n",
      "Unknown char: \n",
      "Word: gnderme\n",
      "Unknown char: \n",
      "Word: yazmadm\n",
      "Unknown char: \n",
      "Word: yazdklarnla\n",
      "Unknown char: \n",
      "Word: yazdklarnla\n",
      "Unknown char: \n",
      "Word: noktasna\n",
      "Unknown char: \n",
      "Word: gidecein\n",
      "Unknown char: \n",
      "Word: uyarsnda\n",
      "Unknown char: \n",
      "Word: uyarsnda\n",
      "Unknown char: \n",
      "Word: hocasnn\n",
      "Unknown char: \n",
      "Word: hocasnn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: deilmi\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \"\n",
      "Word: bitches...\"unless\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: americans=full\n",
      "Unknown char: \n",
      "Word: vl\n",
      "Unknown char: \n",
      "Word: sjlvklarhet\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: lhistoire\n",
      "Unknown char: \n",
      "Word: envoys\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: indication\"of\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: bushs\n",
      "Unknown char: \n",
      "Word: bushs\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: someones\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: someones\n",
      "Unknown char: \n",
      "Word: gun.its\n",
      "Unknown char: \n",
      "Word: sryor\n",
      "Unknown char: \n",
      "Word: sryor\n",
      "Unknown char: \n",
      "Word: dnonce\n",
      "Unknown char: \"\n",
      "Word: much\"im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: persons\n",
      "Unknown char: \n",
      "Word: qubec\n",
      "Unknown char: \n",
      "Word: govt\n",
      "Unknown char: =\n",
      "Word: ideas=fair\n",
      "Unknown char: =\n",
      "Word: this=ridiculous.do\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: comunicacin\n",
      "Unknown char: \n",
      "Word: islam\n",
      "Unknown char: \n",
      "Word: adna\n",
      "Unknown char: \n",
      "Word: terrn\n",
      "Unknown char: \n",
      "Word: terrn\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: hayatn\n",
      "Unknown char: \n",
      "Word: mslman\n",
      "Unknown char: \n",
      "Word: mslman\n",
      "Unknown char: \n",
      "Word: reporters\n",
      "Unknown char: \"\n",
      "Word: truth\"-du-jour\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "Word: tt\n",
      "Unknown char: \n",
      "herd: mention\n",
      "Unknown char: \n",
      "herd: mention\n",
      "Unknown char: \n",
      "Word: mans\n",
      "Unknown char: \n",
      "Word: hy\n",
      "Unknown char: \n",
      "Word: git\n",
      "Unknown char: \n",
      "Word: cht\n",
      "Unknown char: \n",
      "Word: bn\n",
      "Unknown char: \n",
      "Word: khng\n",
      "Unknown char: \n",
      "Word: youve\n",
      "Unknown char: =\n",
      "Word: innocent=kool\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: amrique\n",
      "Unknown char: \n",
      "Word: catstrofe\n",
      "Unknown char: \n",
      "Word: area\n",
      "Unknown char: \n",
      "Word: w/themiddle\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: seor\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: econmico\n",
      "Unknown char: \n",
      "Word: previsvel\n",
      "Unknown char: \n",
      "Word: theyd\n",
      "Unknown char: \n",
      "Word: dorothe\n",
      "Unknown char: \n",
      "Word: enchan\n",
      "Unknown char: \n",
      "Word: bajn\n",
      "Unknown char: \n",
      "Word: psame\n",
      "Unknown char: \n",
      "Word: habls\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: obligacin\n",
      "Unknown char: \n",
      "Word: democrticos\n",
      "Unknown char: \"\n",
      "Word: skyline\"yourselves\n",
      "Unknown char: \"\n",
      "Word: christ-like\"....many\n",
      "Unknown char: \"\n",
      "Word: who\"the\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: mgalomane\n",
      "Unknown char: \n",
      "Word: order.the\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: naf\n",
      "Unknown char: \n",
      "Word: condolances\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: =\n",
      "Word: religion=superstition\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: curs\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \"\n",
      "Word: rt\"why\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: =\n",
      "Word: act=intelligent\n",
      "Unknown char: \n",
      "Word: bni\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dlires\n",
      "Unknown char: \n",
      "Word: islamise\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: franois\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: sptzle\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: ferguson\"monday\n",
      "Unknown char: \n",
      "Word: mxico\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: ningum\n",
      "Unknown char: \n",
      "Word: to\n",
      "Unknown char: \n",
      "Word: dbr\n",
      "Unknown char: \n",
      "Word: dbr\n",
      "Unknown char: \n",
      "Word: deilsin\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: bji\n",
      "Unknown char: \n",
      "Word: krdistan\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wt\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: sacrbleu\n",
      "Unknown char: \n",
      "Word: prophte\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: =\n",
      "Word: ability=no\n",
      "Unknown char: \"\n",
      "Word: negros\"when\n",
      "Unknown char: \"\n",
      "Word: surveillance-\"terrorism\n",
      "Unknown char: \n",
      "Word: lmites\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: mxico\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: journe...c'est\n",
      "Unknown char: \n",
      "Word: scurit\n",
      "Unknown char: \n",
      "Word: hros\n",
      "Unknown char: \n",
      "Word: tiet-sp\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: aid=treasure\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: terrorists\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: youll\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: comunicacin\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: frankrich\n",
      "Unknown char: \"\n",
      "Word: theres\"dryer\n",
      "Unknown char: \"\n",
      "Word: american\"..who\n",
      "Unknown char: \n",
      "Word: sonot\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: to\"heaven\n",
      "Unknown char: \n",
      "Word: tawhd\n",
      "Unknown char: \n",
      "Word: lve\n",
      "Unknown char: \n",
      "Word: cafetera\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: glen's\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: ottawas\n",
      "Unknown char: \"\n",
      "Word: says:\"ray\n",
      "Unknown char: \"\n",
      "Word: k\"c\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: quelquun\n",
      "Unknown char: \n",
      "Word: premire\n",
      "Unknown char: \n",
      "Word: perscutions\n",
      "Unknown char: \n",
      "Word: perscuts\n",
      "Unknown char: \n",
      "Word: perscuts\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: dalgriens\n",
      "Unknown char: \n",
      "Word: dalgriens\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: algrie\n",
      "Unknown char: \n",
      "Word: sicle\n",
      "Unknown char: \n",
      "Word: isral\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: prt\n",
      "Unknown char: \n",
      "Word: extrmistes\n",
      "Unknown char: \n",
      "Word: uks\n",
      "Unknown char: \n",
      "Word: frmmestad\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: solder\n",
      "Unknown char: \n",
      "Word: ddn't\n",
      "Unknown char: \n",
      "Word: ths\n",
      "Unknown char: \"\n",
      "Word: mere\"politics\"to\n",
      "Unknown char: \"\n",
      "Word: mere\"politics\"to\n",
      "Unknown char: \n",
      "Word: servio\n",
      "Unknown char: \n",
      "Word: wre\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: grere\n",
      "Unknown char: \n",
      "Word: grere\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: murder=intentional\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \n",
      "Word: jmfra\n",
      "Unknown char: \n",
      "Word: jmfra\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: frstr\n",
      "Unknown char: \n",
      "Word: frstr\n",
      "Unknown char: \n",
      "Word: snt\n",
      "Unknown char: \n",
      "Word: hr\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: frlorare\n",
      "Unknown char: \n",
      "Word: vldigt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: mglich\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: islmico\n",
      "Unknown char: \n",
      "Word: so\n",
      "Unknown char: \n",
      "Word: esto\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \"\n",
      "Word: war.\"once\n",
      "Unknown char: \"\n",
      "Word: you\"re\n",
      "Unknown char: \"\n",
      "Word: in\"jean\n",
      "Unknown char: \n",
      "Word: barbrie\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: vietas\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: matarn\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: vendrn\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: pen.\"-stephane\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: =\n",
      "Word: is=not=a\n",
      "Unknown char: =\n",
      "Word: is=not=a\n",
      "Unknown char: =\n",
      "Word: western.......=not=here\n",
      "Unknown char: =\n",
      "Word: western.......=not=here\n",
      "Unknown char: \"\n",
      "Word: says..\"i\n",
      "Unknown char: \"\n",
      "Word: can\"t\n",
      "Unknown char: \"\n",
      "Word: breathe!\"...lol\n",
      "Unknown char: \n",
      "Word: lm\n",
      "Unknown char: \n",
      "Word: iekleri\n",
      "Unknown char: \n",
      "Word: iekte\n",
      "Unknown char: \n",
      "Word: klliyen\n",
      "Unknown char: \n",
      "Word: istiyorum\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: insanlarsnz\n",
      "Unknown char: \n",
      "Word: insanlarsnz\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: gnlerde\n",
      "Unknown char: \n",
      "Word: tkanan\n",
      "Unknown char: \n",
      "Word: cierlerimize\n",
      "Unknown char: \n",
      "Word: varsnz\n",
      "Unknown char: \n",
      "Word: varsnz\n",
      "Unknown char: \n",
      "Word: olmayn\n",
      "Unknown char: \n",
      "Word: bey...sayglar\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \\\n",
      "Word: her\\his\n",
      "Unknown char: =\n",
      "Word: us=the\n",
      "Unknown char: \n",
      "Word: matterssophisticated\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: =\n",
      "Word: uber=ugly\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: =\n",
      "Word: snp=ira\n",
      "Unknown char: \n",
      "Word: krdish\n",
      "Unknown char: \n",
      "Word: bibis\n",
      "Unknown char: \"\n",
      "Word: muslim\"...never\n",
      "Unknown char: \"\n",
      "Word: false-flag\"...lol\n",
      "Unknown char: \"\n",
      "Word: danger\".and\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: trocadro\n",
      "Unknown char: =\n",
      "Word: dss=grounds\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: atma\n",
      "Unknown char: \n",
      "Word: atma\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: muslims\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \"\n",
      "Word: the\"hijacking\"of\n",
      "Unknown char: \"\n",
      "Word: the\"hijacking\"of\n",
      "Unknown char: \"\n",
      "Word: insistent\"on\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: rsumer\n",
      "Unknown char: \n",
      "Word: aportacin\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: up?\"...\"find\n",
      "Unknown char: \"\n",
      "Word: up?\"...\"find\n",
      "Unknown char: \n",
      "Word: prophte\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thatso\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: mediaand\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: deportacin\n",
      "Unknown char: \n",
      "Word: prvu\n",
      "Unknown char: \"\n",
      "Word: rumors\"...should\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: =\n",
      "Word: crime=wall\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: oughtnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: polticos\n",
      "Unknown char: \n",
      "Word: seguirn\n",
      "Unknown char: \n",
      "Word: sbado\n",
      "Unknown char: \n",
      "Word: odos\n",
      "Unknown char: \n",
      "Word: aos\n",
      "Unknown char: \n",
      "Word: adems\n",
      "Unknown char: \n",
      "Word: das\n",
      "Unknown char: \"\n",
      "Word: his...er...\"attention\n",
      "Unknown char: \n",
      "Word: weve\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: larticle\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: thiswell\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: =\n",
      "Word: muslims=no\n",
      "Unknown char: =\n",
      "Word: blacks=non\n",
      "Unknown char: =\n",
      "Word: muslims=no\n",
      "Unknown char: =\n",
      "Word: blacks=non\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \"\n",
      "Word: his\"audience\"behind\n",
      "Unknown char: \"\n",
      "Word: his\"audience\"behind\n",
      "Unknown char: \"\n",
      "Word: and\"widescreen\"up\n",
      "Unknown char: \"\n",
      "Word: and\"widescreen\"up\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: yazyor\n",
      "Unknown char: \n",
      "Word: na\n",
      "Unknown char: \n",
      "Word: judasm\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: \n",
      "Word: julians\n",
      "Unknown char: \n",
      "Word: vctms\n",
      "Unknown char: \n",
      "Word: vctms\n",
      "Unknown char: \n",
      "Word: ther\n",
      "Unknown char: \n",
      "Word: famles\n",
      "Unknown char: \n",
      "Word: famles\n",
      "Unknown char: \n",
      "Word: wshes\n",
      "Unknown char: \n",
      "Word: ther\n",
      "Unknown char: \n",
      "Word: quck\n",
      "Unknown char: \n",
      "Word: prayng\n",
      "Unknown char: \n",
      "Word: solder\n",
      "Unknown char: \n",
      "Word: solder's\n",
      "Unknown char: \n",
      "Word: famly\n",
      "Unknown char: \n",
      "Word: chre\n",
      "Unknown char: \n",
      "Word: lcido\n",
      "Unknown char: \n",
      "Word: mritent\n",
      "Unknown char: \n",
      "Word: religionsdeserve\n",
      "Unknown char: \n",
      "Word: andour\n",
      "Unknown char: \"\n",
      "Word: disrespect\"...you\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: be\"fixed\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: tragdia\n",
      "Unknown char: \n",
      "Word: comearem\n",
      "Unknown char: \n",
      "Word: pases\n",
      "Unknown char: \n",
      "Word: wed\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: quran\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: quran\n",
      "Unknown char: \n",
      "Word: schn\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: auer\n",
      "Unknown char: \n",
      "Word: palstinenser\n",
      "Unknown char: \"\n",
      "Word: victim\".everywhere\n",
      "Unknown char: \n",
      "Word: naeve\n",
      "Unknown char: \n",
      "Word: fernndez\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: vous-tes\n",
      "Unknown char: \n",
      "Word: vergenza\n",
      "Unknown char: \n",
      "Word: comparacin\n",
      "Unknown char: \n",
      "Word: aportacin\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: psame\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: hacis\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: prires\n",
      "Unknown char: \n",
      "watd: feiten!\n",
      "Unknown char: \n",
      "Word: einsatzkrfte\n",
      "Unknown char: \"\n",
      "Word: paris\"pres.obama\n",
      "Unknown char: \"\n",
      "Word: off...\"i\n",
      "Unknown char: \n",
      "Word: estlles\n",
      "Unknown char: \n",
      "Word: sontdivischaque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: dreadheads=savages\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: chtiment\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: vritable\n",
      "Unknown char: \n",
      "Word: succs\n",
      "Unknown char: =\n",
      "Word: hispanicsasians=democrat\n",
      "Unknown char: =\n",
      "Word: christians=republicans\n",
      "Unknown char: =\n",
      "Word: christians=dems\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: gehrt\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "make: gun\n",
      "Unknown char: \n",
      "rewardsun\n",
      "Unknown char: \n",
      "puttingead\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: kardelerim\n",
      "Unknown char: \n",
      "Word: cantrk\n",
      "Unknown char: \n",
      "Word: kardein\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: lt;&lt;&lt;kardeimizin\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: aln\n",
      "Unknown char: \n",
      "Word: terr\n",
      "Unknown char: \n",
      "Word: amberin\n",
      "Unknown char: \n",
      "Word: yaatsin\n",
      "Unknown char: \n",
      "Word: zalim\n",
      "Unknown char: \n",
      "Word: dimi\n",
      "Unknown char: \n",
      "Word: irmainin\n",
      "Unknown char: \n",
      "Word: baindadir\n",
      "Unknown char: \n",
      "Word: gnahsizlar\n",
      "Unknown char: \n",
      "Word: amn\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: astrix\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: =\n",
      "Word: search=ferguson\n",
      "Unknown char: =\n",
      "Word: tip=lesson\n",
      "Unknown char: \n",
      "Word: israels\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: localizacin\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: ridculos\n",
      "Unknown char: =\n",
      "Word: photos=boasting\n",
      "Unknown char: =\n",
      "Word: situation=acceptable\n",
      "Unknown char: \n",
      "Word: enneige\n",
      "Unknown char: \n",
      "Word: dbris\n",
      "Unknown char: \n",
      "Word: cest\n",
      "Unknown char: \n",
      "Word: valle\n",
      "Unknown char: \n",
      "Word: hlico\n",
      "Unknown char: \n",
      "Word: dcollent\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: impresso\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: segurana\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: lamentvel\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: ameaas\n",
      "Unknown char: \n",
      "Word: islmicos\n",
      "Unknown char: \n",
      "Word: ameaas\n",
      "Unknown char: \n",
      "Word: tranqilas.tanto\n",
      "Unknown char: \n",
      "Word: sado\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: aps\n",
      "Unknown char: \n",
      "Word: lder\n",
      "Unknown char: \n",
      "Word: islmico\n",
      "Unknown char: \n",
      "Word: religies\n",
      "Unknown char: \n",
      "Word: redao\n",
      "Unknown char: \n",
      "Word: redao\n",
      "Unknown char: \n",
      "Word: horrio\n",
      "Unknown char: \n",
      "Word: reunio\n",
      "Unknown char: \n",
      "Word: vtimas\n",
      "Unknown char: \n",
      "Word: crashthoughts\n",
      "Unknown char: \"\n",
      "Word: radioshow\"=ain't\n",
      "Unknown char: =\n",
      "Word: radioshow\"=ain't\n",
      "Unknown char: =\n",
      "Word: account=prob\n",
      "Unknown char: \"\n",
      "Word: like\"riot\n",
      "Unknown char: \n",
      "Word: policires\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \"\n",
      "Word: recruits\"?first\n",
      "Unknown char: \"\n",
      "Word: terrorism\"&amp\n",
      "Unknown char: =\n",
      "Word: crime=police\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: represin\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: to?\"you\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: journe\n",
      "Unknown char: \n",
      "Word: smtliche\n",
      "Unknown char: \n",
      "Word: lt\n",
      "Unknown char: \n",
      "Word: lt\n",
      "Unknown char: \n",
      "Word: nmlich\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: luft\n",
      "Unknown char: \n",
      "Word: erzhlt\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: lngst\n",
      "Unknown char: \n",
      "Word: berfllig\n",
      "Unknown char: \n",
      "Word: unglcke\n",
      "Unknown char: \n",
      "Word: whod\n",
      "Unknown char: \n",
      "Word: procs\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \n",
      "Word: jrg\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: francs\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: juda\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: yesterdays\n",
      "Unknown char: \n",
      "Word: scholarmay\n",
      "Unknown char: =\n",
      "Word: words=action\n",
      "Unknown char: \n",
      "Word: mastersdo\n",
      "Unknown char: \n",
      "Word: card\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: inquitant\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \"\n",
      "Word: how\"...the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ee5c2c10c07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainCMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainCMModel\u001b[0;34m(sess, rdm_train, cm_train, t_rw, t_steps)\u001b[0m\n\u001b[1;32m    381\u001b[0m                      \u001b[0mrdm_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minit_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                      rdm_train.dropout_keep_prob: 1.0 }\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mt_ssq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdm_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# t_ssq = [batchsize, max_seq, scores]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mssq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_ssq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainCMModel(sess, rdm_train_graph, cm_train_graph, 0.3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
