{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 16:24:33.579910 139972835235648 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"SentiMain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0902 16:24:34.083377 139972835235648 logger.py:24] (300, 64, 101, 31, 2, 2)\n",
      "I0902 16:24:34.084753 139972835235648 logger.py:24] 2019-09-02 16:24:34 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n",
      "300 64 101 31 2 2\n",
      "2019-09-02 16:24:34 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors, word_len = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'], word_len['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "valid_reader = PTB_data_reader.DataReader(word_tensors['valid'], char_tensors['valid'], word_len['valid'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "test_reader = PTB_data_reader.DataReader(word_tensors['test'], char_tensors['test'], word_len['test'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "#load sentiment analysis data\n",
    "# sentiReader = dataloader.SentiDataLoader(\n",
    "#                                         dirpath = '/home/hadoop/trainingandtestdata',\n",
    "#                                         trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "#                                         testfile = 'testdata.manual.2009.06.14.csv', \n",
    "#                                         charVocab = char_vocab\n",
    "#                         )\n",
    "# # sentiReader.load_data()\n",
    "# sentiReader.load_data_fast(\n",
    "#                         '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "#                         '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "#                         '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "#                         '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "#                           )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_len.txt', 'wb') as handle:\n",
    "#     pickle.dump(x_len, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/hadoop/ERD/model.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import adict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "\n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\", reuse=tf.AUTO_REUSE):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCharNet:\n",
    "    def __init__(self, max_word_length, char_vocab_size, \n",
    "                        char_embed_size, embedding_dim):\n",
    "        self.max_word_length = max_word_length\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_embed_size = char_embed_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        with tf.variable_scope('Embedding', reuse=tf.AUTO_REUSE):\n",
    "            self.char_embedding = tf.get_variable('char_embedding', [self.char_vocab_size, self.char_embed_size])\n",
    "            ''' this op clears embedding vector of first symbol (symbol at position 0, which is by convention the position\n",
    "            of the padding symbol). It can be used to mimic Torch7 embedding operator that keeps padding mapped to\n",
    "            zero embedding vector and ignores gradient updates. For that do the following in TF:\n",
    "            1. after parameter initialization, apply this op to zero out padding embedding vector\n",
    "            2. after each gradient update, apply this op to keep padding at zero'''\n",
    "            self.clear_char_embedding_padding = tf.scatter_update(self.char_embedding, [0], tf.constant(0.0, shape=[1, self.char_embed_size]))\n",
    "            self.drop_out_prob_keep = tf.placeholder(tf.float32, name=\"lstm_char_net_dp\")\n",
    "            self.fw_cell = self.create_rnn_cell(self.embedding_dim)\n",
    "            self.bw_cell = self.create_rnn_cell(self.embedding_dim)\n",
    "\n",
    "    def __call__(self, input_words, x_len, fw_init, bw_init):\n",
    "        input_ = input_words\n",
    "        print(\"input_:\", input_)\n",
    "        with tf.variable_scope('Embedding', reuse=tf.AUTO_REUSE):\n",
    "            input_embedded = tf.nn.embedding_lookup(self.char_embedding, input_)\n",
    "            print(\"input_embedded1:\", input_embedded)\n",
    "            input_embedded = tf.reshape(input_embedded, [-1, self.max_word_length, self.char_embed_size])\n",
    "            print(\"input_embedded2:\", input_embedded)\n",
    "            (fw_outs, bw_outs), (fw_final, bw_final) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        self.fw_cell,\n",
    "                                        self.bw_cell,\n",
    "                                        input_embedded,\n",
    "                                        sequence_length=x_len,\n",
    "                                        initial_state_fw=fw_init,\n",
    "                                        initial_state_bw=bw_init,\n",
    "                                        dtype=None,\n",
    "                                        parallel_iterations=None,\n",
    "                                        swap_memory=False,\n",
    "                                        time_major=False,\n",
    "                                        scope=None\n",
    "                                    )\n",
    "        return fw_outs, bw_outs, fw_final, bw_final\n",
    "\n",
    "    def create_rnn_cell(self, rnn_size):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.drop_out_prob_keep)\n",
    "        return cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM:\n",
    "    def __init__(self, batch_size, num_unroll_steps, rnn_size, num_rnn_layers, word_vocab_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unroll_steps = num_unroll_steps\n",
    "        self.rnn_size = rnn_size\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "            self.drop_out = tf.placeholder(tf.float32, name=\"Dropout\")\n",
    "            def create_rnn_cell():\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.drop_out)\n",
    "                return cell\n",
    "            if self.num_rnn_layers > 1:\n",
    "                self.cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
    "            else:\n",
    "                self.cell = create_rnn_cell()\n",
    "            self.initial_rnn_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            \n",
    "    def __call__(self, input_cnn):\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "            input_cnn2 = [tf.squeeze(x, [1]) for x in tf.split(input_cnn, self.num_unroll_steps, 1)]\n",
    "            outputs, final_rnn_state = tf.contrib.rnn.static_rnn(self.cell, input_cnn2,\n",
    "                                             initial_state=self.initial_rnn_state, dtype=tf.float32)     \n",
    "            return outputs, final_rnn_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_train_model(char_rnn_net, LM, \n",
    "                      batch_size, \n",
    "                      num_unroll_steps, \n",
    "                      max_word_length, \n",
    "                      learning_rate,\n",
    "                      max_grad_norm, \n",
    "                     ):\n",
    "    input_ = tf.placeholder(tf.int32, shape=[batch_size, num_unroll_steps, max_word_length], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int64, [batch_size, num_unroll_steps], name='targets')\n",
    "    input_len = tf.placeholder(tf.int32, shape = [batch_size, num_unroll_steps]) \n",
    "    \n",
    "    # input_cnn = word2vec(input_) #[batch_size*num_unroll_steps, k_features]\n",
    "    rnn_len = tf.reshape(input_len, shape = [batch_size*num_unroll_steps])\n",
    "    fw_init = char_rnn_net.fw_cell.zero_state(batch_size*num_unroll_steps, dtype=tf.float32)\n",
    "    bw_init = char_rnn_net.bw_cell.zero_state(batch_size*num_unroll_steps, dtype=tf.float32)\n",
    "    print(\"bw_init:\", bw_init)\n",
    "    fw_outs, bw_outs, fw_final, bw_final = char_rnn_net(input_, rnn_len, fw_init, bw_init)\n",
    "    print(\"fw_out:\", fw_outs)\n",
    "    fw_final = fw_final[-1]\n",
    "    bw_final = bw_final[-1]\n",
    "    \n",
    "#     # add to final tensor\n",
    "#     out_merge = fw_final + bw_final\n",
    "    \n",
    "    # concat two tensor\n",
    "    out_merge = tf.concat([fw_final, bw_final], axis = -1)\n",
    "    \n",
    "#     # max_pooling two tensor\n",
    "#     out_merge = tf.reduce_max(\n",
    "#         tf.transpose(\n",
    "#             tf.identity([fw_final, bw_final]),\n",
    "#             [1, 0, 2]\n",
    "#         ),\n",
    "#         axis = 1\n",
    "#     )\n",
    "    \n",
    "    input_cnn = tf.reshape(out_merge, [batch_size, num_unroll_steps, -1])\n",
    "    outputs, final_rnn_state = LM(input_cnn)\n",
    "    \n",
    "    # linear projection onto output (word) vocab\n",
    "    logits = []\n",
    "    with tf.variable_scope('WordEmbedding') as scope:\n",
    "        for idx, output in enumerate(outputs):\n",
    "            if idx > 0:\n",
    "                scope.reuse_variables()\n",
    "            logits.append(linear(output, LM.word_vocab_size))\n",
    "\n",
    "    word_embedding = tf.identity(outputs, \"lstm_word_embedding\")\n",
    "    sent_embedding = tf.identity(final_rnn_state[-1][-1], \"lstm_sent_embedding\")\n",
    "\n",
    "    with tf.variable_scope('Loss', reuse=tf.AUTO_REUSE):\n",
    "            target_list = [tf.squeeze(x, [1]) for x in tf.split(targets, num_unroll_steps, 1)]\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = target_list), name='loss')\n",
    "            \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    with tf.variable_scope('SGD_Training'):\n",
    "        # SGD learning parameter\n",
    "        learning_rate = tf.Variable(learning_rate, trainable=False, name='learning_rate')\n",
    "        # collect all trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, global_norm = tf.clip_by_global_norm(tf.gradients(loss, tvars), max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "        \n",
    "    return adict(\n",
    "        input = input_,\n",
    "        input_len = input_len, \n",
    "        drop_out_char_lstm = char_rnn_net.drop_out_prob_keep,\n",
    "        drop_out_lm = LM.drop_out,\n",
    "        fw_init = fw_init,\n",
    "        bw_init = bw_init,\n",
    "        initial_rnn_state=LM.initial_rnn_state,\n",
    "        final_rnn_state=final_rnn_state,\n",
    "        rnn_outputs=outputs,\n",
    "        logits = logits,\n",
    "        targets=targets,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        global_step=global_step,\n",
    "        global_norm=global_norm,\n",
    "        train_op=train_op\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(session, train_model, valid_reader, summary_writer, fw_init, bw_init, rnn_state, epoch):\n",
    "    sum_loss = 0\n",
    "    for x, y, x_len in valid_reader.iter():\n",
    "        start_time = time.time()\n",
    "        loss, rnn_state = session.run([\n",
    "            train_model.loss,\n",
    "            train_model.final_rnn_state\n",
    "        ], {\n",
    "            train_model.input: x,\n",
    "            train_model.targets: y,\n",
    "            train_model.input_len: x_len, \n",
    "            train_model.drop_out_char_lstm: 1.0, \n",
    "            train_model.drop_out_lm: 1.0,\n",
    "            train_model.fw_init: fw_init,\n",
    "            train_model.bw_init: bw_init,\n",
    "            train_model.initial_rnn_state: rnn_state\n",
    "        })\n",
    "        sum_loss += loss\n",
    "    \n",
    "    summary = tf.Summary(value=[\n",
    "        tf.Summary.Value(tag=\"step_valid_loss\", simple_value=sum_loss),\n",
    "        tf.Summary.Value(tag=\"step_valid_perplexity\", simple_value=np.exp(sum_loss)),\n",
    "    ])\n",
    "    summary_writer.add_summary(summary, epoch)\n",
    "    print(\"Valid loss:\", sum_loss, \", | Valid perplexity:\", np.exp(sum_loss))\n",
    "\n",
    "def Test(session, train_model, test_reader, summary_writer, fw_init, bw_init, rnn_state):\n",
    "    sum_loss = 0\n",
    "    for x, y, x_len in test_reader.iter():\n",
    "        start_time = time.time()\n",
    "        loss, rnn_state = session.run([\n",
    "            train_model.loss,\n",
    "            train_model.final_rnn_state\n",
    "        ], {\n",
    "            train_model.input: x,\n",
    "            train_model.targets: y,\n",
    "            train_model.input_len: x_len, \n",
    "            train_model.drop_out_char_lstm: 1.0, \n",
    "            train_model.drop_out_lm: 1.0,\n",
    "            train_model.fw_init: fw_init,\n",
    "            train_model.bw_init: bw_init,\n",
    "            train_model.initial_rnn_state: rnn_state\n",
    "        })\n",
    "        sum_loss += loss\n",
    "    \n",
    "    summary = tf.Summary(value=[\n",
    "        tf.Summary.Value(tag=\"step_test_loss\", simple_value=sum_loss),\n",
    "        tf.Summary.Value(tag=\"step_test_perplexity\", simple_value=np.exp(sum_loss)),\n",
    "    ])\n",
    "    summary_writer.add_summary(summary, 0)\n",
    "    print(\"Test loss:\", sum_loss, \", | Test perplexity:\", np.exp(sum_loss))\n",
    "    \n",
    "def Train_Char_Model(session, train_model, train_reader, valid_reader, test_reader,  saver, summary_writer):\n",
    "    best_valid_loss = None\n",
    "    rnn_state = session.run(train_model.initial_rnn_state)\n",
    "    fw_init =  session.run(train_model.fw_init)\n",
    "    bw_init = session.run(train_model.bw_init)\n",
    "    for epoch in range(FLAGS.max_epochs):\n",
    "    # for epoch in range(1):\n",
    "        epoch_start_time = time.time()\n",
    "        avg_train_loss = 0.0\n",
    "        count = 0\n",
    "        for x, y, x_len in train_reader.iter():\n",
    "            count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            loss, _, rnn_state, gradient_norm, step = session.run([\n",
    "                train_model.loss,\n",
    "                train_model.train_op,\n",
    "                train_model.final_rnn_state,\n",
    "                train_model.global_norm,\n",
    "                train_model.global_step\n",
    "            ], {\n",
    "                train_model.input: x,\n",
    "                train_model.targets: y,\n",
    "                train_model.input_len: x_len, \n",
    "                train_model.drop_out_char_lstm:0.8, \n",
    "                train_model.drop_out_lm: 0.8,\n",
    "                train_model.fw_init: fw_init,\n",
    "                train_model.bw_init: bw_init,\n",
    "                train_model.initial_rnn_state: rnn_state\n",
    "            })\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_perplexity\", simple_value=np.exp(loss)),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "            avg_train_loss += 0.05 * (loss - avg_train_loss)\n",
    "\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            if count % FLAGS.print_every == 0:\n",
    "                print('%6d: %d [%5d/%5d], train_loss/perplexity = %6.8f/%6.7f secs/batch = %.4fs, grad.norm=%6.8f' % (step,\n",
    "                                                        epoch, count,\n",
    "                                                        train_reader.length,\n",
    "                                                        loss, np.exp(loss),\n",
    "                                                        time_elapsed,\n",
    "                                                        gradient_norm))\n",
    "        Validation(session, train_model, valid_reader, summary_writer, fw_init, bw_init, rnn_state, epoch)\n",
    "        print('Epoch training time:', time.time()-epoch_start_time)\n",
    "        save_as = '%s/epoch%03d_%.4f.model' % (FLAGS.train_dir, epoch, avg_train_loss)\n",
    "        saver.save(session, save_as)\n",
    "        print('Saved char model', save_as)\n",
    "    Test(session, train_model, test_reader, summary_writer, fw_init, bw_init, rnn_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_init: LSTMStateTuple(c=<tf.Tensor 'DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros:0' shape=(620, 300) dtype=float32>, h=<tf.Tensor 'DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros_1:0' shape=(620, 300) dtype=float32>)\n",
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32, device=/device:CPU:0)\n",
      "input_embedded1: Tensor(\"Embedding_1/embedding_lookup/Identity:0\", shape=(20, 31, 21, 15), dtype=float32, device=/device:CPU:0)\n",
      "input_embedded2: Tensor(\"Embedding_1/Reshape:0\", shape=(620, 21, 15), dtype=float32, device=/device:CPU:0)\n",
      "fw_out: Tensor(\"Embedding_1/bidirectional_rnn/fw/fw/transpose_1:0\", shape=(620, 21, 300), dtype=float32, device=/device:CPU:0)\n",
      "Valid loss: 1085.6619367599487 , | Valid perplexity: inf\n",
      "Epoch training time: 17.682217121124268\n",
      "Test loss: 1214.4328870773315 , | Test perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "# reuse model to train senti model\n",
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# device = '/GPU:0'\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g, config=gpu_config) as sess:\n",
    "        with tf.device('/GPU:0'):\n",
    "            w2v = LSTMCharNet(\n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            char_vocab_size = char_vocab.size, \n",
    "                            char_embed_size = FLAGS.char_embed_size,\n",
    "                            embedding_dim = FLAGS.embedding_dim\n",
    "                        )\n",
    "            lstm_lm = LSTM_LM(\n",
    "                        batch_size = FLAGS.batch_size, \n",
    "                        num_unroll_steps = FLAGS.max_sent_len, \n",
    "                        rnn_size = FLAGS.embedding_dim, \n",
    "                        num_rnn_layers = FLAGS.rnn_layers, \n",
    "                        word_vocab_size = word_vocab.size\n",
    "                    )\n",
    "\n",
    "            char_train_graph = infer_train_model(\n",
    "                                w2v, lstm_lm, \n",
    "                                batch_size = FLAGS.batch_size, \n",
    "                                num_unroll_steps = FLAGS.max_sent_len, \n",
    "                                max_word_length = FLAGS.max_char_num, \n",
    "                                learning_rate = FLAGS.learning_rate,\n",
    "                                max_grad_norm = FLAGS.max_grad_norm\n",
    "                             )\n",
    "            val_list1 = tf.global_variables()\n",
    "            saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "            sess.run(tf.variables_initializer(val_list1))\n",
    "            summary_writer = tf.summary.FileWriter(\"lstm_word_LM/\", graph=sess.graph)\n",
    "        Train_Char_Model(sess, char_train_graph, train_reader, valid_reader, test_reader, saver, summary_writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
