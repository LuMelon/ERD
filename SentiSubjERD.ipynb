{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import MyLogger\n",
    "import time\n",
    "import SubjObjLoader\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from collections import deque\n",
    "# import dataloader\n",
    "from BertRDMLoader import *\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import tsentiLoader\n",
    "import SubjObjLoader\n",
    "import pickle\n",
    "import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert(len(sys.argv)==2)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "#         self.PoolLayer = pooling_layer(word_embedding_dim, sent_embedding_dim) \n",
    "        \n",
    "    def forward(self, x_emb): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "        init_states = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        try:\n",
    "            df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        except:\n",
    "            print(\"Error:\", pool_feature.shape, init_states.shape)\n",
    "            raise\n",
    "        # hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        # final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        # return hidden_outs, final_outs\n",
    "        return df_outputs\n",
    "\n",
    "\n",
    "# ##### CM 模型\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            rl_input, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        print(\"rl_output:\", rl_output,shape)\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state\n",
    "\n",
    "\n",
    "# #### 工具函数\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def Sent2Vec_Generater(tokenizer, bert, cuda):\n",
    "    def fn(sentence):\n",
    "        input_ids = tokenizer.encode(\n",
    "                            sentence,\n",
    "                            add_special_tokens=True\n",
    "                        )\n",
    "        input_ids = torch.tensor([input_ids]).cuda() if cuda else torch.tensor([input_ids])\n",
    "        outs = bert(input_ids)\n",
    "        sentence_emb = outs[1].reshape([1, 1,-1])\n",
    "        return sentence_emb\n",
    "    return fn\n",
    "\n",
    "\n",
    "\n",
    "def Count_Accs(ylabel, preds):\n",
    "    correct_preds = np.array(\n",
    "        [1 if y1==y2 else 0 \n",
    "        for (y1, y2) in zip(ylabel, preds)]\n",
    "    )\n",
    "    y_idxs = [idx if yl >0 else idx - len(ylabel) \n",
    "            for (idx, yl) in enumerate(ylabel)]\n",
    "    pos_idxs = list(filter(lambda x: x >= 0, y_idxs))\n",
    "    neg_idxs = list(filter(lambda x: x < 0, y_idxs))\n",
    "    acc = sum(correct_preds) / (1.0 * len(ylabel))\n",
    "    if len(pos_idxs) > 0:\n",
    "        pos_acc = sum(correct_preds[pos_idxs])/(1.0*len(pos_idxs))\n",
    "    else:\n",
    "        pos_acc = 0\n",
    "    if len(neg_idxs) > 0:\n",
    "        neg_acc = sum(correct_preds[neg_idxs])/(1.0*len(neg_idxs))\n",
    "    else:\n",
    "        neg_acc = 0\n",
    "    return acc, pos_acc, neg_acc, y_idxs, pos_idxs, neg_idxs, correct_preds\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "\n",
    "def rdm_data2bert_tensors(data_X, cuda):\n",
    "    def padding_sent_list(sent_list):\n",
    "        sent_len = [len(sent) for sent in sent_list]\n",
    "        max_sent_len = max(sent_len)\n",
    "        sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "        attn_mask = torch.ones_like(sent_padding)\n",
    "        for i, sent in enumerate(sent_list):\n",
    "            sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "            attn_mask[i][len(sent):].fill_(0)\n",
    "        return sent_padding, attn_mask\n",
    "    sent_list = []\n",
    "    [sent_list.extend(seq) for seq in data_X]\n",
    "    seq_len = [len(seq) for seq in data_X]\n",
    "    sent_tensors, attn_mask = padding_sent_list(sent_list)\n",
    "    if cuda:\n",
    "        sent_tensors = sent_tensors.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_tensors, attn_mask, seq_len\n",
    "\n",
    "def sent_list2bert_tensors(sent_list, cuda):\n",
    "    sent_len = [len(sent) for sent in sent_list]\n",
    "    max_sent_len = max(sent_len)\n",
    "    sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "    attn_mask = torch.ones_like(sent_padding)\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "        attn_mask[i][len(sent):].fill_(0)\n",
    "    if cuda:\n",
    "        sent_padding = sent_padding.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_padding, attn_mask\n",
    "\n",
    "\n",
    "# #### 训练RDM\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def TrainRDMModel(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, t_steps, stage=0, new_data_len=[], logger=None, \n",
    "                        log_dir=\"RDMBertTrain\", cuda=True):\n",
    "    batch_size = 20 \n",
    "    max_gpu_batch = 5 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    splits = int(batch_size/max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    t_acc = 0.9\n",
    "    ret_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    optim = torch.optim.Adagrad([\n",
    "                                {'params': bert.parameters(), 'lr':5e-5},\n",
    "                                {'params': rdm_classifier.parameters(), 'lr': 5e-3},\n",
    "                                {'params': rdm_model.parameters(), 'lr': 5e-3}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(log_dir, filename_suffix=\"_ERD_CM_stage_%3d\"%stage)\n",
    "    acc_l = np.zeros(splits)\n",
    "    loss_l = np.zeros(splits)\n",
    "    for step in range(t_steps):\n",
    "        optim.zero_grad()\n",
    "        try:\n",
    "            for j in range(splits):\n",
    "                if len(new_data_len) > 0:\n",
    "                    x, x_len, y = get_df_batch(step*batch_size+j*max_gpu_batch, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "                else:\n",
    "                    x, x_len, y = get_df_batch(step*batch_size+j*max_gpu_batch, max_gpu_batch, tokenizer=tokenizer) \n",
    "                sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "                bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "                pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "                data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "                rdm_hiddens = rdm_model(data_tensors)\n",
    "                batchsize, _, _ = rdm_hiddens.shape\n",
    "                rdm_outs = torch.cat(\n",
    "                    [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "                    # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                ).reshape(\n",
    "                    [-1, rdm_model.hidden_dim]\n",
    "                )\n",
    "\n",
    "                rdm_scores = rdm_classifier(\n",
    "                    rdm_outs\n",
    "                )\n",
    "                rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "                acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "                loss = loss_fn(rdm_scores, y_label)\n",
    "                loss.backward()\n",
    "                torch.cuda.empty_cache()\n",
    "                loss_l[j] = loss\n",
    "#                 print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "        except RuntimeError as exception:\n",
    "            if \"out of memory\" in str(exception):\n",
    "                print(\"WARNING: out of memory\")\n",
    "                print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "#                     time.sleep(5)\n",
    "                raise exception\n",
    "            else:   \n",
    "                raise exception\n",
    "\n",
    "        optim.step()        \n",
    "        writer.add_scalar('Train Loss', loss_l.mean(), step)\n",
    "        writer.add_scalar('Train Accuracy', acc_l.mean(), step)\n",
    "\n",
    "        sum_loss += loss_l.mean()\n",
    "        sum_acc += acc_l.mean()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if step % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            print('%3d | %d , train_loss/accuracy = %6.8f/%6.7f'             % (step, t_steps, \n",
    "                sum_loss, sum_acc,\n",
    "                ))\n",
    "#                 rdm_model, bert, sentiModel, rdm_classifier\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc\n",
    "\n",
    "\n",
    "# #### 训练CM\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "def TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, stage, t_rw, t_steps, log_dir, logger, FLAGS, cuda=False):\n",
    "    batch_size = 20\n",
    "    t_acc = 0.9\n",
    "    ids = np.array(range(batch_size), dtype=np.int32)\n",
    "    seq_states = np.zeros([batch_size], dtype=np.int32)\n",
    "    isStop = np.zeros([batch_size], dtype=np.int32)\n",
    "    max_id = batch_size\n",
    "    df_init_states = torch.zeros([1, batch_size, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    writer = SummaryWriter(log_dir, filename_suffix=\"_ERD_CM_stage_%3d\"%stage)\n",
    "    state = df_init_states\n",
    "    D = deque()\n",
    "    ssq = []\n",
    "    print(\"in RL the begining\")\n",
    "    rdm_optim = torch.optim.Adagrad([\n",
    "                            {'params': bert.parameters(), 'lr':1e-6},\n",
    "                            {'params': rdm_classifier.parameters(), 'lr': 5e-5},\n",
    "                            {'params': rdm_model.parameters(), 'lr': 5e-5}\n",
    "                         ],\n",
    "                            weight_decay = 0.2\n",
    "    )\n",
    "    rl_optim = torch.optim.Adam([{'params':cm_model.parameters(), 'lr':1e-5}])\n",
    "    # get_new_len(sess, mm)\n",
    "    data_ID = get_data_ID()\n",
    "\n",
    "    if len(data_ID) % batch_size == 0: # the total number of events\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size)\n",
    "    else:\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    \n",
    "    cached_ssq = \"./%s/cached_ssq.pkl\"%log_dir\n",
    "    if os.path.exists(catched_ssq):\n",
    "    #load the cached ssq\n",
    "        with open(cached_ssq, 'rb') as handle:\n",
    "            ssq = pickle.load(handle)    \n",
    "    else:\n",
    "        for i in range(flags):\n",
    "            with torch.no_grad():\n",
    "                x, x_len, y = get_df_batch(i, batch_size, tokenizer=tokenizer)\n",
    "                sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "                bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "                pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "                x_emb = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "                batchsize, max_seq_len, max_sent_len, emb_dim = x_emb.shape\n",
    "                rdm_hiddens = rdm_model(x_emb)\n",
    "                batchsize, _, _ = rdm_hiddens.shape\n",
    "                rdm_outs = torch.cat(\n",
    "                    [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "                    # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                ).reshape(\n",
    "                    [-1, rdm_model.hidden_dim]\n",
    "                )\n",
    "                print(\"batch %d\"%i)\n",
    "                if len(ssq) > 0:\n",
    "                    ssq.extend([rdm_classifier(h) for h in rdm_hiddens])\n",
    "                else:\n",
    "                    ssq = [rdm_classifier(h) for h in rdm_hiddens]\n",
    "        # cache ssq for development\n",
    "        with open(cached_ssq, 'wb') as handle:\n",
    "            pickle.dump(ssq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    print(get_curtime() + \" Now Start RL training ...\")\n",
    "\n",
    "    counter = 0\n",
    "    sum_rw = 0.0 # sum of rewards\n",
    "    data_len = get_data_len()\n",
    "\n",
    "    while True:\n",
    "        if counter > FLAGS.OBSERVE:\n",
    "            sum_rw += rw.mean()\n",
    "            if counter % 200 == 0:\n",
    "                sum_rw = float(sum_rw) / 200\n",
    "                print( get_curtime() + \" Step: \" + str(counter) \n",
    "                       + \" REWARD IS \" + str(sum_rw) \n",
    "                     )\n",
    "                if sum_rw > t_rw:\n",
    "                    print(\"Retch The Target Reward\")\n",
    "                    break\n",
    "                if counter > t_steps:\n",
    "                    print(\"Retch The Target Steps\")\n",
    "                    break\n",
    "                sum_rw = 0.0\n",
    "            sent_tensors, attn_mask = sent_list2bert_tensors(input_x, cuda)\n",
    "            _, x_emb = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            x_emb = x_emb.unsqueeze(-2)\n",
    "            stopScore, isStop, rl_new_state = cm_model(rdm_model, x_emb, s_state)\n",
    "            out_action = (stopScore*s_isStop).sum(axis=1)\n",
    "            rl_cost = torch.mean((s_rw - out_action)*(s_rw - out_action))\n",
    "            rl_cost.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            rl_optim.step()\n",
    "    #             rdm_optim.step() #后期要尝试一下，是否要同时训练整个模型\n",
    "            writer.add_scalar('RL Cost', rl_cost, counter - FLAGS.OBSERVE)\n",
    "            writer.add_scalar('RL Reward', rw.mean(), counter - FLAGS.OBSERVE)\n",
    "\n",
    "        input_x, input_y, ids, seq_states, max_id = get_rl_batch(ids, seq_states, isStop, max_id, 0, FLAGS, tokenizer=tokenizer)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sent_tensors, attn_mask = sent_list2bert_tensors(input_x, cuda)\n",
    "            _, x_emb = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            x_emb = x_emb.unsqueeze(-2)\n",
    "            batchsize, max_sent_len, emb_dim = x_emb.shape\n",
    "            mss, isStop, mNewState = cm_model(rdm_model, x_emb, state)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if random.random() < FLAGS.random_rate:\n",
    "                isStop[j] = int(torch.rand(2).argmax())\n",
    "            if seq_states[j] == data_len[ids[j]]:\n",
    "                isStop[j] = 1\n",
    "\n",
    "        # eval\n",
    "        rw = get_reward(isStop, mss, ssq, ids, seq_states)\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            D.append((state[0][j], input_x[j], isStop[j], rw[j]))\n",
    "            if len(D) > FLAGS.max_memory:\n",
    "                D.popleft()\n",
    "\n",
    "        state = mNewState\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if isStop[j] == 1:\n",
    "                state[0][j].fill_(0)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTLTrainRDMModel(rdm_model, bert, rdm_classifier,\n",
    "                     transformer, task_embedding, senti_classifier, subj_classifier, \n",
    "                     sentiReader, subjReader, \n",
    "                    tokenizer, t_steps, new_data_len=[], logger=None, cuda=False, \n",
    "                        log_dir=\"RDMBertTrain\"):\n",
    "    batch_size = 20 \n",
    "    max_gpu_batch = 5 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    sentiReader.reset_batchsize(max_gpu_batch)\n",
    "    subjReader.reset_batchsize(max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = np.zeros(4)\n",
    "    sum_acc = np.zeros(3)\n",
    "    t_acc = 0.9\n",
    "    ret_acc = 0.0\n",
    "    senti_task_id = torch.tensor([0]) if not cuda else torch.tensor([0]).cuda()\n",
    "    subj_task_id = torch.tensor([1]) if not cuda else torch.tensor([1]).cuda()\n",
    "\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    senti_weights = torch.tensor(\n",
    "            WeightsForUmbalanced(\n",
    "                sentiReader.label\n",
    "            ),\n",
    "            dtype=torch.float32\n",
    "    )\n",
    "    senti_loss_fn = nn.CrossEntropyLoss(weight=senti_weights.cuda()) if cuda else nn.CrossEntropyLoss(weight=senti_weights)\n",
    "\n",
    "    subj_weights = torch.tensor(\n",
    "            WeightsForUmbalanced(\n",
    "                subjReader.label\n",
    "            ),\n",
    "            dtype = torch.float32\n",
    "    )\n",
    "    subj_loss_fn = nn.CrossEntropyLoss(weight=subj_weights) if not cuda else nn.CrossEntropyLoss(weight=subj_weights.cuda())\n",
    "\n",
    "    loss_weight = torch.tensor([0.8, 0.1, 0.1]) if not cuda else torch.tensor([0.8, 0.1, 0.1]).cuda()   \n",
    "    optim = torch.optim.Adagrad([\n",
    "                                {'params': bert.parameters(), 'lr':5e-5},\n",
    "                                {'params': rdm_classifier.parameters(), 'lr': 5e-5},\n",
    "                                {'params': rdm_model.parameters(), 'lr': 5e-5},\n",
    "                                {'params': task_embedding.parameters(), 'lr':1e-6},\n",
    "                                {'params': transformer.parameters(), 'lr': 1e-6},\n",
    "                                {'params': senti_classifier.parameters(), 'lr': 1e-6},\n",
    "                                {'params': subj_classifier.parameters(), 'lr':1e-6}\n",
    "                             ]\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    acc_tmp = np.zeros([3, int(batch_size/max_gpu_batch)])\n",
    "    loss_tmp = np.zeros([4, int(batch_size/max_gpu_batch)])\n",
    "\n",
    "\n",
    "    for step in range(t_steps):\n",
    "        optim.zero_grad()\n",
    "        for j in range(int(batch_size/max_gpu_batch)):\n",
    "            if len(new_data_len) > 0:\n",
    "                x, x_len, y = get_df_batch(step*batch_size+j*max_gpu_batch, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "            else:\n",
    "                x, x_len, y = get_df_batch(step*batch_size+j*max_gpu_batch, max_gpu_batch, tokenizer=tokenizer) \n",
    "            #--------RDM loss----------------------------------\n",
    "            sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "            bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "            data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "            rdm_hiddens = rdm_model(data_tensors)\n",
    "            batchsize, _, _ = rdm_hiddens.shape\n",
    "            rdm_outs = torch.cat(\n",
    "                [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "                # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "            ).reshape(\n",
    "                [-1, rdm_model.hidden_dim]\n",
    "            )\n",
    "\n",
    "            rdm_scores = rdm_classifier(\n",
    "                rdm_outs\n",
    "            )\n",
    "            rdm_preds = rdm_scores.argmax(axis=1)\n",
    "            y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "            acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "            loss = loss_fn(rdm_scores, y_label)\n",
    "            loss_back = loss.mean()*loss_weight[0]\n",
    "            loss_back.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "            #-----------------------------------------------------\n",
    "\n",
    "            # ----------------sentiment analysis------------------------\n",
    "            xst, yst, lst = sentiReader.sample()\n",
    "            sent_tensors, sent_mask = senti_data2bert_tensors(xst, cuda)\n",
    "            xst_embs, _ = bert(sent_tensors, attention_mask = sent_mask)\n",
    "\n",
    "            tensors = xst_embs + task_embedding(senti_task_id)\n",
    "            senti_feature = transformer(tensors.transpose(0, 1)).transpose(0, 1)\n",
    "            cls_feature = senti_feature.max(axis=1)[0]\n",
    "            senti_scores = senti_cls(cls_feature)\n",
    "            y_label = torch.tensor(yst.argmax(axis=1)).cuda() if cuda else torch.tensor(yst.argmax(axis=1))\n",
    "            st_loss = senti_loss_fn(senti_scores, y_label)\n",
    "            st_acc, _, _, _, _, _, _ = Count_Accs(y_label, senti_scores.argmax(axis=1))\n",
    "            st_loss_back = st_loss.mean()*loss_weight[1]\n",
    "            st_loss_back.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "            #-----------------------------------------------------------\n",
    "\n",
    "            # ----------------subjective analysis------------------------\n",
    "            xsj, ysj, lsj = subjReader.sample()\n",
    "            sent_tensors, sent_mask = senti_data2bert_tensors(xsj, cuda)\n",
    "            xsj_embs, _ = bert(sent_tensors, attention_mask = sent_mask)\n",
    "            tensors = xsj_embs + task_embedding(subj_task_id)\n",
    "            subj_feature = transformer(tensors.transpose(0, 1)).transpose(0, 1)\n",
    "            cls_feature = subj_feature.max(axis=1)[0]\n",
    "            subj_scores = subj_cls(cls_feature)\n",
    "            y_label = torch.tensor(ysj.argmax(axis=1)).cuda() if cuda else torch.tensor(ysj.argmax(axis=1))\n",
    "            sj_loss = subj_loss_fn(subj_scores, y_label)\n",
    "            sj_acc, _, _, _, _, _, _ = Count_Accs(y_label, subj_scores.argmax(axis=1))\n",
    "            sj_loss_back = sj_loss.mean()*loss_weight[2]\n",
    "            sj_loss_back.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "            #-----------------------------------------------------------\n",
    "\n",
    "            loss_tmp[:, j] = np.array([loss*loss_weight[0]+st_loss*loss_weight[1]+sj_loss*loss_weight[2], loss, st_loss, sj_loss])\n",
    "            acc_tmp[:, j] = np.array([acc, st_acc, sj_acc])\n",
    "\n",
    "        optim.step()        \n",
    "        writer.add_scalar('Train Loss', loss_tmp[0].mean(), step)\n",
    "        writer.add_scalar('Train Accuracy', acc_tmp[0].mean(), step)\n",
    "\n",
    "        sum_acc += acc_tmp.mean(axis=1)\n",
    "        sum_loss += loss_tmp.mean(axis=1)\n",
    "\n",
    "        print(\"%6d %6d|MTL_Loss:%6.8f, rdm_loss/rdm_acc = %6.8f/%6.7f | senti_loss/senti_acc = %6.8f/%6.7f | subj_loss/subj_acc = %6.8f/%6.7f \" % (\n",
    "                                                                                                step, t_steps, loss_tmp[0].mean(),        \n",
    "                                                                                            loss_tmp[1].mean(), acc_tmp[0].mean(),\n",
    "                                                                                            loss_tmp[2].mean(), acc_tmp[1].mean(),\n",
    "                                                                                            loss_tmp[3].mean(), acc_tmp[2].mean()\n",
    "            )\n",
    "            )\n",
    "\n",
    "        if step % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            print(\"MTL_Loss:%6.8f, rdm_loss/rdm_acc = %6.8f/%6.7f | senti_loss/senti_acc = %6.8f/%6.7f | subj_loss/subj_acc = %6.8f/%6.7f \" % (\n",
    "                                                                                            sum_loss[0],        \n",
    "                                                                                            sum_loss[1], sum_acc[0],\n",
    "                                                                                            sum_loss[2], sum_acc[1],\n",
    "                                                                                            sum_loss[3], sum_acc[2]\n",
    "            )\n",
    "            )\n",
    "    #                 rdm_model, bert, sentiModel, rdm_classifier\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)\n",
    "\n",
    "load_data_fast()\n",
    "\n",
    "\n",
    "# #### 创建模型\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bertModel/\")\n",
    "bert = BertModel.from_pretrained(\"./bertModel/\").cuda()\n",
    "task_embedding = nn.Embedding(3, 768)\n",
    "encoder_layer = nn.TransformerEncoderLayer(768, 8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, 1)\n",
    "subj_cls = nn.Linear(768, 2)\n",
    "\n",
    "transformer = transformer_encoder.cuda()\n",
    "task_embedding = task_embedding.cuda()\n",
    "subj_cls = subj_cls.cuda()\n",
    "senti_cls = nn.Linear(768, 2).cuda()\n",
    "\n",
    "cm_model = CM_Model(300, 256, 2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()\n",
    "cm_log_dir=\"SentiSubj_CMBertTrain\"\n",
    "\n",
    "\n",
    "# #### 导入模型预训练参数\n",
    "joint_save_as = './MTLTrain/jointModel_epoch015.pkl'\n",
    "checkpoint = torch.load(joint_save_as)\n",
    "senti_cls.load_state_dict(checkpoint['senti_classifier'])\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "transformer.load_state_dict(checkpoint['transformer'])\n",
    "task_embedding.load_state_dict(checkpoint['task_embedding'])\n",
    "subj_cls.load_state_dict(checkpoint['subj_classifier'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_file = \"./rotten_imdb/subj.data\"\n",
    "obj_file = \"./rotten_imdb/obj.data\"\n",
    "tr, _, _ = SubjObjLoader.load_data(subj_file, obj_file)\n",
    "subj_train_reader = SubjObjLoader.SubjObjReader(tr, 20, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    # device_ids = [int(device_id) for device_id in sys.argv[1].split(\",\")]\n",
    "    device_ids = list( range( len( sys.argv[1].split(\",\") ) ) )\n",
    "    bert = nn.DataParallel(bert, device_ids=device_ids)\n",
    "    device_name = \"cuda:%d\"%device_ids[0]\n",
    "    device = torch.device(device_name)\n",
    "    bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 标准ERD模型\n",
    "for i in range(20):\n",
    "    if i==0:\n",
    "        TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, i, 0.5, 50000, \"SentiSubjERD/\", None, FLAGS, cuda=True)\n",
    "    else:\n",
    "        TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, i, 0.5, 5000, \"SentiSubjERD/\", None, FLAGS, cuda=True)\n",
    "    erd_save_as = './SentiSubjERD/erdModel_epoch%03d.pkl'% (i)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"bert\":bert.state_dict(),\n",
    "            \"rmdModel\":rdm_model.state_dict(),\n",
    "            \"rdm_classifier\": rdm_classifier.state_dict(),\n",
    "            \"cm_model\":cm_model.state_dict()\n",
    "        },\n",
    "        erd_save_as\n",
    "    )\n",
    "    s2vec = Sent2Vec_Generater(tokenizer, bert, cuda=True)\n",
    "    new_len = get_new_len(s2vec, rdm_model, cm_model, FLAGS, cuda=True)\n",
    "    print(\"after new len:\")\n",
    "    MTLTrainRDMModel(rdm_model, bert, rdm_classifier,\n",
    "                         transformer, task_embedding, senti_cls, subj_cls, \n",
    "                         senti_train_reader, subj_train_reader, \n",
    "                        tt, 1000, new_data_len=[], logger=None, cuda=True, \n",
    "                            log_dir=\"SentiSubjERD\")\n",
    "    \n",
    "    \n",
    "    rdm_save_as = './%s/rdmModel_stage%03d.pkl'% (log_dir, step/100)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"bert\":bert.state_dict(),\n",
    "            \"transformer\":transformer.state_dict(),\n",
    "            \"task_embedding\":task_embedding.state_dict(),\n",
    "            \"senti_classifier\": senti_classifier.state_dict(),\n",
    "            \"subj_classifier\": subj_classifier.state_dict(),\n",
    "            \"rmdModel\":rdm_model.state_dict(),\n",
    "            \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "        },\n",
    "        rdm_save_as\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
