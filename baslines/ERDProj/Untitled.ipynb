{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 21:49:59.171059 139981624010560 deprecation_wrapper.py:119] From /home/hadoop/ERD/baslines/ERDProj/ERDModel.py:3: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load glove finished\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from collections import deque\n",
    "from ERDModel import RL_GRU2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.curdir), '../'))\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "os.chdir(\"/home/hadoop/ERD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "logger = MyLogger(\"ERDMain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_train(sess, summary_writer, mm, t_acc, t_steps, new_data_len=[]):\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    ret_acc = 0.0\n",
    "    init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "\n",
    "    for i in range(t_steps):\n",
    "        if len(new_data_len) > 0:\n",
    "            x, x_len, y = get_df_batch(i, new_data_len)\n",
    "        else:\n",
    "            x, x_len, y = get_df_batch(i)\n",
    "        feed_dic = {mm.input_x: x, mm.x_len: x_len, mm.input_y: y, mm.init_states: init_states, mm.dropout_keep_prob: 0.5}\n",
    "        _, step, loss, acc = sess.run([df_train_op, df_global_step, mm.loss, mm.accuracy], feed_dic)\n",
    "        summary = tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"df_train_loss\", simple_value=loss),\n",
    "                    tf.Summary.Value(tag=\"df_train_accuracy\", simple_value=acc),\n",
    "                ])\n",
    "        summary_writer.add_summary(summary, step)\n",
    "\n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            ret_acc = sum_acc\n",
    "            print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "\n",
    "            if sum_acc > t_acc:\n",
    "                break\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rl_train(sess, mm, t_rw, t_steps):\n",
    "    ids = np.array(range(FLAGS.batch_size), dtype=np.int32)\n",
    "    seq_states = np.zeros([FLAGS.batch_size], dtype=np.int32)\n",
    "    isStop = np.zeros([FLAGS.batch_size], dtype=np.int32)\n",
    "    max_id = FLAGS.batch_size\n",
    "    init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "    feed_dic = {mm.init_states: init_states}\n",
    "    state = sess.run(mm.df_state, feed_dic)\n",
    "    D = deque()\n",
    "    ssq = []\n",
    "    print(\"in RL the begining\")\n",
    "    logger.info(\"in RL the begining\")\n",
    "    # get_new_len(sess, mm)\n",
    "    data_ID = get_data_ID()\n",
    "    if len(data_ID) % FLAGS.batch_size == 0: # the total number of events\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size)\n",
    "    else:\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    for i in range(flags):\n",
    "        x, x_len, y = get_df_batch(i)\n",
    "        feed_dic = {mm.input_x: x, mm.x_len: x_len, mm.input_y: y, mm.init_states:init_states, mm.dropout_keep_prob: 1.0}\n",
    "        t_ssq = sess.run(mm.out_seq, feed_dic)# t_ssq = [batchsize, max_seq, scores]\n",
    "        if len(ssq) > 0:\n",
    "            ssq = np.append(ssq, t_ssq, axis=0)\n",
    "        else:\n",
    "            ssq = t_ssq\n",
    "\n",
    "    print(get_curtime() + \" Now Start RL training ...\")\n",
    "    logger.info(get_curtime() + \" Now Start RL training ...\")\n",
    "    counter = 0\n",
    "    sum_rw = 0.0 # sum of rewards\n",
    "\n",
    "    data_len = get_data_len()\n",
    "    while True:\n",
    "        if counter > FLAGS.OBSERVE:\n",
    "            sum_rw += np.mean(rw)\n",
    "            if counter % 200 == 0:\n",
    "                sum_rw = sum_rw / 2000\n",
    "                print(get_curtime() + \" Step: \" + str(step) + \" REWARD IS \" + str(sum_rw))\n",
    "                logger.info(get_curtime() + \" Step: \" + str(step) + \" REWARD IS \" + str(sum_rw))\n",
    "                if sum_rw > t_rw:\n",
    "                    print(\"Retch The Target Reward\")\n",
    "                    logger.info(\"Retch The Target Reward\")\n",
    "                    break\n",
    "                if counter > t_steps:\n",
    "                    print(\"Retch The Target Steps\")\n",
    "                    logger.info(\"Retch The Target Steps\")\n",
    "                    break\n",
    "                sum_rw = 0.0\n",
    "            s_state, s_x, s_isStop, s_rw = get_RL_Train_batch(D)\n",
    "            feed_dic = {mm.rl_state: s_state, mm.rl_input: s_x, mm.action: s_isStop, mm.reward:s_rw, mm.dropout_keep_prob: 0.5}\n",
    "            _, step = sess.run([rl_train_op, rl_global_step], feed_dic)\n",
    "\n",
    "        x, y, ids, seq_states, max_id = get_rl_batch(ids, seq_states, isStop, max_id, 0, 3150)\n",
    "        batch_dic = {mm.rl_state: state, mm.rl_input: x, mm.dropout_keep_prob: 1.0}\n",
    "        isStop, mss, mNewState = sess.run([mm.isStop, mm.stopScore, mm.rl_new_state], batch_dic)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if random.random() < FLAGS.random_rate:\n",
    "                isStop[j] = np.argmax(np.random.rand(2))\n",
    "            if seq_states[j] == data_len[ids[j]]:\n",
    "                isStop[j] = 1\n",
    "\n",
    "        # eval\n",
    "        rw = get_reward(isStop, mss, ssq, ids, seq_states)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            D.append((state[j], x[j], isStop[j], rw[j]))\n",
    "            if len(D) > FLAGS.max_memory:\n",
    "                D.popleft()\n",
    "\n",
    "        state = mNewState\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if isStop[j] == 1:\n",
    "                # init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "                # feed_dic = {mm.init_states: init_states}\n",
    "                # state[j] = sess.run(mm.df_state, feed_dic)\n",
    "                state[j] = np.zeros([FLAGS.hidden_dim], dtype=np.float32)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(sess, mm):\n",
    "#     start_ef = int(eval_flag / FLAGS.batch_size)\n",
    "#     end_ef = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    data_ID = get_data_ID()\n",
    "    batchs = int( len(data_ID)/FLAGS.batch_size )\n",
    "    init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "\n",
    "    counter = 0\n",
    "    sum_acc = 0.0\n",
    "\n",
    "    for i in range(batchs):\n",
    "        x, x_len, y = get_df_batch(i)\n",
    "        feed_dic = {mm.input_x: x, mm.x_len: x_len, mm.input_y: y, mm.init_states: init_states, mm.dropout_keep_prob: 1.0}\n",
    "        loss, acc = sess.run([mm.loss, mm.accuracy], feed_dic)\n",
    "        print(\"loss:\",loss, \"| acc:\", acc)\n",
    "        counter += 1\n",
    "        sum_acc += acc\n",
    "\n",
    "    print(sum_acc / counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 22:12:15.169942 139981624010560 logger.py:24] 2019-09-24 22:12:15 Loading data ...\n",
      "I0924 22:12:15.370373 139981624010560 logger.py:24] 2019-09-24 22:12:15 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-24 22:12:15 Loading data ...\n",
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n",
      "2019-09-24 22:12:15 Data loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 22:12:15.371721 139981624010560 logger.py:24] (300, 200, 101, 187, 2, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 200 101 187 2 2\n"
     ]
    }
   ],
   "source": [
    "print(get_curtime() + \" Loading data ...\")\n",
    "logger.info(get_curtime() + \" Loading data ...\")\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")\n",
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "\n",
    "gpu_config.gpu_options.allow_growth=True\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_len, class_num, action_num):\n",
    "print(FLAGS.embedding_dim, FLAGS.hidden_dim, FLAGS.max_seq_len, FLAGS.max_sent_len, FLAGS.class_num, FLAGS.action_num)\n",
    "logger.info((FLAGS.embedding_dim, FLAGS.hidden_dim, FLAGS.max_seq_len, FLAGS.max_sent_len, FLAGS.class_num, FLAGS.action_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=gpu_config)\n",
    "with  sess.as_default():\n",
    "    with tf.device('/GPU:0'):\n",
    "        mm = RL_GRU2(FLAGS.embedding_dim, FLAGS.hidden_dim, FLAGS.max_seq_len,\n",
    "                     FLAGS.max_sent_len, FLAGS.class_num, FLAGS.action_num, FLAGS.sent_num)\n",
    "\n",
    "        # df model\n",
    "        df_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        df_train_op = tf.train.AdagradOptimizer(0.05).minimize(mm.loss, df_global_step)\n",
    "\n",
    "        # rl model\n",
    "        rl_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        rl_train_op = tf.train.AdamOptimizer(0.001).minimize(mm.rl_cost, rl_global_step)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_IDs = get_data_ID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9761976 | acc: 0.64\n",
      "loss: 1.0305308 | acc: 0.62\n",
      "loss: 0.93481076 | acc: 0.66\n",
      "loss: 0.79329556 | acc: 0.74\n",
      "loss: 0.9009328 | acc: 0.66\n",
      "loss: 0.9347645 | acc: 0.64\n",
      "loss: 1.0222572 | acc: 0.6\n",
      "loss: 0.8108392 | acc: 0.7\n",
      "loss: 1.1172113 | acc: 0.56\n",
      "loss: 0.99594104 | acc: 0.6\n",
      "loss: 1.008531 | acc: 0.6\n",
      "loss: 0.8904818 | acc: 0.66\n",
      "loss: 0.827948 | acc: 0.7\n",
      "loss: 0.9455811 | acc: 0.64\n",
      "loss: 0.9736242 | acc: 0.62\n",
      "loss: 0.80959874 | acc: 0.72\n",
      "loss: 1.0085737 | acc: 0.62\n",
      "loss: 0.89355296 | acc: 0.68\n",
      "loss: 0.82728916 | acc: 0.7\n",
      "loss: 0.98621595 | acc: 0.62\n",
      "loss: 1.0144815 | acc: 0.62\n",
      "loss: 1.062881 | acc: 0.58\n",
      "loss: 1.1081119 | acc: 0.56\n",
      "loss: 0.7471452 | acc: 0.76\n",
      "loss: 0.8067429 | acc: 0.72\n",
      "loss: 0.9624578 | acc: 0.64\n",
      "loss: 0.7500696 | acc: 0.74\n",
      "loss: 1.0621426 | acc: 0.6\n",
      "loss: 0.8379242 | acc: 0.7\n",
      "loss: 0.95734334 | acc: 0.62\n",
      "loss: 0.8648909 | acc: 0.68\n",
      "loss: 1.1910346 | acc: 0.52\n",
      "loss: 0.85524356 | acc: 0.7\n",
      "loss: 0.7599956 | acc: 0.76\n",
      "loss: 0.76777035 | acc: 0.74\n",
      "loss: 0.9485195 | acc: 0.64\n",
      "loss: 0.9182493 | acc: 0.66\n",
      "loss: 1.0005257 | acc: 0.62\n",
      "loss: 0.9272479 | acc: 0.66\n",
      "loss: 0.9887097 | acc: 0.62\n",
      "loss: 0.9384269 | acc: 0.66\n",
      "loss: 0.95259976 | acc: 0.64\n",
      "loss: 0.95270735 | acc: 0.64\n",
      "loss: 0.791786 | acc: 0.74\n",
      "loss: 0.8459718 | acc: 0.72\n",
      "loss: 0.8431913 | acc: 0.72\n",
      "loss: 1.0921242 | acc: 0.58\n",
      "loss: 1.0490124 | acc: 0.58\n",
      "loss: 1.0298436 | acc: 0.6\n",
      "loss: 1.0624416 | acc: 0.58\n",
      "loss: 0.9395963 | acc: 0.66\n",
      "loss: 0.97410434 | acc: 0.62\n",
      "loss: 0.90335184 | acc: 0.68\n",
      "loss: 0.7873699 | acc: 0.72\n",
      "loss: 1.0036187 | acc: 0.6\n",
      "loss: 0.71186924 | acc: 0.78\n",
      "loss: 0.79488295 | acc: 0.72\n",
      "loss: 1.0705293 | acc: 0.6\n",
      "loss: 0.7866913 | acc: 0.74\n",
      "loss: 0.89582753 | acc: 0.68\n",
      "loss: 0.77635264 | acc: 0.74\n",
      "loss: 0.88922083 | acc: 0.68\n",
      "loss: 0.9327361 | acc: 0.64\n",
      "loss: 0.8397082 | acc: 0.7\n",
      "loss: 1.0990674 | acc: 0.56\n",
      "loss: 0.9117912 | acc: 0.66\n",
      "loss: 0.8180025 | acc: 0.72\n",
      "loss: 0.8686824 | acc: 0.68\n",
      "loss: 0.81436193 | acc: 0.7\n",
      "loss: 0.8881308 | acc: 0.66\n",
      "loss: 0.8591429 | acc: 0.7\n",
      "loss: 0.9122293 | acc: 0.66\n",
      "loss: 0.8254522 | acc: 0.72\n",
      "loss: 0.9481685 | acc: 0.64\n",
      "loss: 0.6155536 | acc: 0.82\n",
      "loss: 0.8803417 | acc: 0.68\n",
      "loss: 0.8033378 | acc: 0.72\n",
      "loss: 0.7179337 | acc: 0.78\n",
      "loss: 0.9458014 | acc: 0.64\n",
      "loss: 0.6787382 | acc: 0.8\n",
      "loss: 0.93450564 | acc: 0.64\n",
      "loss: 0.83442205 | acc: 0.72\n",
      "loss: 0.9903952 | acc: 0.62\n",
      "loss: 0.8282047 | acc: 0.7\n",
      "loss: 0.9377557 | acc: 0.66\n",
      "loss: 0.8226197 | acc: 0.7\n",
      "loss: 0.8939085 | acc: 0.68\n",
      "loss: 0.7491671 | acc: 0.74\n",
      "loss: 0.9500125 | acc: 0.62\n",
      "loss: 0.9045921 | acc: 0.66\n",
      "loss: 0.86662364 | acc: 0.7\n",
      "0.6672527528071142\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    eval(sess, mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 22:15:10.602153 139981624010560 logger.py:24] df_saved_erd/model14 is restored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Debug1------------------\n",
      "df_saved_erd/model14 is restored.\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"df_saved_erd\"\n",
    "checkpoint = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "if checkpoint and checkpoint.model_checkpoint_path:\n",
    "    print(\"--------------Debug1------------------\")\n",
    "    saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "    print(checkpoint.model_checkpoint_path+\" is restored.\")\n",
    "    logger.info(checkpoint.model_checkpoint_path+\" is restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on the trainning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.68944013 | acc: 0.64\n",
      "loss: 0.71958584 | acc: 0.62\n",
      "loss: 0.67800266 | acc: 0.66\n",
      "loss: 0.5913662 | acc: 0.74\n",
      "loss: 0.67395717 | acc: 0.66\n",
      "loss: 0.7013041 | acc: 0.64\n",
      "loss: 0.7399647 | acc: 0.6\n",
      "loss: 0.63522685 | acc: 0.7\n",
      "loss: 0.7940782 | acc: 0.56\n",
      "loss: 0.7490343 | acc: 0.6\n",
      "loss: 0.74350744 | acc: 0.6\n",
      "loss: 0.6772332 | acc: 0.66\n",
      "loss: 0.63835645 | acc: 0.7\n",
      "loss: 0.69375825 | acc: 0.64\n",
      "loss: 0.7240085 | acc: 0.62\n",
      "loss: 0.6174975 | acc: 0.72\n",
      "loss: 0.72881204 | acc: 0.62\n",
      "loss: 0.6437065 | acc: 0.68\n",
      "loss: 0.6357735 | acc: 0.7\n",
      "loss: 0.7205063 | acc: 0.62\n",
      "loss: 0.71788687 | acc: 0.62\n",
      "loss: 0.76473695 | acc: 0.58\n",
      "loss: 0.79534405 | acc: 0.56\n",
      "loss: 0.5719873 | acc: 0.76\n",
      "loss: 0.6108683 | acc: 0.72\n",
      "loss: 0.709652 | acc: 0.64\n",
      "loss: 0.5881363 | acc: 0.74\n",
      "loss: 0.74533266 | acc: 0.6\n",
      "loss: 0.6309216 | acc: 0.7\n",
      "loss: 0.71896046 | acc: 0.62\n",
      "loss: 0.6508034 | acc: 0.68\n",
      "loss: 0.82587147 | acc: 0.52\n",
      "loss: 0.63478255 | acc: 0.7\n",
      "loss: 0.5728295 | acc: 0.76\n",
      "loss: 0.58607113 | acc: 0.74\n",
      "loss: 0.6953647 | acc: 0.64\n",
      "loss: 0.6818204 | acc: 0.66\n",
      "loss: 0.7200105 | acc: 0.62\n",
      "loss: 0.68240064 | acc: 0.66\n",
      "loss: 0.7282064 | acc: 0.62\n",
      "loss: 0.68288374 | acc: 0.66\n",
      "loss: 0.70129794 | acc: 0.64\n",
      "loss: 0.6904971 | acc: 0.64\n",
      "loss: 0.5956611 | acc: 0.74\n",
      "loss: 0.6121293 | acc: 0.72\n",
      "loss: 0.6118969 | acc: 0.72\n",
      "loss: 0.7688915 | acc: 0.58\n",
      "loss: 0.76494235 | acc: 0.58\n",
      "loss: 0.7454592 | acc: 0.6\n",
      "loss: 0.7600797 | acc: 0.58\n",
      "loss: 0.68066484 | acc: 0.66\n",
      "loss: 0.7192093 | acc: 0.62\n",
      "loss: 0.65521044 | acc: 0.68\n",
      "loss: 0.594259 | acc: 0.72\n",
      "loss: 0.7462528 | acc: 0.6\n",
      "loss: 0.546444 | acc: 0.78\n",
      "loss: 0.61217993 | acc: 0.72\n",
      "loss: 0.74317455 | acc: 0.6\n",
      "loss: 0.5832456 | acc: 0.74\n",
      "loss: 0.6523532 | acc: 0.68\n",
      "loss: 0.5901067 | acc: 0.74\n",
      "loss: 0.6521597 | acc: 0.68\n",
      "loss: 0.7062954 | acc: 0.64\n",
      "loss: 0.63622737 | acc: 0.7\n",
      "loss: 0.7823766 | acc: 0.56\n",
      "loss: 0.67763567 | acc: 0.66\n",
      "loss: 0.6107159 | acc: 0.72\n",
      "loss: 0.65603334 | acc: 0.68\n",
      "loss: 0.62304884 | acc: 0.7\n",
      "loss: 0.67531896 | acc: 0.66\n",
      "loss: 0.63565505 | acc: 0.7\n",
      "loss: 0.6822157 | acc: 0.66\n",
      "loss: 0.6088211 | acc: 0.72\n",
      "loss: 0.69923574 | acc: 0.64\n",
      "loss: 0.5001352 | acc: 0.82\n",
      "loss: 0.65338993 | acc: 0.68\n",
      "loss: 0.6084909 | acc: 0.72\n",
      "loss: 0.5444543 | acc: 0.78\n",
      "loss: 0.69909793 | acc: 0.64\n",
      "loss: 0.5300834 | acc: 0.8\n",
      "loss: 0.7093141 | acc: 0.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-91c5a3cc761a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-6fd0c5e5ada5>\u001b[0m in \u001b[0;36meval\u001b[0;34m(sess, mm)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mfeed_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ERD/baslines/ERDProj/dataUtils.py\u001b[0m in \u001b[0;36mget_df_batch\u001b[0;34m(start, new_data_len)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0;31m# data_x[i][j][k] = c2vec.vectorize_words([m_word])[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                     \u001b[0mdata_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m# print(\"word:\", m_word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    eval(sess, mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6896323 | acc: 0.54\n",
      "loss: 2.0606124 | acc: 0.44\n",
      "loss: 1.4072715 | acc: 0.62\n",
      "loss: 2.3717637 | acc: 0.34\n",
      "loss: 1.5512682 | acc: 0.58\n",
      "loss: 1.975043 | acc: 0.46\n",
      "loss: 2.041068 | acc: 0.44\n",
      "loss: 1.9526526 | acc: 0.46\n",
      "loss: 1.7462956 | acc: 0.52\n",
      "loss: 1.8309494 | acc: 0.5\n",
      "0.4900000005960464\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    eval(sess, mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  sess.as_default():\n",
    "    with tf.device('/GPU:0'):\n",
    "        mm = RL_GRU2(FLAGS.embedding_dim, FLAGS.hidden_dim, FLAGS.max_seq_len,\n",
    "                     FLAGS.max_sent_len, FLAGS.class_num, FLAGS.action_num, FLAGS.sent_num)\n",
    "\n",
    "        # df model\n",
    "        df_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        df_train_op = tf.train.AdagradOptimizer(0.05).minimize(mm.loss, df_global_step)\n",
    "\n",
    "        # rl model\n",
    "        rl_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        rl_train_op = tf.train.AdamOptimizer(0.001).minimize(mm.rl_cost, rl_global_step)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"./reports/\", graph=sess.graph)\n",
    "df_train(sess, summary_writer, mm, 0.90, 20000)\n",
    "\n",
    "for i in range(20):\n",
    "    rl_train(sess, mm, 0.5, 50000)\n",
    "    saver.save(sess, \"rl_saved_erd/model\"+str(i))\n",
    "    print(\"rl_model \"+str(i)+\" saved\")\n",
    "    logger.info(\"rl_model \"+str(i)+\" saved\")\n",
    "    new_len = get_new_len(sess, mm)\n",
    "    acc = df_train(sess, summary_writer, mm, 0.9, 1000, new_len)\n",
    "    saver.save(sess, \"df_saved_erd/model\"+str(i))\n",
    "    print(\"df_model \"+str(i)+\" saved\")\n",
    "    logger.info(\"df_model \"+str(i)+\" saved\")\n",
    "    if acc > 0.9:\n",
    "        break\n",
    "\n",
    "print(\"The End of My Program\")\n",
    "logger.info(\"The End of My Program\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
