{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load glove finished\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "sys.path.append(\".\")\n",
    "from dataUtilsV0 import *\n",
    "import json\n",
    "from RDM_Model import *\n",
    "from CM_Model import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 64 ,  max_seq_len: 346\n",
      "5802 data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.conda/envs/torch_B/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LayerNormLSTMCell(nn.LSTMCell):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, bias=True, use_layer_norm=True):\n",
    "        super().__init__(input_size, hidden_size, bias)\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if self.use_layer_norm:\n",
    "            self.ln_ih = nn.LayerNorm(4 * hidden_size)\n",
    "            self.ln_hh = nn.LayerNorm(4 * hidden_size)\n",
    "            self.ln_ho = nn.LayerNorm(hidden_size)\n",
    "        # DropConnect on the recurrent hidden to hidden weight\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        self.check_forward_input(input)\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "        self.check_forward_hidden(input, hx, '[0]')\n",
    "        self.check_forward_hidden(input, cx, '[1]')\n",
    "\n",
    "        weight_hh = nn.functional.dropout(self.weight_hh, p=self.dropout, training=self.training)\n",
    "        if self.use_layer_norm:\n",
    "            gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n",
    "                     + self.ln_hh(F.linear(hx, weight_hh, self.bias_hh))\n",
    "        else:\n",
    "            gates = F.linear(input, self.weight_ih, self.bias_ih) \\\n",
    "                    + F.linear(hx, weight_hh, self.bias_hh)\n",
    "\n",
    "        i, f, c, o = gates.chunk(4, 1)\n",
    "        i_ = torch.sigmoid(i)\n",
    "        f_ = torch.sigmoid(f)\n",
    "        c_ = torch.tanh(c)\n",
    "        o_ = torch.sigmoid(o)\n",
    "        cy = (f_ * cx) + (i_ * c_)\n",
    "        if self.use_layer_norm:\n",
    "            hy = o_ * self.ln_ho(torch.tanh(cy))\n",
    "        else:\n",
    "            hy = o_ * torch.tanh(cy)\n",
    "        return hy, cy\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.0,\n",
    "                 weight_dropout=0.0,\n",
    "                 bias=True,\n",
    "                 bidirectional=False,\n",
    "                 use_layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # using variational dropout\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.hidden0 = nn.ModuleList([\n",
    "            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                              hidden_size=hidden_size, dropout=weight_dropout, bias=bias, use_layer_norm=use_layer_norm)\n",
    "            for layer in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.hidden1 = nn.ModuleList([\n",
    "                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                                  hidden_size=hidden_size, dropout=weight_dropout, bias=bias, use_layer_norm=use_layer_norm)\n",
    "                for layer in range(num_layers)\n",
    "            ])\n",
    "\n",
    "    def copy_parameters(self, rnn_old):\n",
    "        for param in rnn_old.named_parameters():\n",
    "            name_ = param[0].split(\"_\")\n",
    "            layer = int(name_[2].replace(\"l\", \"\"))\n",
    "            sub_name = \"_\".join(name_[:2])\n",
    "            if len(name_) > 3:\n",
    "                self.hidden1[layer].register_parameter(sub_name, param[1])\n",
    "            else:\n",
    "                self.hidden0[layer].register_parameter(sub_name, param[1])\n",
    "\n",
    "    def forward(self, input, hidden=None, seq_lens=None):\n",
    "        seq_len, batch_size, _ = input.size()\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "\n",
    "        ht = []\n",
    "        for i in range(seq_len):\n",
    "            ht.append([None] * (self.num_layers * num_directions))\n",
    "        ct = []\n",
    "        for i in range(seq_len):\n",
    "            ct.append([None] * (self.num_layers * num_directions))\n",
    "\n",
    "        seq_len_mask = input.new_ones(batch_size, seq_len, self.hidden_size, requires_grad=False)\n",
    "        if seq_lens != None:\n",
    "            for i, l in enumerate(seq_lens):\n",
    "                seq_len_mask[i, l:, :] = 0\n",
    "        seq_len_mask = seq_len_mask.transpose(0, 1)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # if use cuda, change 'torch.LongTensor' to 'torch.cuda.LongTensor'\n",
    "            indices_ = (torch.LongTensor(seq_lens) - 1).unsqueeze(1).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                [1, 1, 1, self.hidden_size])\n",
    "            # if use cuda, change 'torch.LongTensor' to 'torch.cuda.LongTensor'\n",
    "            indices_reverse = torch.LongTensor([0] * batch_size).unsqueeze(1).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                [1, 1, 1, self.hidden_size])\n",
    "            indices = torch.cat((indices_, indices_reverse), dim=1)\n",
    "            hy = []\n",
    "            cy = []\n",
    "            xs = input\n",
    "            # Variational Dropout\n",
    "            if not self.training or self.dropout == 0:\n",
    "                dropout_mask = input.new_ones(self.num_layers, 2, batch_size, self.hidden_size)\n",
    "            else:\n",
    "                dropout_mask = input.new(self.num_layers, 2, batch_size, self.hidden_size).bernoulli_(1 - self.dropout)\n",
    "                dropout_mask = Variable(dropout_mask, requires_grad=False) / (1 - self.dropout)\n",
    "\n",
    "            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n",
    "                l0, l1 = 2 * l, 2 * l + 1\n",
    "                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n",
    "                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n",
    "                    ht_, ct_ = layer0(x0, (h0, c0))\n",
    "                    ht[t][l0] = ht_ * seq_len_mask[t]\n",
    "                    ct[t][l0] = ct_ * seq_len_mask[t]\n",
    "                    h0, c0 = ht[t][l0], ct[t][l0]\n",
    "                    t = seq_len - 1 - t\n",
    "                    ht_, ct_ = layer1(x1, (h1, c1))\n",
    "                    ht[t][l1] = ht_ * seq_len_mask[t]\n",
    "                    ct[t][l1] = ct_ * seq_len_mask[t]\n",
    "                    h1, c1 = ht[t][l1], ct[t][l1]\n",
    "\n",
    "                xs = [torch.cat((h[l0]*dropout_mask[l][0], h[l1]*dropout_mask[l][1]), dim=1) for h in ht]\n",
    "                ht_temp = torch.stack([torch.stack([h[l0], h[l1]]) for h in ht])\n",
    "                ct_temp = torch.stack([torch.stack([c[l0], c[l1]]) for c in ct])\n",
    "                if len(hy) == 0:\n",
    "                    hy = torch.stack(list(ht_temp.gather(dim=0, index=indices).squeeze(0)))\n",
    "                else:\n",
    "                    hy = torch.cat((hy, torch.stack(list(ht_temp.gather(dim=0, index=indices).squeeze(0)))), dim=0)\n",
    "                if len(cy) == 0:\n",
    "                    cy = torch.stack(list(ct_temp.gather(dim=0, index=indices).squeeze(0)))\n",
    "                else:\n",
    "                    cy = torch.cat((cy, torch.stack(list(ct_temp.gather(dim=0, index=indices).squeeze(0)))), dim=0)\n",
    "            y  = torch.stack(xs)\n",
    "        else:\n",
    "            # if use cuda, change 'torch.LongTensor' to 'torch.cuda.LongTensor'\n",
    "            indices = (torch.cuda.LongTensor(seq_lens) - 1).unsqueeze(1).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                [1, self.num_layers, 1, self.hidden_size])\n",
    "            h, c = hx, cx\n",
    "            # Variational Dropout\n",
    "            if not self.training or self.dropout == 0:\n",
    "                dropout_mask = input.new_ones(self.num_layers, batch_size, self.hidden_size)\n",
    "            else:\n",
    "                dropout_mask = input.new(self.num_layers, batch_size, self.hidden_size).bernoulli_(1 - self.dropout)\n",
    "                dropout_mask = Variable(dropout_mask, requires_grad=False) / (1 - self.dropout)\n",
    "\n",
    "            for t, x in enumerate(input):\n",
    "                for l, layer in enumerate(self.hidden0):\n",
    "                    ht_, ct_ = layer(x, (h[l], c[l]))\n",
    "                    ht[t][l] = ht_ * seq_len_mask[t]\n",
    "                    ct[t][l] = ct_ * seq_len_mask[t]\n",
    "                    x = ht[t][l] * dropout_mask[l]\n",
    "                ht[t] = torch.stack(ht[t])\n",
    "                ct[t] = torch.stack(ct[t])\n",
    "                h, c = ht[t], ct[t]\n",
    "            y = torch.stack([h[-1]*dropout_mask[-1] for h in ht])\n",
    "            hy = torch.stack(list(torch.stack(ht).gather(dim=0, index=indices).squeeze(0)))\n",
    "            cy = torch.stack(list(torch.stack(ct).gather(dim=0, index=indices).squeeze(0)))\n",
    "\n",
    "        return y, (hy, cy)\n",
    "\n",
    "\n",
    "# ### 模型训练与测试\n",
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs, cuda=True):\n",
    "        inputs_sent = [torch.cat([self.linear(sent_tensor.cuda() if cuda else sent_tensor).max(axis=0)[0].unsqueeze(0) for sent_tensor in seq]) for seq in inputs]\n",
    "        seqs = torch.nn.utils.rnn.pad_sequence(inputs_sent, batch_first=True)\n",
    "        return seqs\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_x): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, emb_dim = input_x.shape\n",
    "        init_states = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        try:\n",
    "            df_outputs, df_last_state = self.gru_model(input_x, init_states)\n",
    "        except:\n",
    "            print(\"Error:\", pool_feature.shape, init_states.shape)\n",
    "            raise\n",
    "        return df_outputs\n",
    "\n",
    "class RDM_Model_V1(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model_V1, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = LayerNormLSTM(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "\n",
    "    def forward(self, input_x, seq_lens): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, emb_dim = input_x.shape\n",
    "        h0 = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        c0 = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        df_outputs, (df_last_state, df_last_cell) = self.gru_model(input_x.transpose(0, 1), (h0, c0), seq_lens)\n",
    "        return df_outputs.transpose(0, 1), df_last_state.transpose(0, 1), df_last_cell.transpose(0, 1)\n",
    "\n",
    "class CM_Model_V1(nn.Module):\n",
    "    def __init__(self, hidden_dim, action_num):\n",
    "        super(CM_Model_V1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_state):\n",
    "        \"\"\"\n",
    "        rdm_state: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, hidden_dim = rdm_state.shape\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "                rdm_state\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop\n",
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            rl_input, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "load_data_fast()\n",
    "\n",
    "rdm_model = RDM_Model(300, 300, 256, 0.2).cuda()\n",
    "sent_pooler = pooling_layer(300, 300).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()\n",
    "cm_model = CM_Model_V1(256, 2).cuda()\n",
    "\n",
    "log_dir = os.path.join(sys.path[0], \"ERD/\")\n",
    "\n",
    "with open(\"../../config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_file = \"ERD/ERD_best.pkl\"\n",
    "if os.path.exists(pretrained_file):\n",
    "    checkpoint = torch.load(pretrained_file)\n",
    "    sent_pooler.load_state_dict(checkpoint['sent_pooler'])\n",
    "    rdm_model.load_state_dict(checkpoint[\"rmdModel\"])\n",
    "    rdm_classifier.load_state_dict(checkpoint[\"rdm_classifier\"])\n",
    "else:\n",
    "    TrainRDMModel(rdm_model, sent_pooler, rdm_classifier, \n",
    "                    t_steps=5000, stage=0, new_data_len=[], valid_new_len=[], logger=None, \n",
    "                        log_dir=log_dir, cuda=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid_data_ID\n",
    "del valid_data_len\n",
    "del valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataUtilsV0 import valid_data_ID\n",
    "from dataUtilsV0 import valid_data_len\n",
    "from dataUtilsV0 import valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 0\n",
    "t_rw = 0.5 \n",
    "t_steps = 10000  \n",
    "logger = None\n",
    "cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "t_acc = 0.9\n",
    "gamma = 1.0\n",
    "lambda1 = -1.0\n",
    "lambda2 = 0.0 #regularizer\n",
    "sum_loss = 0.0\n",
    "sum_acc = 0.0\n",
    "t_acc = 0.9\n",
    "ret_acc = 0.0\n",
    "init_states = torch.zeros([1, batch_size, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight, reduction='mean')\n",
    "optim = torch.optim.Adagrad([\n",
    "                            {'params': sent_pooler.parameters(), 'lr': 2e-5},\n",
    "                            {'params': rdm_model.parameters(), 'lr': 2e-5},\n",
    "                            {'params': rdm_classifier.parameters(), 'lr': 2e-5},\n",
    "                            {'params': cm_model.parameters(), 'lr':2e-3}\n",
    "                         ]\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(log_dir, filename_suffix=\"_ERD_CM_stage_%3d\"%stage)\n",
    "best_valid_acc = 0.0\n",
    "rw_arr = np.zeros(10)\n",
    "len_arr = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing PG*****   0 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.12729970/0.9000000, mean_len = 0.000\n",
      "*****Optimizing PG*****  10 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.35842717/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG*****  20 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.05993899/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG*****  30 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.04908651/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG*****  40 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.03922317/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG*****  50 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.09170857/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG*****  60 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.04401013/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG*****  70 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.01925665/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG*****  80 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.02946398/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG*****  90 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.05616926/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 100 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.12109392/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 110 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.09827054/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 120 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.10612105/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 130 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.26297385/0.9000000, mean_len = 0.000\n",
      "*****Optimizing PG***** 140 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.31256843/0.8500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 150 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.04044061/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG***** 160 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.01787689/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG***** 170 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.11807933/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 180 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.02377208/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG***** 190 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.06027604/1.0000000, mean_len = 0.000\n",
      "*****Optimizing PG***** 200 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.09853941/0.9500000, mean_len = 0.000\n",
      "*****Optimizing PG***** 210 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.06686746/0.9500000, mean_len = 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5e46ff71fa29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch_B/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch_B/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    x, x_len, y = get_df_batch(step*batch_size, batch_size)        \n",
    "    seq = sent_pooler(x)\n",
    "    rdm_hiddens = rdm_model(seq)\n",
    "    batchsize, max_seq_len, hidden_dim = rdm_hiddens.shape\n",
    "    rdm_outs = torch.stack(\n",
    "        [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "    )\n",
    "    stopScore, isStop = cm_model(\n",
    "        rdm_outs\n",
    "    )\n",
    "#     rdm_preds = rdm_scores.argmax(axis=1)\n",
    "    y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "    acc = accuracy_score(y_label.cpu().numpy(), isStop.cpu().numpy())\n",
    "    loss = loss_fn(stopScore, y_label)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step%10 == 0:  \n",
    "        print('*****Optimizing PG***** %3d | %d , train_loss/Expected Reward = %6.8f/%6.7f,  RDM_Loss/RDM Accuracy = %6.8f/%6.7f, mean_len = %2.3f'% (step, t_steps, \n",
    "                0.0, 0 , loss, acc, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM*****   0 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.11310752/0.9500000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  10 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.41237867/0.9500000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  20 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.04482171/0.9500000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  30 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.03326625/1.0000000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  40 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.02917382/1.0000000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  50 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.10453340/0.9500000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  60 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.03556860/1.0000000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  70 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.00990811/1.0000000, mean_len = 0.000\n",
      "*****Optimizing RDM*****  80 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.02363989/1.0000000, mean_len = 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d430a9b6fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_pooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrdm_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_hiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ERD/baslines/ERDProj/dataUtilsV0.py\u001b[0m in \u001b[0;36mget_df_batch\u001b[0;34m(start, batch_size, new_data_len, cuda)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordlist2wordvecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_ID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0msent_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mdata_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch_B/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch_B/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    x, x_len, y = get_df_batch(step*batch_size, batch_size)        \n",
    "    seq = sent_pooler(x)\n",
    "    rdm_hiddens = rdm_model(seq)\n",
    "    batchsize, max_seq_len, hidden_dim = rdm_hiddens.shape\n",
    "    rdm_outs = torch.stack(\n",
    "        [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "    )\n",
    "    rdm_scores = rdm_classifier(\n",
    "        rdm_outs\n",
    "    )\n",
    "    rdm_preds = rdm_scores.argmax(axis=1)\n",
    "    y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "    acc = accuracy_score(y_label.cpu().numpy(), rdm_preds.cpu().numpy())\n",
    "    loss = loss_fn(rdm_scores, y_label)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step%10 == 0:  \n",
    "        print('*****Optimizing RDM***** %3d | %d , train_loss/Expected Reward = %6.8f/%6.7f,  RDM_Loss/RDM Accuracy = %6.8f/%6.7f, mean_len = %2.3f'% (step, t_steps, \n",
    "                0.0, 0 , loss, acc, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG*****   0 | 10000 , train_loss/Expected Reward = 0.00000000/0.0000000,  RDM_Loss/RDM Accuracy = 0.10292125/0.9000000, mean_len = 0.000\n",
      "*****Optimizing RDM & PG*****  10 | 10000 , train_loss/Expected Reward = 1.17797074/1.1779707,  RDM_Loss/RDM Accuracy = 0.39256126/0.9500000, mean_len = 0.246\n",
      "*****Optimizing RDM & PG*****  20 | 10000 , train_loss/Expected Reward = 1.11247169/1.1124717,  RDM_Loss/RDM Accuracy = 0.04693697/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG*****  30 | 10000 , train_loss/Expected Reward = 1.10926030/1.1092603,  RDM_Loss/RDM Accuracy = 0.03507628/1.0000000, mean_len = 0.258\n",
      "*****Optimizing RDM & PG*****  40 | 10000 , train_loss/Expected Reward = 1.16846640/1.1684664,  RDM_Loss/RDM Accuracy = 0.02621892/1.0000000, mean_len = 0.222\n",
      "*****Optimizing RDM & PG*****  50 | 10000 , train_loss/Expected Reward = 0.81764104/0.8176410,  RDM_Loss/RDM Accuracy = 0.08326828/0.9500000, mean_len = 0.233\n",
      "*****Optimizing RDM & PG*****  60 | 10000 , train_loss/Expected Reward = 0.82226546/0.8222655,  RDM_Loss/RDM Accuracy = 0.02440771/1.0000000, mean_len = 0.250\n",
      "*****Optimizing RDM & PG*****  70 | 10000 , train_loss/Expected Reward = 0.77764275/0.7776428,  RDM_Loss/RDM Accuracy = 0.00935637/1.0000000, mean_len = 0.239\n",
      "*****Optimizing RDM & PG*****  80 | 10000 , train_loss/Expected Reward = 0.82801273/0.8280127,  RDM_Loss/RDM Accuracy = 0.02158353/1.0000000, mean_len = 0.235\n",
      "*****Optimizing RDM & PG*****  90 | 10000 , train_loss/Expected Reward = 0.78369950/0.7836995,  RDM_Loss/RDM Accuracy = 0.05397677/1.0000000, mean_len = 0.222\n",
      "*****Optimizing RDM & PG***** 100 | 10000 , train_loss/Expected Reward = 0.58561040/0.5856104,  RDM_Loss/RDM Accuracy = 0.12376381/0.9500000, mean_len = 0.226\n",
      "*****Optimizing RDM & PG***** 110 | 10000 , train_loss/Expected Reward = 0.52931147/0.5293115,  RDM_Loss/RDM Accuracy = 0.12642993/0.9500000, mean_len = 0.250\n",
      "*****Optimizing RDM & PG***** 120 | 10000 , train_loss/Expected Reward = 0.50131432/0.5013143,  RDM_Loss/RDM Accuracy = 0.11561240/0.9500000, mean_len = 0.244\n",
      "*****Optimizing RDM & PG***** 130 | 10000 , train_loss/Expected Reward = 0.46636686/0.4663669,  RDM_Loss/RDM Accuracy = 0.27272570/0.9000000, mean_len = 0.242\n",
      "*****Optimizing RDM & PG***** 140 | 10000 , train_loss/Expected Reward = 0.40128453/0.4012845,  RDM_Loss/RDM Accuracy = 0.33496338/0.8500000, mean_len = 0.241\n",
      "*****Optimizing RDM & PG***** 150 | 10000 , train_loss/Expected Reward = 0.29367160/0.2936716,  RDM_Loss/RDM Accuracy = 0.04519632/1.0000000, mean_len = 0.257\n",
      "*****Optimizing RDM & PG***** 160 | 10000 , train_loss/Expected Reward = 0.27658599/0.2765860,  RDM_Loss/RDM Accuracy = 0.00996874/1.0000000, mean_len = 0.229\n",
      "*****Optimizing RDM & PG***** 170 | 10000 , train_loss/Expected Reward = 0.27586749/0.2758675,  RDM_Loss/RDM Accuracy = 0.15888070/0.9500000, mean_len = 0.231\n",
      "*****Optimizing RDM & PG***** 180 | 10000 , train_loss/Expected Reward = 0.29130765/0.2913076,  RDM_Loss/RDM Accuracy = 0.01978298/1.0000000, mean_len = 0.266\n",
      "*****Optimizing RDM & PG***** 190 | 10000 , train_loss/Expected Reward = 0.29673959/0.2967396,  RDM_Loss/RDM Accuracy = 0.05430230/0.9500000, mean_len = 0.283\n",
      "*****Optimizing RDM & PG***** 200 | 10000 , train_loss/Expected Reward = 0.23511118/0.2351112,  RDM_Loss/RDM Accuracy = 0.09783138/0.9500000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 210 | 10000 , train_loss/Expected Reward = 0.22217100/0.2221710,  RDM_Loss/RDM Accuracy = 0.07108449/0.9500000, mean_len = 0.309\n",
      "*****Optimizing RDM & PG***** 220 | 10000 , train_loss/Expected Reward = 0.23511450/0.2351145,  RDM_Loss/RDM Accuracy = 0.06210690/0.9500000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 230 | 10000 , train_loss/Expected Reward = 0.26079239/0.2607924,  RDM_Loss/RDM Accuracy = 0.06221282/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 240 | 10000 , train_loss/Expected Reward = 0.25164684/0.2516468,  RDM_Loss/RDM Accuracy = 0.00818604/1.0000000, mean_len = 0.260\n",
      "*****Optimizing RDM & PG***** 250 | 10000 , train_loss/Expected Reward = 0.18539255/0.1853925,  RDM_Loss/RDM Accuracy = 0.01657335/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 260 | 10000 , train_loss/Expected Reward = 0.21645910/0.2164591,  RDM_Loss/RDM Accuracy = 0.03094854/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 270 | 10000 , train_loss/Expected Reward = 0.25616355/0.2561635,  RDM_Loss/RDM Accuracy = 0.03581768/1.0000000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 280 | 10000 , train_loss/Expected Reward = 0.25045105/0.2504511,  RDM_Loss/RDM Accuracy = 0.01073303/1.0000000, mean_len = 0.279\n",
      "*****Optimizing RDM & PG***** 290 | 10000 , train_loss/Expected Reward = 0.19145754/0.1914575,  RDM_Loss/RDM Accuracy = 0.20961170/0.8500000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 300 | 10000 , train_loss/Expected Reward = 0.20693025/0.2069303,  RDM_Loss/RDM Accuracy = 0.29823098/0.9000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 310 | 10000 , train_loss/Expected Reward = 0.17617190/0.1761719,  RDM_Loss/RDM Accuracy = 0.13097781/0.9500000, mean_len = 0.313\n",
      "*****Optimizing RDM & PG***** 320 | 10000 , train_loss/Expected Reward = 0.29220131/0.2922013,  RDM_Loss/RDM Accuracy = 0.21023917/0.9500000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 330 | 10000 , train_loss/Expected Reward = 0.21177461/0.2117746,  RDM_Loss/RDM Accuracy = 0.19331384/0.9500000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 340 | 10000 , train_loss/Expected Reward = 0.20994206/0.2099421,  RDM_Loss/RDM Accuracy = 0.02047888/1.0000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 350 | 10000 , train_loss/Expected Reward = 0.24041869/0.2404187,  RDM_Loss/RDM Accuracy = 0.08230403/0.9500000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 360 | 10000 , train_loss/Expected Reward = 0.20799458/0.2079946,  RDM_Loss/RDM Accuracy = 0.24228173/0.9500000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 370 | 10000 , train_loss/Expected Reward = 0.27757735/0.2775774,  RDM_Loss/RDM Accuracy = 0.01263085/1.0000000, mean_len = 0.264\n",
      "*****Optimizing RDM & PG***** 380 | 10000 , train_loss/Expected Reward = 0.22843491/0.2284349,  RDM_Loss/RDM Accuracy = 0.08352187/0.9500000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 390 | 10000 , train_loss/Expected Reward = 0.20466208/0.2046621,  RDM_Loss/RDM Accuracy = 0.00885117/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 400 | 10000 , train_loss/Expected Reward = 0.24632477/0.2463248,  RDM_Loss/RDM Accuracy = 0.04732966/0.9500000, mean_len = 0.266\n",
      "*****Optimizing RDM & PG***** 410 | 10000 , train_loss/Expected Reward = 0.29393257/0.2939326,  RDM_Loss/RDM Accuracy = 0.01454342/1.0000000, mean_len = 0.263\n",
      "*****Optimizing RDM & PG***** 420 | 10000 , train_loss/Expected Reward = 0.20347845/0.2034784,  RDM_Loss/RDM Accuracy = 0.02291337/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 430 | 10000 , train_loss/Expected Reward = 0.21536917/0.2153692,  RDM_Loss/RDM Accuracy = 0.05336331/0.9500000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 440 | 10000 , train_loss/Expected Reward = 0.28431251/0.2843125,  RDM_Loss/RDM Accuracy = 0.10449022/0.9500000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 450 | 10000 , train_loss/Expected Reward = 0.26039843/0.2603984,  RDM_Loss/RDM Accuracy = 0.26457414/0.8000000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 460 | 10000 , train_loss/Expected Reward = 0.25927273/0.2592727,  RDM_Loss/RDM Accuracy = 0.14450455/0.9500000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 470 | 10000 , train_loss/Expected Reward = 0.24638217/0.2463822,  RDM_Loss/RDM Accuracy = 0.05956760/0.9500000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 480 | 10000 , train_loss/Expected Reward = 0.22658502/0.2265850,  RDM_Loss/RDM Accuracy = 0.01672816/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 490 | 10000 , train_loss/Expected Reward = 0.21311554/0.2131155,  RDM_Loss/RDM Accuracy = 0.12662439/0.9500000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 500 | 10000 , train_loss/Expected Reward = 0.17707939/0.1770794,  RDM_Loss/RDM Accuracy = 0.32277623/0.9500000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 510 | 10000 , train_loss/Expected Reward = 0.25626277/0.2562628,  RDM_Loss/RDM Accuracy = 0.00565897/1.0000000, mean_len = 0.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 520 | 10000 , train_loss/Expected Reward = 0.21575976/0.2157598,  RDM_Loss/RDM Accuracy = 0.00859091/1.0000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 530 | 10000 , train_loss/Expected Reward = 0.23222566/0.2322257,  RDM_Loss/RDM Accuracy = 0.12520550/0.9500000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 540 | 10000 , train_loss/Expected Reward = 0.24071087/0.2407109,  RDM_Loss/RDM Accuracy = 0.06673069/1.0000000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 550 | 10000 , train_loss/Expected Reward = 0.19561233/0.1956123,  RDM_Loss/RDM Accuracy = 0.09258128/0.9500000, mean_len = 0.324\n",
      "*****Optimizing RDM & PG***** 560 | 10000 , train_loss/Expected Reward = 0.20127381/0.2012738,  RDM_Loss/RDM Accuracy = 0.11965803/0.9500000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 570 | 10000 , train_loss/Expected Reward = 0.25493366/0.2549337,  RDM_Loss/RDM Accuracy = 0.39542127/0.9000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 580 | 10000 , train_loss/Expected Reward = 0.23422610/0.2342261,  RDM_Loss/RDM Accuracy = 0.09271813/0.9500000, mean_len = 0.283\n",
      "*****Optimizing RDM & PG***** 590 | 10000 , train_loss/Expected Reward = 0.21917922/0.2191792,  RDM_Loss/RDM Accuracy = 0.85628271/0.7500000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 600 | 10000 , train_loss/Expected Reward = 0.26122246/0.2612225,  RDM_Loss/RDM Accuracy = 0.11094294/0.9500000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 610 | 10000 , train_loss/Expected Reward = 0.20003401/0.2000340,  RDM_Loss/RDM Accuracy = 0.20041333/0.9000000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 620 | 10000 , train_loss/Expected Reward = 0.24757240/0.2475724,  RDM_Loss/RDM Accuracy = 0.03472929/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 630 | 10000 , train_loss/Expected Reward = 0.22713978/0.2271398,  RDM_Loss/RDM Accuracy = 0.06311595/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 640 | 10000 , train_loss/Expected Reward = 0.29136605/0.2913661,  RDM_Loss/RDM Accuracy = 0.02165925/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 650 | 10000 , train_loss/Expected Reward = 0.20915045/0.2091504,  RDM_Loss/RDM Accuracy = 0.02937450/1.0000000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 660 | 10000 , train_loss/Expected Reward = 0.25195010/0.2519501,  RDM_Loss/RDM Accuracy = 0.29453403/0.9500000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 670 | 10000 , train_loss/Expected Reward = 0.26812925/0.2681293,  RDM_Loss/RDM Accuracy = 0.12346336/0.9500000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 680 | 10000 , train_loss/Expected Reward = 0.21601773/0.2160177,  RDM_Loss/RDM Accuracy = 0.07813478/1.0000000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 690 | 10000 , train_loss/Expected Reward = 0.24359823/0.2435982,  RDM_Loss/RDM Accuracy = 0.15514457/0.8500000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 700 | 10000 , train_loss/Expected Reward = 0.26967773/0.2696777,  RDM_Loss/RDM Accuracy = 0.11656138/1.0000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 710 | 10000 , train_loss/Expected Reward = 0.20404086/0.2040409,  RDM_Loss/RDM Accuracy = 0.15498282/0.9500000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 720 | 10000 , train_loss/Expected Reward = 0.23215115/0.2321512,  RDM_Loss/RDM Accuracy = 0.15705611/0.9500000, mean_len = 0.287\n",
      "*****Optimizing RDM & PG***** 730 | 10000 , train_loss/Expected Reward = 0.19292896/0.1929290,  RDM_Loss/RDM Accuracy = 0.01551197/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 740 | 10000 , train_loss/Expected Reward = 0.24559562/0.2455956,  RDM_Loss/RDM Accuracy = 0.07407215/1.0000000, mean_len = 0.273\n",
      "*****Optimizing RDM & PG***** 750 | 10000 , train_loss/Expected Reward = 0.24522672/0.2452267,  RDM_Loss/RDM Accuracy = 0.01886232/1.0000000, mean_len = 0.259\n",
      "*****Optimizing RDM & PG***** 760 | 10000 , train_loss/Expected Reward = 0.20018342/0.2001834,  RDM_Loss/RDM Accuracy = 0.00986285/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 770 | 10000 , train_loss/Expected Reward = 0.23401699/0.2340170,  RDM_Loss/RDM Accuracy = 0.03627105/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 780 | 10000 , train_loss/Expected Reward = 0.19554546/0.1955455,  RDM_Loss/RDM Accuracy = 0.11098084/0.9500000, mean_len = 0.316\n",
      "*****Optimizing RDM & PG***** 790 | 10000 , train_loss/Expected Reward = 0.19383873/0.1938387,  RDM_Loss/RDM Accuracy = 0.11856750/0.9000000, mean_len = 0.271\n",
      "*****Optimizing RDM & PG***** 800 | 10000 , train_loss/Expected Reward = 0.21893071/0.2189307,  RDM_Loss/RDM Accuracy = 0.01677061/1.0000000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 810 | 10000 , train_loss/Expected Reward = 0.23643097/0.2364310,  RDM_Loss/RDM Accuracy = 0.34915254/0.9000000, mean_len = 0.279\n",
      "*****Optimizing RDM & PG***** 820 | 10000 , train_loss/Expected Reward = 0.25597409/0.2559741,  RDM_Loss/RDM Accuracy = 0.01649157/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 830 | 10000 , train_loss/Expected Reward = 0.29996322/0.2999632,  RDM_Loss/RDM Accuracy = 0.02483772/1.0000000, mean_len = 0.280\n",
      "*****Optimizing RDM & PG***** 840 | 10000 , train_loss/Expected Reward = 0.20330800/0.2033080,  RDM_Loss/RDM Accuracy = 0.08381330/0.9500000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 850 | 10000 , train_loss/Expected Reward = 0.23417644/0.2341764,  RDM_Loss/RDM Accuracy = 0.06813308/1.0000000, mean_len = 0.296\n",
      "*****Optimizing RDM & PG***** 860 | 10000 , train_loss/Expected Reward = 0.21243224/0.2124322,  RDM_Loss/RDM Accuracy = 0.09835726/0.9000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 870 | 10000 , train_loss/Expected Reward = 0.24240438/0.2424044,  RDM_Loss/RDM Accuracy = 0.02768273/1.0000000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 880 | 10000 , train_loss/Expected Reward = 0.21030789/0.2103079,  RDM_Loss/RDM Accuracy = 0.06260368/1.0000000, mean_len = 0.259\n",
      "*****Optimizing RDM & PG***** 890 | 10000 , train_loss/Expected Reward = 0.22224331/0.2222433,  RDM_Loss/RDM Accuracy = 0.11809637/0.9500000, mean_len = 0.261\n",
      "*****Optimizing RDM & PG***** 900 | 10000 , train_loss/Expected Reward = 0.23276016/0.2327602,  RDM_Loss/RDM Accuracy = 0.01522888/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 910 | 10000 , train_loss/Expected Reward = 0.23892946/0.2389295,  RDM_Loss/RDM Accuracy = 0.10283548/0.9500000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 920 | 10000 , train_loss/Expected Reward = 0.18808533/0.1880853,  RDM_Loss/RDM Accuracy = 0.10837228/0.9500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 930 | 10000 , train_loss/Expected Reward = 0.21311827/0.2131183,  RDM_Loss/RDM Accuracy = 0.28171849/0.9000000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 940 | 10000 , train_loss/Expected Reward = 0.22134440/0.2213444,  RDM_Loss/RDM Accuracy = 0.00892362/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 950 | 10000 , train_loss/Expected Reward = 0.22451709/0.2245171,  RDM_Loss/RDM Accuracy = 0.01676104/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 960 | 10000 , train_loss/Expected Reward = 0.20147407/0.2014741,  RDM_Loss/RDM Accuracy = 0.15167932/0.9500000, mean_len = 0.260\n",
      "*****Optimizing RDM & PG***** 970 | 10000 , train_loss/Expected Reward = 0.23225888/0.2322589,  RDM_Loss/RDM Accuracy = 0.00910162/1.0000000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 980 | 10000 , train_loss/Expected Reward = 0.25290698/0.2529070,  RDM_Loss/RDM Accuracy = 0.08697619/1.0000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 990 | 10000 , train_loss/Expected Reward = 0.22945037/0.2294504,  RDM_Loss/RDM Accuracy = 0.10082661/0.9500000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 1000 | 10000 , train_loss/Expected Reward = 0.19369598/0.1936960,  RDM_Loss/RDM Accuracy = 0.06522126/1.0000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 1010 | 10000 , train_loss/Expected Reward = 0.20208191/0.2020819,  RDM_Loss/RDM Accuracy = 0.14491664/0.9000000, mean_len = 0.313\n",
      "*****Optimizing RDM & PG***** 1020 | 10000 , train_loss/Expected Reward = 0.33263464/0.3326346,  RDM_Loss/RDM Accuracy = 0.03617792/1.0000000, mean_len = 0.254\n",
      "*****Optimizing RDM & PG***** 1030 | 10000 , train_loss/Expected Reward = 0.20913470/0.2091347,  RDM_Loss/RDM Accuracy = 0.01590274/1.0000000, mean_len = 0.293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 1040 | 10000 , train_loss/Expected Reward = 0.20455577/0.2045558,  RDM_Loss/RDM Accuracy = 0.03836829/0.9500000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 1050 | 10000 , train_loss/Expected Reward = 0.20363007/0.2036301,  RDM_Loss/RDM Accuracy = 0.06108401/1.0000000, mean_len = 0.314\n",
      "*****Optimizing RDM & PG***** 1060 | 10000 , train_loss/Expected Reward = 0.25946083/0.2594608,  RDM_Loss/RDM Accuracy = 0.09433495/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 1070 | 10000 , train_loss/Expected Reward = 0.19577198/0.1957720,  RDM_Loss/RDM Accuracy = 0.00378055/1.0000000, mean_len = 0.280\n",
      "*****Optimizing RDM & PG***** 1080 | 10000 , train_loss/Expected Reward = 0.22583198/0.2258320,  RDM_Loss/RDM Accuracy = 0.16336639/0.9500000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 1090 | 10000 , train_loss/Expected Reward = 0.19446086/0.1944609,  RDM_Loss/RDM Accuracy = 0.38705343/0.8500000, mean_len = 0.330\n",
      "*****Optimizing RDM & PG***** 1100 | 10000 , train_loss/Expected Reward = 0.25676622/0.2567662,  RDM_Loss/RDM Accuracy = 0.13148090/0.9500000, mean_len = 0.314\n",
      "*****Optimizing RDM & PG***** 1110 | 10000 , train_loss/Expected Reward = 0.19856493/0.1985649,  RDM_Loss/RDM Accuracy = 0.07490773/0.9500000, mean_len = 0.265\n",
      "*****Optimizing RDM & PG***** 1120 | 10000 , train_loss/Expected Reward = 0.19955191/0.1995519,  RDM_Loss/RDM Accuracy = 0.01618626/1.0000000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 1130 | 10000 , train_loss/Expected Reward = 0.20690587/0.2069059,  RDM_Loss/RDM Accuracy = 0.04393810/1.0000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 1140 | 10000 , train_loss/Expected Reward = 0.20649765/0.2064976,  RDM_Loss/RDM Accuracy = 0.03377957/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 1150 | 10000 , train_loss/Expected Reward = 0.26765738/0.2676574,  RDM_Loss/RDM Accuracy = 0.45477742/0.9000000, mean_len = 0.296\n",
      "*****Optimizing RDM & PG***** 1160 | 10000 , train_loss/Expected Reward = 0.26028873/0.2602887,  RDM_Loss/RDM Accuracy = 0.00981874/1.0000000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 1170 | 10000 , train_loss/Expected Reward = 0.24804948/0.2480495,  RDM_Loss/RDM Accuracy = 0.01275093/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 1180 | 10000 , train_loss/Expected Reward = 0.25948185/0.2594819,  RDM_Loss/RDM Accuracy = 0.00831906/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 1190 | 10000 , train_loss/Expected Reward = 0.24948384/0.2494838,  RDM_Loss/RDM Accuracy = 0.00617465/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 1200 | 10000 , train_loss/Expected Reward = 0.26452325/0.2645233,  RDM_Loss/RDM Accuracy = 0.03481322/1.0000000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 1210 | 10000 , train_loss/Expected Reward = 0.22472445/0.2247244,  RDM_Loss/RDM Accuracy = 0.05614038/1.0000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 1220 | 10000 , train_loss/Expected Reward = 0.18959655/0.1895966,  RDM_Loss/RDM Accuracy = 0.05541558/0.9500000, mean_len = 0.311\n",
      "*****Optimizing RDM & PG***** 1230 | 10000 , train_loss/Expected Reward = 0.24254256/0.2425426,  RDM_Loss/RDM Accuracy = 0.09112476/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 1240 | 10000 , train_loss/Expected Reward = 0.20593942/0.2059394,  RDM_Loss/RDM Accuracy = 0.09810477/0.9500000, mean_len = 0.296\n",
      "*****Optimizing RDM & PG***** 1250 | 10000 , train_loss/Expected Reward = 0.23310240/0.2331024,  RDM_Loss/RDM Accuracy = 0.17608614/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 1260 | 10000 , train_loss/Expected Reward = 0.25591131/0.2559113,  RDM_Loss/RDM Accuracy = 0.02675637/1.0000000, mean_len = 0.265\n",
      "*****Optimizing RDM & PG***** 1270 | 10000 , train_loss/Expected Reward = 0.25838386/0.2583839,  RDM_Loss/RDM Accuracy = 0.10676427/0.9500000, mean_len = 0.253\n",
      "*****Optimizing RDM & PG***** 1280 | 10000 , train_loss/Expected Reward = 0.19215817/0.1921582,  RDM_Loss/RDM Accuracy = 0.18056358/0.9500000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 1290 | 10000 , train_loss/Expected Reward = 0.22903844/0.2290384,  RDM_Loss/RDM Accuracy = 0.01648750/1.0000000, mean_len = 0.283\n",
      "*****Optimizing RDM & PG***** 1300 | 10000 , train_loss/Expected Reward = 0.25197291/0.2519729,  RDM_Loss/RDM Accuracy = 0.17662197/0.9000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 1310 | 10000 , train_loss/Expected Reward = 0.17610871/0.1761087,  RDM_Loss/RDM Accuracy = 0.10600123/0.9000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 1320 | 10000 , train_loss/Expected Reward = 0.22306368/0.2230637,  RDM_Loss/RDM Accuracy = 0.02724102/1.0000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 1330 | 10000 , train_loss/Expected Reward = 0.25880409/0.2588041,  RDM_Loss/RDM Accuracy = 0.06910910/1.0000000, mean_len = 0.302\n",
      "*****Optimizing RDM & PG***** 1340 | 10000 , train_loss/Expected Reward = 0.23878518/0.2387852,  RDM_Loss/RDM Accuracy = 0.03269658/1.0000000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 1350 | 10000 , train_loss/Expected Reward = 0.22270767/0.2227077,  RDM_Loss/RDM Accuracy = 0.10689770/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 1360 | 10000 , train_loss/Expected Reward = 0.22178060/0.2217806,  RDM_Loss/RDM Accuracy = 0.39559364/0.8500000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 1370 | 10000 , train_loss/Expected Reward = 0.27893279/0.2789328,  RDM_Loss/RDM Accuracy = 0.09681234/0.9500000, mean_len = 0.275\n",
      "*****Optimizing RDM & PG***** 1380 | 10000 , train_loss/Expected Reward = 0.26053234/0.2605323,  RDM_Loss/RDM Accuracy = 1.00905764/0.6500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 1390 | 10000 , train_loss/Expected Reward = 0.20418340/0.2041834,  RDM_Loss/RDM Accuracy = 0.04364514/1.0000000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 1400 | 10000 , train_loss/Expected Reward = 0.23789336/0.2378934,  RDM_Loss/RDM Accuracy = 0.53769577/0.8500000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 1410 | 10000 , train_loss/Expected Reward = 0.24283730/0.2428373,  RDM_Loss/RDM Accuracy = 0.02747454/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 1420 | 10000 , train_loss/Expected Reward = 0.21504714/0.2150471,  RDM_Loss/RDM Accuracy = 0.03736331/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 1430 | 10000 , train_loss/Expected Reward = 0.24267715/0.2426771,  RDM_Loss/RDM Accuracy = 0.01508174/1.0000000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 1440 | 10000 , train_loss/Expected Reward = 0.22694594/0.2269459,  RDM_Loss/RDM Accuracy = 0.04338171/1.0000000, mean_len = 0.316\n",
      "*****Optimizing RDM & PG***** 1450 | 10000 , train_loss/Expected Reward = 0.24433270/0.2443327,  RDM_Loss/RDM Accuracy = 0.02998059/1.0000000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 1460 | 10000 , train_loss/Expected Reward = 0.21654641/0.2165464,  RDM_Loss/RDM Accuracy = 0.10295423/0.9500000, mean_len = 0.311\n",
      "*****Optimizing RDM & PG***** 1470 | 10000 , train_loss/Expected Reward = 0.22026491/0.2202649,  RDM_Loss/RDM Accuracy = 0.07034749/1.0000000, mean_len = 0.279\n",
      "*****Optimizing RDM & PG***** 1480 | 10000 , train_loss/Expected Reward = 0.13624764/0.1362476,  RDM_Loss/RDM Accuracy = 0.11958703/0.9000000, mean_len = 0.349\n",
      "*****Optimizing RDM & PG***** 1490 | 10000 , train_loss/Expected Reward = 0.21222765/0.2122276,  RDM_Loss/RDM Accuracy = 0.09845219/1.0000000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 1500 | 10000 , train_loss/Expected Reward = 0.18748826/0.1874883,  RDM_Loss/RDM Accuracy = 0.19860281/0.9000000, mean_len = 0.325\n",
      "*****Optimizing RDM & PG***** 1510 | 10000 , train_loss/Expected Reward = 0.21973518/0.2197352,  RDM_Loss/RDM Accuracy = 0.00803803/1.0000000, mean_len = 0.320\n",
      "*****Optimizing RDM & PG***** 1520 | 10000 , train_loss/Expected Reward = 0.20232688/0.2023269,  RDM_Loss/RDM Accuracy = 0.05177835/1.0000000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 1530 | 10000 , train_loss/Expected Reward = 0.20807673/0.2080767,  RDM_Loss/RDM Accuracy = 0.04567848/1.0000000, mean_len = 0.268\n",
      "*****Optimizing RDM & PG***** 1540 | 10000 , train_loss/Expected Reward = 0.26662350/0.2666235,  RDM_Loss/RDM Accuracy = 0.02081249/1.0000000, mean_len = 0.266\n",
      "*****Optimizing RDM & PG***** 1550 | 10000 , train_loss/Expected Reward = 0.17801805/0.1780180,  RDM_Loss/RDM Accuracy = 0.03944837/1.0000000, mean_len = 0.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 1560 | 10000 , train_loss/Expected Reward = 0.16749661/0.1674966,  RDM_Loss/RDM Accuracy = 0.03922372/1.0000000, mean_len = 0.330\n",
      "*****Optimizing RDM & PG***** 1570 | 10000 , train_loss/Expected Reward = 0.23415815/0.2341582,  RDM_Loss/RDM Accuracy = 0.01895372/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 1580 | 10000 , train_loss/Expected Reward = 0.21277909/0.2127791,  RDM_Loss/RDM Accuracy = 0.10441211/0.9000000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 1590 | 10000 , train_loss/Expected Reward = 0.24609045/0.2460904,  RDM_Loss/RDM Accuracy = 0.03919357/1.0000000, mean_len = 0.332\n",
      "*****Optimizing RDM & PG***** 1600 | 10000 , train_loss/Expected Reward = 0.21059294/0.2105929,  RDM_Loss/RDM Accuracy = 0.33498996/0.9000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 1610 | 10000 , train_loss/Expected Reward = 0.26239321/0.2623932,  RDM_Loss/RDM Accuracy = 0.25978473/0.9500000, mean_len = 0.264\n",
      "*****Optimizing RDM & PG***** 1620 | 10000 , train_loss/Expected Reward = 0.23153846/0.2315385,  RDM_Loss/RDM Accuracy = 0.01672336/1.0000000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 1630 | 10000 , train_loss/Expected Reward = 0.25349314/0.2534931,  RDM_Loss/RDM Accuracy = 0.01365134/1.0000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 1640 | 10000 , train_loss/Expected Reward = 0.22768234/0.2276823,  RDM_Loss/RDM Accuracy = 0.05628078/1.0000000, mean_len = 0.314\n",
      "*****Optimizing RDM & PG***** 1650 | 10000 , train_loss/Expected Reward = 0.21154654/0.2115465,  RDM_Loss/RDM Accuracy = 0.11193705/0.9000000, mean_len = 0.328\n",
      "*****Optimizing RDM & PG***** 1660 | 10000 , train_loss/Expected Reward = 0.23866523/0.2386652,  RDM_Loss/RDM Accuracy = 0.05470249/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 1670 | 10000 , train_loss/Expected Reward = 0.21379904/0.2137990,  RDM_Loss/RDM Accuracy = 0.06588157/1.0000000, mean_len = 0.324\n",
      "*****Optimizing RDM & PG***** 1680 | 10000 , train_loss/Expected Reward = 0.25608170/0.2560817,  RDM_Loss/RDM Accuracy = 0.07090569/0.9500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 1690 | 10000 , train_loss/Expected Reward = 0.19803113/0.1980311,  RDM_Loss/RDM Accuracy = 0.01330980/1.0000000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 1700 | 10000 , train_loss/Expected Reward = 0.22712309/0.2271231,  RDM_Loss/RDM Accuracy = 0.01033599/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 1710 | 10000 , train_loss/Expected Reward = 0.19779710/0.1977971,  RDM_Loss/RDM Accuracy = 0.05394346/1.0000000, mean_len = 0.309\n",
      "*****Optimizing RDM & PG***** 1720 | 10000 , train_loss/Expected Reward = 0.23535318/0.2353532,  RDM_Loss/RDM Accuracy = 0.20062023/0.9500000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 1730 | 10000 , train_loss/Expected Reward = 0.28199090/0.2819909,  RDM_Loss/RDM Accuracy = 0.00986665/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 1740 | 10000 , train_loss/Expected Reward = 0.21694225/0.2169423,  RDM_Loss/RDM Accuracy = 0.06219846/0.9500000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 1750 | 10000 , train_loss/Expected Reward = 0.23321968/0.2332197,  RDM_Loss/RDM Accuracy = 0.10329314/0.9500000, mean_len = 0.266\n",
      "*****Optimizing RDM & PG***** 1760 | 10000 , train_loss/Expected Reward = 0.22768360/0.2276836,  RDM_Loss/RDM Accuracy = 0.02369398/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 1770 | 10000 , train_loss/Expected Reward = 0.28217493/0.2821749,  RDM_Loss/RDM Accuracy = 0.21133126/0.9500000, mean_len = 0.257\n",
      "*****Optimizing RDM & PG***** 1780 | 10000 , train_loss/Expected Reward = 0.20267240/0.2026724,  RDM_Loss/RDM Accuracy = 0.01806842/1.0000000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 1790 | 10000 , train_loss/Expected Reward = 0.19369184/0.1936918,  RDM_Loss/RDM Accuracy = 0.06175099/1.0000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 1800 | 10000 , train_loss/Expected Reward = 0.18677371/0.1867737,  RDM_Loss/RDM Accuracy = 0.15430745/0.9000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 1810 | 10000 , train_loss/Expected Reward = 0.19365079/0.1936508,  RDM_Loss/RDM Accuracy = 0.03442191/0.9500000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 1820 | 10000 , train_loss/Expected Reward = 0.28858668/0.2885867,  RDM_Loss/RDM Accuracy = 0.01663946/1.0000000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 1830 | 10000 , train_loss/Expected Reward = 0.21659833/0.2165983,  RDM_Loss/RDM Accuracy = 0.06485216/0.9500000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 1840 | 10000 , train_loss/Expected Reward = 0.20204117/0.2020412,  RDM_Loss/RDM Accuracy = 0.06692927/1.0000000, mean_len = 0.316\n",
      "*****Optimizing RDM & PG***** 1850 | 10000 , train_loss/Expected Reward = 0.20380009/0.2038001,  RDM_Loss/RDM Accuracy = 0.07501809/0.9500000, mean_len = 0.328\n",
      "*****Optimizing RDM & PG***** 1860 | 10000 , train_loss/Expected Reward = 0.22955976/0.2295598,  RDM_Loss/RDM Accuracy = 0.17183585/0.9500000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 1870 | 10000 , train_loss/Expected Reward = 0.25313308/0.2531331,  RDM_Loss/RDM Accuracy = 0.10875629/1.0000000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 1880 | 10000 , train_loss/Expected Reward = 0.20565106/0.2056511,  RDM_Loss/RDM Accuracy = 0.72589558/0.8000000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 1890 | 10000 , train_loss/Expected Reward = 0.25762825/0.2576283,  RDM_Loss/RDM Accuracy = 0.06514894/0.9500000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 1900 | 10000 , train_loss/Expected Reward = 0.24547123/0.2454712,  RDM_Loss/RDM Accuracy = 0.11220433/0.9500000, mean_len = 0.274\n",
      "*****Optimizing RDM & PG***** 1910 | 10000 , train_loss/Expected Reward = 0.22935930/0.2293593,  RDM_Loss/RDM Accuracy = 0.02876658/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 1920 | 10000 , train_loss/Expected Reward = 0.22957473/0.2295747,  RDM_Loss/RDM Accuracy = 0.06962995/1.0000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 1930 | 10000 , train_loss/Expected Reward = 0.24469090/0.2446909,  RDM_Loss/RDM Accuracy = 0.03544711/1.0000000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 1940 | 10000 , train_loss/Expected Reward = 0.23792183/0.2379218,  RDM_Loss/RDM Accuracy = 0.21716265/0.9500000, mean_len = 0.267\n",
      "*****Optimizing RDM & PG***** 1950 | 10000 , train_loss/Expected Reward = 0.21092504/0.2109250,  RDM_Loss/RDM Accuracy = 0.16897750/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 1960 | 10000 , train_loss/Expected Reward = 0.22012832/0.2201283,  RDM_Loss/RDM Accuracy = 0.01258932/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 1970 | 10000 , train_loss/Expected Reward = 0.23531977/0.2353198,  RDM_Loss/RDM Accuracy = 0.01310926/1.0000000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 1980 | 10000 , train_loss/Expected Reward = 0.20525123/0.2052512,  RDM_Loss/RDM Accuracy = 0.08259047/0.9500000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 1990 | 10000 , train_loss/Expected Reward = 0.21935967/0.2193597,  RDM_Loss/RDM Accuracy = 0.05714796/1.0000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 2000 | 10000 , train_loss/Expected Reward = 0.22595106/0.2259511,  RDM_Loss/RDM Accuracy = 0.08862220/1.0000000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 2010 | 10000 , train_loss/Expected Reward = 0.21027427/0.2102743,  RDM_Loss/RDM Accuracy = 0.05451358/0.9500000, mean_len = 0.307\n",
      "*****Optimizing RDM & PG***** 2020 | 10000 , train_loss/Expected Reward = 0.19175496/0.1917550,  RDM_Loss/RDM Accuracy = 0.22353363/0.9500000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 2030 | 10000 , train_loss/Expected Reward = 0.21270007/0.2127001,  RDM_Loss/RDM Accuracy = 0.02705660/1.0000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 2040 | 10000 , train_loss/Expected Reward = 0.19273346/0.1927335,  RDM_Loss/RDM Accuracy = 0.03732220/1.0000000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 2050 | 10000 , train_loss/Expected Reward = 0.22164349/0.2216435,  RDM_Loss/RDM Accuracy = 0.02986907/1.0000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 2060 | 10000 , train_loss/Expected Reward = 0.20225746/0.2022575,  RDM_Loss/RDM Accuracy = 0.10046721/0.9500000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 2070 | 10000 , train_loss/Expected Reward = 0.16290624/0.1629062,  RDM_Loss/RDM Accuracy = 0.20517842/0.9500000, mean_len = 0.343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 2080 | 10000 , train_loss/Expected Reward = 0.19376960/0.1937696,  RDM_Loss/RDM Accuracy = 0.01510480/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 2090 | 10000 , train_loss/Expected Reward = 0.21779183/0.2177918,  RDM_Loss/RDM Accuracy = 0.43525994/0.8500000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 2100 | 10000 , train_loss/Expected Reward = 0.27215149/0.2721515,  RDM_Loss/RDM Accuracy = 0.12491938/0.9000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 2110 | 10000 , train_loss/Expected Reward = 0.22681377/0.2268138,  RDM_Loss/RDM Accuracy = 0.01255062/1.0000000, mean_len = 0.315\n",
      "*****Optimizing RDM & PG***** 2120 | 10000 , train_loss/Expected Reward = 0.20298302/0.2029830,  RDM_Loss/RDM Accuracy = 0.06105240/1.0000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 2130 | 10000 , train_loss/Expected Reward = 0.27512446/0.2751245,  RDM_Loss/RDM Accuracy = 0.03290658/1.0000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 2140 | 10000 , train_loss/Expected Reward = 0.22481243/0.2248124,  RDM_Loss/RDM Accuracy = 0.04400295/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 2150 | 10000 , train_loss/Expected Reward = 0.19915297/0.1991530,  RDM_Loss/RDM Accuracy = 0.35741177/0.7500000, mean_len = 0.331\n",
      "*****Optimizing RDM & PG***** 2160 | 10000 , train_loss/Expected Reward = 0.24141317/0.2414132,  RDM_Loss/RDM Accuracy = 0.04151392/1.0000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 2170 | 10000 , train_loss/Expected Reward = 0.20749438/0.2074944,  RDM_Loss/RDM Accuracy = 0.65707213/0.7500000, mean_len = 0.331\n",
      "*****Optimizing RDM & PG***** 2180 | 10000 , train_loss/Expected Reward = 0.18150439/0.1815044,  RDM_Loss/RDM Accuracy = 0.08751576/1.0000000, mean_len = 0.327\n",
      "*****Optimizing RDM & PG***** 2190 | 10000 , train_loss/Expected Reward = 0.23822282/0.2382228,  RDM_Loss/RDM Accuracy = 0.34655264/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 2200 | 10000 , train_loss/Expected Reward = 0.23409570/0.2340957,  RDM_Loss/RDM Accuracy = 0.01069115/1.0000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 2210 | 10000 , train_loss/Expected Reward = 0.22131113/0.2213111,  RDM_Loss/RDM Accuracy = 0.14330001/0.9500000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 2220 | 10000 , train_loss/Expected Reward = 0.21804709/0.2180471,  RDM_Loss/RDM Accuracy = 0.03504459/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 2230 | 10000 , train_loss/Expected Reward = 0.18354268/0.1835427,  RDM_Loss/RDM Accuracy = 0.03511108/1.0000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 2240 | 10000 , train_loss/Expected Reward = 0.25625112/0.2562511,  RDM_Loss/RDM Accuracy = 0.10197219/1.0000000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 2250 | 10000 , train_loss/Expected Reward = 0.20878225/0.2087823,  RDM_Loss/RDM Accuracy = 0.13148296/0.9500000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 2260 | 10000 , train_loss/Expected Reward = 0.21506676/0.2150668,  RDM_Loss/RDM Accuracy = 0.26244438/0.9500000, mean_len = 0.287\n",
      "*****Optimizing RDM & PG***** 2270 | 10000 , train_loss/Expected Reward = 0.22492482/0.2249248,  RDM_Loss/RDM Accuracy = 0.03216701/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 2280 | 10000 , train_loss/Expected Reward = 0.23478201/0.2347820,  RDM_Loss/RDM Accuracy = 0.06116039/1.0000000, mean_len = 0.287\n",
      "*****Optimizing RDM & PG***** 2290 | 10000 , train_loss/Expected Reward = 0.21508923/0.2150892,  RDM_Loss/RDM Accuracy = 0.31822217/0.8500000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 2300 | 10000 , train_loss/Expected Reward = 0.24584669/0.2458467,  RDM_Loss/RDM Accuracy = 0.00565730/1.0000000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 2310 | 10000 , train_loss/Expected Reward = 0.23121862/0.2312186,  RDM_Loss/RDM Accuracy = 0.04845370/1.0000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 2320 | 10000 , train_loss/Expected Reward = 0.20171702/0.2017170,  RDM_Loss/RDM Accuracy = 0.01497717/1.0000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 2330 | 10000 , train_loss/Expected Reward = 0.30217678/0.3021768,  RDM_Loss/RDM Accuracy = 0.00947972/1.0000000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 2340 | 10000 , train_loss/Expected Reward = 0.18494213/0.1849421,  RDM_Loss/RDM Accuracy = 0.24655698/0.9500000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 2350 | 10000 , train_loss/Expected Reward = 0.21418689/0.2141869,  RDM_Loss/RDM Accuracy = 0.02265313/1.0000000, mean_len = 0.282\n",
      "*****Optimizing RDM & PG***** 2360 | 10000 , train_loss/Expected Reward = 0.18513445/0.1851345,  RDM_Loss/RDM Accuracy = 0.05692543/0.9500000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 2370 | 10000 , train_loss/Expected Reward = 0.25618595/0.2561859,  RDM_Loss/RDM Accuracy = 0.04568005/0.9500000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 2380 | 10000 , train_loss/Expected Reward = 0.20046147/0.2004615,  RDM_Loss/RDM Accuracy = 0.03056283/1.0000000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 2390 | 10000 , train_loss/Expected Reward = 0.17812475/0.1781247,  RDM_Loss/RDM Accuracy = 0.07694693/0.9500000, mean_len = 0.326\n",
      "*****Optimizing RDM & PG***** 2400 | 10000 , train_loss/Expected Reward = 0.27242526/0.2724253,  RDM_Loss/RDM Accuracy = 0.23123460/0.9500000, mean_len = 0.267\n",
      "*****Optimizing RDM & PG***** 2410 | 10000 , train_loss/Expected Reward = 0.21377435/0.2137743,  RDM_Loss/RDM Accuracy = 0.03888443/1.0000000, mean_len = 0.313\n",
      "*****Optimizing RDM & PG***** 2420 | 10000 , train_loss/Expected Reward = 0.19845867/0.1984587,  RDM_Loss/RDM Accuracy = 0.01386074/1.0000000, mean_len = 0.279\n",
      "*****Optimizing RDM & PG***** 2430 | 10000 , train_loss/Expected Reward = 0.22105156/0.2210516,  RDM_Loss/RDM Accuracy = 0.05616968/1.0000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 2440 | 10000 , train_loss/Expected Reward = 0.22017705/0.2201771,  RDM_Loss/RDM Accuracy = 0.02612771/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 2450 | 10000 , train_loss/Expected Reward = 0.20472259/0.2047226,  RDM_Loss/RDM Accuracy = 0.07085072/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 2460 | 10000 , train_loss/Expected Reward = 0.20647038/0.2064704,  RDM_Loss/RDM Accuracy = 0.03800225/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 2470 | 10000 , train_loss/Expected Reward = 0.23726876/0.2372688,  RDM_Loss/RDM Accuracy = 0.15040986/0.9000000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 2480 | 10000 , train_loss/Expected Reward = 0.22890086/0.2289009,  RDM_Loss/RDM Accuracy = 0.00932332/1.0000000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 2490 | 10000 , train_loss/Expected Reward = 0.22217878/0.2221788,  RDM_Loss/RDM Accuracy = 0.10333957/0.9000000, mean_len = 0.264\n",
      "*****Optimizing RDM & PG***** 2500 | 10000 , train_loss/Expected Reward = 0.26660654/0.2666065,  RDM_Loss/RDM Accuracy = 0.05035629/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 2510 | 10000 , train_loss/Expected Reward = 0.26847047/0.2684705,  RDM_Loss/RDM Accuracy = 0.21047834/0.9500000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 2520 | 10000 , train_loss/Expected Reward = 0.21766497/0.2176650,  RDM_Loss/RDM Accuracy = 0.03074869/1.0000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 2530 | 10000 , train_loss/Expected Reward = 0.20901257/0.2090126,  RDM_Loss/RDM Accuracy = 0.33175924/0.9000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 2540 | 10000 , train_loss/Expected Reward = 0.20972307/0.2097231,  RDM_Loss/RDM Accuracy = 0.04332810/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 2550 | 10000 , train_loss/Expected Reward = 0.22373895/0.2237390,  RDM_Loss/RDM Accuracy = 0.41413215/0.9500000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 2560 | 10000 , train_loss/Expected Reward = 0.26292353/0.2629235,  RDM_Loss/RDM Accuracy = 0.15001372/0.9500000, mean_len = 0.259\n",
      "*****Optimizing RDM & PG***** 2570 | 10000 , train_loss/Expected Reward = 0.21537650/0.2153765,  RDM_Loss/RDM Accuracy = 0.03182627/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 2580 | 10000 , train_loss/Expected Reward = 0.19080027/0.1908003,  RDM_Loss/RDM Accuracy = 0.30727741/0.9500000, mean_len = 0.311\n",
      "*****Optimizing RDM & PG***** 2590 | 10000 , train_loss/Expected Reward = 0.22397845/0.2239785,  RDM_Loss/RDM Accuracy = 0.08975212/0.9500000, mean_len = 0.324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 2600 | 10000 , train_loss/Expected Reward = 0.23711057/0.2371106,  RDM_Loss/RDM Accuracy = 0.07090273/0.9500000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 2610 | 10000 , train_loss/Expected Reward = 0.24152178/0.2415218,  RDM_Loss/RDM Accuracy = 0.05450480/1.0000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 2620 | 10000 , train_loss/Expected Reward = 0.23017306/0.2301731,  RDM_Loss/RDM Accuracy = 0.04295926/1.0000000, mean_len = 0.274\n",
      "*****Optimizing RDM & PG***** 2630 | 10000 , train_loss/Expected Reward = 0.23178140/0.2317814,  RDM_Loss/RDM Accuracy = 0.07192231/1.0000000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 2640 | 10000 , train_loss/Expected Reward = 0.22234944/0.2223494,  RDM_Loss/RDM Accuracy = 0.30893037/0.8500000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 2650 | 10000 , train_loss/Expected Reward = 0.29186646/0.2918665,  RDM_Loss/RDM Accuracy = 0.19960357/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 2660 | 10000 , train_loss/Expected Reward = 0.22907161/0.2290716,  RDM_Loss/RDM Accuracy = 0.05364069/1.0000000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 2670 | 10000 , train_loss/Expected Reward = 0.22190687/0.2219069,  RDM_Loss/RDM Accuracy = 0.49366733/0.8500000, mean_len = 0.320\n",
      "*****Optimizing RDM & PG***** 2680 | 10000 , train_loss/Expected Reward = 0.22829380/0.2282938,  RDM_Loss/RDM Accuracy = 0.00978088/1.0000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 2690 | 10000 , train_loss/Expected Reward = 0.26096199/0.2609620,  RDM_Loss/RDM Accuracy = 0.05671191/1.0000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 2700 | 10000 , train_loss/Expected Reward = 0.20322357/0.2032236,  RDM_Loss/RDM Accuracy = 0.04001577/1.0000000, mean_len = 0.306\n",
      "*****Optimizing RDM & PG***** 2710 | 10000 , train_loss/Expected Reward = 0.18607657/0.1860766,  RDM_Loss/RDM Accuracy = 0.11440910/0.9500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 2720 | 10000 , train_loss/Expected Reward = 0.19808236/0.1980824,  RDM_Loss/RDM Accuracy = 0.09631837/0.9500000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 2730 | 10000 , train_loss/Expected Reward = 0.22467291/0.2246729,  RDM_Loss/RDM Accuracy = 0.59556311/0.9000000, mean_len = 0.285\n",
      "*****Optimizing RDM & PG***** 2740 | 10000 , train_loss/Expected Reward = 0.22392100/0.2239210,  RDM_Loss/RDM Accuracy = 0.15955204/0.9500000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 2750 | 10000 , train_loss/Expected Reward = 0.24785656/0.2478566,  RDM_Loss/RDM Accuracy = 0.01611108/1.0000000, mean_len = 0.332\n",
      "*****Optimizing RDM & PG***** 2760 | 10000 , train_loss/Expected Reward = 0.21681552/0.2168155,  RDM_Loss/RDM Accuracy = 0.02244138/1.0000000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 2770 | 10000 , train_loss/Expected Reward = 0.21665130/0.2166513,  RDM_Loss/RDM Accuracy = 0.08623298/0.9500000, mean_len = 0.307\n",
      "*****Optimizing RDM & PG***** 2780 | 10000 , train_loss/Expected Reward = 0.21903190/0.2190319,  RDM_Loss/RDM Accuracy = 0.09444620/1.0000000, mean_len = 0.304\n",
      "*****Optimizing RDM & PG***** 2790 | 10000 , train_loss/Expected Reward = 0.28709533/0.2870953,  RDM_Loss/RDM Accuracy = 0.08683448/1.0000000, mean_len = 0.268\n",
      "*****Optimizing RDM & PG***** 2800 | 10000 , train_loss/Expected Reward = 0.24844444/0.2484444,  RDM_Loss/RDM Accuracy = 0.01172286/1.0000000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 2810 | 10000 , train_loss/Expected Reward = 0.24517041/0.2451704,  RDM_Loss/RDM Accuracy = 0.25409648/0.9000000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 2820 | 10000 , train_loss/Expected Reward = 0.20678158/0.2067816,  RDM_Loss/RDM Accuracy = 0.00652909/1.0000000, mean_len = 0.322\n",
      "*****Optimizing RDM & PG***** 2830 | 10000 , train_loss/Expected Reward = 0.23872663/0.2387266,  RDM_Loss/RDM Accuracy = 0.17622535/0.9000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 2840 | 10000 , train_loss/Expected Reward = 0.25044385/0.2504439,  RDM_Loss/RDM Accuracy = 0.03059669/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 2850 | 10000 , train_loss/Expected Reward = 0.19460626/0.1946063,  RDM_Loss/RDM Accuracy = 0.16675630/0.9500000, mean_len = 0.317\n",
      "*****Optimizing RDM & PG***** 2860 | 10000 , train_loss/Expected Reward = 0.19075489/0.1907549,  RDM_Loss/RDM Accuracy = 0.35428417/0.8500000, mean_len = 0.320\n",
      "*****Optimizing RDM & PG***** 2870 | 10000 , train_loss/Expected Reward = 0.23690990/0.2369099,  RDM_Loss/RDM Accuracy = 0.21040563/0.9500000, mean_len = 0.271\n",
      "*****Optimizing RDM & PG***** 2880 | 10000 , train_loss/Expected Reward = 0.21777675/0.2177767,  RDM_Loss/RDM Accuracy = 0.26695678/0.9500000, mean_len = 0.320\n",
      "*****Optimizing RDM & PG***** 2890 | 10000 , train_loss/Expected Reward = 0.21911465/0.2191146,  RDM_Loss/RDM Accuracy = 0.08887878/0.9500000, mean_len = 0.256\n",
      "*****Optimizing RDM & PG***** 2900 | 10000 , train_loss/Expected Reward = 0.21053953/0.2105395,  RDM_Loss/RDM Accuracy = 0.03225746/1.0000000, mean_len = 0.312\n",
      "*****Optimizing RDM & PG***** 2910 | 10000 , train_loss/Expected Reward = 0.19868807/0.1986881,  RDM_Loss/RDM Accuracy = 0.13696100/0.9500000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 2920 | 10000 , train_loss/Expected Reward = 0.21149809/0.2114981,  RDM_Loss/RDM Accuracy = 0.04683277/0.9500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 2930 | 10000 , train_loss/Expected Reward = 0.22126514/0.2212651,  RDM_Loss/RDM Accuracy = 0.03186980/1.0000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 2940 | 10000 , train_loss/Expected Reward = 0.22711844/0.2271184,  RDM_Loss/RDM Accuracy = 0.25091735/0.8000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 2950 | 10000 , train_loss/Expected Reward = 0.23096096/0.2309610,  RDM_Loss/RDM Accuracy = 0.02877466/1.0000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 2960 | 10000 , train_loss/Expected Reward = 0.20981951/0.2098195,  RDM_Loss/RDM Accuracy = 0.07422485/0.9500000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 2970 | 10000 , train_loss/Expected Reward = 0.21485040/0.2148504,  RDM_Loss/RDM Accuracy = 0.07096815/1.0000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 2980 | 10000 , train_loss/Expected Reward = 0.20063680/0.2006368,  RDM_Loss/RDM Accuracy = 0.41012618/0.9000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 2990 | 10000 , train_loss/Expected Reward = 0.19694241/0.1969424,  RDM_Loss/RDM Accuracy = 0.02416036/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 3000 | 10000 , train_loss/Expected Reward = 0.24082177/0.2408218,  RDM_Loss/RDM Accuracy = 0.13416114/0.9500000, mean_len = 0.310\n",
      "*****Optimizing RDM & PG***** 3010 | 10000 , train_loss/Expected Reward = 0.17661475/0.1766148,  RDM_Loss/RDM Accuracy = 0.26976302/0.9000000, mean_len = 0.311\n",
      "*****Optimizing RDM & PG***** 3020 | 10000 , train_loss/Expected Reward = 0.18183700/0.1818370,  RDM_Loss/RDM Accuracy = 0.04256598/1.0000000, mean_len = 0.315\n",
      "*****Optimizing RDM & PG***** 3030 | 10000 , train_loss/Expected Reward = 0.27814386/0.2781439,  RDM_Loss/RDM Accuracy = 0.37887925/0.8500000, mean_len = 0.290\n",
      "*****Optimizing RDM & PG***** 3040 | 10000 , train_loss/Expected Reward = 0.23235984/0.2323598,  RDM_Loss/RDM Accuracy = 0.02867263/1.0000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 3050 | 10000 , train_loss/Expected Reward = 0.20987655/0.2098765,  RDM_Loss/RDM Accuracy = 0.39920312/0.9000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 3060 | 10000 , train_loss/Expected Reward = 0.20601781/0.2060178,  RDM_Loss/RDM Accuracy = 0.02274654/1.0000000, mean_len = 0.283\n",
      "*****Optimizing RDM & PG***** 3070 | 10000 , train_loss/Expected Reward = 0.16987011/0.1698701,  RDM_Loss/RDM Accuracy = 0.13894451/0.9500000, mean_len = 0.320\n",
      "*****Optimizing RDM & PG***** 3080 | 10000 , train_loss/Expected Reward = 0.25943298/0.2594330,  RDM_Loss/RDM Accuracy = 0.21538828/0.9000000, mean_len = 0.269\n",
      "*****Optimizing RDM & PG***** 3090 | 10000 , train_loss/Expected Reward = 0.23589344/0.2358934,  RDM_Loss/RDM Accuracy = 0.00761301/1.0000000, mean_len = 0.296\n",
      "*****Optimizing RDM & PG***** 3100 | 10000 , train_loss/Expected Reward = 0.23065521/0.2306552,  RDM_Loss/RDM Accuracy = 0.01150380/1.0000000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 3110 | 10000 , train_loss/Expected Reward = 0.19817423/0.1981742,  RDM_Loss/RDM Accuracy = 0.01518051/1.0000000, mean_len = 0.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 3120 | 10000 , train_loss/Expected Reward = 0.23814025/0.2381403,  RDM_Loss/RDM Accuracy = 0.06603328/0.9500000, mean_len = 0.296\n",
      "*****Optimizing RDM & PG***** 3130 | 10000 , train_loss/Expected Reward = 0.22489734/0.2248973,  RDM_Loss/RDM Accuracy = 0.24646990/0.9500000, mean_len = 0.293\n",
      "*****Optimizing RDM & PG***** 3140 | 10000 , train_loss/Expected Reward = 0.25460239/0.2546024,  RDM_Loss/RDM Accuracy = 0.01588176/1.0000000, mean_len = 0.313\n",
      "*****Optimizing RDM & PG***** 3150 | 10000 , train_loss/Expected Reward = 0.22159871/0.2215987,  RDM_Loss/RDM Accuracy = 0.04992288/0.9500000, mean_len = 0.330\n",
      "*****Optimizing RDM & PG***** 3160 | 10000 , train_loss/Expected Reward = 0.27174877/0.2717488,  RDM_Loss/RDM Accuracy = 0.01291234/1.0000000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 3170 | 10000 , train_loss/Expected Reward = 0.27554596/0.2755460,  RDM_Loss/RDM Accuracy = 0.03324949/1.0000000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 3180 | 10000 , train_loss/Expected Reward = 0.22831939/0.2283194,  RDM_Loss/RDM Accuracy = 0.02673062/1.0000000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 3190 | 10000 , train_loss/Expected Reward = 0.20923876/0.2092388,  RDM_Loss/RDM Accuracy = 0.33328289/0.8500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 3200 | 10000 , train_loss/Expected Reward = 0.21882117/0.2188212,  RDM_Loss/RDM Accuracy = 0.07145756/0.9500000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 3210 | 10000 , train_loss/Expected Reward = 0.24685836/0.2468584,  RDM_Loss/RDM Accuracy = 0.01987414/1.0000000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 3220 | 10000 , train_loss/Expected Reward = 0.24944048/0.2494405,  RDM_Loss/RDM Accuracy = 0.09942103/0.9500000, mean_len = 0.244\n",
      "*****Optimizing RDM & PG***** 3230 | 10000 , train_loss/Expected Reward = 0.24648982/0.2464898,  RDM_Loss/RDM Accuracy = 0.03886829/1.0000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 3240 | 10000 , train_loss/Expected Reward = 0.23420373/0.2342037,  RDM_Loss/RDM Accuracy = 0.06305519/0.9500000, mean_len = 0.262\n",
      "*****Optimizing RDM & PG***** 3250 | 10000 , train_loss/Expected Reward = 0.26219329/0.2621933,  RDM_Loss/RDM Accuracy = 0.07094274/1.0000000, mean_len = 0.269\n",
      "*****Optimizing RDM & PG***** 3260 | 10000 , train_loss/Expected Reward = 0.22397605/0.2239761,  RDM_Loss/RDM Accuracy = 0.16795710/0.9000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 3270 | 10000 , train_loss/Expected Reward = 0.25342680/0.2534268,  RDM_Loss/RDM Accuracy = 0.03191715/1.0000000, mean_len = 0.277\n",
      "*****Optimizing RDM & PG***** 3280 | 10000 , train_loss/Expected Reward = 0.20774620/0.2077462,  RDM_Loss/RDM Accuracy = 0.09813186/0.9000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 3290 | 10000 , train_loss/Expected Reward = 0.23932534/0.2393253,  RDM_Loss/RDM Accuracy = 0.05852328/1.0000000, mean_len = 0.305\n",
      "*****Optimizing RDM & PG***** 3300 | 10000 , train_loss/Expected Reward = 0.26003076/0.2600308,  RDM_Loss/RDM Accuracy = 0.00797112/1.0000000, mean_len = 0.269\n",
      "*****Optimizing RDM & PG***** 3310 | 10000 , train_loss/Expected Reward = 0.27107648/0.2710765,  RDM_Loss/RDM Accuracy = 0.04857297/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 3320 | 10000 , train_loss/Expected Reward = 0.24158908/0.2415891,  RDM_Loss/RDM Accuracy = 0.30903310/0.9500000, mean_len = 0.276\n",
      "*****Optimizing RDM & PG***** 3330 | 10000 , train_loss/Expected Reward = 0.21949758/0.2194976,  RDM_Loss/RDM Accuracy = 0.17026661/0.9500000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 3340 | 10000 , train_loss/Expected Reward = 0.22043000/0.2204300,  RDM_Loss/RDM Accuracy = 0.39813507/0.9500000, mean_len = 0.273\n",
      "*****Optimizing RDM & PG***** 3350 | 10000 , train_loss/Expected Reward = 0.22830874/0.2283087,  RDM_Loss/RDM Accuracy = 0.17648432/0.9500000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 3360 | 10000 , train_loss/Expected Reward = 0.18267330/0.1826733,  RDM_Loss/RDM Accuracy = 0.06578543/0.9500000, mean_len = 0.327\n",
      "*****Optimizing RDM & PG***** 3370 | 10000 , train_loss/Expected Reward = 0.22585018/0.2258502,  RDM_Loss/RDM Accuracy = 0.32293299/0.9500000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 3380 | 10000 , train_loss/Expected Reward = 0.24496079/0.2449608,  RDM_Loss/RDM Accuracy = 0.09356540/0.9500000, mean_len = 0.268\n",
      "*****Optimizing RDM & PG***** 3390 | 10000 , train_loss/Expected Reward = 0.17379679/0.1737968,  RDM_Loss/RDM Accuracy = 0.14321022/0.9500000, mean_len = 0.318\n",
      "*****Optimizing RDM & PG***** 3400 | 10000 , train_loss/Expected Reward = 0.26224113/0.2622411,  RDM_Loss/RDM Accuracy = 0.05336244/1.0000000, mean_len = 0.231\n",
      "*****Optimizing RDM & PG***** 3410 | 10000 , train_loss/Expected Reward = 0.19549422/0.1954942,  RDM_Loss/RDM Accuracy = 0.00813700/1.0000000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 3420 | 10000 , train_loss/Expected Reward = 0.27903571/0.2790357,  RDM_Loss/RDM Accuracy = 0.02311635/1.0000000, mean_len = 0.253\n",
      "*****Optimizing RDM & PG***** 3430 | 10000 , train_loss/Expected Reward = 0.23770036/0.2377004,  RDM_Loss/RDM Accuracy = 0.22646491/0.9000000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 3440 | 10000 , train_loss/Expected Reward = 0.21787842/0.2178784,  RDM_Loss/RDM Accuracy = 0.12114094/0.9500000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 3450 | 10000 , train_loss/Expected Reward = 0.20420554/0.2042055,  RDM_Loss/RDM Accuracy = 0.03043554/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 3460 | 10000 , train_loss/Expected Reward = 0.17666858/0.1766686,  RDM_Loss/RDM Accuracy = 0.43910879/0.8500000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 3470 | 10000 , train_loss/Expected Reward = 0.21573996/0.2157400,  RDM_Loss/RDM Accuracy = 0.20796458/0.9500000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 3480 | 10000 , train_loss/Expected Reward = 0.20236811/0.2023681,  RDM_Loss/RDM Accuracy = 0.08573956/1.0000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 3490 | 10000 , train_loss/Expected Reward = 0.23821557/0.2382156,  RDM_Loss/RDM Accuracy = 0.04783246/1.0000000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 3500 | 10000 , train_loss/Expected Reward = 0.18718674/0.1871867,  RDM_Loss/RDM Accuracy = 0.18506740/0.9000000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 3510 | 10000 , train_loss/Expected Reward = 0.20270947/0.2027095,  RDM_Loss/RDM Accuracy = 0.14241804/0.9000000, mean_len = 0.274\n",
      "*****Optimizing RDM & PG***** 3520 | 10000 , train_loss/Expected Reward = 0.22923626/0.2292363,  RDM_Loss/RDM Accuracy = 0.42583653/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 3530 | 10000 , train_loss/Expected Reward = 0.28293705/0.2829370,  RDM_Loss/RDM Accuracy = 0.15790729/0.9500000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 3540 | 10000 , train_loss/Expected Reward = 0.24393134/0.2439313,  RDM_Loss/RDM Accuracy = 0.01996458/1.0000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 3550 | 10000 , train_loss/Expected Reward = 0.21679612/0.2167961,  RDM_Loss/RDM Accuracy = 0.02684752/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 3560 | 10000 , train_loss/Expected Reward = 0.23480284/0.2348028,  RDM_Loss/RDM Accuracy = 0.39775571/0.9000000, mean_len = 0.278\n",
      "*****Optimizing RDM & PG***** 3570 | 10000 , train_loss/Expected Reward = 0.20730198/0.2073020,  RDM_Loss/RDM Accuracy = 0.08451256/1.0000000, mean_len = 0.291\n",
      "*****Optimizing RDM & PG***** 3580 | 10000 , train_loss/Expected Reward = 0.22977452/0.2297745,  RDM_Loss/RDM Accuracy = 0.20637615/0.9500000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 3590 | 10000 , train_loss/Expected Reward = 0.21241965/0.2124196,  RDM_Loss/RDM Accuracy = 0.01673803/1.0000000, mean_len = 0.270\n",
      "*****Optimizing RDM & PG***** 3600 | 10000 , train_loss/Expected Reward = 0.20747600/0.2074760,  RDM_Loss/RDM Accuracy = 0.07029501/0.9500000, mean_len = 0.259\n",
      "*****Optimizing RDM & PG***** 3610 | 10000 , train_loss/Expected Reward = 0.24783950/0.2478395,  RDM_Loss/RDM Accuracy = 0.00559364/1.0000000, mean_len = 0.271\n",
      "*****Optimizing RDM & PG***** 3620 | 10000 , train_loss/Expected Reward = 0.27092282/0.2709228,  RDM_Loss/RDM Accuracy = 0.14883901/0.9000000, mean_len = 0.256\n",
      "*****Optimizing RDM & PG***** 3630 | 10000 , train_loss/Expected Reward = 0.21576117/0.2157612,  RDM_Loss/RDM Accuracy = 0.01446854/1.0000000, mean_len = 0.292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Optimizing RDM & PG***** 3640 | 10000 , train_loss/Expected Reward = 0.25997037/0.2599704,  RDM_Loss/RDM Accuracy = 0.15793009/0.9500000, mean_len = 0.269\n",
      "*****Optimizing RDM & PG***** 3650 | 10000 , train_loss/Expected Reward = 0.17479647/0.1747965,  RDM_Loss/RDM Accuracy = 0.12125248/0.9000000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 3660 | 10000 , train_loss/Expected Reward = 0.25612660/0.2561266,  RDM_Loss/RDM Accuracy = 0.22856781/0.9500000, mean_len = 0.292\n",
      "*****Optimizing RDM & PG***** 3670 | 10000 , train_loss/Expected Reward = 0.29542180/0.2954218,  RDM_Loss/RDM Accuracy = 0.03097360/1.0000000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 3680 | 10000 , train_loss/Expected Reward = 0.19086605/0.1908661,  RDM_Loss/RDM Accuracy = 0.26477662/0.9500000, mean_len = 0.303\n",
      "*****Optimizing RDM & PG***** 3690 | 10000 , train_loss/Expected Reward = 0.20057129/0.2005713,  RDM_Loss/RDM Accuracy = 0.07536831/0.9500000, mean_len = 0.299\n",
      "*****Optimizing RDM & PG***** 3700 | 10000 , train_loss/Expected Reward = 0.18766077/0.1876608,  RDM_Loss/RDM Accuracy = 0.34341851/0.9000000, mean_len = 0.284\n",
      "*****Optimizing RDM & PG***** 3710 | 10000 , train_loss/Expected Reward = 0.19439988/0.1943999,  RDM_Loss/RDM Accuracy = 0.04407991/0.9500000, mean_len = 0.300\n",
      "*****Optimizing RDM & PG***** 3720 | 10000 , train_loss/Expected Reward = 0.22219445/0.2221944,  RDM_Loss/RDM Accuracy = 0.15021060/0.9500000, mean_len = 0.287\n",
      "*****Optimizing RDM & PG***** 3730 | 10000 , train_loss/Expected Reward = 0.22015284/0.2201528,  RDM_Loss/RDM Accuracy = 0.30444157/0.9000000, mean_len = 0.273\n",
      "*****Optimizing RDM & PG***** 3740 | 10000 , train_loss/Expected Reward = 0.23098855/0.2309886,  RDM_Loss/RDM Accuracy = 0.01570862/1.0000000, mean_len = 0.286\n",
      "*****Optimizing RDM & PG***** 3750 | 10000 , train_loss/Expected Reward = 0.21681685/0.2168168,  RDM_Loss/RDM Accuracy = 0.06921136/0.9500000, mean_len = 0.281\n",
      "*****Optimizing RDM & PG***** 3760 | 10000 , train_loss/Expected Reward = 0.22697611/0.2269761,  RDM_Loss/RDM Accuracy = 0.03817661/1.0000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 3770 | 10000 , train_loss/Expected Reward = 0.19709044/0.1970904,  RDM_Loss/RDM Accuracy = 0.14559303/0.9500000, mean_len = 0.308\n",
      "*****Optimizing RDM & PG***** 3780 | 10000 , train_loss/Expected Reward = 0.26153596/0.2615360,  RDM_Loss/RDM Accuracy = 0.06536666/1.0000000, mean_len = 0.294\n",
      "*****Optimizing RDM & PG***** 3790 | 10000 , train_loss/Expected Reward = 0.20027140/0.2002714,  RDM_Loss/RDM Accuracy = 0.23337780/0.9000000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 3800 | 10000 , train_loss/Expected Reward = 0.22540255/0.2254026,  RDM_Loss/RDM Accuracy = 0.25935093/0.9000000, mean_len = 0.301\n",
      "*****Optimizing RDM & PG***** 3810 | 10000 , train_loss/Expected Reward = 0.20396260/0.2039626,  RDM_Loss/RDM Accuracy = 0.03959050/1.0000000, mean_len = 0.323\n",
      "*****Optimizing RDM & PG***** 3820 | 10000 , train_loss/Expected Reward = 0.26080383/0.2608038,  RDM_Loss/RDM Accuracy = 0.32227585/0.9000000, mean_len = 0.267\n",
      "*****Optimizing RDM & PG***** 3830 | 10000 , train_loss/Expected Reward = 0.25744808/0.2574481,  RDM_Loss/RDM Accuracy = 0.02809719/1.0000000, mean_len = 0.289\n",
      "*****Optimizing RDM & PG***** 3840 | 10000 , train_loss/Expected Reward = 0.24118952/0.2411895,  RDM_Loss/RDM Accuracy = 0.26038179/0.8500000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 3850 | 10000 , train_loss/Expected Reward = 0.20792299/0.2079230,  RDM_Loss/RDM Accuracy = 0.03121135/1.0000000, mean_len = 0.335\n",
      "*****Optimizing RDM & PG***** 3860 | 10000 , train_loss/Expected Reward = 0.21590747/0.2159075,  RDM_Loss/RDM Accuracy = 0.10910209/0.9500000, mean_len = 0.279\n",
      "*****Optimizing RDM & PG***** 3870 | 10000 , train_loss/Expected Reward = 0.20775622/0.2077562,  RDM_Loss/RDM Accuracy = 0.02297944/1.0000000, mean_len = 0.307\n",
      "*****Optimizing RDM & PG***** 3880 | 10000 , train_loss/Expected Reward = 0.21657413/0.2165741,  RDM_Loss/RDM Accuracy = 0.02144339/1.0000000, mean_len = 0.321\n",
      "*****Optimizing RDM & PG***** 3890 | 10000 , train_loss/Expected Reward = 0.26499008/0.2649901,  RDM_Loss/RDM Accuracy = 0.03011324/1.0000000, mean_len = 0.280\n",
      "*****Optimizing RDM & PG***** 3900 | 10000 , train_loss/Expected Reward = 0.27990066/0.2799007,  RDM_Loss/RDM Accuracy = 0.04472691/1.0000000, mean_len = 0.298\n",
      "*****Optimizing RDM & PG***** 3910 | 10000 , train_loss/Expected Reward = 0.20589439/0.2058944,  RDM_Loss/RDM Accuracy = 0.06125230/0.9500000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 3920 | 10000 , train_loss/Expected Reward = 0.24455312/0.2445531,  RDM_Loss/RDM Accuracy = 0.03536956/1.0000000, mean_len = 0.314\n",
      "*****Optimizing RDM & PG***** 3930 | 10000 , train_loss/Expected Reward = 0.22190713/0.2219071,  RDM_Loss/RDM Accuracy = 0.02755662/1.0000000, mean_len = 0.327\n",
      "*****Optimizing RDM & PG***** 3940 | 10000 , train_loss/Expected Reward = 0.21760400/0.2176040,  RDM_Loss/RDM Accuracy = 0.01420600/1.0000000, mean_len = 0.288\n",
      "*****Optimizing RDM & PG***** 3950 | 10000 , train_loss/Expected Reward = 0.21192544/0.2119254,  RDM_Loss/RDM Accuracy = 0.04171583/1.0000000, mean_len = 0.313\n",
      "*****Optimizing RDM & PG***** 3960 | 10000 , train_loss/Expected Reward = 0.22116856/0.2211686,  RDM_Loss/RDM Accuracy = 0.07722046/1.0000000, mean_len = 0.297\n",
      "*****Optimizing RDM & PG***** 3970 | 10000 , train_loss/Expected Reward = 0.22851221/0.2285122,  RDM_Loss/RDM Accuracy = 0.03257092/1.0000000, mean_len = 0.316\n",
      "*****Optimizing RDM & PG***** 3980 | 10000 , train_loss/Expected Reward = 0.25931772/0.2593177,  RDM_Loss/RDM Accuracy = 0.09861423/0.9000000, mean_len = 0.295\n",
      "*****Optimizing RDM & PG***** 3990 | 10000 , train_loss/Expected Reward = 0.25054539/0.2505454,  RDM_Loss/RDM Accuracy = 0.06741166/0.9500000, mean_len = 0.282\n"
     ]
    }
   ],
   "source": [
    "for step in range(4000):\n",
    "    x, x_len, y = get_df_batch(step*batch_size, batch_size)        \n",
    "    seq = sent_pooler(x)\n",
    "    rdm_hiddens = rdm_model(seq)\n",
    "    tau_len = 5\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batchsize, _, _ = rdm_hiddens.shape\n",
    "        rdm_outs = torch.cat(\n",
    "            [ rdm_hiddens[i][x_len[i]-1].unsqueeze(0) for i in range(batchsize)] \n",
    "            # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "        )\n",
    "        rdm_scores = rdm_classifier(\n",
    "            rdm_outs\n",
    "        )\n",
    "        rdm_preds = rdm_scores.argmax(axis=1)\n",
    "        y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "        acc = accuracy_score(y_label.cpu().numpy(), rdm_preds.cpu().numpy())\n",
    "        loss = loss_fn(rdm_scores, y_label)\n",
    "\n",
    "    batchsize, max_seq_len, hidden_dim = rdm_hiddens.shape\n",
    "    stopScore, isStop = cm_model(rdm_hiddens.reshape(-1, 256))\n",
    "    isStop = isStop.reshape([batchsize, max_seq_len, -1])\n",
    "    stopProb = stopScore.reshape([batchsize, max_seq_len, -1]).softmax(axis=-1)         \n",
    "\n",
    "    sum_rw = torch.zeros(len(x_len)).cuda()\n",
    "    prob = torch.ones(len(x_len)).cuda()\n",
    "    sum_len = 0.0\n",
    "\n",
    "    preds_list = []\n",
    "    label_list = []\n",
    "\n",
    "    preds = rdm_classifier(rdm_hiddens.reshape(-1, hidden_dim)).reshape(batchsize, max_seq_len, -1)\n",
    "    for j in range(len(x_len)):\n",
    "        start = random.randint(0, x_len[j]-1)\n",
    "        delay_punish = -0.1*torch.arange(1, x_len[j]+1).cuda()\n",
    "        for t in range(start, x_len[j]):\n",
    "            rnd = random.random()\n",
    "            if rnd > stopProb[j][t][1]:\n",
    "                prob[j] *= stopProb[j][t][0]            \n",
    "            else:\n",
    "                prob[j] *= stopProb[j][t][1]\n",
    "                rnd2 = random.random()\n",
    "                if  rnd2 < 0.4:\n",
    "                    break\n",
    "        label_list.append( torch.tensor(y[j]).repeat(x_len[j]-t, 1).cuda() )\n",
    "        sum_rw[j] = -1*delay_punish[t]+ loss_fn(preds[j][t:x_len[j]], label_list[-1].argmax(axis=1))\n",
    "        sum_len += (t-start+1)*1.0/x_len[j]\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    if step %2 == 0:\n",
    "        E_rw = (prob.detach().cuda()*sum_rw).mean()\n",
    "        E_rw.backward()\n",
    "    else:\n",
    "        E_rw = (prob*sum_rw.detach().cuda()).mean()\n",
    "        E_rw.backward()\n",
    "    optim.step()       \n",
    "    if step%10 == 0:  \n",
    "        print('*****Optimizing RDM & PG***** %3d | %d , train_loss/Expected Reward = %6.8f/%6.7f,  RDM_Loss/RDM Accuracy = %6.8f/%6.7f, mean_len = %2.3f'             % (step, t_steps, \n",
    "                rw_arr.mean(), rw_arr.mean(), loss, acc, len_arr.mean()\n",
    "                ))\n",
    "    rw_arr[int(step%10)] = float(E_rw)\n",
    "    len_arr[int(step%10)] = sum_len*1.0/batch_size\n",
    "\n",
    "    writer.add_scalar('RDM Loss', loss, step)\n",
    "    writer.add_scalar('RDM Accuracy', acc, step)\n",
    "    writer.add_scalar('Train Loss', float(E_rw), step)\n",
    "    writer.add_scalar('Expected Reward', float(E_rw), step)\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "erd_save_as = '%s/erdModel_test1.pkl'% (log_dir)\n",
    "torch.save(\n",
    "    {\n",
    "        \"sent_pooler\":sent_pooler.state_dict(),\n",
    "        \"rmdModel\":rdm_model.state_dict(),\n",
    "        \"rdm_classifier\": rdm_classifier.state_dict(),\n",
    "        \"cm_model\":cm_model.state_dict()\n",
    "    },\n",
    "    erd_save_as\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_ID\n",
    "del data_y\n",
    "del data_len\n",
    "del valid_data_ID\n",
    "del valid_data_len\n",
    "del valid_data_y\n",
    "\n",
    "from dataUtilsV0 import data\n",
    "from dataUtilsV0 import data_ID\n",
    "from dataUtilsV0 import data_len\n",
    "from dataUtilsV0 import data_y\n",
    "from dataUtilsV0 import valid_data_ID\n",
    "from dataUtilsV0 import valid_data_len\n",
    "from dataUtilsV0 import valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_new_len = get_new_len_on_valid_data(sent_pooler, rdm_model, cm_model, FLAGS, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(valid_data_len) - np.array(valid_new_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.854, 0.8960244648318043, 0.8825301204819277)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_on_valid_data(rdm_model, sent_pooler, rdm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisrt_len = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_on_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.782, 0.9305019305019305, 0.7259036144578314)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_on_valid_data(rdm_model, sent_pooler, rdm_classifier, new_data_len=fisrt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del accuracy_on_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 64 ,  max_seq_len: 346\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_ID\n",
    "del data_y\n",
    "del data_len\n",
    "del valid_data_ID\n",
    "del valid_data_len\n",
    "del valid_data_y\n",
    "\n",
    "from dataUtilsV0 import data\n",
    "from dataUtilsV0 import data_ID\n",
    "from dataUtilsV0 import data_len\n",
    "from dataUtilsV0 import data_y\n",
    "from dataUtilsV0 import valid_data_ID\n",
    "from dataUtilsV0 import valid_data_len\n",
    "from dataUtilsV0 import valid_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_valid_data(rdm_model = None, sent_pooler = None, rdm_classifier=None, new_data_len=[], cuda=True):\n",
    "    batch_size = 20\n",
    "    t_steps = int(len(valid_data_ID)/batch_size)\n",
    "    \n",
    "    sum_acc = 0.0\n",
    "    sum_pres = 0.0\n",
    "    sum_recall = 0.0\n",
    "    \n",
    "    miss_vec = 0\n",
    "    mts = 0\n",
    "    hit_vec = 0\n",
    "    if len(new_data_len) > 0:\n",
    "        t_data_len = new_data_len\n",
    "    else:\n",
    "        t_data_len = valid_data_len\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for step in range(t_steps):\n",
    "        data_x = []\n",
    "        m_data_y = np.zeros([batch_size, 2], dtype=np.int32)\n",
    "        m_data_len = np.zeros([batch_size], dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            m_data_y[i] = valid_data_y[mts]\n",
    "            m_data_len[i] = t_data_len[mts]\n",
    "            seq = []\n",
    "            for j in range(t_data_len[mts]):\n",
    "                sent = wordlist2wordvecs(data[valid_data_ID[mts]]['text'][j])\n",
    "                sent_tensor = torch.tensor(np.stack(sent))\n",
    "                seq.append(sent_tensor)\n",
    "            data_x.append(seq)\n",
    "            mts += 1\n",
    "            if mts >= len(valid_data_ID): # read data looply\n",
    "                mts = mts % len(valid_data_ID)\n",
    "        \n",
    "        \n",
    "        if rdm_model is not None and sent_pooler is not None and rdm_classifier is not None:\n",
    "            with torch.no_grad():\n",
    "                seq = sent_pooler(data_x)\n",
    "                rdm_hiddens = rdm_model(seq)\n",
    "                batchsize, _, _ = rdm_hiddens.shape\n",
    "                rdm_outs = torch.stack(\n",
    "                    [ rdm_hiddens[i][m_data_len[i]-1] for i in range(batchsize)] \n",
    "#                     [ rdm_hiddens[i][0] for i in range(batchsize)] \n",
    "                )\n",
    "                rdm_scores = rdm_classifier(\n",
    "                    rdm_outs\n",
    "                )\n",
    "                rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                y_label = torch.tensor(m_data_y).argmax(axis=1).cuda() if cuda else torch.tensor(m_data_y).argmax(axis=1)\n",
    "                preds.append(rdm_preds)\n",
    "                labels.append(y_label)\n",
    "    \n",
    "    pred_array = torch.cat(preds).cpu().numpy()\n",
    "    label_array = torch.cat(labels).cpu().numpy()\n",
    "    return accuracy_score(y_true=label_array, y_pred=pred_array), precision_score(y_true=label_array, y_pred=pred_array), recall_score(y_true=label_array, y_pred=pred_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.836, 0.8255208333333334, 0.9548192771084337)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_on_valid_data(rdm_model, sent_pooler, rdm_classifier) # 使用第0条推特"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.874, 0.8898550724637682, 0.9246987951807228)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_on_valid_data(rdm_model, sent_pooler, rdm_classifier) # 使用最后1条推特"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
