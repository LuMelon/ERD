{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import MyLogger\n",
    "import SubjObjLoader\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from collections import deque\n",
    "# import dataloader\n",
    "from BertRDMLoader import *\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import tsentiLoader\n",
    "from emotionLoader import *\n",
    "from SubjObjLoader import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "#         self.PoolLayer = pooling_layer(word_embedding_dim, sent_embedding_dim) \n",
    "        \n",
    "    def forward(self, x_emb): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "        init_states = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        try:\n",
    "            df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        except:\n",
    "            print(\"Error:\", pool_feature.shape, init_states.shape)\n",
    "            raise\n",
    "        # hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        # final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        # return hidden_outs, final_outs\n",
    "        return df_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Count_Accs(ylabel, preds):\n",
    "    correct_preds = np.array(\n",
    "        [1 if y1==y2 else 0 \n",
    "        for (y1, y2) in zip(ylabel, preds)]\n",
    "    )\n",
    "    y_idxs = [idx if yl >0 else idx - len(ylabel) \n",
    "            for (idx, yl) in enumerate(ylabel)]\n",
    "    pos_idxs = list(filter(lambda x: x >= 0, y_idxs))\n",
    "    neg_idxs = list(filter(lambda x: x < 0, y_idxs))\n",
    "    acc = sum(correct_preds) / (1.0 * len(ylabel))\n",
    "    if len(pos_idxs) > 0:\n",
    "        pos_acc = sum(correct_preds[pos_idxs])/(1.0*len(pos_idxs))\n",
    "    else:\n",
    "        pos_acc = 0\n",
    "    if len(neg_idxs) > 0:\n",
    "        neg_acc = sum(correct_preds[neg_idxs])/(1.0*len(neg_idxs))\n",
    "    else:\n",
    "        neg_acc = 0\n",
    "    return acc, pos_acc, neg_acc, y_idxs, pos_idxs, neg_idxs, correct_preds\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "    \n",
    "def WeightsForUmbalanced(data_label):\n",
    "    _, _, labels = data_label.shape\n",
    "    label_cnt = data_label.reshape([-1, labels]).sum(axis=0)\n",
    "    weights = 1.0/label_cnt\n",
    "    normalized_weights = weights/sum(weights)\n",
    "    return normalized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_new = [sent1, sent2, sent3, ...]　\n",
    "# x_new -> x_old_emb [batchsize, seq_len, sent_emb]：#使用seq_info 将sent组装回去\n",
    "# ---> [batchsize, max_seq_len, sent_emb] # padding 成一个可以计算的batch, 从而可以切分\n",
    "def rdm_data2bert_tensors(data_X, cuda):\n",
    "    def padding_sent_list(sent_list):\n",
    "        sent_len = [len(sent) for sent in sent_list]\n",
    "        max_sent_len = max(sent_len)\n",
    "        sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "        attn_mask = torch.ones_like(sent_padding)\n",
    "        for i, sent in enumerate(sent_list):\n",
    "            sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "            attn_mask[i][len(sent):].fill_(0)\n",
    "        return sent_padding, attn_mask\n",
    "    sent_list = []\n",
    "    [sent_list.extend(seq) for seq in data_X]\n",
    "    seq_len = [len(seq) for seq in data_X]\n",
    "    sent_tensors, attn_mask = padding_sent_list(sent_list)\n",
    "    if cuda:\n",
    "        sent_tensors = sent_tensors.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_tensors, attn_mask, seq_len\n",
    "\n",
    "def senti_data2bert_tensors(sent_list, cuda):\n",
    "    sent_len = [len(sent) for sent in sent_list]\n",
    "    max_sent_len = max(sent_len)\n",
    "    sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "    attn_mask = torch.ones_like(sent_padding)\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "        attn_mask[i][len(sent):].fill_(0)\n",
    "    if cuda:\n",
    "        sent_padding = sent_padding.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_padding, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdmGraph_generate(rdm_model):\n",
    "    def fn(x_emb):\n",
    "        rdm_hiddens = rdm_model(x_emb)\n",
    "        return rdm_hiddens\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clsGraph_generater(cls):\n",
    "    def fn(tensor):\n",
    "        return cls(tensor)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这样做的好处在于，一方面他能保证在运行的时候数据被切分到不同的ＧＰＵ上，另一方面通过返回函数将需要更新的模型放在计算模型之外\n",
    "class RDMLossGraph(nn.Module):\n",
    "    def __init__(self, rdm_func, cls_func, loss_fn, cuda):\n",
    "        super(RDMLossGraph, self).__init__()\n",
    "        self.rdm_func = rdm_func\n",
    "        self.cls_func = cls_func\n",
    "        self.loss_fn = loss_fn\n",
    "        self.cuda = cuda\n",
    "    def forward(self, x, y, x_len):\n",
    "        rdm_hiddens = self.rdm_func(x)\n",
    "        batchsize, _, _ = rdm_hiddens.shape\n",
    "        rdm_outs = torch.cat(\n",
    "            [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "            # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "        ).reshape(\n",
    "            [-1, rdm_model.hidden_dim]\n",
    "        )\n",
    "        \n",
    "        rdm_scores = self.cls_func(\n",
    "            rdm_outs\n",
    "        )\n",
    "        rdm_preds = rdm_scores.argmax(axis=1)\n",
    "        y_label = torch.tensor(y).argmax(axis=1).cuda() if self.cuda else torch.tensor(y).argmax(axis=1)\n",
    "        acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "        loss = loss_fn(rdm_scores, y_label)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiGraph_generate(bert, transformer, task_emb):\n",
    "    def fn(xst_embs):\n",
    "        tensors = xst_embs + task_emb\n",
    "        senti_feature = transformer(tensors.transpose(0, 1)).transpose(0, 1)\n",
    "        cls_feature = senti_feature.max(axis=1)[0]\n",
    "        return cls_feature\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiLossGraph(nn.Module):\n",
    "    def __init__(self, senti_graph, cls_func, loss_fn, cuda):\n",
    "        super(SentiLossGraph, self).__init__()\n",
    "        self.senti_graph = senti_graph\n",
    "        self.cls_func = cls_func\n",
    "        self.loss_fn = loss_fn\n",
    "        self.cuda = cuda\n",
    "    def forward(self, xst_embs, yst, lst):\n",
    "        cls_feature = self.senti_graph(xst_embs)\n",
    "        senti_scores = self.cls_func(cls_feature)\n",
    "        y_label = torch.tensor(yst.argmax(axis=1)).cuda() if self.cuda else torch.tensor(yst.argmax(axis=1))\n",
    "        loss = senti_loss_fn(senti_scores, y_label)\n",
    "        acc, _, _, _, _, _, _ = Count_Accs(y_label, senti_scores.argmax(axis=1))\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRDMWithSenti(rdm_model, bert, rdm_classifier,\n",
    "                     transformer, task_embedding, senti_classifier, \n",
    "                     sentiReader,\n",
    "                    tokenizer, t_steps, new_data_len=[], logger=None, cuda=False, \n",
    "                        log_dir=\"SentiRDM\"):\n",
    "    batch_size = 10\n",
    "    max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    sentiReader.reset_batchsize(max_gpu_batch)\n",
    "\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "\n",
    "    sum_loss = np.zeros(3)\n",
    "    sum_acc = np.zeros(2)\n",
    "\n",
    "    t_acc = 0.9\n",
    "    ret_acc = 0.0\n",
    "\n",
    "    #  rdm loss graph    \n",
    "    #-----------------------------------------\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    #------------------------------------------------    \n",
    "\n",
    "    #  senti loss graph\n",
    "    #-----------------------------------------------------------\n",
    "    senti_weights = torch.tensor(\n",
    "            WeightsForUmbalanced(\n",
    "                sentiReader.label\n",
    "            ),\n",
    "            dtype=torch.float32\n",
    "    )\n",
    "    senti_loss_fn = nn.CrossEntropyLoss(weight=senti_weights.cuda()) if cuda else nn.CrossEntropyLoss(weight=senti_weights)\n",
    "    senti_task_id = torch.tensor([0]) if not cuda else torch.tensor([0]).cuda()\n",
    "    #-------------------------------------------------------------    \n",
    "\n",
    "    loss_weight = torch.tensor([0.8, 0.2]) if not cuda else torch.tensor([0.8, 0.2]).cuda()   \n",
    "    optim = torch.optim.Adagrad([\n",
    "                                {'params': bert.parameters(), 'lr':5e-7},\n",
    "                                {'params': rdm_classifier.parameters(), 'lr': 5e-5},\n",
    "                                {'params': rdm_model.parameters(), 'lr': 5e-5},\n",
    "                                {'params': task_embedding.parameters(), 'lr':1e-6},\n",
    "                                {'params': transformer.parameters(), 'lr': 1e-6},\n",
    "                                {'params': senti_classifier.parameters(), 'lr': 1e-6}\n",
    "                             ]\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    acc_tmp = np.zeros([2, int(batch_size/max_gpu_batch)])\n",
    "    loss_tmp = np.zeros([3, int(batch_size/max_gpu_batch)])\n",
    "\n",
    "    for step in range(t_steps):\n",
    "        optim.zero_grad()\n",
    "        for j in range(int(batch_size/max_gpu_batch)):\n",
    "            # RDM loss 用的是bert的pooled emb，　其它模型用的是cls emb\n",
    "\n",
    "            #----------------RDM loss computation--------------------------\n",
    "            if len(new_data_len) > 0:\n",
    "                x, x_len, y = get_df_batch(step, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "            else:\n",
    "                x, x_len, y = get_df_batch(step*batch_size+j*max_gpu_batch, max_gpu_batch, tokenizer=tokenizer) \n",
    "            sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "            bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "            data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "            rdm_hiddens = rdm_model(data_tensors)\n",
    "            batchsize, _, _ = rdm_hiddens.shape\n",
    "            rdm_outs = torch.cat(\n",
    "                [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "                # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "            ).reshape(\n",
    "                [-1, rdm_model.hidden_dim]\n",
    "            )\n",
    "\n",
    "            rdm_scores = rdm_classifier(\n",
    "                rdm_outs\n",
    "            )\n",
    "            rdm_preds = rdm_scores.argmax(axis=1)\n",
    "            y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "            acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "            loss = loss_fn(rdm_scores, y_label)\n",
    "            loss_back = loss.mean()*loss_weight[0]\n",
    "            loss_back.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"rdm j:\", j)\n",
    "            #----------------------------------------------------------------------\n",
    "\n",
    "            # ----------------sentiment analysis------------------------\n",
    "            xst, yst, lst = sentiReader.sample()\n",
    "            sent_tensors, sent_mask = senti_data2bert_tensors(xst, cuda)\n",
    "            xst_embs, _ = bert(sent_tensors, attention_mask = sent_mask)\n",
    "\n",
    "            tensors = xst_embs + task_embedding(senti_task_id)\n",
    "            senti_feature = transformer(tensors.transpose(0, 1)).transpose(0, 1)\n",
    "            cls_feature = senti_feature.max(axis=1)[0]\n",
    "            senti_scores = senti_cls(cls_feature)\n",
    "            y_label = torch.tensor(yst.argmax(axis=1)).cuda() if cuda else torch.tensor(yst.argmax(axis=1))\n",
    "            st_loss = senti_loss_fn(senti_scores, y_label)\n",
    "            st_acc, _, _, _, _, _, _ = Count_Accs(y_label, senti_scores.argmax(axis=1))\n",
    "            st_loss_back = st_loss.mean()*loss_weight[1]\n",
    "            st_loss_back.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"senti j:\", j)\n",
    "            #-----------------------------------------------------------\n",
    "            loss_tmp[:, j] = np.array([loss_back+st_loss_back, loss.mean(), st_loss.mean()])\n",
    "            acc_tmp[:, j] = np.array([acc.mean(), st_acc.mean()])\n",
    "            torch.cuda.empty_cache()\n",
    "        time.sleep(1)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        writer.add_scalar('Train Loss', loss_tmp[0].mean(), step)\n",
    "        writer.add_scalar('Train Accuracy', acc_tmp[0].mean(), step)\n",
    "\n",
    "        sum_acc += acc_tmp.mean(axis=1)\n",
    "        sum_loss += loss_tmp.mean(axis=1)\n",
    "\n",
    "        print(\"%6d %6d|MTL_Loss:%6.8f, rdm_loss/rdm_acc = %6.8f/%6.7f | senti_loss/senti_acc = %6.8f/%6.7f \" % (\n",
    "                                                                                                step, t_steps, loss_tmp[0].mean(),        \n",
    "                                                                                            loss_tmp[1].mean(), acc_tmp[0].mean(),\n",
    "                                                                                            loss_tmp[2].mean(), acc_tmp[1].mean()\n",
    "            )\n",
    "            )\n",
    "\n",
    "        if step % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            print(\"MTL_Loss:%6.8f, rdm_loss/rdm_acc = %6.8f/%6.7f | senti_loss/senti_acc = %6.8f/%6.7f\" % (\n",
    "                                                                                            sum_loss[0],        \n",
    "                                                                                            sum_loss[1], sum_acc[0],\n",
    "                                                                                            sum_loss[2], sum_acc[1]\n",
    "            )\n",
    "            )\n",
    "            if step%100 == 99:\n",
    "                rdm_save_as = './%s/sentiRDMModel_epoch%03d.pkl'% (log_dir, step/100)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"bert\":bert.state_dict(),\n",
    "                        \"transformer\":transformer.state_dict(),\n",
    "                        \"task_embedding\":task_embedding.state_dict(),\n",
    "                        \"senti_classifier\": senti_classifier.state_dict(),\n",
    "                        \"rmdModel\":rdm_model.state_dict(),\n",
    "                        \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "                    },\n",
    "                    rdm_save_as\n",
    "                )\n",
    "    #                 rdm_model, bert, sentiModel, rdm_classifier\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "tt = BertTokenizer.from_pretrained(\"./bertModel/\")\n",
    "bb = BertModel.from_pretrained(\"./bertModel/\")\n",
    "task_embedding = nn.Embedding(3, 768)\n",
    "encoder_layer = nn.TransformerEncoderLayer(768, 8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, 1)\n",
    "\n",
    "bert = bb.cuda()\n",
    "transformer = transformer_encoder.cuda()\n",
    "task_embedding = task_embedding.cuda()\n",
    "senti_cls = nn.Linear(768, 2).cuda()\n",
    "rdm_model = RDM_Model(768, 300, 256, 0.2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trainingandtestdata/training.1600000.processed.noemoticon.csv\n",
      "./trainingandtestdata/testdata.manual.2009.06.14.csv\n",
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "# #### 各个任务的数据\n",
    "train_file = \"./trainingandtestdata/training.1600000.processed.noemoticon.csv\"\n",
    "test_file = \"./trainingandtestdata/testdata.manual.2009.06.14.csv\"\n",
    "train_set, test_set = tsentiLoader.load_data(train_file, test_file)\n",
    "senti_train_reader = tsentiLoader.tSentiReader(train_set[:10000], 20, tt)\n",
    "senti_train_reader.label = np.delete(senti_train_reader.label, 1, axis=2)\n",
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #### 导入预训练模型\n",
    "joint_save_as = './MTLTrain/jointModel_epoch015.pkl'\n",
    "checkpoint = torch.load(joint_save_as)\n",
    "senti_cls.load_state_dict(checkpoint['senti_classifier'])\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "transformer.load_state_dict(checkpoint['transformer'])\n",
    "task_embedding.load_state_dict(checkpoint['task_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainRDMWithSenti(rdm_model, bert, rdm_classifier,\n",
    "                     transformer, task_embedding, senti_cls, \n",
    "                     senti_train_reader,\n",
    "                    tt, 1000, new_data_len=[], logger=None, cuda=True, \n",
    "                        log_dir=\"SentiRDM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "sentiReader = senti_train_reader\n",
    "senti_classifier = senti_cls\n",
    "log_dir = \"./SentiRDM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "sentiReader.reset_batchsize(max_gpu_batch)\n",
    "\n",
    "assert(batch_size%max_gpu_batch == 0)\n",
    "\n",
    "sum_loss = np.zeros(3)\n",
    "sum_acc = np.zeros(2)\n",
    "\n",
    "t_acc = 0.9\n",
    "ret_acc = 0.0\n",
    "\n",
    "#  rdm loss graph    \n",
    "#-----------------------------------------\n",
    "weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "#------------------------------------------------    \n",
    "\n",
    "#  senti loss graph\n",
    "#-----------------------------------------------------------\n",
    "senti_weights = torch.tensor(\n",
    "        WeightsForUmbalanced(\n",
    "            sentiReader.label\n",
    "        ),\n",
    "        dtype=torch.float32\n",
    ")\n",
    "senti_loss_fn = nn.CrossEntropyLoss(weight=senti_weights.cuda()) if cuda else nn.CrossEntropyLoss(weight=senti_weights)\n",
    "senti_task_id = torch.tensor([0]) if not cuda else torch.tensor([0]).cuda()\n",
    "#-------------------------------------------------------------    \n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "acc_tmp = np.zeros([2, int(batch_size/max_gpu_batch)])\n",
    "loss_tmp = np.zeros([3, int(batch_size/max_gpu_batch)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = torch.tensor([0.8, 0.2]) if not cuda else torch.tensor([0.8, 0.2]).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 60, 1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_len, y = get_df_batch(4, 5, tokenizer=tt) \n",
    "sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outs = bert(sent_tensors, attention_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 10.73 GiB total capacity; 8.91 GiB already allocated; 8.31 MiB free; 325.39 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1dd13f4d463a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 10.73 GiB total capacity; 8.91 GiB already allocated; 8.31 MiB free; 325.39 MiB cached)"
     ]
    }
   ],
   "source": [
    "data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([123, 58])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 15, 23, 60, 6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-39a39aa64430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "data_tensors.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 10.73 GiB total capacity; 8.89 GiB already allocated; 86.44 MiB free; 269.04 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b47768a70608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msent_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_data2bert_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbert_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpooled_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbert_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    713\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    714\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                                        head_mask=head_mask)\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 10.73 GiB total capacity; 8.89 GiB already allocated; 86.44 MiB free; 269.04 MiB cached)"
     ]
    }
   ],
   "source": [
    "\n",
    "rdm_hiddens = rdm_model(data_tensors)\n",
    "batchsize, _, _ = rdm_hiddens.shape\n",
    "rdm_outs = torch.cat(\n",
    "    [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "    # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    ").reshape(\n",
    "    [-1, rdm_model.hidden_dim]\n",
    ")\n",
    "\n",
    "rdm_scores = rdm_classifier(\n",
    "    rdm_outs\n",
    ")\n",
    "rdm_preds = rdm_scores.argmax(axis=1)\n",
    "y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "loss = loss_fn(rdm_scores, y_label)\n",
    "loss_back = loss.mean()*loss_weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_可以用于实时地监控显存的使用情况_\n",
    "``` python\n",
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(meminfo.used)\n",
    "\n",
    "x, x_len, y = get_df_batch(0, 5, tokenizer=tokenizer) \n",
    "\n",
    "x, x_len, y = get_df_batch(0, 5, tokenizer=tokenizer) \n",
    "sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "rdm_hiddens = rdm_model(data_tensors)\n",
    "batchsize, _, _ = rdm_hiddens.shape\n",
    "rdm_outs = torch.cat(\n",
    "    [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "    # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    ").reshape(\n",
    "    [-1, rdm_model.hidden_dim]\n",
    ")\n",
    "\n",
    "rdm_scores = rdm_classifier(\n",
    "    rdm_outs\n",
    ")\n",
    "rdm_preds = rdm_scores.argmax(axis=1)\n",
    "y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "acc, _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "loss = loss_fn(rdm_scores, y_label)\n",
    "loss_back = loss.mean()*loss_weight[0]\n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(meminfo.used)\n",
    "\n",
    "loss_back.backward()\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "xst, yst, lst = sentiReader.sample()\n",
    "sent_tensors, sent_mask = senti_data2bert_tensors(xst, cuda)\n",
    "xst_embs, _ = bert(sent_tensors, attention_mask = sent_mask)\n",
    "\n",
    "tensors = xst_embs + task_embedding(senti_task_id)\n",
    "senti_feature = transformer(tensors.transpose(0, 1)).transpose(0, 1)\n",
    "cls_feature = senti_feature.max(axis=1)[0]\n",
    "senti_scores = senti_cls(cls_feature)\n",
    "y_label = torch.tensor(yst.argmax(axis=1)).cuda() if cuda else torch.tensor(yst.argmax(axis=1))\n",
    "st_loss = senti_loss_fn(senti_scores, y_label)\n",
    "st_acc, _, _, _, _, _, _ = Count_Accs(y_label, senti_scores.argmax(axis=1))\n",
    "st_loss_back = st_loss.mean()*loss_weight[1]\n",
    "st_loss_back.backward()\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
