{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:42:15.496636 140377599600448 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"SentiMain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 18:42:31.175283 140377599600448 logger.py:24] (300, 64, 101, 31, 2, 2)\n",
      "I0901 18:42:31.176598 140377599600448 logger.py:24] 2019-09-01 18:42:31 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 64 101 31 2 2\n",
      "2019-09-01 18:42:31 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.losses import Reduction\n",
    "from model import adict\n",
    "\n",
    "def InferSentiTrainGraph(char_model, lm, senti_model, \n",
    "                            batchsize, max_word_num, max_char_num, \n",
    "                            embedding_dim, hidden_dim, sent_num, learning_rate):\n",
    "    sent_x = tf.placeholder(tf.int32, shape = [batchsize, max_word_num, max_char_num])\n",
    "    sent_y = tf.placeholder(tf.float32, shape = [batchsize, sent_num])\n",
    "    words2vec = char_model(sent_x) #[None, max_word_num, kernerl_size]\n",
    "    cnn_outs = tf.reshape(words2vec, [batchsize, max_word_num, sum(char_model.kernel_features)])\n",
    "    \n",
    "    cnn_outs_list = [tf.squeeze(x, [1]) \n",
    "    for x in tf.split(cnn_outs, max_word_num, 1)]\n",
    "    lm_init_state = lm.cell.zero_state(\n",
    "                            batchsize, \n",
    "                            dtype=tf.float32\n",
    "                        )\n",
    "    words_embedding, sentence_embedding = tf.contrib.rnn.static_rnn(\n",
    "                                        lm.cell, \n",
    "                                        cnn_outs_list,\n",
    "                                        initial_state=lm_init_state, \n",
    "                                        dtype=tf.float32\n",
    "                                    )     \n",
    "    \n",
    "    words_embedding = tf.identity(words_embedding, \"rnn_out_puts\")\n",
    "    words_embedding = tf.transpose(words_embedding, [1, 0, 2])\n",
    "    words_feature = tf.math.reduce_max( words_embedding , axis=1)\n",
    "    print(\"words_feature:\", words_feature)\n",
    "    print(\"sentence_embedding:\", sentence_embedding)\n",
    "    senti_features =  sentence_embedding[-1][-1] + words_feature \n",
    "    print(\"senti_features:\", senti_features)\n",
    "#     senti_features = senti_model(words_embedding)\n",
    "    classifier = tf.layers.Dense(sent_num, activation=tf.nn.relu, trainable=True)\n",
    "    senti_rst = classifier(senti_features)\n",
    "    sent_scores = tf.nn.softmax(senti_rst, axis=1)\n",
    "    sent_pred = tf.argmax(sent_scores, 1, name=\"predictions\")\n",
    "#     sent_loss = tf.losses.softmax_cross_entropy(\n",
    "#                         sent_y,\n",
    "#                         senti_rst,\n",
    "#                         weights=1.0,\n",
    "#                         label_smoothing=0,\n",
    "#                         scope=None,\n",
    "#                         loss_collection=tf.GraphKeys.LOSSES,\n",
    "#                         reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "#                     )\n",
    "    sent_loss = tf.reduce_mean((sent_y-senti_rst)*(sent_y-senti_rst))\n",
    "    sent_correct_predictions = tf.equal(sent_pred, tf.argmax(sent_y, 1))\n",
    "    sent_acc = tf.reduce_mean(tf.cast(sent_correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    sent_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    sent_train_op = tf.train.AdagradOptimizer(0.1).minimize(sent_loss, sent_global_step)        \n",
    "    return adict(\n",
    "                # dropout_keep_prob = self.dropout_keep_prob,\n",
    "                sent_x = sent_x,\n",
    "                sent_y = sent_y,\n",
    "                drop_out = lm.drop_out,\n",
    "                initial_rnn_state = lm_init_state, \n",
    "                feature = senti_features,\n",
    "                sent_scores = sent_scores,\n",
    "                sent_pred = sent_pred,\n",
    "                sent_loss = sent_loss,\n",
    "                sent_acc = sent_acc,\n",
    "                sent_global_step = sent_global_step,\n",
    "                sent_train_op = sent_train_op\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSentiModel(sess, saver, logger, summary_writer, model_dir, train_model, senti_reader, train_batch, test_batch):\n",
    "    train_iter = 100000\n",
    "    rnn_state = sess.run(train_model.initial_rnn_state)\n",
    "    for t_epoch in range(10): \n",
    "        # for validation\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for t_iter in range(train_iter):\n",
    "            data_X, data_Y = senti_reader.GetTrainingBatch(\n",
    "                                            t_iter, \n",
    "                                            train_batch\n",
    "                            )\n",
    "            feed_dic = {\n",
    "                        train_model.sent_x: data_X, \n",
    "                        train_model.drop_out: 0.8,\n",
    "                        train_model.sent_y: data_Y,\n",
    "                        train_model.initial_rnn_state:rnn_state\n",
    "            }\n",
    "            _, step, loss, acc = sess.run(\n",
    "                                        [train_model.sent_train_op, \n",
    "                                         train_model.sent_global_step, \n",
    "                                         train_model.sent_loss, \n",
    "                                         train_model.sent_acc], \n",
    "                                        feed_dic)\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_acc\", simple_value=acc),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            \n",
    "            sum_loss += loss\n",
    "            sum_acc += acc\n",
    "            if t_iter % 10 == 9:\n",
    "                sum_loss = sum_loss / 10\n",
    "                sum_acc = sum_acc / 10\n",
    "                ret_acc = sum_acc\n",
    "                print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                sum_acc = 0.0\n",
    "                sum_loss = 0.0\n",
    "        saver.save(sess, model_dir)\n",
    "        # for validation\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for t_iter in range(100):\n",
    "            data_X, data_Y = senti_reader.GetTestData(t_iter, test_batch)\n",
    "            feed_dic = {\n",
    "                        train_model.sent_x: data_X, \n",
    "                        train_model.drop_out: 1.0,\n",
    "                        train_model.sent_y: data_Y,\n",
    "                        train_model.initial_rnn_state:rnn_state\n",
    "            }\n",
    "            loss, acc = sess.run([train_model.sent_loss, train_model.sent_acc], feed_dic)\n",
    "            sum_loss += loss\n",
    "            sum_acc += acc    \n",
    "        sum_loss = sum_loss / 100\n",
    "        sum_acc = sum_acc / 100\n",
    "        ret_acc = sum_acc\n",
    "        print(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        logger.info(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        \n",
    "        summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_valid_loss\", simple_value=sum_loss),\n",
    "                tf.Summary.Value(tag=\"step_valid_acc\", simple_value=sum_acc),\n",
    "            ])\n",
    "        summary_writer.add_summary(summary, t_epoch)\n",
    "        \n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/hadoop/ERD/model.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32)\n",
      "input_: Tensor(\"Placeholder_1:0\", shape=(20, 41, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_cnn: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(820, 1100), dtype=float32, device=/device:GPU:0)\n",
      "words_feature: Tensor(\"Max:0\", shape=(20, 300), dtype=float32, device=/device:GPU:0)\n",
      "sentence_embedding: (LSTMStateTuple(c=<tf.Tensor 'LSTM_1/rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/Add_143:0' shape=(20, 300) dtype=float32>, h=<tf.Tensor 'LSTM_1/rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_215:0' shape=(20, 300) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'LSTM_1/rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/Add_143:0' shape=(20, 300) dtype=float32>, h=<tf.Tensor 'LSTM_1/rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_215:0' shape=(20, 300) dtype=float32>))\n",
      "senti_features: Tensor(\"add:0\", shape=(20, 300), dtype=float32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:55.632224 140123195463488 logger.py:24] 2019-09-01 10:18:55 Step: 10 Training loss: 1.0986123085021973 accuracy: 0.5049999982118607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:55 Step: 10 Training loss: 1.0986123085021973 accuracy: 0.5049999982118607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:56.046760 140123195463488 logger.py:24] 2019-09-01 10:18:56 Step: 20 Training loss: 1.1245155692100526 accuracy: 0.5150000095367432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:56 Step: 20 Training loss: 1.1245155692100526 accuracy: 0.5150000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:56.457043 140123195463488 logger.py:24] 2019-09-01 10:18:56 Step: 30 Training loss: 1.0986123085021973 accuracy: 0.5450000047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:56 Step: 30 Training loss: 1.0986123085021973 accuracy: 0.5450000047683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:56.858850 140123195463488 logger.py:24] 2019-09-01 10:18:56 Step: 40 Training loss: 1.0986123085021973 accuracy: 0.5049999952316284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:56 Step: 40 Training loss: 1.0986123085021973 accuracy: 0.5049999952316284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:57.283736 140123195463488 logger.py:24] 2019-09-01 10:18:57 Step: 50 Training loss: 1.0986123085021973 accuracy: 0.5099999994039536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:57 Step: 50 Training loss: 1.0986123085021973 accuracy: 0.5099999994039536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:57.709704 140123195463488 logger.py:24] 2019-09-01 10:18:57 Step: 60 Training loss: 1.0986123085021973 accuracy: 0.5300000041723252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:57 Step: 60 Training loss: 1.0986123085021973 accuracy: 0.5300000041723252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:58.127620 140123195463488 logger.py:24] 2019-09-01 10:18:58 Step: 70 Training loss: 1.0986123085021973 accuracy: 0.4900000005960464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:58 Step: 70 Training loss: 1.0986123085021973 accuracy: 0.4900000005960464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:58.540694 140123195463488 logger.py:24] 2019-09-01 10:18:58 Step: 80 Training loss: 1.0986123085021973 accuracy: 0.5149999976158142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:58 Step: 80 Training loss: 1.0986123085021973 accuracy: 0.5149999976158142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:58.933269 140123195463488 logger.py:24] 2019-09-01 10:18:58 Step: 90 Training loss: 1.0986123085021973 accuracy: 0.5249999970197677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:58 Step: 90 Training loss: 1.0986123085021973 accuracy: 0.5249999970197677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:59.331932 140123195463488 logger.py:24] 2019-09-01 10:18:59 Step: 100 Training loss: 1.0986123085021973 accuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:59 Step: 100 Training loss: 1.0986123085021973 accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:18:59.723435 140123195463488 logger.py:24] 2019-09-01 10:18:59 Step: 110 Training loss: 1.0986123085021973 accuracy: 0.4599999934434891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:18:59 Step: 110 Training loss: 1.0986123085021973 accuracy: 0.4599999934434891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:00.123543 140123195463488 logger.py:24] 2019-09-01 10:19:00 Step: 120 Training loss: 1.0986123085021973 accuracy: 0.5349999994039536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:00 Step: 120 Training loss: 1.0986123085021973 accuracy: 0.5349999994039536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:00.507950 140123195463488 logger.py:24] 2019-09-01 10:19:00 Step: 130 Training loss: 1.0986123085021973 accuracy: 0.5300000041723252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:00 Step: 130 Training loss: 1.0986123085021973 accuracy: 0.5300000041723252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:00.894550 140123195463488 logger.py:24] 2019-09-01 10:19:00 Step: 140 Training loss: 1.0986123085021973 accuracy: 0.4150000035762787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:00 Step: 140 Training loss: 1.0986123085021973 accuracy: 0.4150000035762787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:01.326805 140123195463488 logger.py:24] 2019-09-01 10:19:01 Step: 150 Training loss: 1.0986123085021973 accuracy: 0.4799999982118607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:01 Step: 150 Training loss: 1.0986123085021973 accuracy: 0.4799999982118607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:01.754597 140123195463488 logger.py:24] 2019-09-01 10:19:01 Step: 160 Training loss: 1.0986123085021973 accuracy: 0.4499999970197678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:01 Step: 160 Training loss: 1.0986123085021973 accuracy: 0.4499999970197678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:02.141881 140123195463488 logger.py:24] 2019-09-01 10:19:02 Step: 170 Training loss: 1.0986123085021973 accuracy: 0.5649999946355819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:02 Step: 170 Training loss: 1.0986123085021973 accuracy: 0.5649999946355819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:02.529832 140123195463488 logger.py:24] 2019-09-01 10:19:02 Step: 180 Training loss: 1.0986123085021973 accuracy: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:02 Step: 180 Training loss: 1.0986123085021973 accuracy: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:02.924097 140123195463488 logger.py:24] 2019-09-01 10:19:02 Step: 190 Training loss: 1.0986123085021973 accuracy: 0.42000000476837157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:02 Step: 190 Training loss: 1.0986123085021973 accuracy: 0.42000000476837157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:03.311193 140123195463488 logger.py:24] 2019-09-01 10:19:03 Step: 200 Training loss: 1.0986123085021973 accuracy: 0.4850000023841858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:03 Step: 200 Training loss: 1.0986123085021973 accuracy: 0.4850000023841858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 10:19:15.429938 140123195463488 logger.py:24] 2019-09-01 10:19:15 Step: 210 Training loss: 1.0986123085021973 accuracy: 0.4299999997019768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 10:19:15 Step: 210 Training loss: 1.0986123085021973 accuracy: 0.4299999997019768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1aa2ca4325ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SentiCNNGPU/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mTrainSentiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SentiCNNGPU/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenti_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5e706b576cc9>\u001b[0m in \u001b[0;36mTrainSentiModel\u001b[0;34m(sess, saver, logger, summary_writer, model_dir, train_model, senti_reader, train_batch, test_batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                          \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                          train_model.sent_acc], \n\u001b[0;32m---> 24\u001b[0;31m                                         feed_dic)\n\u001b[0m\u001b[1;32m     25\u001b[0m             summary = tf.Summary(value=[\n\u001b[1;32m     26\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"step_train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reuse model to train senti model\n",
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "device = '/GPU:0'\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g, config=gpu_config) as sess:\n",
    "        w2v = lstm_char_cnn.WordEmbedding(\n",
    "                        max_word_length = FLAGS.max_char_num , \n",
    "                        char_vocab_size = char_vocab.size, \n",
    "                        char_embed_size = FLAGS.char_embed_size, \n",
    "                        kernels = eval(FLAGS.kernels), \n",
    "                        kernel_features = eval(FLAGS.kernel_features), \n",
    "                        num_highway_layers = FLAGS.highway_layers,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "        lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    rnn_size = FLAGS.embedding_dim, \n",
    "                    num_rnn_layers = FLAGS.rnn_layers, \n",
    "                    word_vocab_size = word_vocab.size\n",
    "                )\n",
    "\n",
    "        char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                            w2v, lstm_lm, \n",
    "                            batch_size = FLAGS.batch_size, \n",
    "                            num_unroll_steps = FLAGS.max_sent_len, \n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            learning_rate = FLAGS.learning_rate,\n",
    "                            max_grad_norm = FLAGS.max_grad_norm\n",
    "                         )\n",
    "        val_list1 = tf.global_variables()\n",
    "        saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "        sess.run(tf.variables_initializer(val_list1))\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"lstmCharCNNGPU/\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "            \n",
    "        with tf.device(device):\n",
    "            #sentiment analysis model\n",
    "            s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "            senti_train_graph = InferSentiTrainGraph(\n",
    "                                    w2v, \n",
    "                                    lstm_lm, \n",
    "                                    s_model, \n",
    "                                    batchsize=20,\n",
    "                                    max_word_num = sentiReader.max_sent_len, \n",
    "                                    max_char_num = FLAGS.max_char_num, \n",
    "                                    hidden_dim = FLAGS.hidden_dim, \n",
    "                                    sent_num = FLAGS.sent_num,\n",
    "                                    embedding_dim = FLAGS.embedding_dim,\n",
    "                                    learning_rate = 0.1\n",
    "                                )\n",
    "        val_list2 = tf.global_variables()\n",
    "        saver2 = tf.train.Saver(val_list2, max_to_keep=4)\n",
    "        uninitialized_vars = list( filter(lambda var: var not in val_list1, val_list2) )\n",
    "        sess.run(tf.variables_initializer(uninitialized_vars))\n",
    "        summary_writer = tf.summary.FileWriter(\"SentiCNNGPU/\", graph=sess.graph)\n",
    "        TrainSentiModel(sess, saver, logger, summary_writer, \"SentiCNNGPU/\", senti_train_graph, sentiReader, 20, 20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
