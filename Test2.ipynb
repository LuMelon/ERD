{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 16:10:10.932252 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:3: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "W0813 16:10:10.975089 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0813 16:10:10.975697 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 16:10:10.977313 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0813 16:10:11.624479 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0813 16:10:11.648278 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0813 16:10:11.648894 139974489069376 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from model import RL_GRU2\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train\n",
      "reading valid\n",
      "reading test\n",
      "\n",
      "actual longest token length is: 21\n",
      "size of word vocabulary: 10000\n",
      "size of char vocabulary: 51\n",
      "number of tokens in train: 929589\n",
      "number of tokens in valid: 73760\n",
      "number of tokens in test: 82430\n"
     ]
    }
   ],
   "source": [
    "import lstm_char_cnn\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "import PTB_data_reader\n",
    "import config\n",
    "FLAGS = tf.flags.FLAGS\n",
    "word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, eos=FLAGS.EOS)\n",
    "\n",
    "train_reader =PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                              FLAGS.batch_size, FLAGS.num_unroll_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 21:03:58.122577 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:129: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0813 21:03:58.123331 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:130: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0813 21:03:58.124018 140435558864704 deprecation.py:506] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 21:03:58.131158 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:137: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.\n",
      "\n",
      "W0813 21:03:58.147720 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:99: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0813 21:03:58.644132 140435558864704 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0813 21:03:58.644644 140435558864704 deprecation.py:323] From /home/hadoop/ERD/lstm_char_cnn.py:155: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0813 21:03:58.648414 140435558864704 deprecation.py:323] From /home/hadoop/ERD/lstm_char_cnn.py:161: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0813 21:03:58.678512 140435558864704 deprecation.py:323] From /home/hadoop/ERD/lstm_char_cnn.py:171: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "W0813 21:03:58.826826 140435558864704 deprecation.py:506] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 21:04:00.184902 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:196: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 21:04:00.217486 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:216: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0813 21:04:02.136613 140435558864704 deprecation.py:323] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 21:04:02.162136 140435558864704 deprecation_wrapper.py:119] From /home/hadoop/ERD/lstm_char_cnn.py:219: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_train = tf.placeholder(tf.int32, shape=[FLAGS.batch_size, FLAGS.num_unroll_steps, max_word_length], name=\"input\")\n",
    "char_model = lstm_char_cnn.inference_graph(\n",
    "                    char_vocab_size=char_vocab.size,\n",
    "                    word_vocab_size=word_vocab.size,\n",
    "                    input_ = input_train,\n",
    "                    char_embed_size=FLAGS.char_embed_size,\n",
    "                    batch_size=FLAGS.batch_size,\n",
    "                    num_highway_layers=FLAGS.highway_layers,\n",
    "                    num_rnn_layers=FLAGS.rnn_layers,\n",
    "                    rnn_size=FLAGS.rnn_size,\n",
    "                    max_word_length=50,\n",
    "                    kernels=eval(FLAGS.kernels),\n",
    "                    kernel_features=eval(FLAGS.kernel_features),\n",
    "                    num_unroll_steps=FLAGS.num_unroll_steps,\n",
    "                    dropout=FLAGS.dropout\n",
    "    )\n",
    "char_model.update(lstm_char_cnn.loss_graph(char_model.logits, FLAGS.batch_size, FLAGS.num_unroll_steps))\n",
    "char_model.update(lstm_char_cnn.training_graph(char_model.loss * FLAGS.num_unroll_steps,\n",
    "                FLAGS.learning_rate, FLAGS.max_grad_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "session = tf.Session()\n",
    "with session.as_default():\n",
    "    session.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def Train_Char_Model(session, input_train, train_model, train_reader, saver, summary_writer):\n",
    "    best_valid_loss = None\n",
    "    rnn_state = session.run(train_model.initial_rnn_state)\n",
    "#     for epoch in range(FLAGS.max_epochs):\n",
    "    for epoch in range(1):\n",
    "        epoch_start_time = time.time()\n",
    "        avg_train_loss = 0.0\n",
    "        count = 0\n",
    "        for x, y in train_reader.iter():\n",
    "            count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            loss, _, rnn_state, gradient_norm, step, _ = session.run([\n",
    "                train_model.loss,\n",
    "                train_model.train_op,\n",
    "                train_model.final_rnn_state,\n",
    "                train_model.global_norm,\n",
    "                train_model.global_step,\n",
    "                train_model.clear_char_embedding_padding\n",
    "            ], {\n",
    "                input_train: x,\n",
    "                train_model.targets: y,\n",
    "                train_model.initial_rnn_state: rnn_state\n",
    "            })\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_perplexity\", simple_value=np.exp(loss)),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "            avg_train_loss += 0.05 * (loss - avg_train_loss)\n",
    "\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            if count % FLAGS.print_every == 0:\n",
    "                print('%6d: %d [%5d/%5d], train_loss/perplexity = %6.8f/%6.7f secs/batch = %.4fs, grad.norm=%6.8f' % (step,\n",
    "                                                        epoch, count,\n",
    "                                                        train_reader.length,\n",
    "                                                        loss, np.exp(loss),\n",
    "                                                        time_elapsed,\n",
    "                                                        gradient_norm))\n",
    "                break\n",
    "        print('Epoch training time:', time.time()-epoch_start_time)\n",
    "        save_as = '%s/epoch%03d_%.4f.model' % (FLAGS.train_dir, epoch, avg_train_loss)\n",
    "        saver.save(session, save_as)\n",
    "        print('Saved char model', save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11: 0 [    5/  531], train_loss/perplexity = 7.78428316/2402.5432129 secs/batch = 2.2163s, grad.norm=7.60929680\n",
      "Epoch training time: 10.185387372970581\n",
      "Saved char model cv/epoch000_1.7867.model\n"
     ]
    }
   ],
   "source": [
    "Train_Char_Model(session, input_train, char_model, train_reader, saver, summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_state = session.run(char_model.initial_rnn_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.random.randint(0, char_vocab.size, size = [FLAGS.batch_size, FLAGS.num_unroll_steps, max_word_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_o= session.run([\n",
    "                char_model.input_cnn\n",
    "            ], {\n",
    "                input_train: x_in,\n",
    "                char_model.initial_rnn_state: rnn_state\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 35, 462)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
