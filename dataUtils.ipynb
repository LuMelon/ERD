{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # get data files path\n",
    "    data = {}\n",
    "    files = []\n",
    "    data_ID = []\n",
    "    data_len = []\n",
    "    list_files(data_path) #load all filepath to files\n",
    "    max_sent = 0\n",
    "    # load data to json\n",
    "    for file in files:\n",
    "        td = data_process(file) # read out the information from json file, and organized it as {dataID:{'key':val}}\n",
    "        for key in td.keys(): # use temporary data to organize the final whole data\n",
    "            if key in data:\n",
    "                data[key]['text'].append(td[key]['text'][0])\n",
    "                data[key]['created_at'].append(td[key]['created_at'][0])\n",
    "            else:\n",
    "                data[key] = td[key]\n",
    "    # convert to my data style\n",
    "    for key, value in data.items():\n",
    "        temp_list = []\n",
    "        for i in range(len(data[key]['text'])):\n",
    "            temp_list.append([data[key]['created_at'][i], data[key]['text'][i]])\n",
    "        temp_list = sortTempList(temp_list)\n",
    "        data[key]['text'] = []\n",
    "        data[key]['created_at'] = []\n",
    "        ttext = \"\"\n",
    "        last = 0\n",
    "        for i in range(len(temp_list)):\n",
    "            if temp_list[i][0] - temp_list[0][0] > FLAGS.time_limit * 3600 or len(data[key]['created_at']) >= 100:\n",
    "                break\n",
    "            if i % FLAGS.post_fn == 0: # merge the fixed number of texts in a time interval\n",
    "                if len(ttext) > 0: # if there are data already in ttext, output it as a new instance\n",
    "                    data[key]['text'].append(ttext)\n",
    "                    data[key]['created_at'].append(temp_list[i][0])\n",
    "                ttext = temp_list[i][1]\n",
    "            else:\n",
    "                ttext += \" \" + temp_list[i][1]\n",
    "            last = i\n",
    "        # keep the last one\n",
    "        if len(ttext) > 0:\n",
    "            words = list( filter(lambda s: len(s)>0, [transIrregularWord(word) for word in re.split('([,\\n ]+)', ttext.strip() )]) )\n",
    "#             words = list(filter(lambda x:x!=' ' and x!='', re.split('([,\\n .]+)', ttext.strip()) ))\n",
    "            if len(words) > max_sent:\n",
    "                max_sent = len(words)\n",
    "            data[key]['text'].append(words)\n",
    "            data[key]['created_at'].append(temp_list[last][0])\n",
    "    for key in data.keys():\n",
    "        data_ID.append(key)\n",
    "    data_ID = random.sample(data_ID, len(data_ID)) #shuffle the data id\n",
    "    for i in range(len(data_ID)): #pre processing the extra informations\n",
    "        data_len.append(len(data[data_ID[i]]['text']))\n",
    "        if data[data_ID[i]]['label'] == \"rumours\":\n",
    "            data_y.append([1.0, 0.0])\n",
    "        else:\n",
    "            data_y.append([0.0, 1.0])\n",
    "    print(\"max_sent:\", max_sent, \",  max_seq_len:\", max(data_len))\n",
    "    eval_flag = int(len(data_ID) / 4) * 3\n",
    "    print(\"{} data loaded\".format(len(data)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
