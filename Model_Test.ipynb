{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "\n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\", reuse=tf.AUTO_REUSE):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    def __init__(self, max_word_length, char_vocab_size, char_embed_size, kernels, kernel_features, num_highway_layers):\n",
    "#             self.input_ = tf.placeholder(tf.int32, [None, max_word_length, char_vocab_size],name=\"W2V_input\")\n",
    "        self.max_word_length = max_word_length\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_embed_size = char_embed_size\n",
    "        self.kernels = kernels\n",
    "        self.kernel_features = kernel_features\n",
    "        self.num_highway_layers = num_highway_layers\n",
    "        with tf.variable_scope('Embedding', reuse=tf.AUTO_REUSE):\n",
    "            self.char_embedding = tf.get_variable('char_embedding', [self.char_vocab_size, self.char_embed_size])\n",
    "            ''' this op clears embedding vector of first symbol (symbol at position 0, which is by convention the position\n",
    "            of the padding symbol). It can be used to mimic Torch7 embedding operator that keeps padding mapped to\n",
    "            zero embedding vector and ignores gradient updates. For that do the following in TF:\n",
    "            1. after parameter initialization, apply this op to zero out padding embedding vector\n",
    "            2. after each gradient update, apply this op to keep padding at zero'''\n",
    "            self.clear_char_embedding_padding = tf.scatter_update(self.char_embedding, [0], tf.constant(0.0, shape=[1, self.char_embed_size]))\n",
    "            \n",
    "    def __call__(self, input_words):\n",
    "        input_ = input_words\n",
    "        with tf.variable_scope('Embedding', reuse=tf.AUTO_REUSE):\n",
    "            # [batch_size x max_word_length, num_unroll_steps, char_embed_size]\n",
    "            input_embedded = tf.nn.embedding_lookup(self.char_embedding, input_)\n",
    "            input_embedded = tf.reshape(input_embedded, [-1, self.max_word_length, self.char_embed_size])\n",
    "        input_cnn = self.tdnn(input_embedded, self.kernels, self.kernel_features)\n",
    "        ''' Maybe apply Highway '''\n",
    "#             if num_highway_layers > 0:\n",
    "        assert self.num_highway_layers > 0\n",
    "        input_cnn = self.highway(input_cnn, input_cnn.get_shape()[-1], num_layers=self.num_highway_layers, scope=\"CNN_OUT\")\n",
    "        return input_cnn\n",
    "    \n",
    "    def conv2d(self, input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "            b = tf.get_variable('b', [output_dim])\n",
    "        return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b\n",
    "\n",
    "    def highway(self, input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "        \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "        t = sigmoid(Wy + b)\n",
    "        z = t * g(Wy + b) + (1 - t) * y\n",
    "        where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            for idx in range(num_layers):\n",
    "                g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "                t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "                output = t * g + (1. - t) * input_\n",
    "                input_ = output\n",
    "        print(output)\n",
    "        return output\n",
    "\n",
    "    def tdnn(self, input_, kernels, kernel_features, scope='TDNN'):\n",
    "        '''\n",
    "        :input:           input float tensor of shape [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "        :kernels:         array of kernel sizes\n",
    "        :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "        '''\n",
    "        assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "        max_word_length = input_.get_shape()[1]\n",
    "        embed_size = input_.get_shape()[-1]\n",
    "        # input_: [batch_size*num_unroll_steps, 1, max_word_length, embed_size]\n",
    "        input_ = tf.expand_dims(input_, 1)\n",
    "        layers = []\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "                reduced_length = max_word_length - kernel_size + 1\n",
    "                # [batch_size*num_unroll_steps, 1, reduced_length, kernel_feature_size]\n",
    "                conv = self.conv2d(input_, kernel_feature_size, 1, kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "                # [batch_size*num_unroll_steps, 1, 1, kernel_feature_size]\n",
    "                pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "                layers.append(tf.squeeze(pool, [1, 2]))\n",
    "            if len(kernels) > 1:\n",
    "                output = tf.concat(layers, 1)\n",
    "            else:\n",
    "                output = layers[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 19:56:43.344120 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:3: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "W0815 19:56:43.389121 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0815 19:56:43.389778 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0815 19:56:43.391288 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0815 19:56:44.056740 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0815 19:56:44.083391 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0815 19:56:44.083859 139842992813888 deprecation_wrapper.py:119] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 40 ,  max_seq_len: 346\n",
      "5802 data loaded\n",
      "reading train\n",
      "reading valid\n",
      "reading test\n",
      "\n",
      "actual longest token length is: 21\n",
      "size of word vocabulary: 10000\n",
      "size of char vocabulary: 51\n",
      "number of tokens in train: 929589\n",
      "number of tokens in valid: 73760\n",
      "number of tokens in test: 82430\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from model import RL_GRU2\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "\n",
    "import lstm_char_cnn\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "import PTB_data_reader\n",
    "import config\n",
    "from dataUtils import *\n",
    "load_data_fast()\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, eos=FLAGS.EOS)\n",
    "\n",
    "train_reader =PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                              FLAGS.batch_size, FLAGS.max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 19:56:51.214496 139842992813888 deprecation.py:506] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "w2v = WordEmbedding(\n",
    "                    max_word_length=21, \n",
    "                    char_vocab_size=char_vocab.size, \n",
    "                    char_embed_size=FLAGS.char_embed_size, \n",
    "                    kernels=eval(FLAGS.kernels), \n",
    "                    kernel_features=eval(FLAGS.kernel_features), \n",
    "                    num_highway_layers=FLAGS.highway_layers\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM:\n",
    "    def __init__(self, batch_size, num_unroll_steps, input_dim, rnn_size, num_rnn_layers, word_vocab_size, dropout):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unroll_steps = num_unroll_steps\n",
    "        self.input_dim = input_dim\n",
    "        self.rnn_size = rnn_size\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "            def create_rnn_cell():\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.-dropout)\n",
    "                return cell\n",
    "            if self.num_rnn_layers > 1:\n",
    "                self.cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
    "            else:\n",
    "                self.cell = create_rnn_cell()\n",
    "            self.initial_rnn_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            \n",
    "    def __call__(self, input_cnn):\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "            input_cnn2 = [tf.squeeze(x, [1]) for x in tf.split(input_cnn, self.num_unroll_steps, 1)]\n",
    "            outputs, final_rnn_state = tf.contrib.rnn.static_rnn(self.cell, input_cnn2,\n",
    "                                             initial_state=self.initial_rnn_state, dtype=tf.float32)\n",
    "            # linear projection onto output (word) vocab\n",
    "            logits = []\n",
    "            with tf.variable_scope('WordEmbedding') as scope:\n",
    "                for idx, output in enumerate(outputs):\n",
    "                    if idx > 0:\n",
    "                        scope.reuse_variables()\n",
    "                    logits.append(linear(output, self.word_vocab_size))\n",
    "            return logits, outputs, final_rnn_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 19:56:56.496309 139842992813888 deprecation.py:323] From <ipython-input-5-ff52d24ad250>:11: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0815 19:56:56.500741 139842992813888 deprecation.py:323] From <ipython-input-5-ff52d24ad250>:15: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "input_dim = sum(eval(FLAGS.kernel_features))\n",
    "lm_model = LSTM_LM(batch_size=FLAGS.batch_size,\n",
    "                   num_unroll_steps=FLAGS.max_sent_len, \n",
    "                   input_dim=input_dim, \n",
    "                   rnn_size=FLAGS.rnn_size,\n",
    "                   num_rnn_layers=FLAGS.rnn_layers, \n",
    "                   word_vocab_size=word_vocab.size,\n",
    "                   dropout=0.9\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def infer_train_model(word2vec, LM, \n",
    "                      batch_size, \n",
    "                      num_unroll_steps, \n",
    "                      max_word_length, \n",
    "                      learning_rate,\n",
    "                      max_grad_norm\n",
    "                     ):\n",
    "    drop_out = tf.placeholder(tf.float32)\n",
    "    input_ = tf.placeholder(tf.int32, shape=[batch_size, num_unroll_steps, max_word_length], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int64, [batch_size, num_unroll_steps], name='targets')\n",
    "    \n",
    "    input_cnn = word2vec(input_)\n",
    "    input_cnn = tf.reshape(input_cnn, [batch_size, num_unroll_steps, -1])\n",
    "    logits, outputs, final_rnn_state = LM(input_cnn)\n",
    "    \n",
    "    with tf.variable_scope('Loss', reuse=tf.AUTO_REUSE):\n",
    "            target_list = [tf.squeeze(x, [1]) for x in tf.split(targets, num_unroll_steps, 1)]\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = target_list), name='loss')\n",
    "            \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    with tf.variable_scope('SGD_Training'):\n",
    "        # SGD learning parameter\n",
    "        learning_rate = tf.Variable(learning_rate, trainable=False, name='learning_rate')\n",
    "        # collect all trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, global_norm = tf.clip_by_global_norm(tf.gradients(loss, tvars), max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "        \n",
    "    return adict(\n",
    "        input = input_,\n",
    "        clear_char_embedding_padding=word2vec.clear_char_embedding_padding,\n",
    "        initial_rnn_state=LM.initial_rnn_state,\n",
    "        final_rnn_state=final_rnn_state,\n",
    "        rnn_outputs=outputs,\n",
    "        logits = logits,\n",
    "        targets=targets,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        global_step=global_step,\n",
    "        global_norm=global_norm,\n",
    "        train_op=train_op\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 19:56:59.089799 139842992813888 deprecation.py:323] From <ipython-input-5-ff52d24ad250>:24: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "W0815 19:56:59.234540 139842992813888 deprecation.py:506] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"CNN_OUT/add_7:0\", shape=(2000, 1100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 19:56:59.431271 139842992813888 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 19:56:59.455509 139842992813888 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 19:56:59.472136 139842992813888 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 19:56:59.487415 139842992813888 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 19:56:59.503583 139842992813888 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 19:57:02.471161 139842992813888 deprecation.py:323] From /home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "train_model = infer_train_model(  w2v, lm_model,\n",
    "                                  batch_size=lm_model.batch_size, \n",
    "                                  num_unroll_steps=lm_model.num_unroll_steps, \n",
    "                                  max_word_length=w2v.max_word_length,\n",
    "                                  learning_rate = 1.0,\n",
    "                                  max_grad_norm = 5.0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Char_Model(session, train_model, train_reader, saver, summary_writer):\n",
    "    best_valid_loss = None\n",
    "    rnn_state = session.run(train_model.initial_rnn_state)\n",
    "    for epoch in range(1):\n",
    "        epoch_start_time = time.time()\n",
    "        avg_train_loss = 0.0\n",
    "        count = 0\n",
    "        for x, y in train_reader.iter():\n",
    "            count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            loss, _, rnn_state, gradient_norm, step, _ = session.run([\n",
    "                train_model.loss,\n",
    "                train_model.train_op,\n",
    "                train_model.final_rnn_state,\n",
    "                train_model.global_norm,\n",
    "                train_model.global_step,\n",
    "                train_model.clear_char_embedding_padding\n",
    "            ], {\n",
    "                train_model.input: x,\n",
    "                train_model.targets: y,\n",
    "                train_model.initial_rnn_state: rnn_state\n",
    "            })\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_perplexity\", simple_value=np.exp(loss)),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "            avg_train_loss += 0.05 * (loss - avg_train_loss)\n",
    "\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            if count % FLAGS.print_every == 0:\n",
    "                print('%6d: %d [%5d/%5d], train_loss/perplexity = %6.8f/%6.7f secs/batch = %.4fs, grad.norm=%6.8f' % (step,\n",
    "                                                        epoch, count,\n",
    "                                                        train_reader.length,\n",
    "                                                        loss, np.exp(loss),\n",
    "                                                        time_elapsed,\n",
    "                                                        gradient_norm))\n",
    "        print('Epoch training time:', time.time()-epoch_start_time)\n",
    "        save_as = '%s/epoch%03d_%.4f.model' % (FLAGS.train_dir, epoch, avg_train_loss)\n",
    "        saver.save(session, save_as)\n",
    "        print('Saved char model', save_as)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
