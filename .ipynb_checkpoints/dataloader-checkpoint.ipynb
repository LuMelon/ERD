{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def transIrregularWord(word):\n",
    "    if not word:\n",
    "        return ''\n",
    "    pattern1 = \"[^A-Za-z]*$\" #punctuation at the end of sentence\n",
    "    pattern2 = \"^[^A-Za-z@#]*\" #punctuation at the start of sentence\n",
    "    word = re.sub(pattern2, \"\", re.sub(pattern1, \"\", word))\n",
    "    pattern3 = '(.*)http(.?)://(.*)' # url\n",
    "    pattern4 = '^[0-9]+.?[0-9]+$' # number\n",
    "    if not word:\n",
    "        return ''\n",
    "    elif word.__contains__('@'):\n",
    "        return 'person'\n",
    "    elif word.__contains__('#'):\n",
    "        return 'topic'\n",
    "    elif re.match(r'(.*)http?://(.*)', word, re.M|re.I|re.S):    \n",
    "        return 'links'\n",
    "    elif re.match(pattern4, word, re.M|re.I):\n",
    "        return 'number'\n",
    "    else:\n",
    "        return  word.lower()\n",
    "def sentence2words(line):\n",
    "    words = re.split('([,\\n ]+)', line.strip() )\n",
    "    words = list( filter(lambda s: len(s)>0, [transIrregularWord(word) for word in words]) )\n",
    "    return words\n",
    "\n",
    "def CSVFile2Dataset(filepath):\n",
    "    print(filepath)\n",
    "    df = pd.read_csv(filepath,encoding='latin-1')\n",
    "    instances = [(line[-1], line[0]) for line in df.values]\n",
    "    del df\n",
    "    texts = [sentence2words(instance[0]) for instance in instances]\n",
    "    labels = [instance[1] for instance in instances]\n",
    "    return texts, labels\n",
    "\n",
    "dirpath = '/Users/lumenglong/Downloads/trainingandtestdata'\n",
    "trainfile = 'training.1600000.processed.noemoticon.csv'\n",
    "testfile = 'testdata.manual.2009.06.14'\n",
    "trainset, trainlabel = CSVFile2Dataset(os.path.join(dirpath, trainfile))\n",
    "testset, testlabel = CSVFile2Dataset(os.path.join(dirpath, testfile))\n",
    "max_sent_len = max([(len(sent) for sent in texts) for texts in [trainset, testset]])\n",
    "print(max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/Users/lumenglong/word2vec.model')\n",
    "\n",
    "def GetTrainingBatch(batchId, batchsize, embedding_dim):\n",
    "    data_x = np.zeros([data_x = np.zeros([batch, max_sent_len, embedding_dim], dtype=np.float32)\n",
    "    data_y = np.zeros([batch_size, 3], dtype=np.int32)])\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(trainset):\n",
    "        startIdx = startIdx%len(trainset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(trainset):\n",
    "            mts = mts%len(trainset)\n",
    "        data_y[i][trainlabel[mts]/2] = 1\n",
    "        for j in range(len(trainset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[trainset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", FLAGS.batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n",
    "def GetTestData(batchId, batchsize, embedding_dim):\n",
    "    data_x = np.zeros([data_x = np.zeros([batchsize, max_sent_len, embedding_dim], dtype=np.float32)\n",
    "    data_y = np.zeros([batch_size, 3], dtype=np.int32)])\n",
    "    startIdx = batchId*batchsize\n",
    "    miss_vec = 0\n",
    "    hit_vec = 0\n",
    "    if startIdx >= len(testset):\n",
    "        startIdx = startIdx%len(testset)\n",
    "    for i in range(batchsize):\n",
    "        mts = startIdx + i\n",
    "        if mts >= len(testset):\n",
    "            mts = mts%len(testset)\n",
    "        data_y[i][testlabel[mts]/2] = 1\n",
    "        for j in range(len(testset[mts])):\n",
    "            try:\n",
    "                data_x[i][j] = word2vec[testset[mts][j]]\n",
    "            except KeyError:\n",
    "                print(\"word:\", m_word)\n",
    "                miss_vec += 1\n",
    "            except IndexError:\n",
    "                print(\"i, j, k:\", batch_size, '|',t_data_len[mts] ,'|', len(t_words))\n",
    "                print(\"word:\", m_word, \"(\", i, j, k, \")\")\n",
    "                raise\n",
    "            else:\n",
    "                hit_vec += 1\n",
    "    print(\"hit_vec | miss_vec:\", hit_vec, '|', miss_vec)\n",
    "    return data_x, data_y\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
