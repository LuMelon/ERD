{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 19:48:18.894572 139768431343424 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"RDMTrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors, word_len = \\\n",
    "    PTB_data_reader.load_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "charsVec = ['UNK', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', \n",
    "     '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4',\n",
    "     '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']',\n",
    "     '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "     'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', \n",
    "     't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '—', \n",
    "     '“', '”', '\"', \"’\"]\n",
    "charsVocab = {char:idx for (idx, char) in enumerate(charsVec)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "# gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "device = \"/CPU:0\"\n",
    "# device = \"/GPU:0\"\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph, config=gpu_config)\n",
    "with graph.as_default():\n",
    "    with  sess.as_default():\n",
    "        with tf.device('/GPU:0'):\n",
    "            w2v = lstm_char_cnn.WordEmbedding(\n",
    "                            max_word_length = FLAGS.max_char_num , \n",
    "                            char_vocab_size = char_vocab.size, \n",
    "                            char_embed_size = FLAGS.char_embed_size, \n",
    "                            kernels = eval(FLAGS.kernels), \n",
    "                            kernel_features = eval(FLAGS.kernel_features), \n",
    "                            num_highway_layers = FLAGS.highway_layers,\n",
    "                            embedding_dim = FLAGS.embedding_dim\n",
    "                        )\n",
    "            lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                        batch_size = FLAGS.batch_size, \n",
    "                        num_unroll_steps = FLAGS.max_sent_len, \n",
    "                        rnn_size = FLAGS.embedding_dim, \n",
    "                        num_rnn_layers = FLAGS.rnn_layers, \n",
    "                        word_vocab_size = word_vocab.size\n",
    "                    )\n",
    "\n",
    "            char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                                w2v, lstm_lm, \n",
    "                                batch_size = FLAGS.batch_size, \n",
    "                                num_unroll_steps = FLAGS.max_sent_len, \n",
    "                                max_word_length = FLAGS.max_char_num, \n",
    "                                learning_rate = FLAGS.learning_rate,\n",
    "                                max_grad_norm = FLAGS.max_grad_norm\n",
    "                             )\n",
    "#             s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "#             senti_train_graph = model.InferSentiTrainGraph(\n",
    "#                                     w2v, \n",
    "#                                     lstm_lm, \n",
    "#                                     s_model, \n",
    "#                                     batchsize=20,\n",
    "#                                     max_word_num = sentiReader.max_sent_len, \n",
    "#                                     max_char_num = FLAGS.max_char_num, \n",
    "#                                     hidden_dim = FLAGS.hidden_dim, \n",
    "#                                     sent_num = FLAGS.sent_num,\n",
    "#                                     embedding_dim = FLAGS.embedding_dim\n",
    "#                                 )\n",
    "            val_list1 = tf.global_variables()\n",
    "            saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "            sess.run(tf.variables_initializer(val_list1))\n",
    "            checkpoint = tf.train.get_checkpoint_state(\"lstmCharCNNModel/\")\n",
    "            if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkpoint.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2IDs(word):\n",
    "    rst = [charsVocab.get(char) for char in '{'+word.lower()+'}']\n",
    "    for i in range(len(rst)):\n",
    "        if rst[i] is None:\n",
    "            rst[i] = 0\n",
    "            # print(\"Unknown char:\", word[i])\n",
    "            # print(\"Word:\", word)\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 54, 39, 53, 54, 62]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2IDs(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"test\", \"fork\", \"learn\", \"man\", \"women\", \"girls\", \"boys\", \"king\", \"queen\"]\n",
    "\n",
    "word_vecs = [Word2IDs(w) for w in words]\n",
    "sent_arr = keras.preprocessing.sequence.pad_sequences(\n",
    "                                                word_vecs, \n",
    "                                                maxlen= 21, \n",
    "                                                dtype='int32', \n",
    "                                                padding='post', \n",
    "                                                truncating='post',\n",
    "                                                value=0.0\n",
    "                    )\n",
    "sent_arr = sent_arr.reshape([1, len(sent_arr), 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Words(words, g, sess, w2v, lm):\n",
    "    word_vecs = [Word2IDs(w) for w in words]\n",
    "    sent_arr = keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    word_vecs, \n",
    "                                                    maxlen= 21, \n",
    "                                                    dtype='int32', \n",
    "                                                    padding='post', \n",
    "                                                    truncating='post',\n",
    "                                                    value=0.0\n",
    "                        )\n",
    "    sent_arr = sent_arr.reshape([1, len(sent_arr), 21])\n",
    "    with g.as_default():\n",
    "        input_ = tf.placeholder(tf.int32, shape=[1, sent_arr.shape[1], 21], name=\"input\")\n",
    "        input_cnn = w2v(input_)\n",
    "        rst = tf.reshape(input_cnn, [1, sent_arr.shape[1], -1])\n",
    "    embeddings = sess.run(rst, feed_dict={input_:sent_arr})\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chars2vec\n",
    "c2vec = chars2vec.load_model('eng_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input_7:0\", shape=(1, 9, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_8/CNN_OUT/add_7:0\", shape=(9, 1100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "words = [\"look\", \"looook\", \"learn\", \"man\", \"women\", \"girls\", \"boys\", \"king\", \"queen\"]\n",
    "e = Predict_Words(words, graph, sess, w2v, lstm_lm)\n",
    "c = c2vec.vectorize_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5030319100763636, 0.9786026766256501)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(e[0], e[1]), cos_sim(c[0], c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4548039770992554, 0.4696885826166507)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(e[3]-e[4], e[6]-e[5]), cos_sim(c[3]-c[4], c[6]-c[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47212130539169267, 0.4755729040081983)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(e[3], e[0]), cos_sim(c[3], c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0902 19:46:30.816870 140631244523328 logger.py:24] (300, 64, 101, 31, 2, 2)\n",
      "I0902 19:46:30.817985 140631244523328 logger.py:24] 2019-09-02 19:46:30 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n",
      "300 64 101 31 2 2\n",
      "2019-09-02 19:46:30 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# load_data(FLAGS.data_file_path)\n",
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors, word_len = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# # save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import adict\n",
    "\n",
    "\n",
    "def InferRDMTrainGraph(char_model, lm, senti_model, rdm_model, batchsize,\n",
    "                            max_seq_len, max_word_num, max_char_num, \n",
    "                                hidden_dim, embedding_dim, class_num):\n",
    "    input_x = tf.placeholder(\n",
    "                        tf.int32, \n",
    "                        shape = [\n",
    "                                 batchsize, \n",
    "                                 max_seq_len, \n",
    "                                 max_word_num, \n",
    "                                 max_char_num\n",
    "                                 ], \n",
    "                        name=\"input_x\"\n",
    "                    )\n",
    "    input_y = tf.placeholder(\n",
    "                        tf.float32, \n",
    "                        shape = [batchsize, class_num], \n",
    "                        name=\"input_y\"\n",
    "                    )\n",
    "    x_len = tf.placeholder(\n",
    "                        tf.int32, \n",
    "                        [batchsize], \n",
    "                        name=\"x_len\"\n",
    "                    )\n",
    "    init_states = tf.placeholder(\n",
    "                        tf.float32, \n",
    "                        [batchsize, hidden_dim], \n",
    "                        name=\"init_states\"\n",
    "                    )\n",
    "    x_reshape = tf.reshape(\n",
    "                        input_x, \n",
    "                        [\n",
    "                         batchsize*max_seq_len, \n",
    "                         max_word_num, \n",
    "                         max_char_num\n",
    "                        ]\n",
    "                    )\n",
    "    print(\"x_reshape:\", x_reshape)\n",
    "    x_embedding = char_model(x_reshape)\n",
    "    print(\"x_embedding:\", x_embedding)\n",
    "    cnn_outs = tf.reshape(\n",
    "                        x_embedding, \n",
    "                        [\n",
    "                         batchsize*max_seq_len, \n",
    "                         max_word_num, \n",
    "                         sum(char_model.kernel_features)\n",
    "                        ]\n",
    "                    )\n",
    "    print(\"cnn_outs:\", cnn_outs)\n",
    "    # words_embedding, sentence_embedding = lm(cnn_outs)\n",
    "    cnn_outs_list = [tf.squeeze(x, [1]) \n",
    "    for x in tf.split(cnn_outs, max_word_num, 1)]\n",
    "    rdm_init_state = lm.cell.zero_state(\n",
    "                            batchsize*max_seq_len, \n",
    "                            dtype=tf.float32\n",
    "                        )\n",
    "    words_embedding, sentence_embedding = tf.contrib.rnn.static_rnn(\n",
    "                                        lm.cell, \n",
    "                                        cnn_outs_list,\n",
    "                                        initial_state=rdm_init_state, \n",
    "                                        dtype=tf.float32\n",
    "                                    )     \n",
    "    words_embedding = tf.identity(words_embedding, \n",
    "                                    \"rnn_out_puts\")\n",
    "    words_embedding = tf.transpose(words_embedding, \n",
    "                                        [1, 0, 2])\n",
    "    print(\"RDM words_embedding:\", words_embedding)\n",
    "#     x_senti = senti_model(words_embedding)\n",
    "    words_feature = tf.math.reduce_max( words_embedding , axis=1)\n",
    "    \n",
    "    with tf.variable_scope(\"Train_RDM\", reuse=tf.AUTO_REUSE):\n",
    "        fcn_layer = tf.layers.Dense(hidden_dim, activation=tf.compat.v1.keras.activations.sigmoid)\n",
    "        x_senti =  fcn_layer(sentence_embedding[-1][-1] + words_feature )\n",
    "        print(\"x_senti:\", x_senti)\n",
    "        RDM_Input = tf.reshape(\n",
    "                            x_senti, \n",
    "                            [\n",
    "                             batchsize, \n",
    "                             max_seq_len, \n",
    "                             hidden_dim\n",
    "                            ]\n",
    "                        )  \n",
    "        df_outputs, df_last_state = rdm_model(RDM_Input, x_len, init_states)\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        w_ps = tf.Variable(tf.truncated_normal([hidden_dim, class_num], stddev=0.1)) #\n",
    "        b_ps = tf.Variable(tf.constant(0.01, shape=[class_num])) #\n",
    "        l2_loss += tf.nn.l2_loss(w_ps) \n",
    "        l2_loss += tf.nn.l2_loss(b_ps) \n",
    "\n",
    "        pre_scores = tf.nn.xw_plus_b(df_last_state, w_ps, b_ps, name=\"p_scores\")\n",
    "        predictions = tf.argmax(pre_scores, 1, name=\"predictions\")\n",
    "\n",
    "        r_outputs = tf.reshape(df_outputs, [-1, hidden_dim]) #[batchsize*max_seq_len, output_dim]\n",
    "        scores_seq = tf.nn.softmax(tf.nn.xw_plus_b(r_outputs, w_ps, b_ps)) # [batchsize * max_seq_len, class_num] \n",
    "        out_seq = tf.reshape(scores_seq, [-1, max_seq_len, class_num], name=\"out_seq\") #[batchsize, max_seq_len, class_num]\n",
    "\n",
    "        df_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pre_scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(df_losses) + 0.1 * l2_loss\n",
    "\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "    df_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    df_train_op = tf.train.AdamOptimizer(0.01).minimize(loss, df_global_step)\n",
    "    return adict(\n",
    "                lm_drop_out = lm.drop_out,\n",
    "                dropout_keep_prob = rdm_model.dropout_keep_prob,\n",
    "                input_x = input_x,\n",
    "                input_y = input_y,\n",
    "                x_len = x_len,\n",
    "                init_states = init_states,\n",
    "                pre_scores = pre_scores,\n",
    "                predictions = predictions,\n",
    "                r_outputs = r_outputs,\n",
    "                scores_seq = scores_seq,\n",
    "                out_seq = out_seq,\n",
    "                df_losses = df_losses,\n",
    "                loss = loss,\n",
    "                correct_predictions = correct_predictions,\n",
    "                accuracy = accuracy,\n",
    "                df_global_step = df_global_step,\n",
    "                df_train_op = df_train_op\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRDMModel(sess, saver, summary_writter, logger, mm, batch_size, t_acc, t_steps, model_dir, new_data_len=[]):\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    ret_acc = 0.0\n",
    "    init_states = np.zeros([batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "\n",
    "    for i in range(t_steps):\n",
    "        if len(new_data_len) > 0:\n",
    "            x, x_len, y = get_df_batch(i, batch_size, new_data_len)\n",
    "        else:\n",
    "            x, x_len, y = get_df_batch(i, batch_size)\n",
    "        feed_dic = {\n",
    "                        mm.input_x: x, \n",
    "                        mm.x_len: x_len, \n",
    "                        mm.input_y: y, \n",
    "                        mm.init_states: init_states, \n",
    "                        mm.dropout_keep_prob: 0.8,\n",
    "                        mm.lm_drop_out: 0.8\n",
    "        }\n",
    "        _, step, loss, acc = sess.run([mm.df_train_op, mm.df_global_step, mm.loss, mm.accuracy], feed_dic)\n",
    "        \n",
    "        summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_acc\", simple_value=acc),\n",
    "            ])\n",
    "        \n",
    "        summary_writer.add_summary(summary, step)    \n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            ret_acc = sum_acc\n",
    "            print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            if sum_acc > t_acc:\n",
    "                break\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "        if i % 1000 == 999:\n",
    "            save_as = '%s/epoch%03d_%.4f.model' % (model_dir, epoch, avg_train_loss)\n",
    "            saver.save(session, save_as)\n",
    "            print('Saved char model', save_as)\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32, device=/device:GPU:0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b9f9c98bff59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                 \u001b[0mmax_word_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_char_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                 \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                 \u001b[0mmax_grad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                              )\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#             s_model = model.SentiModel(FLAGS.hidden_dim, 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ERD/lstm_char_cnn.py\u001b[0m in \u001b[0;36minfer_train_model\u001b[0;34m(word2vec, LM, batch_size, num_unroll_steps, max_word_length, learning_rate, max_grad_norm)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# collect all trainable variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mtvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    745\u001b[0m                       ignore_existing=True):\n\u001b[1;32m    746\u001b[0m                     \u001b[0min_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           \u001b[0m_LogOpGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m           \u001b[0;31m# If no grad_fn is defined or none of out_grads is available,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_LogOpGradients\u001b[0;34m(op, out_grads, in_grads)\u001b[0m\n\u001b[1;32m    906\u001b[0m                \", \".join([x.name for x in out_grads if _FilterGrad(x)]))\n\u001b[1;32m    907\u001b[0m   logging.vlog(1, \"  out --> %s\",\n\u001b[0;32m--> 908\u001b[0;31m                \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    906\u001b[0m                \", \".join([x.name for x in out_grads if _FilterGrad(x)]))\n\u001b[1;32m    907\u001b[0m   logging.vlog(1, \"  out --> %s\",\n\u001b[0;32m--> 908\u001b[0;31m                \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reuse model to train RDMModel\n",
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "# gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "device = \"/CPU:0\"\n",
    "# device = \"/GPU:0\"\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g, config=gpu_config) as sess:\n",
    "        with tf.device('/GPU:0'):\n",
    "            w2v = lstm_char_cnn.WordEmbedding(\n",
    "                            max_word_length = FLAGS.max_char_num , \n",
    "                            char_vocab_size = char_vocab.size, \n",
    "                            char_embed_size = FLAGS.char_embed_size, \n",
    "                            kernels = eval(FLAGS.kernels), \n",
    "                            kernel_features = eval(FLAGS.kernel_features), \n",
    "                            num_highway_layers = FLAGS.highway_layers,\n",
    "                            embedding_dim = FLAGS.embedding_dim\n",
    "                        )\n",
    "            lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                        batch_size = FLAGS.batch_size, \n",
    "                        num_unroll_steps = FLAGS.max_sent_len, \n",
    "                        rnn_size = FLAGS.embedding_dim, \n",
    "                        num_rnn_layers = FLAGS.rnn_layers, \n",
    "                        word_vocab_size = word_vocab.size\n",
    "                    )\n",
    "\n",
    "            char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                                w2v, lstm_lm, \n",
    "                                batch_size = FLAGS.batch_size, \n",
    "                                num_unroll_steps = FLAGS.max_sent_len, \n",
    "                                max_word_length = FLAGS.max_char_num, \n",
    "                                learning_rate = FLAGS.learning_rate,\n",
    "                                max_grad_norm = FLAGS.max_grad_norm\n",
    "                             )\n",
    "#             s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "#             senti_train_graph = model.InferSentiTrainGraph(\n",
    "#                                     w2v, \n",
    "#                                     lstm_lm, \n",
    "#                                     s_model, \n",
    "#                                     batchsize=20,\n",
    "#                                     max_word_num = sentiReader.max_sent_len, \n",
    "#                                     max_char_num = FLAGS.max_char_num, \n",
    "#                                     hidden_dim = FLAGS.hidden_dim, \n",
    "#                                     sent_num = FLAGS.sent_num,\n",
    "#                                     embedding_dim = FLAGS.embedding_dim\n",
    "#                                 )\n",
    "            val_list1 = tf.global_variables()\n",
    "            saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "            sess.run(tf.variables_initializer(val_list1))\n",
    "            checkpoint = tf.train.get_checkpoint_state(\"lstmCharCNNModel/\")\n",
    "            if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "            #RDMModel\n",
    "            rdm_model = model.RDM_Model(\n",
    "                    max_seq_len = FLAGS.max_seq_len, \n",
    "                    max_word_num = FLAGS.max_sent_len, \n",
    "                    embedding_dim = FLAGS.embedding_dim, \n",
    "                    hidden_dim = FLAGS.hidden_dim\n",
    "                )\n",
    "            rdm_train_graph = InferRDMTrainGraph(\n",
    "                            w2v, lstm_lm, None, rdm_model, \n",
    "                            batchsize=5,\n",
    "                            max_seq_len = FLAGS.max_seq_len, \n",
    "                            max_word_num = FLAGS.max_sent_len, \n",
    "                            max_char_num = FLAGS.max_char_num, \n",
    "                            hidden_dim = FLAGS.hidden_dim, \n",
    "                            embedding_dim = FLAGS.embedding_dim,\n",
    "                            class_num = FLAGS.class_num\n",
    "                    )\n",
    "            val_list2 = tf.global_variables()\n",
    "            saver2 = tf.train.Saver(val_list2, max_to_keep=4)\n",
    "            uninitialized_vars = list( filter(lambda var: var not in val_list1, val_list2) )\n",
    "#             print(\"uninitialized_vars:\", uninitialized_vars)\n",
    "            sess.run(tf.variables_initializer(uninitialized_vars))\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(\"RDMGPUTrain/\", graph=sess.graph)\n",
    "        TrainRDMModel(sess, saver, summary_writer, logger, rdm_train_graph, 5, 0.9, 100000, \"RDMGPUTrain/\", new_data_len=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
