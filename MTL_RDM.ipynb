{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 12:17:45.989879 139700702275392 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 12:18:02.012879 139700702275392 logger.py:24] (300, 64, 101, 31, 2, 2)\n",
      "I0901 12:18:02.013772 139700702275392 logger.py:24] 2019-09-01 12:18:02 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 64 101 31 2 2\n",
      "2019-09-01 12:18:02 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"RDMTrain\")\n",
    "\n",
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")\n",
    "\n",
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "from model import adict\n",
    "\n",
    "\n",
    "def InferRDMTrainGraph(char_model, lm, senti_model, rdm_model, batchsize,\n",
    "                            max_seq_len, max_word_num, max_char_num, \n",
    "                                hidden_dim, embedding_dim, class_num):\n",
    "    input_x = tf.placeholder(\n",
    "                        tf.int32, \n",
    "                        shape = [\n",
    "                                 batchsize, \n",
    "                                 max_seq_len, \n",
    "                                 max_word_num, \n",
    "                                 max_char_num\n",
    "                                 ], \n",
    "                        name=\"input_x\"\n",
    "                    )\n",
    "    input_y = tf.placeholder(\n",
    "                        tf.float32, \n",
    "                        shape = [batchsize, class_num], \n",
    "                        name=\"input_y\"\n",
    "                    )\n",
    "    x_len = tf.placeholder(\n",
    "                        tf.int32, \n",
    "                        [batchsize], \n",
    "                        name=\"x_len\"\n",
    "                    )\n",
    "    init_states = tf.placeholder(\n",
    "                        tf.float32, \n",
    "                        [batchsize, hidden_dim], \n",
    "                        name=\"init_states\"\n",
    "                    )\n",
    "    x_reshape = tf.reshape(\n",
    "                        input_x, \n",
    "                        [\n",
    "                         batchsize*max_seq_len, \n",
    "                         max_word_num, \n",
    "                         max_char_num\n",
    "                        ]\n",
    "                    )\n",
    "    print(\"x_reshape:\", x_reshape)\n",
    "    x_embedding = char_model(x_reshape)\n",
    "    print(\"x_embedding:\", x_embedding)\n",
    "    cnn_outs = tf.reshape(\n",
    "                        x_embedding, \n",
    "                        [\n",
    "                         batchsize*max_seq_len, \n",
    "                         max_word_num, \n",
    "                         sum(char_model.kernel_features)\n",
    "                        ]\n",
    "                    )\n",
    "    print(\"cnn_outs:\", cnn_outs)\n",
    "    # words_embedding, sentence_embedding = lm(cnn_outs)\n",
    "    cnn_outs_list = [tf.squeeze(x, [1]) \n",
    "    for x in tf.split(cnn_outs, max_word_num, 1)]\n",
    "    rdm_init_state = lm.cell.zero_state(\n",
    "                            batchsize*max_seq_len, \n",
    "                            dtype=tf.float32\n",
    "                        )\n",
    "    words_embedding, sentence_embedding = tf.contrib.rnn.static_rnn(\n",
    "                                        lm.cell, \n",
    "                                        cnn_outs_list,\n",
    "                                        initial_state=rdm_init_state, \n",
    "                                        dtype=tf.float32\n",
    "                                    )     \n",
    "    words_embedding = tf.identity(words_embedding, \n",
    "                                    \"rnn_out_puts\")\n",
    "    words_embedding = tf.transpose(words_embedding, \n",
    "                                        [1, 0, 2])\n",
    "    print(\"RDM words_embedding:\", words_embedding)\n",
    "#     x_senti = senti_model(words_embedding)\n",
    "    words_feature = tf.math.reduce_max( words_embedding , axis=1)\n",
    "    \n",
    "    with tf.variable_scope(\"Train_RDM\", reuse=tf.AUTO_REUSE):\n",
    "        fcn_layer = tf.layers.Dense(hidden_dim, activation=tf.compat.v1.keras.activations.sigmoid)\n",
    "        x_senti =  fcn_layer(sentence_embedding[-1][-1] + words_feature )\n",
    "        print(\"x_senti:\", x_senti)\n",
    "        RDM_Input = tf.reshape(\n",
    "                            x_senti, \n",
    "                            [\n",
    "                             batchsize, \n",
    "                             max_seq_len, \n",
    "                             hidden_dim\n",
    "                            ]\n",
    "                        )  \n",
    "        df_outputs, df_last_state = rdm_model(RDM_Input, x_len, init_states)\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        w_ps = tf.Variable(tf.truncated_normal([hidden_dim, class_num], stddev=0.1)) #\n",
    "        b_ps = tf.Variable(tf.constant(0.01, shape=[class_num])) #\n",
    "        l2_loss += tf.nn.l2_loss(w_ps) \n",
    "        l2_loss += tf.nn.l2_loss(b_ps) \n",
    "\n",
    "        pre_scores = tf.nn.xw_plus_b(df_last_state, w_ps, b_ps, name=\"p_scores\")\n",
    "        predictions = tf.argmax(pre_scores, 1, name=\"predictions\")\n",
    "\n",
    "        r_outputs = tf.reshape(df_outputs, [-1, hidden_dim]) #[batchsize*max_seq_len, output_dim]\n",
    "        scores_seq = tf.nn.softmax(tf.nn.xw_plus_b(r_outputs, w_ps, b_ps)) # [batchsize * max_seq_len, class_num] \n",
    "        out_seq = tf.reshape(scores_seq, [-1, max_seq_len, class_num], name=\"out_seq\") #[batchsize, max_seq_len, class_num]\n",
    "\n",
    "        df_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pre_scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(df_losses) + 0.1 * l2_loss\n",
    "\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "    df_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    df_train_op = tf.train.AdamOptimizer(0.01).minimize(loss, df_global_step)\n",
    "    return adict(\n",
    "                lm_drop_out = lm.drop_out,\n",
    "                dropout_keep_prob = rdm_model.dropout_keep_prob,\n",
    "                input_x = input_x,\n",
    "                input_y = input_y,\n",
    "                x_len = x_len,\n",
    "                init_states = init_states,\n",
    "                pre_scores = pre_scores,\n",
    "                predictions = predictions,\n",
    "                r_outputs = r_outputs,\n",
    "                scores_seq = scores_seq,\n",
    "                out_seq = out_seq,\n",
    "                df_losses = df_losses,\n",
    "                loss = loss,\n",
    "                correct_predictions = correct_predictions,\n",
    "                accuracy = accuracy,\n",
    "                df_global_step = df_global_step,\n",
    "                df_train_op = df_train_op\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRDMModel(sess, saver, summary_writter, logger, mm, batch_size, t_acc, t_steps, model_dir, new_data_len=[]):\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    ret_acc = 0.0\n",
    "    init_states = np.zeros([batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "\n",
    "    for i in range(t_steps):\n",
    "        if len(new_data_len) > 0:\n",
    "            x, x_len, y = get_df_batch(i, batch_size, new_data_len)\n",
    "        else:\n",
    "            x, x_len, y = get_df_batch(i, batch_size)\n",
    "        feed_dic = {\n",
    "                        mm.input_x: x, \n",
    "                        mm.x_len: x_len, \n",
    "                        mm.input_y: y, \n",
    "                        mm.init_states: init_states, \n",
    "                        mm.dropout_keep_prob: 0.8,\n",
    "                        mm.lm_drop_out: 0.8\n",
    "        }\n",
    "        _, step, loss, acc = sess.run([mm.df_train_op, mm.df_global_step, mm.loss, mm.accuracy], feed_dic)\n",
    "        \n",
    "        summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_acc\", simple_value=acc),\n",
    "            ])\n",
    "        \n",
    "        summary_writer.add_summary(summary, step)    \n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            ret_acc = sum_acc\n",
    "            print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            if sum_acc > t_acc:\n",
    "                break\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "        if i % 1000 == 999:\n",
    "            save_as = '%s/epoch%03d_%.4f.model' % (model_dir, epoch, avg_train_loss)\n",
    "            saver.save(session, save_as)\n",
    "            print('Saved char model', save_as)\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32, device=/device:GPU:0)\n",
      "x_reshape: Tensor(\"Reshape_1:0\", shape=(505, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_: Tensor(\"Reshape_1:0\", shape=(505, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "input_cnn: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(15655, 1100), dtype=float32, device=/device:GPU:0)\n",
      "x_embedding: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(15655, 1100), dtype=float32, device=/device:GPU:0)\n",
      "cnn_outs: Tensor(\"Reshape_2:0\", shape=(505, 31, 1100), dtype=float32, device=/device:GPU:0)\n",
      "RDM words_embedding: Tensor(\"transpose:0\", shape=(505, 31, 300), dtype=float32, device=/device:GPU:0)\n",
      "x_senti: Tensor(\"Train_RDM/dense/Sigmoid:0\", shape=(505, 64), dtype=float32, device=/device:GPU:0)\n",
      "Unknown char: 😞\n",
      "Word: crash😞thoughts\n",
      "################step 0: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: =\n",
      "Word: lt;=&gt\n",
      "################step 1: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: méxico\n",
      "################step 2: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: evéchés\n",
      "Unknown char: é\n",
      "Word: evéchés\n",
      "################step 3: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: résumer\n",
      "################step 4: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 5: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 6: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 7: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: ñ\n",
      "Word: cañazo\n",
      "################step 8: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 9: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 12:18:23.005650 139700702275392 logger.py:24] 2019-09-01 12:18:23 Step: 10 Training loss: 0.7795779228210449 accuracy: 0.5600000143051147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 12:18:23 Step: 10 Training loss: 0.7795779228210449 accuracy: 0.5600000143051147\n",
      "################step 10: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: gérard\n",
      "################step 11: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: l'amérique\n",
      "################step 12: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 13: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 14: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: béni\n",
      "Unknown char: ê\n",
      "Word: même\n",
      "################step 15: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: ã\n",
      "Word: pensã©es\n",
      "Unknown char: ©\n",
      "Word: pensã©es\n",
      "################step 16: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: c'était\n",
      "################step 17: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 18: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 19: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 12:18:26.484260 139700702275392 logger.py:24] 2019-09-01 12:18:26 Step: 20 Training loss: 0.7528831422328949 accuracy: 0.5000000104308129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 12:18:26 Step: 20 Training loss: 0.7528831422328949 accuracy: 0.5000000104308129\n",
      "################step 20: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 21: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: …\n",
      "Word: me…thats\n",
      "################step 22: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 23: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: engagée\n",
      "Unknown char: é\n",
      "Word: décrit\n",
      "Unknown char: è\n",
      "Word: très\n",
      "Unknown char: é\n",
      "Word: occupée\n",
      "################step 24: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: l'amérique\n",
      "Unknown char: ü\n",
      "Word: dürfte\n",
      "Unknown char: ü\n",
      "Word: für\n",
      "Unknown char: ô\n",
      "Word: tôt\n",
      "Unknown char: è\n",
      "Word: hypothèse\n",
      "Unknown char: é\n",
      "Word: crée\n",
      "Unknown char: é\n",
      "Word: déj\n",
      "Unknown char: è\n",
      "Word: après\n",
      "Unknown char: é\n",
      "Word: m'écrire\n",
      "Unknown char: ê\n",
      "Word: même\n",
      "Unknown char: é\n",
      "Word: l'état\n",
      "Unknown char: è\n",
      "Word: procèderais\n",
      "Unknown char: ô\n",
      "Word: côt\n",
      "Unknown char: é\n",
      "Word: d'écoute\n",
      "Unknown char: ê\n",
      "Word: honnêtement\n",
      "Unknown char: é\n",
      "Word: médias\n",
      "Unknown char: ê\n",
      "Word: d'être\n",
      "Unknown char: é\n",
      "Word: l'événement\n",
      "Unknown char: é\n",
      "Word: l'événement\n",
      "Unknown char: è\n",
      "Word: prière\n",
      "Unknown char: é\n",
      "Word: avérer\n",
      "Unknown char: è\n",
      "Word: pièce\n",
      "Unknown char: è\n",
      "Word: mène\n",
      "Unknown char: é\n",
      "Word: québec\n",
      "Unknown char: é\n",
      "Word: réellement\n",
      "Unknown char: ô\n",
      "Word: côt\n",
      "Unknown char: ê\n",
      "Word: toi-même\n",
      "Unknown char: é\n",
      "Word: média\n",
      "Unknown char: é\n",
      "Word: médias\n",
      "Unknown char: è\n",
      "Word: gère\n",
      "Unknown char: é\n",
      "Word: médias\n",
      "Unknown char: ô\n",
      "Word: contrôle\n",
      "Unknown char: é\n",
      "Word: l'intérêt\n",
      "Unknown char: ê\n",
      "Word: l'intérêt\n",
      "Unknown char: é\n",
      "Word: créer\n",
      "Unknown char: é\n",
      "Word: infondée\n",
      "Unknown char: é\n",
      "Word: erronées\n",
      "Unknown char: é\n",
      "Word: communiquées\n",
      "Unknown char: è\n",
      "Word: après\n",
      "Unknown char: ê\n",
      "Word: arrêter\n",
      "Unknown char: è\n",
      "Word: caractère\n",
      "Unknown char: é\n",
      "Word: réponde\n",
      "Unknown char: û\n",
      "Word: sûrement\n",
      "Unknown char: è\n",
      "Word: t'inquiète\n",
      "Unknown char: ô\n",
      "Word: plutôt\n",
      "################step 25: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 26: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: amérique\n",
      "################step 27: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 28: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 29: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 12:18:29.898418 139700702275392 logger.py:24] 2019-09-01 12:18:29 Step: 30 Training loss: 0.6584962010383606 accuracy: 0.5800000101327896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 12:18:29 Step: 30 Training loss: 0.6584962010383606 accuracy: 0.5800000101327896\n",
      "Unknown char: í\n",
      "Word: magníficas\n",
      "################step 30: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: crémer\n",
      "################step 31: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: ü\n",
      "Word: gülen's\n",
      "Unknown char: ̇\n",
      "Word: i̇srailli\n",
      "Unknown char: ü\n",
      "Word: tümgeneral\n",
      "Unknown char: ç\n",
      "Word: koçavi\n",
      "Unknown char: ü\n",
      "Word: türkiye’de\n",
      "Unknown char: ü\n",
      "Word: türkiye’de\n",
      "Unknown char: ş\n",
      "Word: işi̇d\n",
      "Unknown char: ̇\n",
      "Word: işi̇d\n",
      "Unknown char: ö\n",
      "Word: teröristlerini\n",
      "Unknown char: ğ\n",
      "Word: eğiten\n",
      "Unknown char: ı\n",
      "Word: sayıda\n",
      "Unknown char: ̇\n",
      "Word: i̇slam\n",
      "Unknown char: ı\n",
      "Word: adına\n",
      "Unknown char: ö\n",
      "Word: terörün\n",
      "Unknown char: ü\n",
      "Word: terörün\n",
      "Unknown char: ı\n",
      "Word: saldırısında\n",
      "Unknown char: ı\n",
      "Word: saldırısında\n",
      "Unknown char: ı\n",
      "Word: saldırısında\n",
      "Unknown char: ı\n",
      "Word: hayatın\n",
      "Unknown char: ü\n",
      "Word: müslüman\n",
      "Unknown char: ü\n",
      "Word: müslüman\n",
      "################step 32: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 33: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 34: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: …\n",
      "Word: ok…every\n",
      "Unknown char: ‼\n",
      "Word: jews‼️now\n",
      "Unknown char: ️\n",
      "Word: jews‼️now\n",
      "Unknown char: …\n",
      "Word: there…foolish\n",
      "Unknown char: …\n",
      "Word: well…posted\n",
      "Unknown char: ‼\n",
      "Word: note‼️then\n",
      "Unknown char: ️\n",
      "Word: note‼️then\n",
      "Unknown char: …\n",
      "Word: jews…so‼️i\n",
      "Unknown char: ‼\n",
      "Word: jews…so‼️i\n",
      "Unknown char: ️\n",
      "Word: jews…so‼️i\n",
      "Unknown char: …\n",
      "Word: deplorable…right\n",
      "Unknown char: …\n",
      "Word: now…now\n",
      "################step 35: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 36: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: l'amérique\n",
      "Unknown char: á\n",
      "Word: cuántas\n",
      "Unknown char: á\n",
      "Word: pakistán\n",
      "Unknown char: á\n",
      "Word: irán\n",
      "Unknown char: í\n",
      "Word: víctimas\n",
      "################step 37: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: í\n",
      "Word: incrível\n",
      "################step 38: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 39: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0901 12:18:33.354232 139700702275392 logger.py:24] 2019-09-01 12:18:33 Step: 40 Training loss: 0.6275402277708053 accuracy: 0.7400000154972076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-01 12:18:33 Step: 40 Training loss: 0.6275402277708053 accuracy: 0.7400000154972076\n",
      "Unknown char: í\n",
      "Word: vía\n",
      "################step 40: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: l'amérique\n",
      "################step 41: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 42: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 43: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "################step 44: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: í\n",
      "Word: vía\n",
      "Unknown char: ó\n",
      "Word: situación\n",
      "Unknown char: í\n",
      "Word: parís\n",
      "Unknown char: é\n",
      "Word: rehén\n",
      "Unknown char: í\n",
      "Word: estadísticas\n",
      "Unknown char: ó\n",
      "Word: discriminación\n",
      "Unknown char: á\n",
      "Word: bán\n",
      "################step 45: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: č\n",
      "Word: počasi\n",
      "Unknown char: č\n",
      "Word: nesrečna\n",
      "Unknown char: č\n",
      "Word: slučaj\n",
      "################step 46: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: ề\n",
      "Word: iều\n",
      "Unknown char: ò\n",
      "Word: hòa\n",
      "Unknown char: ầ\n",
      "Word: trần\n",
      "################step 47: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: \\\n",
      "Word: nat\\f\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: é\n",
      "Word: pensées\n",
      "Unknown char: ç\n",
      "Word: français\n",
      "################step 48: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n",
      "Unknown char: é\n",
      "Word: gérard\n",
      "Unknown char: é\n",
      "Word: spéisiúl\n",
      "Unknown char: ú\n",
      "Word: spéisiúl\n",
      "Unknown char: ó\n",
      "Word: fóill\n",
      "Unknown char: í\n",
      "Word: shuíomh\n",
      "Unknown char: é\n",
      "Word: déj\n",
      "################step 49: Tensor(\"input_x:0\", shape=(5, 101, 31, 21), dtype=int32, device=/device:GPU:0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9f9c98bff59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RDMGPUTrain/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mTrainRDMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RDMGPUTrain/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_data_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8d6fc1704062>\u001b[0m in \u001b[0;36mTrainRDMModel\u001b[0;34m(sess, saver, summary_writter, logger, mm, batch_size, t_acc, t_steps, model_dir, new_data_len)\u001b[0m\n\u001b[1;32m     19\u001b[0m         }\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"################step %d:\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_train_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_global_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         summary = tf.Summary(value=[\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF_GPU/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reuse model to train RDMModel\n",
    "gpu_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "# gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "device = \"/CPU:0\"\n",
    "# device = \"/GPU:0\"\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g, config=gpu_config) as sess:\n",
    "        with tf.device('/GPU:0'):\n",
    "            w2v = lstm_char_cnn.WordEmbedding(\n",
    "                            max_word_length = FLAGS.max_char_num , \n",
    "                            char_vocab_size = char_vocab.size, \n",
    "                            char_embed_size = FLAGS.char_embed_size, \n",
    "                            kernels = eval(FLAGS.kernels), \n",
    "                            kernel_features = eval(FLAGS.kernel_features), \n",
    "                            num_highway_layers = FLAGS.highway_layers,\n",
    "                            embedding_dim = FLAGS.embedding_dim\n",
    "                        )\n",
    "            lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                        batch_size = FLAGS.batch_size, \n",
    "                        num_unroll_steps = FLAGS.max_sent_len, \n",
    "                        rnn_size = FLAGS.embedding_dim, \n",
    "                        num_rnn_layers = FLAGS.rnn_layers, \n",
    "                        word_vocab_size = word_vocab.size\n",
    "                    )\n",
    "\n",
    "            char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                                w2v, lstm_lm, \n",
    "                                batch_size = FLAGS.batch_size, \n",
    "                                num_unroll_steps = FLAGS.max_sent_len, \n",
    "                                max_word_length = FLAGS.max_char_num, \n",
    "                                learning_rate = FLAGS.learning_rate,\n",
    "                                max_grad_norm = FLAGS.max_grad_norm\n",
    "                             )\n",
    "#             s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "#             senti_train_graph = model.InferSentiTrainGraph(\n",
    "#                                     w2v, \n",
    "#                                     lstm_lm, \n",
    "#                                     s_model, \n",
    "#                                     batchsize=20,\n",
    "#                                     max_word_num = sentiReader.max_sent_len, \n",
    "#                                     max_char_num = FLAGS.max_char_num, \n",
    "#                                     hidden_dim = FLAGS.hidden_dim, \n",
    "#                                     sent_num = FLAGS.sent_num,\n",
    "#                                     embedding_dim = FLAGS.embedding_dim\n",
    "#                                 )\n",
    "            val_list1 = tf.global_variables()\n",
    "            saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "            sess.run(tf.variables_initializer(val_list1))\n",
    "            checkpoint = tf.train.get_checkpoint_state(\"lstmCharCNNModel/\")\n",
    "            if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "            #RDMModel\n",
    "            rdm_model = model.RDM_Model(\n",
    "                    max_seq_len = FLAGS.max_seq_len, \n",
    "                    max_word_num = FLAGS.max_sent_len, \n",
    "                    embedding_dim = FLAGS.embedding_dim, \n",
    "                    hidden_dim = FLAGS.hidden_dim\n",
    "                )\n",
    "            rdm_train_graph = InferRDMTrainGraph(\n",
    "                            w2v, lstm_lm, None, rdm_model, \n",
    "                            batchsize=5,\n",
    "                            max_seq_len = FLAGS.max_seq_len, \n",
    "                            max_word_num = FLAGS.max_sent_len, \n",
    "                            max_char_num = FLAGS.max_char_num, \n",
    "                            hidden_dim = FLAGS.hidden_dim, \n",
    "                            embedding_dim = FLAGS.embedding_dim,\n",
    "                            class_num = FLAGS.class_num\n",
    "                    )\n",
    "            val_list2 = tf.global_variables()\n",
    "            saver2 = tf.train.Saver(val_list2, max_to_keep=4)\n",
    "            uninitialized_vars = list( filter(lambda var: var not in val_list1, val_list2) )\n",
    "#             print(\"uninitialized_vars:\", uninitialized_vars)\n",
    "            sess.run(tf.variables_initializer(uninitialized_vars))\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(\"RDMGPUTrain/\", graph=sess.graph)\n",
    "        TrainRDMModel(sess, saver, summary_writer, logger, rdm_train_graph, 5, 0.9, 100000, \"RDMGPUTrain/\", new_data_len=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
