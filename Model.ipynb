{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.losses import Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_pooling_layer(inputs, input_dim, hidden_dim, max_seq_len, max_word_num, output_dim):\n",
    "    with tf.variable_scope('share_pooling', reuse=tf.AUTO_REUSE):\n",
    "        w_t = tf.Variable(tf.random_uniform([input_dim, hidden_dim], -1.0, 1.0), name=\"w_t\")\n",
    "        b_t = tf.Variable(tf.constant(0.01, shape=[hidden_dim]), name=\"b_t\")\n",
    "        t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "        t_h = tf.nn.xw_plus_b(t_inputs, w_t, b_t)\n",
    "        # t_h = tf.matmul(t_inputs, self.w_t)\n",
    "        t_h = tf.reshape(t_h, [-1, max_word_num, hidden_dim])\n",
    "        t_h_expended = tf.expand_dims(t_h, -1)\n",
    "        pooled = tf.nn.max_pool(\n",
    "            t_h_expended,\n",
    "            ksize=[1, max_word_num, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        outs = tf.reshape(pooled, [-1, max_seq_len, hidden_dim])\n",
    "    return outs\n",
    "\n",
    "def pooling_layer(inputs, input_dim, max_seq_len, max_word_num, output_dim):\n",
    "    t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "    with tf.variable_scope('pooling_layer', reuse=tf.AUTO_REUSE):\n",
    "        w = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[output_dim]))\n",
    "\n",
    "        h = tf.nn.xw_plus_b(t_inputs, w, b)\n",
    "        hs = tf.reshape(h, [-1, max_word_num, output_dim])\n",
    "\n",
    "        inputs_expended = tf.expand_dims(hs, -1)\n",
    "        # [seq, words, out] --> [seq, words, out, 1] --> [seq, 1, out, 1] --> [1, seq, out]\n",
    "        pooled = tf.nn.max_pool(\n",
    "            inputs_expended,\n",
    "            ksize=[1, max_word_len, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        cnn_outs = tf.reshape(pooled, [-1, max_seq_len, output_dim]) \n",
    "    return cnn_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Model:\n",
    "    def __init__(self, max_word_num, embedding_dim, hidden_dim, action_num):\n",
    "        self.max_word_num = max_word_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "    def __call__(self, rdm_model, rl_state, rl_input):\n",
    "        pooled_rl_input = shared_pooling_layer(rl_input, self.embedding_dim, 1, self.max_word_num, self.hidden_dim)\n",
    "        pooled_rl_input = tf.reshape(pooled_rl_input, [-1, self.hidden_dim])\n",
    "        rl_output, rl_new_state = rdm_model.df_cell(pooled_rl_input, rl_state)\n",
    "        with tf.varibale_scope(\"CM_Model\", reuse=tf.AUTO_REUSE):\n",
    "            w_ss1 = tf.Variable(tf.truncated_normal([self.hidden_dim, 64], stddev=0.01))\n",
    "            b_ss1 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "            rl_h1 = tf.nn.relu(tf.nn.xw_plus_b(rl_state, w_ss1, b_ss1))  # replace the process here\n",
    "            w_ss2 = tf.Variable(tf.truncated_normal([64, action_num], stddev=0.01))\n",
    "            b_ss2 = tf.Variable(tf.constant(0.01, shape=[action_num]))\n",
    "            stopScore = tf.nn.xw_plus_b(rl_h1, w_ss2, b_ss2, name=\"stopScore\")\n",
    "            isStop = tf.argmax(stopScore, 1, name=\"isStop\")\n",
    "        return stopScore, isStop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDM_Model:\n",
    "    def __init__(self, max_seq_len, max_word_num, embedding_dim, hidden_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_word_num = max_word_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        with tf.variable_scope(\"RDM_Model\", reuse=tf.AUTO_REUSE):\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout\")\n",
    "            self.df_cell = rnn.GRUCell(self.hidden_dim)\n",
    "            self.df_cell = rnn.DropoutWrapper(self.df_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "        \n",
    "    def __call__(self, input_x, x_len, init_states):\n",
    "        with tf.variable_scope('pooling_layer', reuse=tf.AUTO_REUSE):\n",
    "            pooled_input_x = shared_pooling_layer(input_x, self.embedding_dim, self.max_seq_len, self.max_word_num, self.hidden_dim) # replace the shared_pooling_layer with a sentiment analysis model\n",
    "            # dropout layer\n",
    "            pooled_input_x_dp = tf.nn.dropout(pooled_input_x, self.dropout_keep_prob)\n",
    "            df_outputs, df_last_state = tf.nn.dynamic_rnn(\n",
    "                                                            self.df_cell, \n",
    "                                                            pooled_input_x_dp, \n",
    "                                                            x_len, \n",
    "                                                            initial_state=init_states, \n",
    "                                                            dtype=tf.float32\n",
    "                                                          )\n",
    "        return df_outputs, df_last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiModel:\n",
    "    def __init__(self, num_filters, kernel_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "    def __call__(self, sentence):\n",
    "        with tf.variable_scope(\"SentiModel\", reuse=tf.AUTO_REUSE):\n",
    "            conv_input = tf.layers.conv1d(sentence, num_filters, kernel_size, strides=1, padding='valid', name='conv2', trainable=True)\n",
    "            feature_map = tf.nn.relu(conv_input) # [batchsize, conv_feats, filters]\n",
    "            pooled_feat = tf.reduce_max(feature_map, 1) #[batchsize, 1, filters]\n",
    "        return pooled_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferSentiTrainGraph(char_model, senti_model, max_word_num, max_char_num):\n",
    "    sent_x = tf.placeholder(tf.int32, shape = [None, self.max_word_num, max_char_num])\n",
    "    sent_y = tf.placeholder(tf.float32, shape = [None, sent_num])\n",
    "    sentence = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferRDMTrainGraph(char_model, senti_model, rdm_model, ):\n",
    "        \n",
    "    with tf.variable_scope(\"Train_RDM\", reuse=tf.AUTO_REUSE):\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        w_ps = tf.Variable(tf.truncated_normal([self.hidden_dim, class_num], stddev=0.1)) #\n",
    "        b_ps = tf.Variable(tf.constant(0.01, shape=[class_num])) #\n",
    "        l2_loss += tf.nn.l2_loss(w_ps) \n",
    "        l2_loss += tf.nn.l2_loss(b_ps) \n",
    "\n",
    "        pre_scores = tf.nn.xw_plus_b(df_last_state, w_ps, b_ps, name=\"p_scores\")\n",
    "        predictions = tf.argmax(pre_scores, 1, name=\"predictions\")\n",
    "\n",
    "        r_outputs = tf.reshape(df_outputs, [-1, self.hidden_dim]) #[batchsize*max_seq_len, output_dim]\n",
    "        scores_seq = tf.nn.softmax(tf.nn.xw_plus_b(r_outputs, w_ps, b_ps)) # [batchsize * max_seq_len, class_num] \n",
    "        out_seq = tf.reshape(scores_seq, [-1, self.max_seq_len, class_num], name=\"out_seq\") #[batchsize, max_seq_len, class_num]\n",
    "\n",
    "        df_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pre_scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(df_losses) + 0.1 * l2_loss\n",
    "\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferCMTrainGraph(char_model, senti_model, rdm_model, cm_model,):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSentiModel(sess, train_model, ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRDMModel(sess, train_model, ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainCMModel(sess, train_model, ):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
