{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.losses import Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_pooling_layer(inputs, input_dim, hidden_dim, max_seq_len, max_word_num, output_dim):\n",
    "    with tf.variable_scope('share_pooling', reuse=tf.AUTO_REUSE):\n",
    "        w_t = tf.Variable(tf.random_uniform([input_dim, hidden_dim], -1.0, 1.0), name=\"w_t\")\n",
    "        b_t = tf.Variable(tf.constant(0.01, shape=[hidden_dim]), name=\"b_t\")\n",
    "        t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "        t_h = tf.nn.xw_plus_b(t_inputs, w_t, b_t)\n",
    "        # t_h = tf.matmul(t_inputs, self.w_t)\n",
    "        t_h = tf.reshape(t_h, [-1, max_word_num, hidden_dim])\n",
    "        t_h_expended = tf.expand_dims(t_h, -1)\n",
    "        pooled = tf.nn.max_pool(\n",
    "            t_h_expended,\n",
    "            ksize=[1, max_word_num, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        outs = tf.reshape(pooled, [-1, max_seq_len, hidden_dim])\n",
    "    return outs\n",
    "\n",
    "def pooling_layer(inputs, input_dim, max_seq_len, max_word_num, output_dim):\n",
    "    t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "    with tf.variable_scope('pooling_layer', reuse=tf.AUTO_REUSE):\n",
    "        w = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[output_dim]))\n",
    "\n",
    "        h = tf.nn.xw_plus_b(t_inputs, w, b)\n",
    "        hs = tf.reshape(h, [-1, max_word_num, output_dim])\n",
    "\n",
    "        inputs_expended = tf.expand_dims(hs, -1)\n",
    "        # [seq, words, out] --> [seq, words, out, 1] --> [seq, 1, out, 1] --> [1, seq, out]\n",
    "        pooled = tf.nn.max_pool(\n",
    "            inputs_expended,\n",
    "            ksize=[1, max_word_len, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        cnn_outs = tf.reshape(pooled, [-1, max_seq_len, output_dim]) \n",
    "    return cnn_outs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Model:\n",
    "    def __init__(self, max_word_num, embedding_dim, hidden_dim, action_num):\n",
    "        self.max_word_num = max_word_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "    def __call__(self, rdm_model, rl_state, rl_input):\n",
    "        pooled_rl_input = shared_pooling_layer(rl_input, self.embedding_dim, 1, self.max_word_num, self.hidden_dim)\n",
    "        pooled_rl_input = tf.reshape(pooled_rl_input, [-1, self.hidden_dim])\n",
    "        rl_output, rl_new_state = rdm_model.df_cell(pooled_rl_input, rl_state)\n",
    "        with tf.varibale_scope(\"CM_Model\", reuse=tf.AUTO_REUSE):\n",
    "            w_ss1 = tf.Variable(tf.truncated_normal([self.hidden_dim, 64], stddev=0.01))\n",
    "            b_ss1 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "            rl_h1 = tf.nn.relu(tf.nn.xw_plus_b(rl_state, w_ss1, b_ss1))  # replace the process here\n",
    "            w_ss2 = tf.Variable(tf.truncated_normal([64, action_num], stddev=0.01))\n",
    "            b_ss2 = tf.Variable(tf.constant(0.01, shape=[action_num]))\n",
    "            stopScore = tf.nn.xw_plus_b(rl_h1, w_ss2, b_ss2, name=\"stopScore\")\n",
    "            isStop = tf.argmax(stopScore, 1, name=\"isStop\")\n",
    "        return stopScore, isStop, rl_new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDM_Model:\n",
    "    def __init__(self, max_seq_len, max_word_num, embedding_dim, hidden_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_word_num = max_word_num\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        with tf.variable_scope(\"RDM_Model\", reuse=tf.AUTO_REUSE):\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout\")\n",
    "            self.df_cell = rnn.GRUCell(self.hidden_dim)\n",
    "            self.df_cell = rnn.DropoutWrapper(self.df_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "        \n",
    "    def __call__(self, input_x, x_len, init_states): #input_x: [batchsize, max_seq_len, max_word_num, max_char_num] \n",
    "        with tf.variable_scope('pooling_layer', reuse=tf.AUTO_REUSE):\n",
    "            pooled_input_x = shared_pooling_layer(input_x, self.embedding_dim, self.max_seq_len, self.max_word_num, self.hidden_dim) # replace the shared_pooling_layer with a sentiment analysis model\n",
    "            # dropout layer\n",
    "            pooled_input_x_dp = tf.nn.dropout(pooled_input_x, self.dropout_keep_prob)\n",
    "            df_outputs, df_last_state = tf.nn.dynamic_rnn(\n",
    "                                                            self.df_cell, \n",
    "                                                            pooled_input_x_dp, \n",
    "                                                            x_len, \n",
    "                                                            initial_state=init_states, \n",
    "                                                            dtype=tf.float32\n",
    "                                                          )\n",
    "        return df_outputs, df_last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiModel:\n",
    "    def __init__(self, num_filters, kernel_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "    def __call__(self, sentence):\n",
    "        with tf.variable_scope(\"SentiModel\", reuse=tf.AUTO_REUSE):\n",
    "            conv_input = tf.layers.conv1d(sentence, num_filters, kernel_size, strides=1, padding='valid', name='conv2', trainable=True)\n",
    "            feature_map = tf.nn.relu(conv_input) # [batchsize, conv_feats, filters]\n",
    "            pooled_feat = tf.reduce_max(feature_map, 1) #[batchsize, 1, filters]\n",
    "        return pooled_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferSentiTrainGraph(char_model, senti_model, max_word_num, max_char_num, hidden_dim, sent_num):\n",
    "    sent_x = tf.placeholder(tf.int32, shape = [None, max_word_num, max_char_num])\n",
    "    sent_y = tf.placeholder(tf.float32, shape = [None, sent_num])\n",
    "    sentence = char_model(sent_x) #[None, max_word_num, kernerl_size]\n",
    "    fcn_layer = tf.layers.Dense(hidden_dim, activation=tf.keras.activations.sigmoid, trainable=True)\n",
    "    sentence_input = fcn_layer(sentence) # [None, max_word_num, hidden_dim]\n",
    "    senti_features = senti_model(sentence_input)\n",
    "    classifier = tf.layers.Dense(sent_num, activation=tf.nn.relu, trainable=True)\n",
    "    senti_rst = classifier(senti_features)\n",
    "    sent_scores = tf.nn.softmax(senti_features, axis=1)\n",
    "    sent_pred = tf.argmax(sent_scores, 1, name=\"predictions\")\n",
    "    sent_loss = tf.losses.softmax_cross_entropy(\n",
    "                        sent_y,\n",
    "                        sent_scores,\n",
    "                        weights=1.0,\n",
    "                        label_smoothing=0,\n",
    "                        scope=None,\n",
    "                        loss_collection=tf.GraphKeys.LOSSES,\n",
    "                        reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "                    )\n",
    "    sent_correct_predictions = tf.equal(sent_pred, tf.argmax(sent_y, 1))\n",
    "    sent_acc = tf.reduce_mean(tf.cast(sent_correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    sent_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    sent_train_op = tf.train.AdagradOptimizer(0.01).minimize(sent_loss, sent_global_step)        \n",
    "    return adict(\n",
    "                dropout_keep_prob = self.dropout_keep_prob,\n",
    "                sent_x = sent_x,\n",
    "                sent_y = sent_y,\n",
    "                feature = pooled_feat,\n",
    "                sent_scores = sent_scores,\n",
    "                sent_pred = sent_pred,\n",
    "                sent_loss = sent_loss,\n",
    "                sent_acc = sent_acc,\n",
    "                sent_global_step = sent_global_step,\n",
    "                sent_train_op = sent_train_op\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferRDMTrainGraph(char_model, senti_model, rdm_model, max_seq_len, max_word_num, max_char_num, hidden_dim, class_num):\n",
    "    input_x = tf.placeholder(tf.int32, shape = [None, max_seq_len, max_word_num, max_char_num], name=\"input_x\")\n",
    "    input_y = tf.placeholder(tf.float32, shape = [None, class_num], name=\"input_y\")\n",
    "    x_len = tf.placeholder(tf.int32, [None], name=\"x_len\")\n",
    "    init_states = tf.placeholder(tf.float32, [None, hidden_dim], name=\"init_states\")\n",
    "    x_reshape = tf.reshape(input_x, [-1, max_word_num, max_char_num])\n",
    "    x_embedding = tf.reshape(x_reshape)\n",
    "    x_senti = senti_model(x_embedding)\n",
    "    \n",
    "    with tf.variable_scope(\"Train_RDM\", reuse=tf.AUTO_REUSE):\n",
    "        fcn_layer = tf.layers.Dense(rdm_model.embedding_dim, activation=tf.keras.activation.sigmoid)\n",
    "        x_features = fcn_layer(x_senti)\n",
    "        RDM_Input = tf.reshape(x_features, [-1, max_seq_len, max_word_num, hidden_dim])\n",
    "        df_outputs, df_last_state = rdm_model(RDM_Input, x_len, init_states)\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        w_ps = tf.Variable(tf.truncated_normal([self.hidden_dim, class_num], stddev=0.1)) #\n",
    "        b_ps = tf.Variable(tf.constant(0.01, shape=[class_num])) #\n",
    "        l2_loss += tf.nn.l2_loss(w_ps) \n",
    "        l2_loss += tf.nn.l2_loss(b_ps) \n",
    "\n",
    "        pre_scores = tf.nn.xw_plus_b(df_last_state, w_ps, b_ps, name=\"p_scores\")\n",
    "        predictions = tf.argmax(pre_scores, 1, name=\"predictions\")\n",
    "\n",
    "        r_outputs = tf.reshape(df_outputs, [-1, self.hidden_dim]) #[batchsize*max_seq_len, output_dim]\n",
    "        scores_seq = tf.nn.softmax(tf.nn.xw_plus_b(r_outputs, w_ps, b_ps)) # [batchsize * max_seq_len, class_num] \n",
    "        out_seq = tf.reshape(scores_seq, [-1, self.max_seq_len, class_num], name=\"out_seq\") #[batchsize, max_seq_len, class_num]\n",
    "\n",
    "        df_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pre_scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(df_losses) + 0.1 * l2_loss\n",
    "\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "    df_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    df_train_op = tf.train.AdamOptimizer(0.01).minimize(loss, df_global_step)\n",
    "    return adict(\n",
    "                dropout_keep_prob = rdm_model.dropout_keep_prob,\n",
    "                input_x = input_x,\n",
    "                input_y = input_y,\n",
    "                x_len = x_len,\n",
    "                init_states = init_states,\n",
    "                pre_scores = pre_scores,\n",
    "                predictions = predictions,\n",
    "                r_outputs = r_outputs,\n",
    "                scores_seq = scores_seq,\n",
    "                out_seq = out_seq,\n",
    "                df_losses = df_losses,\n",
    "                loss = loss,\n",
    "                correct_predictions = correct_predictions,\n",
    "                accuracy = accuracy,\n",
    "                df_global_step = df_global_step,\n",
    "                df_train_op = df_train_op\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferCMTrainGraph(char_model, senti_model, rdm_model, cm_model, max_word_num, embedding_dim, hidden_dim, action_num):\n",
    "    rl_state = tf.placeholder(tf.float32, [None, hidden_dim], name=\"rl_states\")\n",
    "    rl_input = tf.placeholder(tf.float32, [None, max_word_num, embedding_dim], name=\"rl_input\")\n",
    "    action = tf.placeholder(tf.float32, [None, action_num], name=\"action\")\n",
    "    reward = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "    \n",
    "    stopScore, isStop, rl_new_state = cm_model(rdm_model, rl_input, rl_state)\n",
    "\n",
    "    out_action = tf.reduce_sum(tf.multiply(stopScore, action), reduction_indices=1)\n",
    "    rl_cost = tf.reduce_mean(tf.square(reward - out_action), name=\"rl_cost\")\n",
    "    \n",
    "    rl_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    rl_train_op = tf.train.AdamOptimizer(0.001).minimize(rl_cost, rl_global_step)\n",
    "    \n",
    "    return adict(\n",
    "        dropout_keep_prob = rdm_model.dropout_keep_prob,\n",
    "        rl_state = rl_state, \n",
    "        rl_input = rl_input,\n",
    "        action = action,\n",
    "        reward = reward,  \n",
    "        rl_new_state = rl_new_state,\n",
    "        stopScore = stopScore,\n",
    "        isStop = isStop,\n",
    "        rl_cost = rl_cost,\n",
    "        rl_global_step = rl_global_step,\n",
    "        rl_train_op = rl_train_op\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSentiModel(sess, saver, train_model, train_batch, test_batch):\n",
    "    \n",
    "    for t_epoch in range(100): \n",
    "        for t_iter in range(train_iter):\n",
    "            data_X, data_Y = dataloader.GetTrainingBatch(t_iter, train_batch, 300)\n",
    "            feed_dic = {sent_mm.input_x: data_X, sent_mm.input_y: data_Y}\n",
    "            _, step, loss, acc = sess.run([sent_train_op, sent_global_step, sent_mm.loss, sent_mm.accuracy], feed_dic)\n",
    "            sum_loss += loss\n",
    "            sum_acc += acc\n",
    "            if t_iter % 100 == 99:\n",
    "                sum_loss = sum_loss / 100\n",
    "                sum_acc = sum_acc / 100\n",
    "                ret_acc = sum_acc\n",
    "                print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                sum_acc = 0.0\n",
    "                sum_loss = 0.0\n",
    "        # for validation\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for t_iter in range(10):\n",
    "            data_X, data_Y = dataloader.GetTestData(t_iter, test_batch, 300)\n",
    "            feed_dic = {sent_mm.input_x: data_X, sent_mm.input_y: data_Y}\n",
    "            loss, acc = sess.run([sent_mm.loss, sent_mm.accuracy], feed_dic)\n",
    "            sum_loss += loss\n",
    "            sum_acc += acc    \n",
    "        sum_loss = sum_loss / 100\n",
    "        sum_acc = sum_acc / 100\n",
    "        ret_acc = sum_acc\n",
    "        print(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        logger.info(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "\n",
    "        saver.save(sess, \"df_saved/sent_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRDMModel(sess, mm, t_acc, t_steps, new_data_len=[]):\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    ret_acc = 0.0\n",
    "    init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "\n",
    "    for i in range(t_steps):\n",
    "        if len(new_data_len) > 0:\n",
    "            x, x_len, y = get_df_batch(i, new_data_len)\n",
    "        else:\n",
    "            x, x_len, y = get_df_batch(i)\n",
    "        feed_dic = {mm.input_x: x, mm.x_len: x_len, mm.input_y: y, mm.init_states: init_states, mm.dropout_keep_prob: 0.8}\n",
    "        _, step, loss, acc = sess.run([mm.df_train_op, mm.df_global_step, mm.loss, mm.accuracy], feed_dic)\n",
    "        sum_loss += loss\n",
    "        sum_acc += acc\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            ret_acc = sum_acc\n",
    "            print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "            if sum_acc > t_acc:\n",
    "                break\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainCMModel(sess, rdm_train, cm_train, t_rw, t_steps):\n",
    "    ids = np.array(range(FLAGS.batch_size), dtype=np.int32)\n",
    "    seq_states = np.zeros([FLAGS.batch_size], dtype=np.int32)\n",
    "    isStop = np.zeros([FLAGS.batch_size], dtype=np.int32)\n",
    "    max_id = FLAGS.batch_size\n",
    "    init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "    state = init_states\n",
    "    D = deque()\n",
    "    ssq = []\n",
    "    print(\"in RL the begining\")\n",
    "    logger.info(\"in RL the begining\")\n",
    "    # get_new_len(sess, mm)\n",
    "    data_ID = get_data_ID()\n",
    "    if len(data_ID) % FLAGS.batch_size == 0: # the total number of events\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size)\n",
    "    else:\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    for i in range(flags):\n",
    "        x, x_len, y = get_df_batch(i)\n",
    "        feed_dic = { rdm_train.input_x: x, \n",
    "                     rdm_train.x_len: x_len, \n",
    "                     rdm_train.input_y: y, \n",
    "                     rdm_train.init_states:init_states, \n",
    "                     rdm_train.dropout_keep_prob: 1.0 }\n",
    "        t_ssq = sess.run(rdm_train.out_seq, feed_dic)# t_ssq = [batchsize, max_seq, scores]\n",
    "        if len(ssq) > 0:\n",
    "            ssq = np.append(ssq, t_ssq, axis=0)\n",
    "        else:\n",
    "            ssq = t_ssq\n",
    "\n",
    "    print(get_curtime() + \" Now Start RL training ...\")\n",
    "    logger.info(get_curtime() + \" Now Start RL training ...\")\n",
    "    counter = 0\n",
    "    sum_rw = 0.0 # sum of rewards\n",
    "\n",
    "    data_len = get_data_len()\n",
    "    while True:\n",
    "        if counter > FLAGS.OBSERVE:\n",
    "            sum_rw += np.mean(rw)\n",
    "            if counter % 200 == 0:\n",
    "                sum_rw = sum_rw / 2000\n",
    "                print( get_curtime() + \" Step: \" + str(step) \n",
    "                       + \" REWARD IS \" + str(sum_rw) \n",
    "                     )\n",
    "                logger.info( get_curtime() + \n",
    "                             \" Step: \" + str(step) + \n",
    "                            \" REWARD IS \" + str(sum_rw)\n",
    "                           )\n",
    "                if sum_rw > t_rw:\n",
    "                    print(\"Retch The Target Reward\")\n",
    "                    logger.info(\"Retch The Target Reward\")\n",
    "                    break\n",
    "                if counter > t_steps:\n",
    "                    print(\"Retch The Target Steps\")\n",
    "                    logger.info(\"Retch The Target Steps\")\n",
    "                    break\n",
    "                sum_rw = 0.0\n",
    "            s_state, s_x, s_isStop, s_rw = get_RL_Train_batch(D)\n",
    "            feed_dic = {\n",
    "                        cm_train.rl_state: s_state, \n",
    "                        cm_train.rl_input: s_x, \n",
    "                        cm_train.action: s_isStop, \n",
    "                        cm_train.reward: s_rw, \n",
    "                        cm_train.dropout_keep_prob: 0.8\n",
    "            }\n",
    "            \n",
    "            _, step = sess.run([rl_train_op, rl_global_step], feed_dic)\n",
    "\n",
    "        x, y, ids, seq_states, max_id = get_rl_batch(ids, seq_states, isStop, max_id, 0, 3150)\n",
    "        batch_dic = {\n",
    "                     cm_train.rl_state: state, \n",
    "                     cm_train.rl_input: x, \n",
    "                     cm_train.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        \n",
    "        isStop, mss, mNewState = sess.run(\n",
    "            [cm_train.isStop, cm_train.stopScore, cm_train.rl_new_state], \n",
    "            batch_dic\n",
    "        )\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if random.random() < FLAGS.random_rate:\n",
    "                isStop[j] = np.argmax(np.random.rand(2))\n",
    "            if seq_states[j] == data_len[ids[j]]:\n",
    "                isStop[j] = 1\n",
    "\n",
    "        # eval\n",
    "        rw = get_reward(isStop, mss, ssq, ids, seq_states)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            D.append((state[j], x[j], isStop[j], rw[j]))\n",
    "            if len(D) > FLAGS.max_memory:\n",
    "                D.popleft()\n",
    "\n",
    "        state = mNewState\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if isStop[j] == 1:\n",
    "                # init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "                # feed_dic = {rl_model.init_states: init_states}\n",
    "                # state[j] = sess.run(rl_model.df_state, feed_dic)\n",
    "                state[j] = np.zeros([FLAGS.hidden_dim], dtype=np.float32)\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
