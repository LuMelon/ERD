{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import MyLogger\n",
    "import time\n",
    "import SubjObjLoader\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from collections import deque\n",
    "# import dataloader\n",
    "from BertRDMLoader import *\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import tsentiLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "#         self.PoolLayer = pooling_layer(word_embedding_dim, sent_embedding_dim) \n",
    "        \n",
    "    def forward(self, x_emb, x_len, init_states): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "#         pool_feature = self.PoolLayer(x_emb)\n",
    "#         sent_feature = sentiModel( \n",
    "#                 x_emb.reshape(\n",
    "#                     [-1, max_sent_len, emb_dim]\n",
    "#                 ) \n",
    "#             ).reshape(\n",
    "#                 [batchsize, max_seq_len, -1]\n",
    "#             )\n",
    "#         pooled_input_x_dp = self.DropLayer(input_x)\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        return hidden_outs, final_outs\n",
    "\n",
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, s_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "#         assert(embedding_dim==self.embedding_dim)\n",
    "        sentence = s_model(rl_input).reshape(batch_size, 1, self.sentence_embedding_dim)\n",
    "#         pooled_rl_input = self.PoolLayer(\n",
    "#             rl_input.reshape(\n",
    "#                 [-1, 1, max_word_num, self.embedding_dim]\n",
    "#             )\n",
    "#         ).reshape([-1, 1, self.hidden_dim])\n",
    "        \n",
    "#         print(\"sentence:\", sentence.shape)\n",
    "#         print(\"rl_state:\", rl_state.shape)\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            sentence, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def layer2seq(bert, layer, cuda=False):\n",
    "    if cuda:\n",
    "        outs = [bert( torch.tensor([input_]).cuda())\n",
    "                for input_ in layer]   \n",
    "    else: \n",
    "        outs = [bert( torch.tensor([input_]))\n",
    "                    for input_ in layer]\n",
    "    states = [item[1] for item in outs]\n",
    "    return rnn_utils.pad_sequence(states, batch_first=True)\n",
    "\n",
    "def Word_ids2SeqStates(word_ids, bert, ndim, cuda=False):\n",
    "    assert(ndim == 3)\n",
    "    if cuda:\n",
    "        embedding = [layer2seq(bert, layer, cuda) for layer in word_ids]\n",
    "    else:\n",
    "        embedding = [layer2seq(bert, layer) for layer in word_ids]\n",
    "    return padding_sequence(embedding)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def Count_Accs(ylabel, preds):\n",
    "    correct_preds = np.array(\n",
    "        [1 if y1==y2 else 0 \n",
    "        for (y1, y2) in zip(ylabel, preds)]\n",
    "    )\n",
    "    y_idxs = [idx if yl >0 else idx - len(ylabel) \n",
    "            for (idx, yl) in enumerate(ylabel)]\n",
    "    pos_idxs = list(filter(lambda x: x >= 0, y_idxs))\n",
    "    neg_idxs = list(filter(lambda x: x < 0, y_idxs))\n",
    "    acc = sum(correct_preds) / (1.0 * len(ylabel))\n",
    "    if len(pos_idxs) > 0:\n",
    "        pos_acc = sum(correct_preds[pos_idxs])/(1.0*len(pos_idxs))\n",
    "    else:\n",
    "        pos_acc = 0\n",
    "    if len(neg_idxs) > 0:\n",
    "        neg_acc = sum(correct_preds[neg_idxs])/(1.0*len(neg_idxs))\n",
    "    else:\n",
    "        neg_acc = 0\n",
    "    return acc, pos_acc, neg_acc, y_idxs, pos_idxs, neg_idxs, correct_preds\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def TrainRDMModel(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, t_steps, new_data_len=[], logger=None, \n",
    "                        log_dir=\"RDMBertTrain\"):\n",
    "    batch_size = 20 \n",
    "    max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    splits = int(batch_size/max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    t_acc = 0.9\n",
    "    ret_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    optim = torch.optim.Adagrad([\n",
    "                                {'params': bert.parameters(), 'lr':5e-5},\n",
    "                                {'params': rdm_classifier.parameters(), 'lr': 5e-3},\n",
    "                                {'params': rdm_model.parameters(), 'lr': 5e-3}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(log_dir)\n",
    "    acc_l = np.zeros(splits)\n",
    "    loss_l = np.zeros(splits)\n",
    "    for step in range(t_steps):\n",
    "        optim.zero_grad()\n",
    "        try:\n",
    "            for j in range(splits):\n",
    "                if len(new_data_len) > 0:\n",
    "                    x, x_len, y = get_df_batch(step*splits+j, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "                else:\n",
    "                    x, x_len, y = get_df_batch(step, max_gpu_batch, tokenizer=tokenizer)\n",
    "                x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "                batchsize, max_seq_len, max_sent_len,                                     emb_dim = x_emb.shape\n",
    "                rdm_hiddens, rdm_outs = rdm_model(x_emb, x_len, init_states)\n",
    "                rdm_scores = rdm_classifier(\n",
    "                    torch.cat(\n",
    "                        rdm_outs # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                    ).reshape(\n",
    "                        [-1, rdm_model.hidden_dim]\n",
    "                    )\n",
    "                )\n",
    "                rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                y_label = y.argmax(axis=1)\n",
    "                acc_l[j], _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "                loss = loss_fn(rdm_scores, torch.tensor(y_label).cuda())\n",
    "                loss.backward()\n",
    "                loss_l[j] = float(loss)\n",
    "#                 print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "        except RuntimeError as exception:\n",
    "            if \"out of memory\" in str(exception):\n",
    "                print(\"WARNING: out of memory\")\n",
    "                print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "#                     time.sleep(5)\n",
    "                raise exception\n",
    "            else:   \n",
    "                raise exception\n",
    "\n",
    "        optim.step()        \n",
    "        writer.add_scalar('Train Loss', loss_l.mean(), step)\n",
    "        writer.add_scalar('Train Accuracy', acc_l.mean(), step)\n",
    "\n",
    "        sum_loss += loss_l.mean()\n",
    "        sum_acc += acc_l.mean()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if step % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            print('%3d | %d , train_loss/accuracy = %6.8f/%6.7f'             % (step, t_steps, \n",
    "                sum_loss, sum_acc,\n",
    "                ))\n",
    "            logger.info('%3d | %d , train_loss/accuracy = %6.8f/%6.7f'             % (step, t_steps, \n",
    "                sum_loss, sum_acc,\n",
    "                ))\n",
    "            if step%500 == 499:\n",
    "                rdm_save_as = '/home/hadoop/ERD/%s/rdmModel_epoch%03d.pkl'                                    % (log_dir, step/500)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"bert\":bert.state_dict(),\n",
    "                        \"rmdModel\":rdm_model.state_dict(),\n",
    "                        \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "                    },\n",
    "                    rdm_save_as\n",
    "                )\n",
    "#                 rdm_model, bert, sentiModel, rdm_classifier\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, log_dir, logger, FLAGS):\n",
    "    batch_size = 20\n",
    "    t_acc = 0.9\n",
    "    ids = np.array(range(batch_size), dtype=np.int32)\n",
    "    seq_states = np.zeros([batch_size], dtype=np.int32)\n",
    "    isStop = np.zeros([batch_size], dtype=np.int32)\n",
    "    max_id = batch_size\n",
    "    df_init_states = torch.zeros([1, batch_size, FLAGS.hidden_dim], dtype=torch.float32).cuda()\n",
    "    state = df_init_states\n",
    "    D = deque()\n",
    "    ssq = []\n",
    "    print(\"in RL the begining\")\n",
    "    rdm_optim = torch.optim.Adagrad([\n",
    "                            {'params': bert.parameters(), 'lr':1e-3},\n",
    "    #                                 {'params': rdm_classifier.parameters(), 'lr': 5e-2},\n",
    "                            {'params': rdm_model.parameters(), 'lr': 5e-2},\n",
    "                            {'params': sentiModel.parameters(), 'lr': 1e-2}\n",
    "                         ],\n",
    "                            weight_decay = 0.2\n",
    "    )\n",
    "    rl_optim = torch.optim.Adam([{'params':cm_model.parameters(), 'lr':1e-3}])\n",
    "    # get_new_len(sess, mm)\n",
    "    data_ID = get_data_ID()\n",
    "\n",
    "    if len(data_ID) % batch_size == 0: # the total number of events\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size)\n",
    "    else:\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    for i in range(flags):\n",
    "        with torch.no_grad():\n",
    "            x, x_len, y = get_df_batch(i, batch_size, tokenizer=tokenizer)\n",
    "            x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "            batchsize, max_seq_len, max_sent_len,                                     emb_dim = x_emb.shape\n",
    "            sent_feature = sentiModel( \n",
    "                x_emb.reshape(\n",
    "                    [-1, max_sent_len, emb_dim]\n",
    "                ) \n",
    "            ).reshape(\n",
    "                [batchsize, max_seq_len, -1]\n",
    "            )\n",
    "            rdm_hiddens, rdm_outs = rdm_model(sent_feature, x_len, df_init_states)\n",
    "        #         t_ssq = sess.run(rdm_train.out_seq, feed_dic)# t_ssq = [batchsize, max_seq, scores]\n",
    "            print(\"batch %d\"%i)\n",
    "            if len(ssq) > 0:\n",
    "                ssq.extend([rdm_classifier(h) for h in rdm_hiddens])\n",
    "            else:\n",
    "                ssq = [rdm_classifier(h) for h in rdm_hiddens]\n",
    "\n",
    "    print(get_curtime() + \" Now Start RL training ...\")\n",
    "    counter = 0\n",
    "    sum_rw = 0.0 # sum of rewards\n",
    "\n",
    "    data_len = get_data_len()\n",
    "\n",
    "    while True:\n",
    "        if counter > FLAGS.OBSERVE:\n",
    "            sum_rw += np.mean(rw)\n",
    "            if counter % 200 == 0:\n",
    "                sum_rw = sum_rw / 2000\n",
    "                print( get_curtime() + \" Step: \" + str(step) \n",
    "                       + \" REWARD IS \" + str(sum_rw) \n",
    "                     )\n",
    "                logger.info( get_curtime() + \n",
    "                             \" Step: \" + str(step) + \n",
    "                            \" REWARD IS \" + str(sum_rw)\n",
    "                           )\n",
    "                if sum_rw > t_rw:\n",
    "                    print(\"Retch The Target Reward\")\n",
    "                    logger.info(\"Retch The Target Reward\")\n",
    "                    break\n",
    "                if counter > t_steps:\n",
    "                    print(\"Retch The Target Steps\")\n",
    "                    logger.info(\"Retch The Target Steps\")\n",
    "                    break\n",
    "                sum_rw = 0.0\n",
    "            s_state, s_x, s_isStop, s_rw = get_RL_Train_batch(D, FLAGS)\n",
    "            stopScore, isStop, rl_new_state = cm_model(rdm_model, sentiModel, s_x, s_state)\n",
    "            out_action = (stopScore*s_isStop).sum(axis=1)\n",
    "            rl_cost = torch.mean((s_rw - out_action)*(s_rw - out_action))\n",
    "            rl_cost.backward()\n",
    "            rl_optim.step()\n",
    "\n",
    "        input_x, input_y, ids, seq_states, max_id = get_rl_batch(ids, seq_states, isStop, max_id, 0, FLAGS, tokenizer=tokenizer)\n",
    "        with torch.no_grad():\n",
    "            x_emb = layer2seq(bert, input_x, cuda=True)\n",
    "            batchsize, max_sent_len, emb_dim = x_emb.shape\n",
    "            mss, isStop, mNewState = cm_model(rdm_model, sentiModel, x_emb, state)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if random.random() < FLAGS.random_rate:\n",
    "    #             isStop[j] = np.argmax(np.random.rand(2))\n",
    "                isStop[j] = int(torch.rand(2).argmax())\n",
    "            if seq_states[j] == data_len[ids[j]]:\n",
    "                isStop[j] = 1\n",
    "\n",
    "        # eval\n",
    "        rw = get_reward(isStop, mss, ssq, ids, seq_states)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            D.append((state[0][j], input_x[j], isStop[j], rw[j]))\n",
    "            if len(D) > FLAGS.max_memory:\n",
    "                D.popleft()\n",
    "\n",
    "        state = mNewState\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if isStop[j] == 1:\n",
    "                # init_states = np.zeros([FLAGS.batch_size, FLAGS.hidden_dim], dtype=np.float32)\n",
    "                # feed_dic = {rl_model.init_states: init_states}\n",
    "                # state[j] = sess.run(rl_model.df_state, feed_dic)\n",
    "    #             state[j] = np.zeros([FLAGS.hidden_dim], dtype=np.float32)\n",
    "                state[0][j].fill_(0)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, new_data_len=[]):\n",
    "    batch_size = 20 \n",
    "    d_IDs = get_data_ID()\n",
    "    t_steps=int(len(d_IDs)/batch_size)\n",
    "    \n",
    "    max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    splits = int(batch_size/max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    acc_l = np.zeros(splits)\n",
    "    loss_l = np.zeros(splits)\n",
    "    with torch.no_grad():\n",
    "        for step in range(t_steps):\n",
    "            try:\n",
    "                for j in range(splits):\n",
    "                    if len(new_data_len) > 0:\n",
    "                        x, x_len, y = get_df_batch(step*splits+j, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "                    else:\n",
    "                        x, x_len, y = get_df_batch(step, max_gpu_batch, tokenizer=tokenizer)\n",
    "                    x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "                    batchsize, max_seq_len, max_sent_len,                                     emb_dim = x_emb.shape\n",
    "                    rdm_hiddens, rdm_outs = rdm_model(x_emb, x_len, init_states)\n",
    "                    rdm_scores = rdm_classifier(\n",
    "                        torch.cat(\n",
    "                            rdm_outs # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                        ).reshape(\n",
    "                            [-1, rdm_model.hidden_dim]\n",
    "                        )\n",
    "                    )\n",
    "                    rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                    y_label = y.argmax(axis=1)\n",
    "                    acc_l[j], _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "                    loss = loss_fn(rdm_scores, torch.tensor(y_label).cuda())\n",
    "                    loss_l[j] = float(loss)\n",
    "    #                 print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "            except RuntimeError as exception:\n",
    "                if \"out of memory\" in str(exception):\n",
    "                    print(\"WARNING: out of memory\")\n",
    "                    print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "                    if hasattr(torch.cuda, 'empty_cache'):\n",
    "                        torch.cuda.empty_cache()\n",
    "    #                     time.sleep(5)\n",
    "                    raise exception\n",
    "                else:   \n",
    "                    raise exception\n",
    "            sum_loss += loss_l.mean()\n",
    "            sum_acc += acc_l.mean()\n",
    "            print(\"loss:\", loss_l.mean(), \"acc:\", acc_l.mean())\n",
    "    mean_loss = sum_loss/(1.0*t_steps)\n",
    "    mean_acc = sum_acc/(1.0*t_steps)\n",
    "    print(\"mean_loss\", mean_loss, \" | mean_acc:\", mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cached_dir = \"/home/hadoop/transformer_pretrained_models/bert-base-uncased-pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)\n",
    "# In[10]:\n",
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分原数据集 | 4084:500\n",
    "_马晶那个数据集跟这个数据集差异太大，无法迁移_\n",
    "\n",
    "``` python\n",
    "data_ID = get_data_ID()\n",
    "data_len = get_data_len()\n",
    "data_y = get_data_y()\n",
    "\n",
    "test_data_y = data_y[-500:]\n",
    "\n",
    "test_data_len = data_len[-500:]\n",
    "\n",
    "test_data_ID = data_ID[-500:]\n",
    "\n",
    "np.save(\"data/data_ID.npy\", np.array(data_ID)[:-500])\n",
    "np.save(\"data/data_len.npy\", np.array(data_len)[:-500])\n",
    "np.save(\"data/data_y.npy\", np.array(data_y)[:-500])\n",
    "\n",
    "np.save(\"data/test_data_ID.npy\", np.array(test_data_ID))\n",
    "np.save(\"data/test_data_len.npy\", np.array(test_data_len))\n",
    "np.save(\"data/test_data_y.npy\", np.array(test_data_y))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "rdm_model = RDM_Model(768, 300, 256, 0.2).cuda()\n",
    "cm_model = CM_Model(300, 256, 2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_save_as = '/home/hadoop/ERD/RDMBertTrain/rdmModel_epoch019.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(senti_save_as)\n",
    "bert.load_state_dict(checkpoint['bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "torch.save(\n",
    "                    {\n",
    "                        \"bert\":bert.state_dict(),\n",
    "                        \"rmdModel\":rdm_model.state_dict(),\n",
    "                        \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "                    },\n",
    "                    rdm_save_as\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdm_model.load_state_dict(checkpoint['rmdModel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdm_classifier.load_state_dict(checkpoint['rdm_classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the bert embedding model on the trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.020524263381958008 acc: 1.0\n",
      "loss: 0.10758360475301743 acc: 1.0\n",
      "loss: 0.9045018553733826 acc: 0.5\n",
      "loss: 0.24011637270450592 acc: 1.0\n",
      "loss: 1.6844463348388672 acc: 0.5\n",
      "loss: 1.0140597820281982 acc: 0.5\n",
      "loss: 0.16554677486419678 acc: 1.0\n",
      "loss: 1.2934188842773438 acc: 0.5\n",
      "loss: 0.02928406000137329 acc: 1.0\n",
      "loss: 1.8285801410675049 acc: 0.5\n",
      "loss: 0.01852881908416748 acc: 1.0\n",
      "loss: 0.14462363719940186 acc: 1.0\n",
      "loss: 0.03732529282569885 acc: 1.0\n",
      "loss: 0.12282135337591171 acc: 1.0\n",
      "loss: 0.13216009736061096 acc: 1.0\n",
      "loss: 0.025502383708953857 acc: 1.0\n",
      "loss: 0.02906098961830139 acc: 1.0\n",
      "loss: 0.037511587142944336 acc: 1.0\n",
      "loss: 0.6612767577171326 acc: 0.5\n",
      "loss: 0.016743063926696777 acc: 1.0\n",
      "loss: 0.05478944256901741 acc: 1.0\n",
      "loss: 0.20837311446666718 acc: 1.0\n",
      "loss: 0.15018148720264435 acc: 1.0\n",
      "loss: 0.9488863945007324 acc: 0.5\n",
      "loss: 0.05902600288391113 acc: 1.0\n",
      "loss: 0.048886675387620926 acc: 1.0\n",
      "loss: 0.035547852516174316 acc: 1.0\n",
      "loss: 0.023428678512573242 acc: 1.0\n",
      "loss: 0.06916701048612595 acc: 1.0\n",
      "loss: 0.1756153702735901 acc: 1.0\n",
      "loss: 0.24071946740150452 acc: 1.0\n",
      "loss: 0.03172056004405022 acc: 1.0\n",
      "loss: 0.025862157344818115 acc: 1.0\n",
      "loss: 0.39347273111343384 acc: 0.5\n",
      "loss: 0.14893701672554016 acc: 1.0\n",
      "loss: 0.14638952910900116 acc: 1.0\n",
      "loss: 0.037431199103593826 acc: 1.0\n",
      "loss: 0.041146159172058105 acc: 1.0\n",
      "loss: 0.41830942034721375 acc: 1.0\n",
      "loss: 0.038979560136795044 acc: 1.0\n",
      "loss: 0.03716035559773445 acc: 1.0\n",
      "loss: 0.2829364538192749 acc: 1.0\n",
      "loss: 0.05540703237056732 acc: 1.0\n",
      "loss: 0.08450403809547424 acc: 1.0\n",
      "loss: 0.7165133357048035 acc: 0.5\n",
      "loss: 0.09797263145446777 acc: 1.0\n",
      "loss: 0.050018925219774246 acc: 1.0\n",
      "loss: 0.02301555871963501 acc: 1.0\n",
      "loss: 0.029187321662902832 acc: 1.0\n",
      "loss: 0.02845662832260132 acc: 1.0\n",
      "loss: 0.08933056145906448 acc: 1.0\n",
      "loss: 0.03327568247914314 acc: 1.0\n",
      "loss: 0.2406732141971588 acc: 1.0\n",
      "loss: 0.01624816656112671 acc: 1.0\n",
      "loss: 0.5140324234962463 acc: 0.5\n",
      "loss: 0.042638182640075684 acc: 1.0\n",
      "loss: 0.03556889295578003 acc: 1.0\n",
      "loss: 0.2434571385383606 acc: 1.0\n",
      "loss: 0.06274750828742981 acc: 1.0\n",
      "loss: 0.15961404144763947 acc: 1.0\n",
      "loss: 0.385846883058548 acc: 1.0\n",
      "loss: 0.04831405356526375 acc: 1.0\n",
      "loss: 0.027648210525512695 acc: 1.0\n",
      "loss: 0.04325541853904724 acc: 1.0\n",
      "loss: 1.2149726152420044 acc: 0.5\n",
      "loss: 0.016743957996368408 acc: 1.0\n",
      "loss: 0.15574012696743011 acc: 1.0\n",
      "loss: 0.13293017446994781 acc: 1.0\n",
      "loss: 0.050745438784360886 acc: 1.0\n",
      "loss: 0.014863371849060059 acc: 1.0\n",
      "loss: 0.02574855089187622 acc: 1.0\n",
      "loss: 0.04044675827026367 acc: 1.0\n",
      "loss: 0.08350566029548645 acc: 1.0\n",
      "loss: 0.030758917331695557 acc: 1.0\n",
      "loss: 0.4604189395904541 acc: 1.0\n",
      "loss: 0.019829630851745605 acc: 1.0\n",
      "loss: 0.20382773876190186 acc: 1.0\n",
      "loss: 0.019040405750274658 acc: 1.0\n",
      "loss: 0.029521971940994263 acc: 1.0\n",
      "loss: 0.015493154525756836 acc: 1.0\n",
      "loss: 0.023328006267547607 acc: 1.0\n",
      "loss: 0.07859202474355698 acc: 1.0\n",
      "loss: 0.03382560610771179 acc: 1.0\n",
      "loss: 0.023205241188406944 acc: 1.0\n",
      "loss: 0.025631746277213097 acc: 1.0\n",
      "loss: 0.02564561367034912 acc: 1.0\n",
      "loss: 0.028127551078796387 acc: 1.0\n",
      "loss: 0.1123739629983902 acc: 1.0\n",
      "loss: 0.023350536823272705 acc: 1.0\n",
      "loss: 0.025671323761343956 acc: 1.0\n",
      "loss: 0.04843727871775627 acc: 1.0\n",
      "loss: 0.026006758213043213 acc: 1.0\n",
      "loss: 0.03065558336675167 acc: 1.0\n",
      "loss: 0.03612375259399414 acc: 1.0\n",
      "loss: 0.08846624940633774 acc: 1.0\n",
      "loss: 0.22947293519973755 acc: 1.0\n",
      "loss: 0.3954660892486572 acc: 0.5\n",
      "loss: 0.31018581986427307 acc: 1.0\n",
      "loss: 0.029524624347686768 acc: 1.0\n",
      "loss: 0.02010369300842285 acc: 1.0\n",
      "loss: 0.04850113391876221 acc: 1.0\n",
      "loss: 0.24923892319202423 acc: 1.0\n",
      "loss: 0.03370912745594978 acc: 1.0\n",
      "loss: 1.5531654357910156 acc: 0.5\n",
      "loss: 0.06331473588943481 acc: 1.0\n",
      "loss: 0.052564144134521484 acc: 1.0\n",
      "loss: 0.03138858079910278 acc: 1.0\n",
      "loss: 0.022846877574920654 acc: 1.0\n",
      "loss: 0.02446913719177246 acc: 1.0\n",
      "loss: 0.059547025710344315 acc: 1.0\n",
      "loss: 1.6329009532928467 acc: 0.5\n",
      "loss: 0.03578938916325569 acc: 1.0\n",
      "loss: 0.07599849998950958 acc: 1.0\n",
      "loss: 0.028976261615753174 acc: 1.0\n",
      "loss: 1.2199885845184326 acc: 0.5\n",
      "loss: 0.08630650490522385 acc: 1.0\n",
      "loss: 0.4321770966053009 acc: 1.0\n",
      "loss: 0.031213322654366493 acc: 1.0\n",
      "loss: 0.023234963417053223 acc: 1.0\n",
      "loss: 0.27361026406288147 acc: 1.0\n",
      "loss: 0.20488430559635162 acc: 1.0\n",
      "loss: 0.11263354867696762 acc: 1.0\n",
      "loss: 0.029436767101287842 acc: 1.0\n",
      "loss: 0.31413090229034424 acc: 1.0\n",
      "loss: 0.024635275825858116 acc: 1.0\n",
      "loss: 0.024628043174743652 acc: 1.0\n",
      "loss: 0.03870481252670288 acc: 1.0\n",
      "loss: 0.029634634032845497 acc: 1.0\n",
      "loss: 0.16304732859134674 acc: 1.0\n",
      "loss: 0.03278613090515137 acc: 1.0\n",
      "loss: 0.031007686629891396 acc: 1.0\n",
      "loss: 0.03544729948043823 acc: 1.0\n",
      "loss: 0.04120686650276184 acc: 1.0\n",
      "loss: 0.025769054889678955 acc: 1.0\n",
      "loss: 0.2799108028411865 acc: 1.0\n",
      "loss: 0.06116024777293205 acc: 1.0\n",
      "loss: 0.04815787076950073 acc: 1.0\n",
      "loss: 0.02941441535949707 acc: 1.0\n",
      "loss: 0.18751771748065948 acc: 1.0\n",
      "loss: 0.04477858543395996 acc: 1.0\n",
      "loss: 0.03698837757110596 acc: 1.0\n",
      "loss: 0.033591706305742264 acc: 1.0\n",
      "loss: 0.08811040967702866 acc: 1.0\n",
      "loss: 0.03162413835525513 acc: 1.0\n",
      "loss: 0.02832847833633423 acc: 1.0\n",
      "loss: 0.027717411518096924 acc: 1.0\n",
      "loss: 0.04430091381072998 acc: 1.0\n",
      "loss: 0.025275826454162598 acc: 1.0\n",
      "loss: 0.6672351956367493 acc: 0.5\n",
      "loss: 0.3740041255950928 acc: 0.5\n",
      "loss: 0.04029363393783569 acc: 1.0\n",
      "loss: 0.07546684890985489 acc: 1.0\n",
      "loss: 0.042083222419023514 acc: 1.0\n",
      "loss: 0.2552522122859955 acc: 1.0\n",
      "loss: 0.1751074641942978 acc: 1.0\n",
      "loss: 0.018580853939056396 acc: 1.0\n",
      "loss: 0.03604242205619812 acc: 1.0\n",
      "loss: 0.028136610984802246 acc: 1.0\n",
      "loss: 0.04416462779045105 acc: 1.0\n",
      "loss: 0.01049947738647461 acc: 1.0\n",
      "loss: 0.043658822774887085 acc: 1.0\n",
      "loss: 0.03364706039428711 acc: 1.0\n",
      "loss: 0.03645249083638191 acc: 1.0\n",
      "loss: 1.2889907360076904 acc: 0.5\n",
      "loss: 0.1375994086265564 acc: 1.0\n",
      "loss: 0.10228335857391357 acc: 1.0\n",
      "loss: 0.02937975525856018 acc: 1.0\n",
      "loss: 0.03870570659637451 acc: 1.0\n",
      "loss: 0.07338956743478775 acc: 1.0\n",
      "loss: 0.039908915758132935 acc: 1.0\n",
      "loss: 0.09192699193954468 acc: 1.0\n",
      "loss: 0.3907923698425293 acc: 1.0\n",
      "loss: 0.05104442313313484 acc: 1.0\n",
      "loss: 0.12007149308919907 acc: 1.0\n",
      "loss: 0.26818981766700745 acc: 1.0\n",
      "loss: 0.02942601777613163 acc: 1.0\n",
      "loss: 0.03600986674427986 acc: 1.0\n",
      "loss: 0.04607188701629639 acc: 1.0\n",
      "loss: 0.035548049956560135 acc: 1.0\n",
      "loss: 0.2126936912536621 acc: 1.0\n",
      "loss: 0.03040260076522827 acc: 1.0\n",
      "loss: 0.30593380331993103 acc: 1.0\n",
      "loss: 0.019906818866729736 acc: 1.0\n",
      "loss: 0.03212207555770874 acc: 1.0\n",
      "loss: 1.1706773042678833 acc: 0.5\n",
      "loss: 0.029166758060455322 acc: 1.0\n",
      "loss: 0.02432173490524292 acc: 1.0\n",
      "loss: 0.03834536671638489 acc: 1.0\n",
      "loss: 0.04630729556083679 acc: 1.0\n",
      "loss: 0.03921810910105705 acc: 1.0\n",
      "loss: 0.3364279270172119 acc: 1.0\n",
      "loss: 0.12483704090118408 acc: 1.0\n",
      "loss: 0.032289307564496994 acc: 1.0\n",
      "loss: 0.010135054588317871 acc: 1.0\n",
      "loss: 0.4883192479610443 acc: 1.0\n",
      "loss: 0.5018894672393799 acc: 0.5\n",
      "loss: 0.028206506744027138 acc: 1.0\n",
      "loss: 0.1086525097489357 acc: 1.0\n",
      "loss: 0.010098159313201904 acc: 1.0\n",
      "loss: 0.030619442462921143 acc: 1.0\n",
      "loss: 0.04792207479476929 acc: 1.0\n",
      "loss: 0.16359487175941467 acc: 1.0\n",
      "loss: 0.07547102868556976 acc: 1.0\n",
      "loss: 0.054954349994659424 acc: 1.0\n",
      "loss: 0.03603116795420647 acc: 1.0\n",
      "loss: 0.026719629764556885 acc: 1.0\n",
      "loss: 0.5831045508384705 acc: 0.5\n",
      "loss: 0.6013824343681335 acc: 0.5\n",
      "loss: 0.07358014583587646 acc: 1.0\n",
      "loss: 0.19634149968624115 acc: 1.0\n",
      "loss: 0.02535092830657959 acc: 1.0\n",
      "loss: 0.037051279097795486 acc: 1.0\n",
      "loss: 0.5206092596054077 acc: 0.5\n",
      "loss: 0.041847724467515945 acc: 1.0\n",
      "loss: 0.21519573032855988 acc: 1.0\n",
      "loss: 0.04386672377586365 acc: 1.0\n",
      "loss: 0.009331285953521729 acc: 1.0\n",
      "loss: 0.011514623649418354 acc: 1.0\n",
      "loss: 0.14780111610889435 acc: 1.0\n",
      "loss: 0.1760738641023636 acc: 1.0\n",
      "loss: 0.03601399064064026 acc: 1.0\n",
      "loss: 0.10528608411550522 acc: 1.0\n",
      "loss: 0.030569374561309814 acc: 1.0\n",
      "loss: 0.03163468837738037 acc: 1.0\n",
      "loss: 0.03455895185470581 acc: 1.0\n",
      "loss: 0.03312021493911743 acc: 1.0\n",
      "loss: 0.0240364670753479 acc: 1.0\n",
      "loss: 0.050893384963274 acc: 1.0\n",
      "loss: 1.3327584266662598 acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475982532751092"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(\"/home/hadoop/tmp_loss\").readlines()\n",
    "\n",
    "accs = [float(line.strip(\"\\n\").split(\"acc:\")[1]) for line in lines]\n",
    "\n",
    "mean_acc = np.array(accs).mean()\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the bert model on another data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([163., 337.]),\n",
       " [[0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array(get_data_y())\n",
    "test_labels.sum(axis=0), test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.07612740993499756 acc: 1.0\n",
      "loss: 0.017561832442879677 acc: 1.0\n",
      "loss: 0.6489768028259277 acc: 0.5\n",
      "loss: 1.1129690408706665 acc: 0.5\n",
      "loss: 0.18138177692890167 acc: 1.0\n",
      "loss: 0.036225467920303345 acc: 1.0\n",
      "loss: 0.00855553150177002 acc: 1.0\n",
      "loss: 0.781512975692749 acc: 0.5\n",
      "loss: 5.100790500640869 acc: 0.5\n",
      "loss: 0.005118608474731445 acc: 1.0\n",
      "loss: 0.01620030403137207 acc: 1.0\n",
      "loss: 1.5932191610336304 acc: 0.5\n",
      "loss: 0.04462093114852905 acc: 1.0\n",
      "loss: 0.06475254893302917 acc: 1.0\n",
      "loss: 0.9104501008987427 acc: 0.5\n",
      "loss: 0.03745266795158386 acc: 1.0\n",
      "loss: 0.0005619525909423828 acc: 1.0\n",
      "loss: 2.9406898021698 acc: 0.5\n",
      "loss: 0.1600847989320755 acc: 1.0\n",
      "loss: 0.24916815757751465 acc: 1.0\n",
      "loss: 0.1096452847123146 acc: 1.0\n",
      "loss: 0.08292627334594727 acc: 1.0\n",
      "loss: 0.0026209354400634766 acc: 1.0\n",
      "loss: 0.2198190838098526 acc: 1.0\n",
      "loss: 0.005079030990600586 acc: 1.0\n",
      "mean_loss 0.5762604392319918  | mean_acc: 0.86\n"
     ]
    }
   ],
   "source": [
    "eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
