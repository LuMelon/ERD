{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import MyLogger\n",
    "import time\n",
    "import SubjObjLoader\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from collections import deque\n",
    "# import dataloader\n",
    "from BertRDMLoader import *\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import tsentiLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工具函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画柱状图表达特征指标的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(heatmap, data_label, heat_max, heat_min):\n",
    "    #attention: [batch, seq_len]\n",
    "    #data_label: [batch]\n",
    "    batch = len(data_label)\n",
    "    height = 0.5\n",
    "    plt.figure(figsize=(50, batch*height))\n",
    "    plt.ylim(0, batch*height)\n",
    "    plt.xlim(0, 50)\n",
    "    plt.yticks(np.arange(0, batch*height, height), data_label)\n",
    "    plt.yticks(np.arange(0, 50, 5), ['']*10)\n",
    "    for idx, h in enumerate(heatmap):\n",
    "        color_list = [((b-heat_min)*1.0/(heat_max - heat_min), 0.0, 0.0) for b in h]\n",
    "        d = np.arange(0, 50, 49.99/len(h)).tolist()\n",
    "        d.reverse()\n",
    "        color_list.reverse()\n",
    "        for color, length in zip(color_list, d):\n",
    "            plt.barh([idx*0.5], length, 1.0, color=color) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = [np.random.randint(5, size = [np.random.randint(5, 100)]).tolist() for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x3600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_heatmap(heat, list(range(100)), 4, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 旧模型的评价\n",
    "\n",
    "bert没有分到多块GPU上去进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "#         self.PoolLayer = pooling_layer(word_embedding_dim, sent_embedding_dim) \n",
    "        \n",
    "    def forward(self, x_emb, x_len, init_states): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "#         pool_feature = self.PoolLayer(x_emb)\n",
    "#         sent_feature = sentiModel( \n",
    "#                 x_emb.reshape(\n",
    "#                     [-1, max_sent_len, emb_dim]\n",
    "#                 ) \n",
    "#             ).reshape(\n",
    "#                 [batchsize, max_seq_len, -1]\n",
    "#             )\n",
    "#         pooled_input_x_dp = self.DropLayer(input_x)\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        return hidden_outs, final_outs\n",
    "\n",
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, s_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "#         assert(embedding_dim==self.embedding_dim)\n",
    "        sentence = s_model(rl_input).reshape(batch_size, 1, self.sentence_embedding_dim)\n",
    "#         pooled_rl_input = self.PoolLayer(\n",
    "#             rl_input.reshape(\n",
    "#                 [-1, 1, max_word_num, self.embedding_dim]\n",
    "#             )\n",
    "#         ).reshape([-1, 1, self.hidden_dim])\n",
    "        \n",
    "#         print(\"sentence:\", sentence.shape)\n",
    "#         print(\"rl_state:\", rl_state.shape)\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            sentence, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def layer2seq(bert, layer, cuda=False):\n",
    "    if cuda:\n",
    "        outs = [bert( torch.tensor([input_]).cuda())\n",
    "                for input_ in layer]   \n",
    "    else: \n",
    "        outs = [bert( torch.tensor([input_]))\n",
    "                    for input_ in layer]\n",
    "    states = [item[1] for item in outs]\n",
    "    return rnn_utils.pad_sequence(states, batch_first=True)\n",
    "\n",
    "def Word_ids2SeqStates(word_ids, bert, ndim, cuda=False):\n",
    "    assert(ndim == 3)\n",
    "    if cuda:\n",
    "        embedding = [layer2seq(bert, layer, cuda) for layer in word_ids]\n",
    "    else:\n",
    "        embedding = [layer2seq(bert, layer) for layer in word_ids]\n",
    "    return padding_sequence(embedding)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def Count_Accs(ylabel, preds):\n",
    "    correct_preds = np.array(\n",
    "        [1 if y1==y2 else 0 \n",
    "        for (y1, y2) in zip(ylabel, preds)]\n",
    "    )\n",
    "    y_idxs = [idx if yl >0 else idx - len(ylabel) \n",
    "            for (idx, yl) in enumerate(ylabel)]\n",
    "    pos_idxs = list(filter(lambda x: x >= 0, y_idxs))\n",
    "    neg_idxs = list(filter(lambda x: x < 0, y_idxs))\n",
    "    acc = sum(correct_preds) / (1.0 * len(ylabel))\n",
    "    if len(pos_idxs) > 0:\n",
    "        pos_acc = sum(correct_preds[pos_idxs])/(1.0*len(pos_idxs))\n",
    "    else:\n",
    "        pos_acc = 0\n",
    "    if len(neg_idxs) > 0:\n",
    "        neg_acc = sum(correct_preds[neg_idxs])/(1.0*len(neg_idxs))\n",
    "    else:\n",
    "        neg_acc = 0\n",
    "    return acc, pos_acc, neg_acc, y_idxs, pos_idxs, neg_idxs, correct_preds\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, new_data_len=[]):\n",
    "    batch_size = 20 \n",
    "    d_IDs = get_data_ID()\n",
    "    t_steps=int(len(d_IDs)/batch_size)\n",
    "    \n",
    "    max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    splits = int(batch_size/max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    acc_l = np.zeros(splits)\n",
    "    loss_l = np.zeros(splits)\n",
    "    with torch.no_grad():\n",
    "        for step in range(t_steps):\n",
    "            try:\n",
    "                for j in range(splits):\n",
    "                    if len(new_data_len) > 0:\n",
    "                        x, x_len, y = get_df_batch(step*splits+j, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "                    else:\n",
    "                        x, x_len, y = get_df_batch(step, max_gpu_batch, tokenizer=tokenizer)\n",
    "                    x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "                    batchsize, max_seq_len, max_sent_len,                                     emb_dim = x_emb.shape\n",
    "                    rdm_hiddens, rdm_outs = rdm_model(x_emb, x_len, init_states)\n",
    "                    rdm_scores = rdm_classifier(\n",
    "                        torch.cat(\n",
    "                            rdm_outs # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                        ).reshape(\n",
    "                            [-1, rdm_model.hidden_dim]\n",
    "                        )\n",
    "                    )\n",
    "                    rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                    y_label = y.argmax(axis=1)\n",
    "                    acc_l[j], _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "                    loss = loss_fn(rdm_scores, torch.tensor(y_label).cuda())\n",
    "                    loss_l[j] = float(loss)\n",
    "    #                 print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "            except RuntimeError as exception:\n",
    "                if \"out of memory\" in str(exception):\n",
    "                    print(\"WARNING: out of memory\")\n",
    "                    print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "                    if hasattr(torch.cuda, 'empty_cache'):\n",
    "                        torch.cuda.empty_cache()\n",
    "    #                     time.sleep(5)\n",
    "                    raise exception\n",
    "                else:   \n",
    "                    raise exception\n",
    "            sum_loss += loss_l.mean()\n",
    "            sum_acc += acc_l.mean()\n",
    "            print(\"loss:\", loss_l.mean(), \"acc:\", acc_l.mean())\n",
    "    mean_loss = sum_loss/(1.0*t_steps)\n",
    "    mean_acc = sum_acc/(1.0*t_steps)\n",
    "    print(\"mean_loss\", mean_loss, \" | mean_acc:\", mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cached_dir = \"/home/hadoop/transformer_pretrained_models/bert-base-uncased-pytorch_model.bin\")\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "\n",
    "with open(\"config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)\n",
    "# In[10]:\n",
    "load_data_fast()\n",
    "\n",
    "data_ID = get_data_ID()\n",
    "\n",
    "len(data_ID)\n",
    "\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 切分原数据集 | 3884: 200 :500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_马晶那个数据集跟这个数据集差异太大，无法迁移_\n",
    "\n",
    "``` python\n",
    "data_ID = get_data_ID()\n",
    "data_len = get_data_len()\n",
    "data_y = get_data_y()\n",
    "\n",
    "test_data_y = data_y[-500:]\n",
    "\n",
    "test_data_len = data_len[-500:]\n",
    "\n",
    "test_data_ID = data_ID[-500:]\n",
    "\n",
    "np.save(\"data/data_ID.npy\", np.array(data_ID)[:-500])\n",
    "np.save(\"data/data_len.npy\", np.array(data_len)[:-500])\n",
    "np.save(\"data/data_y.npy\", np.array(data_y)[:-500])\n",
    "\n",
    "np.save(\"data/test_data_ID.npy\", np.array(test_data_ID))\n",
    "np.save(\"data/test_data_len.npy\", np.array(test_data_len))\n",
    "np.save(\"data/test_data_y.npy\", np.array(test_data_y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "data_ID = get_data_ID()\n",
    "data_len = get_data_len()\n",
    "data_y = get_data_y()\n",
    "\n",
    "valid_data_y = data_y[-200:]\n",
    "\n",
    "valid_data_len = data_len[-200:]\n",
    "\n",
    "valid_data_ID = data_ID[-200:]\n",
    "\n",
    "np.save(\"data/data_ID.npy\", np.array(data_ID)[:-200])\n",
    "np.save(\"data/data_len.npy\", np.array(data_len)[:-200])\n",
    "np.save(\"data/data_y.npy\", np.array(data_y)[:-200])\n",
    "\n",
    "np.save(\"data/valid_data_ID.npy\", np.array(valid_data_ID))\n",
    "np.save(\"data/valid_data_len.npy\", np.array(valid_data_len))\n",
    "np.save(\"data/valid_data_y.npy\", np.array(valid_data_y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "del data               \n",
    "del data_ID              \n",
    "del data_len               \n",
    "del data_y             \n",
    "del valid_data_ID            \n",
    "del valid_data_len       \n",
    "del valid_data_y            \n",
    "\n",
    "from BertRDMLoader import data          \n",
    "from BertRDMLoader import data_ID       \n",
    "from BertRDMLoader import data_len      \n",
    "from BertRDMLoader import data_y        \n",
    "from BertRDMLoader import valid_data_ID \n",
    "from BertRDMLoader import valid_data_len\n",
    "from BertRDMLoader import valid_data_y  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 创建测试所用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rdm_model = RDM_Model(768, 300, 256, 0.2).cuda()\n",
    "cm_model = CM_Model(300, 256, 2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "senti_save_as = '/home/hadoop/ERD/RDMBertTrain/rdmModel_epoch019.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(senti_save_as)\n",
    "bert.load_state_dict(checkpoint['bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "``` python\n",
    "torch.save(\n",
    "                    {\n",
    "                        \"bert\":bert.state_dict(),\n",
    "                        \"rmdModel\":rdm_model.state_dict(),\n",
    "                        \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "                    },\n",
    "                    rdm_save_as\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdm_model.load_state_dict(checkpoint['rmdModel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdm_classifier.load_state_dict(checkpoint['rdm_classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### test the bert embedding model on the trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.11101210117340088 acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-afd2bcba5c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m eval(rdm_model, bert, rdm_classifier, \n\u001b[0;32m----> 2\u001b[0;31m                     tokenizer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-220f3a1020df>\u001b[0m in \u001b[0;36meval\u001b[0;34m(rdm_model, bert, rdm_classifier, tokenizer, new_data_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gpu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0mx_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord_ids2SeqStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sent_len\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mrdm_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae56a178560>\u001b[0m in \u001b[0;36mWord_ids2SeqStates\u001b[0;34m(word_ids, bert, ndim, cuda)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae56a178560>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae56a178560>\u001b[0m in \u001b[0;36mlayer2seq\u001b[0;34m(bert, layer, cuda)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         outs = [bert( torch.tensor([input_]).cuda())\n\u001b[0;32m--> 113\u001b[0;31m                 for input_ in layer]   \n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         outs = [bert( torch.tensor([input_]))\n",
      "\u001b[0;32m<ipython-input-37-bae56a178560>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         outs = [bert( torch.tensor([input_]).cuda())\n\u001b[0;32m--> 113\u001b[0;31m                 for input_ in layer]   \n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         outs = [bert( torch.tensor([input_]))\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    713\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    714\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                                        head_mask=head_mask)\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# initialize self.training separately from the rest of the internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# state, as it is managed differently by nn.Module and ScriptModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthnn_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475982532751092"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(\"/home/hadoop/tmp_loss\").readlines()\n",
    "\n",
    "accs = [float(line.strip(\"\\n\").split(\"acc:\")[1]) for line in lines]\n",
    "\n",
    "mean_acc = np.array(accs).mean()\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = torch.randn([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9957, -0.4725,  0.5221,  0.0872]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### test the bert model on another data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_test_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_ID =  get_data_ID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([163., 337.]),\n",
       " [[0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0],\n",
       "  [1.0, 0.0],\n",
       "  [1.0, 0.0],\n",
       "  [0.0, 1.0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array(get_data_y())\n",
    "test_labels.sum(axis=0), test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.07612740993499756 acc: 1.0\n",
      "loss: 0.017561832442879677 acc: 1.0\n",
      "loss: 0.6489768028259277 acc: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-afd2bcba5c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m eval(rdm_model, bert, rdm_classifier, \n\u001b[0;32m----> 2\u001b[0;31m                     tokenizer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-220f3a1020df>\u001b[0m in \u001b[0;36meval\u001b[0;34m(rdm_model, bert, rdm_classifier, tokenizer, new_data_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gpu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0mx_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord_ids2SeqStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sent_len\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mrdm_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bae56a178560>\u001b[0m in \u001b[0;36mWord_ids2SeqStates\u001b[0;34m(word_ids, bert, ndim, cuda)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bae56a178560>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bae56a178560>\u001b[0m in \u001b[0;36mlayer2seq\u001b[0;34m(bert, layer, cuda)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         outs = [bert( torch.tensor([input_]).cuda())\n\u001b[0;32m--> 113\u001b[0;31m                 for input_ in layer]   \n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         outs = [bert( torch.tensor([input_]))\n",
      "\u001b[0;32m<ipython-input-2-bae56a178560>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         outs = [bert( torch.tensor([input_]).cuda())\n\u001b[0;32m--> 113\u001b[0;31m                 for input_ in layer]   \n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         outs = [bert( torch.tensor([input_]))\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    713\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    714\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                                        head_mask=head_mask)\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新模型的评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新模型的模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "#         self.PoolLayer = pooling_layer(word_embedding_dim, sent_embedding_dim) \n",
    "        \n",
    "    def forward(self, x_emb): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "        init_states = torch.zeros([1, batchsize, self.hidden_dim], dtype=torch.float32).cuda()\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        try:\n",
    "            df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        except:\n",
    "            print(\"Error:\", pool_feature.shape, init_states.shape)\n",
    "            raise\n",
    "        # hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        # final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        # return hidden_outs, final_outs\n",
    "        return df_outputs\n",
    "\n",
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            rl_input, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "    \n",
    "def WeightsForUmbalanced(data_label):\n",
    "    _, _, labels = data_label.shape\n",
    "    label_cnt = data_label.reshape([-1, labels]).sum(axis=0)\n",
    "    weights = 1.0/label_cnt\n",
    "    normalized_weights = weights/sum(weights)\n",
    "    return normalized_weights\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# x_new = [sent1, sent2, sent3, ...]　\n",
    "# x_new -> x_old_emb [batchsize, seq_len, sent_emb]：#使用seq_info 将sent组装回去\n",
    "# ---> [batchsize, max_seq_len, sent_emb] # padding 成一个可以计算的batch, 从而可以切分\n",
    "def rdm_data2bert_tensors(data_X, cuda):\n",
    "    def padding_sent_list(sent_list):\n",
    "        sent_len = [len(sent) for sent in sent_list]\n",
    "        max_sent_len = max(sent_len)\n",
    "        sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "        attn_mask = torch.ones_like(sent_padding)\n",
    "        for i, sent in enumerate(sent_list):\n",
    "            sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "            attn_mask[i][len(sent):].fill_(0)\n",
    "        return sent_padding, attn_mask\n",
    "    sent_list = []\n",
    "    [sent_list.extend(seq) for seq in data_X]\n",
    "    seq_len = [len(seq) for seq in data_X]\n",
    "    sent_tensors, attn_mask = padding_sent_list(sent_list)\n",
    "    if cuda:\n",
    "        sent_tensors = sent_tensors.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_tensors, attn_mask, seq_len\n",
    "\n",
    "def subj_data2bert_tensors(sent_list, cuda):\n",
    "    sent_len = [len(sent) for sent in sent_list]\n",
    "    max_sent_len = max(sent_len)\n",
    "    sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "    attn_mask = torch.ones_like(sent_padding)\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "        attn_mask[i][len(sent):].fill_(0)\n",
    "    if cuda:\n",
    "        sent_padding = sent_padding.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_padding, attn_mask\n",
    "\n",
    "def senti_data2bert_tensors(sent_list, cuda):\n",
    "    sent_len = [len(sent) for sent in sent_list]\n",
    "    max_sent_len = max(sent_len)\n",
    "    sent_padding = torch.zeros([len(sent_list), max_sent_len], dtype=torch.int64)\n",
    "    attn_mask = torch.ones_like(sent_padding)\n",
    "    for i, sent in enumerate(sent_list):\n",
    "        sent_padding[i][:len(sent)] = torch.tensor(sent, dtype=torch.int32)\n",
    "        attn_mask[i][len(sent):].fill_(0)\n",
    "    if cuda:\n",
    "        sent_padding = sent_padding.cuda()\n",
    "        attn_mask = attn_mask.cuda()\n",
    "    return sent_padding, attn_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型的评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rdm(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, new_data_len=[], cuda=True):\n",
    "    batch_size = 20 \n",
    "    d_IDs = get_data_ID()\n",
    "    t_steps=int(len(d_IDs)/batch_size)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for step in range(t_steps):\n",
    "            x, x_len, y = get_df_batch(step, batch_size, tokenizer=tokenizer)\n",
    "            sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "            bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            pooled_sents = [bert_outs[1][sum(seq_len[:idx]):sum(seq_len[:idx])+seq_len[idx]] for idx, s_len in enumerate(seq_len)]\n",
    "            data_tensors = rnn_utils.pad_sequence(pooled_sents, batch_first=True).unsqueeze(-2)\n",
    "            rdm_hiddens = rdm_model(data_tensors)\n",
    "            batchsize, _, _ = rdm_hiddens.shape\n",
    "            rdm_outs = torch.cat(\n",
    "                [ rdm_hiddens[i][x_len[i]-1] for i in range(batchsize)] \n",
    "                # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "            ).reshape(\n",
    "                [-1, rdm_model.hidden_dim]\n",
    "            )\n",
    "            rdm_scores = rdm_classifier(\n",
    "                rdm_outs\n",
    "            )\n",
    "            rdm_preds = rdm_scores.argmax(axis=1)\n",
    "            y_label = torch.tensor(y).argmax(axis=1).cuda() if cuda else torch.tensor(y).argmax(axis=1)\n",
    "\n",
    "            labels.append(y_label)\n",
    "            preds.append(rdm_preds)\n",
    "\n",
    "            loss = loss_fn(rdm_scores, y_label)\n",
    "            sum_loss += loss\n",
    "            torch.cuda.empty_cache()\n",
    "    mean_loss = sum_loss/(1.0*t_steps)\n",
    "\n",
    "    rdm_preds = torch.cat(preds, axis=0) if not cuda else torch.cat(preds, axis=0).cpu()\n",
    "    y_label = torch.cat(labels, axis=0) if not cuda else torch.cat(labels, axis=0).cpu()\n",
    "    macro_precision, micro_precision, precision = precision_score(y_label, rdm_preds, average=\"macro\"), precision_score(y_label, rdm_preds, average=\"micro\"), precision_score(y_label, rdm_preds, average=None)\n",
    "    macro_recall, micro_recall, recall = recall_score(y_label, rdm_preds, average=\"macro\"), recall_score(y_label, rdm_preds, average=\"micro\"), recall_score(y_label, rdm_preds, average=None)\n",
    "    acc = accuracy_score(y_label, rdm_preds)\n",
    "    macro_f1, micro_f1, f1 = f1_score(y_label, rdm_preds, average=\"macro\"), f1_score(y_label, rdm_preds, average=\"micro\"), f1_score(y_label, rdm_preds, average=None)\n",
    "    return mean_loss, (macro_precision, micro_precision, precision), (macro_recall, micro_recall, recall), acc, (macro_f1, micro_f1, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建模型并导入测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./bertModel/\")\n",
    "bert = BertModel.from_pretrained(\"./bertModel/\").cuda()\n",
    "rdm_model = RDM_Model(768, 300, 256, 0.2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_ids = [0]\n",
    "bert = nn.DataParallel(bert, device_ids=device_ids)\n",
    "device_name = \"cuda:%d\"%device_ids[0]\n",
    "device = torch.device(device_name)\n",
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 测试RDM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_显然这个模型的挑选时机不好，过拟合了，应当挑选一个在开发集上最好的模型_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_file = \"./MTLRDM/rdmModel_epoch199.pkl\"\n",
    "checkpoint = torch.load(pretrained_file)\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "rdm_model.load_state_dict(checkpoint[\"rmdModel\"])\n",
    "rdm_classifier.load_state_dict(checkpoint[\"rdm_classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n",
      "loss: 0.0013149792794138193 acc: 1.0\n",
      "loss: 0.0008682012557983398 acc: 1.0\n",
      "loss: 0.0011640170123428106 acc: 1.0\n",
      "loss: 0.0019624531269073486 acc: 1.0\n",
      "loss: 0.0012935230042785406 acc: 1.0\n",
      "loss: 0.0010040828492492437 acc: 1.0\n",
      "loss: 0.0012245089747011662 acc: 1.0\n",
      "loss: 0.03089812584221363 acc: 1.0\n",
      "loss: 0.0006210895953699946 acc: 1.0\n",
      "loss: 0.0008620548178441823 acc: 1.0\n",
      "loss: 0.0008964042062871158 acc: 1.0\n",
      "loss: 0.0012937907595187426 acc: 1.0\n",
      "loss: 0.0007344117620959878 acc: 1.0\n",
      "loss: 0.0011110963532701135 acc: 1.0\n",
      "loss: 0.0009626635583117604 acc: 1.0\n",
      "loss: 0.0013735622633248568 acc: 1.0\n",
      "loss: 0.001495623611845076 acc: 1.0\n",
      "loss: 0.0015656640753149986 acc: 1.0\n",
      "loss: 0.001469327020458877 acc: 1.0\n",
      "loss: 0.001017270260490477 acc: 1.0\n",
      "loss: 0.0010514497989788651 acc: 1.0\n",
      "loss: 0.0009182266076095402 acc: 1.0\n",
      "loss: 0.0010460523189976811 acc: 1.0\n",
      "loss: 0.002764412434771657 acc: 1.0\n",
      "loss: 0.0009817044483497739 acc: 1.0\n",
      "loss: 0.0009120305185206234 acc: 1.0\n",
      "loss: 0.0009324635611847043 acc: 1.0\n",
      "loss: 0.0007556629134342074 acc: 1.0\n",
      "loss: 0.0010860516922548413 acc: 1.0\n",
      "loss: 0.001237954362295568 acc: 1.0\n",
      "loss: 0.00093841552734375 acc: 1.0\n",
      "loss: 0.0012408593902364373 acc: 1.0\n",
      "loss: 0.0010895913001149893 acc: 1.0\n",
      "loss: 0.0012248887214809656 acc: 1.0\n",
      "loss: 0.0011911392211914062 acc: 1.0\n",
      "loss: 0.000963398371823132 acc: 1.0\n",
      "loss: 0.0008591898949816823 acc: 1.0\n",
      "loss: 0.0012306059943512082 acc: 1.0\n",
      "loss: 0.0010384846245869994 acc: 1.0\n",
      "loss: 0.0017500590765848756 acc: 1.0\n",
      "loss: 0.0014520457480102777 acc: 1.0\n",
      "loss: 0.001205179374665022 acc: 1.0\n",
      "loss: 0.0007400676840916276 acc: 1.0\n",
      "loss: 0.0007810684619471431 acc: 1.0\n",
      "loss: 0.0010892009595409036 acc: 1.0\n",
      "loss: 0.0010371208190917969 acc: 1.0\n",
      "loss: 0.0009419459383934736 acc: 1.0\n",
      "loss: 0.001387719763442874 acc: 1.0\n",
      "loss: 0.0010444223880767822 acc: 1.0\n",
      "loss: 0.0009412318468093872 acc: 1.0\n",
      "loss: 0.001095356303267181 acc: 1.0\n",
      "loss: 0.0007423214265145361 acc: 1.0\n",
      "loss: 0.001696729683317244 acc: 1.0\n",
      "loss: 0.0011089423205703497 acc: 1.0\n",
      "loss: 0.0011621458688750863 acc: 1.0\n",
      "loss: 0.0011781002394855022 acc: 1.0\n",
      "loss: 0.0010005811927840114 acc: 1.0\n",
      "loss: 0.0006505684577859938 acc: 1.0\n",
      "loss: 0.0013092940207570791 acc: 1.0\n",
      "loss: 0.0008088074973784387 acc: 1.0\n",
      "loss: 0.002396596362814307 acc: 1.0\n",
      "loss: 0.0007900595664978027 acc: 1.0\n",
      "loss: 0.0016605942510068417 acc: 1.0\n",
      "loss: 0.0014764070510864258 acc: 1.0\n",
      "loss: 0.0017024462576955557 acc: 1.0\n",
      "loss: 0.0010665787849575281 acc: 1.0\n",
      "loss: 0.0008024236303754151 acc: 1.0\n",
      "loss: 0.0009129230747930706 acc: 1.0\n",
      "loss: 0.0012889078352600336 acc: 1.0\n",
      "loss: 0.0007127810968086123 acc: 1.0\n",
      "loss: 0.0013098539784550667 acc: 1.0\n",
      "loss: 0.0009724426199682057 acc: 1.0\n",
      "loss: 0.0015028970083221793 acc: 1.0\n",
      "loss: 0.0009602175559848547 acc: 1.0\n",
      "loss: 0.001069987309165299 acc: 1.0\n",
      "loss: 0.0007270145579241216 acc: 1.0\n",
      "loss: 0.001442308770492673 acc: 1.0\n",
      "loss: 0.0007454395527020097 acc: 1.0\n",
      "loss: 0.0009263072861358523 acc: 1.0\n",
      "loss: 0.0010785579215735197 acc: 1.0\n",
      "loss: 0.0016188091831281781 acc: 1.0\n",
      "loss: 0.0009730358724482358 acc: 1.0\n",
      "loss: 0.0008481884142383933 acc: 1.0\n",
      "loss: 0.0023365479428321123 acc: 1.0\n",
      "loss: 0.0010721731232479215 acc: 1.0\n",
      "loss: 0.0008632307290099561 acc: 1.0\n",
      "loss: 0.0012313586194068193 acc: 1.0\n",
      "loss: 0.001210664864629507 acc: 1.0\n",
      "loss: 0.0008304119110107422 acc: 1.0\n",
      "loss: 0.002780128503218293 acc: 1.0\n",
      "loss: 0.0006673622410744429 acc: 1.0\n",
      "loss: 0.001028159516863525 acc: 1.0\n",
      "loss: 0.0008328667609021068 acc: 1.0\n",
      "loss: 0.0011273111449554563 acc: 1.0\n",
      "loss: 0.0012942773755639791 acc: 1.0\n",
      "loss: 0.0009214419405907393 acc: 1.0\n",
      "loss: 0.001112929661758244 acc: 1.0\n",
      "loss: 0.0012114247074350715 acc: 1.0\n",
      "loss: 0.0014329084660857916 acc: 1.0\n",
      "loss: 0.0012953034602105618 acc: 1.0\n",
      "loss: 0.0008450666791759431 acc: 1.0\n",
      "loss: 0.0008107324247248471 acc: 1.0\n",
      "loss: 0.0011810516007244587 acc: 1.0\n",
      "loss: 0.0010390722891315818 acc: 1.0\n",
      "loss: 0.0010450744302943349 acc: 1.0\n",
      "loss: 0.005982222035527229 acc: 1.0\n",
      "loss: 0.0008703003986738622 acc: 1.0\n",
      "loss: 0.0009285944979637861 acc: 1.0\n",
      "loss: 0.0012923204340040684 acc: 1.0\n",
      "loss: 0.0023166032042354345 acc: 1.0\n",
      "loss: 0.0007601793040521443 acc: 1.0\n",
      "loss: 0.0015653830487281084 acc: 1.0\n",
      "loss: 0.0008654594421386719 acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f70387af2ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mload_data_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m eval_rdm(rdm_model, bert, rdm_classifier, \n\u001b[0;32m----> 3\u001b[0;31m                     tokenizer, new_data_len=[])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-724d13c2cdd4>\u001b[0m in \u001b[0;36meval_rdm\u001b[0;34m(rdm_model, bert, rdm_classifier, tokenizer, new_data_len, cuda)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mpooled_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbert_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdata_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mrdm_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_hiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             rdm_outs = torch.cat(\n",
      "\u001b[0;32m~/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0cb5b7df4f7f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_emb)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \"\"\"\n\u001b[1;32m     43\u001b[0m         \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0minit_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         pool_feature = x_emb.reshape(\n\u001b[1;32m     46\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_data_fast()\n",
    "eval_rdm(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, new_data_len=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.4481, device='cuda:0'),\n",
       " (0.6215083798882681, 0.674, array([0.5       , 0.74301676])),\n",
       " (0.6124501647521436, 0.674, array([0.43558282, 0.78931751])),\n",
       " 0.674,\n",
       " (0.615520698195542, 0.674, array([0.46557377, 0.76546763])))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_test_data_fast()\n",
    "eval_rdm(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, new_data_len=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试数据集中的情感变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embedding = nn.Embedding(3, 768)\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(768, 8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, 1)\n",
    "\n",
    "subj_cls = nn.Linear(768, 2)\n",
    "\n",
    "transformer = transformer_encoder.cuda()\n",
    "task_embedding = task_embedding.cuda()\n",
    "subj_cls = subj_cls.cuda()\n",
    "\n",
    "senti_cls = nn.Linear(768, 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_save_as = '/home/hadoop/ERD/MTLTrain/jointModel_epoch015.pkl'\n",
    "checkpoint = torch.load(joint_save_as)\n",
    "senti_cls.load_state_dict(checkpoint['senti_classifier'])\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "transformer.load_state_dict(checkpoint['transformer'])\n",
    "task_embedding.load_state_dict(checkpoint['task_embedding'])\n",
    "subj_cls.load_state_dict(checkpoint['subj_classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_test_data_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = adict({\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"num_labels\": 2,\n",
    "  \"output_attentions\": False,\n",
    "  \"output_hidden_states\": False,\n",
    "  \"torchscript\": False,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "})\n",
    "\n",
    "import transformer_utils\n",
    "\n",
    "importlib.reload(transformer_utils)\n",
    "\n",
    "BertEncoder = transformer_utils.BertEncoder\n",
    "\n",
    "trans = BertEncoder(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_ID\n",
    "del data_len\n",
    "del data_y\n",
    "\n",
    "from BertRDMLoader import data\n",
    "from BertRDMLoader import data_ID\n",
    "from BertRDMLoader import data_len\n",
    "from BertRDMLoader import data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idxs = [idx for idx, d_y in enumerate(data_y) if d_y[1]==1]\n",
    "neg_idxs = [idx for idx, d_y in enumerate(data_y) if d_y[0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_samples = random.sample(pos_idxs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = random.sample(pos_idxs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_idxs(data_idxs, new_data_len=[], tokenizer=None):\n",
    "    batchsize = len(data_idxs)\n",
    "    m_data_y = np.zeros([batchsize, 2], dtype=np.int32)\n",
    "    m_data_len = np.zeros([batchsize], dtype=np.int32)\n",
    "    data_x = [] #[batchsize, seq_len, sent_len]\n",
    "    if len(new_data_len) > 0:\n",
    "        t_data_len = new_data_len\n",
    "    else:\n",
    "        t_data_len = data_len\n",
    "    \n",
    "    for i in range(batchsize):\n",
    "        idx = data_idxs[i]\n",
    "        m_data_y[i] = data_y[idx]\n",
    "        m_data_len[i] = t_data_len[idx]\n",
    "        seq_x = [\n",
    "            tokenizer.encode(\n",
    "                data[data_ID[idx]]['text'][j],\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "            for j in range(t_data_len[idx])\n",
    "        ]\n",
    "        data_x.append(seq_x)\n",
    "    return data_x, m_data_len, m_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_len, labels = get_data_by_idxs(pos_samples, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x[0*20:1*20]\n",
    "sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([578, 58])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outs = bert(sent_tensors, attention_mask= attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([578, 58, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([58, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdm_seq_data2scores_seq(rdm_data, bert, task_emb, classifier, label_num =2):\n",
    "    batch_size = 20\n",
    "    assert len(rdm_data)%batch_size == 0\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(rdm_data)/batch_size):\n",
    "            x = rdm_data[i*batch_size:(i+1)*batch_size]\n",
    "            sent_tensors, attn_mask, seq_len = rdm_data2bert_tensors(x, cuda)\n",
    "            bert_outs = bert(sent_tensors, attention_mask=attn_mask)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
