{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 12:51:26.128711 140337803204416 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import config\n",
    "import pickle\n",
    "import dataloader\n",
    "import importlib\n",
    "from tensorflow import graph_util\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "logger = MyLogger(\"ERDMain\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.num_unroll_steps) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lstm_char_cnn' from '/home/hadoop/ERD/lstm_char_cnn.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(lstm_char_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 35, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(700, 1100), dtype=float32)\n",
      "     5: 0 [    5/ 1327], train_loss/perplexity = 9.15713978/9481.8974609 secs/batch = 0.6161s, grad.norm=0.11757042\n",
      "    10: 0 [   10/ 1327], train_loss/perplexity = 9.07616043/8744.3271484 secs/batch = 0.6236s, grad.norm=0.14040542\n",
      "    15: 0 [   15/ 1327], train_loss/perplexity = 8.97761822/7923.7368164 secs/batch = 0.7529s, grad.norm=0.17943056\n",
      "    20: 0 [   20/ 1327], train_loss/perplexity = 8.69668484/5983.0444336 secs/batch = 0.6355s, grad.norm=0.39473239\n",
      "    25: 0 [   25/ 1327], train_loss/perplexity = 7.77400303/2377.9714355 secs/batch = 0.6412s, grad.norm=0.67980880\n",
      "    30: 0 [   30/ 1327], train_loss/perplexity = 7.22767782/1377.0211182 secs/batch = 0.6140s, grad.norm=0.46658877\n",
      "    35: 0 [   35/ 1327], train_loss/perplexity = 7.45424461/1727.1788330 secs/batch = 0.6122s, grad.norm=0.63674241\n",
      "    40: 0 [   40/ 1327], train_loss/perplexity = 7.00763464/1105.0375977 secs/batch = 0.6098s, grad.norm=0.57256657\n",
      "    45: 0 [   45/ 1327], train_loss/perplexity = 6.73136282/838.2889404 secs/batch = 0.6078s, grad.norm=0.47526455\n",
      "    50: 0 [   50/ 1327], train_loss/perplexity = 7.29534531/1473.4255371 secs/batch = 0.6124s, grad.norm=0.73359561\n",
      "    55: 0 [   55/ 1327], train_loss/perplexity = 7.16874456/1298.2137451 secs/batch = 0.6070s, grad.norm=0.50202626\n",
      "    60: 0 [   60/ 1327], train_loss/perplexity = 6.88619757/978.6729736 secs/batch = 0.6118s, grad.norm=0.62220109\n",
      "    65: 0 [   65/ 1327], train_loss/perplexity = 7.52996016/1863.0312500 secs/batch = 0.6143s, grad.norm=0.67499417\n",
      "    70: 0 [   70/ 1327], train_loss/perplexity = 6.63228607/759.2158203 secs/batch = 0.6312s, grad.norm=0.61625093\n",
      "    75: 0 [   75/ 1327], train_loss/perplexity = 6.71117878/821.5384521 secs/batch = 0.6425s, grad.norm=0.46376446\n",
      "    80: 0 [   80/ 1327], train_loss/perplexity = 6.79192114/890.6229248 secs/batch = 0.6425s, grad.norm=0.77888560\n",
      "    85: 0 [   85/ 1327], train_loss/perplexity = 6.84550619/939.6488037 secs/batch = 0.6421s, grad.norm=0.44331887\n",
      "    90: 0 [   90/ 1327], train_loss/perplexity = 7.09905434/1210.8215332 secs/batch = 0.6239s, grad.norm=0.60750335\n",
      "    95: 0 [   95/ 1327], train_loss/perplexity = 6.76332664/865.5166626 secs/batch = 0.6360s, grad.norm=0.76749730\n",
      "   100: 0 [  100/ 1327], train_loss/perplexity = 6.79442453/892.8552856 secs/batch = 0.6121s, grad.norm=0.49907345\n",
      "   105: 0 [  105/ 1327], train_loss/perplexity = 6.96552229/1059.4681396 secs/batch = 0.7491s, grad.norm=0.88035256\n",
      "   110: 0 [  110/ 1327], train_loss/perplexity = 6.68043995/796.6695557 secs/batch = 0.7443s, grad.norm=0.79230702\n",
      "   115: 0 [  115/ 1327], train_loss/perplexity = 6.75799084/860.9107666 secs/batch = 0.6881s, grad.norm=0.67528659\n",
      "   120: 0 [  120/ 1327], train_loss/perplexity = 6.97569180/1070.2973633 secs/batch = 0.7020s, grad.norm=0.62222934\n",
      "   125: 0 [  125/ 1327], train_loss/perplexity = 6.82597017/921.4699707 secs/batch = 0.7582s, grad.norm=0.49266246\n",
      "   130: 0 [  130/ 1327], train_loss/perplexity = 6.80286121/900.4199219 secs/batch = 0.6209s, grad.norm=0.84913653\n",
      "   135: 0 [  135/ 1327], train_loss/perplexity = 7.08875608/1198.4161377 secs/batch = 0.6415s, grad.norm=0.74546415\n",
      "   140: 0 [  140/ 1327], train_loss/perplexity = 6.82491302/920.4963379 secs/batch = 0.6247s, grad.norm=0.54534155\n",
      "   145: 0 [  145/ 1327], train_loss/perplexity = 6.87556219/968.3195801 secs/batch = 0.6409s, grad.norm=0.52609998\n",
      "   150: 0 [  150/ 1327], train_loss/perplexity = 6.81129551/908.0464478 secs/batch = 0.6142s, grad.norm=0.49268419\n",
      "   155: 0 [  155/ 1327], train_loss/perplexity = 6.92443895/1016.8236084 secs/batch = 0.6665s, grad.norm=0.60222477\n",
      "   160: 0 [  160/ 1327], train_loss/perplexity = 6.65461683/776.3604126 secs/batch = 0.6376s, grad.norm=0.65084934\n",
      "   165: 0 [  165/ 1327], train_loss/perplexity = 6.71678782/826.1594849 secs/batch = 0.6115s, grad.norm=0.75118363\n",
      "   170: 0 [  170/ 1327], train_loss/perplexity = 6.73135376/838.2813110 secs/batch = 0.6231s, grad.norm=0.61091387\n",
      "   175: 0 [  175/ 1327], train_loss/perplexity = 6.73170280/838.5739746 secs/batch = 0.7457s, grad.norm=0.40383935\n",
      "   180: 0 [  180/ 1327], train_loss/perplexity = 6.74976206/853.8555908 secs/batch = 0.6153s, grad.norm=0.42251909\n",
      "   185: 0 [  185/ 1327], train_loss/perplexity = 6.80743170/904.5446777 secs/batch = 0.7246s, grad.norm=0.37356946\n",
      "   190: 0 [  190/ 1327], train_loss/perplexity = 6.68610334/801.1942139 secs/batch = 0.7319s, grad.norm=0.41467905\n",
      "   195: 0 [  195/ 1327], train_loss/perplexity = 6.57346869/715.8485718 secs/batch = 0.6272s, grad.norm=0.59284371\n",
      "   200: 0 [  200/ 1327], train_loss/perplexity = 6.89571905/988.0358887 secs/batch = 0.6262s, grad.norm=0.64784700\n",
      "   205: 0 [  205/ 1327], train_loss/perplexity = 6.65225506/774.5289917 secs/batch = 0.6360s, grad.norm=0.38393953\n",
      "   210: 0 [  210/ 1327], train_loss/perplexity = 6.56799126/711.9382935 secs/batch = 0.6562s, grad.norm=0.38780367\n",
      "   215: 0 [  215/ 1327], train_loss/perplexity = 6.72135735/829.9432373 secs/batch = 0.6653s, grad.norm=1.12076890\n",
      "   220: 0 [  220/ 1327], train_loss/perplexity = 6.75248480/856.1835938 secs/batch = 0.7123s, grad.norm=0.59490812\n",
      "   225: 0 [  225/ 1327], train_loss/perplexity = 6.85578108/949.3533325 secs/batch = 0.7618s, grad.norm=0.53800493\n",
      "   230: 0 [  230/ 1327], train_loss/perplexity = 6.72536850/833.2789917 secs/batch = 0.6999s, grad.norm=0.52116084\n",
      "   235: 0 [  235/ 1327], train_loss/perplexity = 6.66846561/787.1868286 secs/batch = 0.6747s, grad.norm=0.84789699\n",
      "   240: 0 [  240/ 1327], train_loss/perplexity = 6.51203775/673.1968384 secs/batch = 0.7517s, grad.norm=0.48734689\n",
      "   245: 0 [  245/ 1327], train_loss/perplexity = 6.69475842/808.1586914 secs/batch = 0.7476s, grad.norm=0.35250005\n",
      "   250: 0 [  250/ 1327], train_loss/perplexity = 6.60170412/736.3489380 secs/batch = 0.6728s, grad.norm=0.37860584\n",
      "   255: 0 [  255/ 1327], train_loss/perplexity = 6.72267294/831.0358276 secs/batch = 0.6635s, grad.norm=0.44237360\n",
      "   260: 0 [  260/ 1327], train_loss/perplexity = 6.84073019/935.1717529 secs/batch = 0.6876s, grad.norm=0.57214135\n",
      "   265: 0 [  265/ 1327], train_loss/perplexity = 6.74078608/846.2256470 secs/batch = 0.6409s, grad.norm=1.10792363\n",
      "   270: 0 [  270/ 1327], train_loss/perplexity = 6.72336483/831.6110229 secs/batch = 0.6539s, grad.norm=0.64300013\n",
      "   275: 0 [  275/ 1327], train_loss/perplexity = 6.96061230/1054.2789307 secs/batch = 0.7286s, grad.norm=0.75279832\n",
      "   280: 0 [  280/ 1327], train_loss/perplexity = 6.60378981/737.8863525 secs/batch = 0.6838s, grad.norm=0.50359350\n",
      "   285: 0 [  285/ 1327], train_loss/perplexity = 6.75042820/854.4245605 secs/batch = 0.6352s, grad.norm=0.41337788\n",
      "   290: 0 [  290/ 1327], train_loss/perplexity = 6.71104908/821.4319458 secs/batch = 0.6271s, grad.norm=0.52606958\n",
      "   295: 0 [  295/ 1327], train_loss/perplexity = 6.84343815/937.7075806 secs/batch = 0.6364s, grad.norm=0.62943327\n",
      "   300: 0 [  300/ 1327], train_loss/perplexity = 6.56643629/710.8321533 secs/batch = 0.6296s, grad.norm=0.51669270\n",
      "   305: 0 [  305/ 1327], train_loss/perplexity = 6.57182550/714.6732788 secs/batch = 0.6131s, grad.norm=0.57085568\n",
      "   310: 0 [  310/ 1327], train_loss/perplexity = 6.62938118/757.0135498 secs/batch = 0.7444s, grad.norm=0.44336358\n",
      "   315: 0 [  315/ 1327], train_loss/perplexity = 6.46200180/640.3416138 secs/batch = 0.7081s, grad.norm=0.43699113\n",
      "   320: 0 [  320/ 1327], train_loss/perplexity = 6.58768559/726.0984497 secs/batch = 0.6127s, grad.norm=0.41496888\n",
      "   325: 0 [  325/ 1327], train_loss/perplexity = 6.51283073/673.7308960 secs/batch = 0.7506s, grad.norm=0.58884919\n",
      "   330: 0 [  330/ 1327], train_loss/perplexity = 6.72959900/836.8116455 secs/batch = 0.6091s, grad.norm=0.53768754\n",
      "   335: 0 [  335/ 1327], train_loss/perplexity = 6.25192022/519.0084839 secs/batch = 0.6804s, grad.norm=0.49781412\n",
      "   340: 0 [  340/ 1327], train_loss/perplexity = 6.65370941/775.6562500 secs/batch = 0.6638s, grad.norm=0.42545697\n",
      "   345: 0 [  345/ 1327], train_loss/perplexity = 6.63417816/760.6536865 secs/batch = 0.6134s, grad.norm=0.30050287\n",
      "   350: 0 [  350/ 1327], train_loss/perplexity = 6.61276436/744.5383301 secs/batch = 0.6167s, grad.norm=0.48760349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   355: 0 [  355/ 1327], train_loss/perplexity = 6.71332455/823.3032227 secs/batch = 0.6169s, grad.norm=0.69381249\n",
      "   360: 0 [  360/ 1327], train_loss/perplexity = 6.76541567/867.3266602 secs/batch = 0.6074s, grad.norm=0.37876713\n",
      "   365: 0 [  365/ 1327], train_loss/perplexity = 6.68111181/797.2049561 secs/batch = 0.6213s, grad.norm=0.41956562\n",
      "   370: 0 [  370/ 1327], train_loss/perplexity = 6.79958057/897.4707642 secs/batch = 0.6053s, grad.norm=0.53257513\n",
      "   375: 0 [  375/ 1327], train_loss/perplexity = 6.51513338/675.2839966 secs/batch = 0.6098s, grad.norm=0.39017209\n",
      "   380: 0 [  380/ 1327], train_loss/perplexity = 6.68172073/797.6905518 secs/batch = 0.6667s, grad.norm=0.45927042\n",
      "   385: 0 [  385/ 1327], train_loss/perplexity = 6.84981155/943.7030640 secs/batch = 0.7422s, grad.norm=0.49560401\n",
      "   390: 0 [  390/ 1327], train_loss/perplexity = 6.61472511/745.9996338 secs/batch = 0.7155s, grad.norm=0.36983743\n",
      "   395: 0 [  395/ 1327], train_loss/perplexity = 6.82563639/921.1624146 secs/batch = 0.6244s, grad.norm=0.35590389\n",
      "   400: 0 [  400/ 1327], train_loss/perplexity = 6.53318214/687.5827026 secs/batch = 0.6519s, grad.norm=0.71191001\n",
      "   405: 0 [  405/ 1327], train_loss/perplexity = 6.81073093/907.5339355 secs/batch = 0.6483s, grad.norm=1.01563478\n",
      "   410: 0 [  410/ 1327], train_loss/perplexity = 6.69098902/805.1181641 secs/batch = 0.7663s, grad.norm=0.50716895\n",
      "   415: 0 [  415/ 1327], train_loss/perplexity = 6.53740025/690.4891357 secs/batch = 0.6476s, grad.norm=0.36395863\n",
      "   420: 0 [  420/ 1327], train_loss/perplexity = 6.57814121/719.2012329 secs/batch = 0.8103s, grad.norm=0.33201805\n",
      "   425: 0 [  425/ 1327], train_loss/perplexity = 6.75934982/862.0814819 secs/batch = 0.7583s, grad.norm=0.32939380\n",
      "   430: 0 [  430/ 1327], train_loss/perplexity = 6.69823503/810.9732056 secs/batch = 0.6610s, grad.norm=0.34307754\n",
      "   435: 0 [  435/ 1327], train_loss/perplexity = 6.70911980/819.8486938 secs/batch = 0.6746s, grad.norm=0.44343418\n",
      "   440: 0 [  440/ 1327], train_loss/perplexity = 6.69101763/805.1411743 secs/batch = 0.7150s, grad.norm=1.27989650\n",
      "   445: 0 [  445/ 1327], train_loss/perplexity = 6.65780210/778.8372803 secs/batch = 0.6186s, grad.norm=1.04665768\n",
      "   450: 0 [  450/ 1327], train_loss/perplexity = 6.60962200/742.2023926 secs/batch = 0.6079s, grad.norm=0.33897269\n",
      "   455: 0 [  455/ 1327], train_loss/perplexity = 6.38264942/591.4927368 secs/batch = 0.6138s, grad.norm=0.41161606\n",
      "   460: 0 [  460/ 1327], train_loss/perplexity = 6.64338160/767.6865845 secs/batch = 0.6082s, grad.norm=0.35875580\n",
      "   465: 0 [  465/ 1327], train_loss/perplexity = 6.57216167/714.9135742 secs/batch = 0.6033s, grad.norm=0.42553216\n",
      "   470: 0 [  470/ 1327], train_loss/perplexity = 6.74283695/847.9629517 secs/batch = 0.6102s, grad.norm=0.40618092\n",
      "   475: 0 [  475/ 1327], train_loss/perplexity = 6.74338293/848.4260254 secs/batch = 0.6128s, grad.norm=0.30411246\n",
      "   480: 0 [  480/ 1327], train_loss/perplexity = 6.60705233/740.2976685 secs/batch = 0.6155s, grad.norm=0.39951491\n",
      "   485: 0 [  485/ 1327], train_loss/perplexity = 6.65178967/774.1685791 secs/batch = 0.6030s, grad.norm=0.38874459\n",
      "   490: 0 [  490/ 1327], train_loss/perplexity = 6.66117239/781.4666138 secs/batch = 0.6071s, grad.norm=0.50563246\n",
      "   495: 0 [  495/ 1327], train_loss/perplexity = 6.38591528/593.4276123 secs/batch = 0.6138s, grad.norm=0.86413991\n",
      "   500: 0 [  500/ 1327], train_loss/perplexity = 6.71099901/821.3908081 secs/batch = 0.6105s, grad.norm=0.46379066\n",
      "   505: 0 [  505/ 1327], train_loss/perplexity = 6.62522793/753.8760376 secs/batch = 0.6156s, grad.norm=0.48537987\n",
      "   510: 0 [  510/ 1327], train_loss/perplexity = 6.66564035/784.9659424 secs/batch = 0.6303s, grad.norm=0.37651265\n",
      "   515: 0 [  515/ 1327], train_loss/perplexity = 6.53028440/685.5931396 secs/batch = 0.6109s, grad.norm=0.48852134\n",
      "   520: 0 [  520/ 1327], train_loss/perplexity = 6.69494057/808.3059082 secs/batch = 0.6118s, grad.norm=0.53369743\n",
      "   525: 0 [  525/ 1327], train_loss/perplexity = 6.55090475/699.8770752 secs/batch = 0.6123s, grad.norm=0.36500803\n",
      "   530: 0 [  530/ 1327], train_loss/perplexity = 6.59410381/730.7736816 secs/batch = 0.6127s, grad.norm=0.54736602\n",
      "   535: 0 [  535/ 1327], train_loss/perplexity = 6.65761852/778.6942749 secs/batch = 0.6076s, grad.norm=0.29889843\n",
      "   540: 0 [  540/ 1327], train_loss/perplexity = 6.69933128/811.8627319 secs/batch = 0.6114s, grad.norm=0.31701806\n",
      "   545: 0 [  545/ 1327], train_loss/perplexity = 6.71172094/821.9840088 secs/batch = 0.6152s, grad.norm=0.33393323\n",
      "   550: 0 [  550/ 1327], train_loss/perplexity = 6.72972536/836.9173584 secs/batch = 0.6370s, grad.norm=0.44242233\n",
      "   555: 0 [  555/ 1327], train_loss/perplexity = 6.60445786/738.3794556 secs/batch = 0.6129s, grad.norm=0.43655857\n",
      "   560: 0 [  560/ 1327], train_loss/perplexity = 6.72389174/832.0493164 secs/batch = 0.6081s, grad.norm=0.36178634\n",
      "   565: 0 [  565/ 1327], train_loss/perplexity = 6.65120268/773.7142944 secs/batch = 0.6104s, grad.norm=0.39521691\n",
      "   570: 0 [  570/ 1327], train_loss/perplexity = 6.56037092/706.5336914 secs/batch = 0.6231s, grad.norm=0.39196506\n",
      "   575: 0 [  575/ 1327], train_loss/perplexity = 6.60274363/737.1148071 secs/batch = 0.6116s, grad.norm=0.49751520\n",
      "   580: 0 [  580/ 1327], train_loss/perplexity = 6.63731289/763.0418701 secs/batch = 0.6174s, grad.norm=0.44588283\n",
      "   585: 0 [  585/ 1327], train_loss/perplexity = 6.54628849/696.6537476 secs/batch = 0.6173s, grad.norm=0.46997973\n",
      "   590: 0 [  590/ 1327], train_loss/perplexity = 6.71265745/822.7541504 secs/batch = 0.6155s, grad.norm=0.37280500\n",
      "   595: 0 [  595/ 1327], train_loss/perplexity = 6.64884615/771.8931885 secs/batch = 0.6135s, grad.norm=0.34054041\n",
      "   600: 0 [  600/ 1327], train_loss/perplexity = 6.72213221/830.5866089 secs/batch = 0.6390s, grad.norm=0.37009636\n",
      "   605: 0 [  605/ 1327], train_loss/perplexity = 6.67614746/793.2571411 secs/batch = 0.6156s, grad.norm=0.47266573\n",
      "   610: 0 [  610/ 1327], train_loss/perplexity = 6.81192732/908.6203003 secs/batch = 0.6120s, grad.norm=0.48268792\n",
      "   615: 0 [  615/ 1327], train_loss/perplexity = 6.34602833/570.2234497 secs/batch = 0.6164s, grad.norm=0.36026812\n",
      "   620: 0 [  620/ 1327], train_loss/perplexity = 6.55824852/705.0357666 secs/batch = 0.6098s, grad.norm=0.33320782\n",
      "   625: 0 [  625/ 1327], train_loss/perplexity = 6.68557262/800.7691040 secs/batch = 0.6080s, grad.norm=0.29411897\n",
      "   630: 0 [  630/ 1327], train_loss/perplexity = 6.68824482/802.9117432 secs/batch = 0.6095s, grad.norm=0.32870689\n",
      "   635: 0 [  635/ 1327], train_loss/perplexity = 6.63280821/759.6123047 secs/batch = 0.6339s, grad.norm=0.58769608\n",
      "   640: 0 [  640/ 1327], train_loss/perplexity = 6.55298328/701.3333130 secs/batch = 0.6123s, grad.norm=0.70644194\n",
      "   645: 0 [  645/ 1327], train_loss/perplexity = 6.64619493/769.8494263 secs/batch = 0.6164s, grad.norm=0.28808278\n",
      "   650: 0 [  650/ 1327], train_loss/perplexity = 6.47418404/648.1901245 secs/batch = 0.6318s, grad.norm=0.48654738\n",
      "   655: 0 [  655/ 1327], train_loss/perplexity = 6.52571201/682.4655151 secs/batch = 0.6159s, grad.norm=0.45555690\n",
      "   660: 0 [  660/ 1327], train_loss/perplexity = 6.57202578/714.8164062 secs/batch = 0.6141s, grad.norm=0.32803902\n",
      "   665: 0 [  665/ 1327], train_loss/perplexity = 6.68092585/797.0567017 secs/batch = 0.6074s, grad.norm=0.38437364\n",
      "   670: 0 [  670/ 1327], train_loss/perplexity = 6.61954784/749.6060791 secs/batch = 0.6267s, grad.norm=0.36155730\n",
      "   675: 0 [  675/ 1327], train_loss/perplexity = 6.42987251/620.0949097 secs/batch = 0.6160s, grad.norm=0.34193605\n",
      "   680: 0 [  680/ 1327], train_loss/perplexity = 6.82602692/921.5222168 secs/batch = 0.6130s, grad.norm=0.53370160\n",
      "   685: 0 [  685/ 1327], train_loss/perplexity = 6.73707676/843.0925903 secs/batch = 0.6096s, grad.norm=0.57524627\n",
      "   690: 0 [  690/ 1327], train_loss/perplexity = 6.69433165/807.8138428 secs/batch = 0.6122s, grad.norm=0.43191871\n",
      "   695: 0 [  695/ 1327], train_loss/perplexity = 6.56717443/711.3569946 secs/batch = 0.6056s, grad.norm=0.37884170\n",
      "   700: 0 [  700/ 1327], train_loss/perplexity = 6.62664318/754.9437256 secs/batch = 0.6092s, grad.norm=0.28740805\n",
      "   705: 0 [  705/ 1327], train_loss/perplexity = 6.45990992/639.0034790 secs/batch = 0.6147s, grad.norm=0.41408908\n",
      "   710: 0 [  710/ 1327], train_loss/perplexity = 6.57272530/715.3166504 secs/batch = 0.6164s, grad.norm=0.28479242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   715: 0 [  715/ 1327], train_loss/perplexity = 6.61808443/748.5098877 secs/batch = 0.6127s, grad.norm=0.37769234\n",
      "   720: 0 [  720/ 1327], train_loss/perplexity = 6.63069248/758.0068970 secs/batch = 0.6114s, grad.norm=0.47281492\n",
      "   725: 0 [  725/ 1327], train_loss/perplexity = 6.42002869/614.0207520 secs/batch = 0.6069s, grad.norm=0.58388084\n",
      "   730: 0 [  730/ 1327], train_loss/perplexity = 6.45088720/633.2638550 secs/batch = 0.6132s, grad.norm=0.32802877\n",
      "   735: 0 [  735/ 1327], train_loss/perplexity = 6.58612442/724.9657593 secs/batch = 0.6158s, grad.norm=0.30253953\n",
      "   740: 0 [  740/ 1327], train_loss/perplexity = 6.46294165/640.9437256 secs/batch = 0.6105s, grad.norm=0.44014961\n",
      "   745: 0 [  745/ 1327], train_loss/perplexity = 6.61271048/744.4982300 secs/batch = 0.6219s, grad.norm=0.40359932\n",
      "   750: 0 [  750/ 1327], train_loss/perplexity = 6.45377731/635.0967407 secs/batch = 0.6074s, grad.norm=0.40300721\n",
      "   755: 0 [  755/ 1327], train_loss/perplexity = 6.49482632/661.7092896 secs/batch = 0.6075s, grad.norm=0.29278630\n",
      "   760: 0 [  760/ 1327], train_loss/perplexity = 6.43341875/622.2977905 secs/batch = 0.6108s, grad.norm=0.45155582\n",
      "   765: 0 [  765/ 1327], train_loss/perplexity = 6.45234108/634.1852417 secs/batch = 0.6147s, grad.norm=0.47039267\n",
      "   770: 0 [  770/ 1327], train_loss/perplexity = 6.47199917/646.7754517 secs/batch = 0.6104s, grad.norm=0.32594115\n",
      "   775: 0 [  775/ 1327], train_loss/perplexity = 6.54664564/696.9025879 secs/batch = 0.6111s, grad.norm=0.34560463\n",
      "   780: 0 [  780/ 1327], train_loss/perplexity = 6.66062832/781.0415039 secs/batch = 0.6048s, grad.norm=0.34971055\n",
      "   785: 0 [  785/ 1327], train_loss/perplexity = 6.53420115/688.2837524 secs/batch = 0.6147s, grad.norm=0.35240889\n",
      "   790: 0 [  790/ 1327], train_loss/perplexity = 6.40630150/605.6495361 secs/batch = 0.6220s, grad.norm=0.42042142\n",
      "   795: 0 [  795/ 1327], train_loss/perplexity = 6.63362885/760.2359619 secs/batch = 0.6548s, grad.norm=0.33936566\n",
      "   800: 0 [  800/ 1327], train_loss/perplexity = 6.59504461/731.4615479 secs/batch = 0.6168s, grad.norm=0.35632014\n",
      "   805: 0 [  805/ 1327], train_loss/perplexity = 6.69168854/805.6815186 secs/batch = 0.6083s, grad.norm=0.39738658\n",
      "   810: 0 [  810/ 1327], train_loss/perplexity = 6.65315104/775.2232666 secs/batch = 0.6160s, grad.norm=0.41196525\n",
      "   815: 0 [  815/ 1327], train_loss/perplexity = 6.45359850/634.9831543 secs/batch = 0.6111s, grad.norm=0.40973678\n",
      "   820: 0 [  820/ 1327], train_loss/perplexity = 6.31539154/553.0185547 secs/batch = 0.6135s, grad.norm=0.42038026\n",
      "   825: 0 [  825/ 1327], train_loss/perplexity = 6.45130110/633.5260620 secs/batch = 0.6205s, grad.norm=0.30288267\n",
      "   830: 0 [  830/ 1327], train_loss/perplexity = 6.32965040/560.9604492 secs/batch = 0.6151s, grad.norm=0.36322713\n",
      "   835: 0 [  835/ 1327], train_loss/perplexity = 6.55945873/705.8895264 secs/batch = 0.6132s, grad.norm=0.31184888\n",
      "   840: 0 [  840/ 1327], train_loss/perplexity = 6.63920975/764.4906006 secs/batch = 0.6155s, grad.norm=0.27225459\n",
      "   845: 0 [  845/ 1327], train_loss/perplexity = 6.57305431/715.5520020 secs/batch = 0.6139s, grad.norm=0.33068690\n",
      "   850: 0 [  850/ 1327], train_loss/perplexity = 6.51165724/672.9407349 secs/batch = 0.6199s, grad.norm=0.38266096\n",
      "   855: 0 [  855/ 1327], train_loss/perplexity = 6.54418898/695.1926270 secs/batch = 0.6139s, grad.norm=0.33964571\n",
      "   860: 0 [  860/ 1327], train_loss/perplexity = 6.35746574/576.7827759 secs/batch = 0.6134s, grad.norm=0.35759795\n",
      "   865: 0 [  865/ 1327], train_loss/perplexity = 6.66818428/786.9653931 secs/batch = 0.6134s, grad.norm=0.34487355\n",
      "   870: 0 [  870/ 1327], train_loss/perplexity = 6.72331047/831.5658569 secs/batch = 0.6080s, grad.norm=0.39865589\n",
      "   875: 0 [  875/ 1327], train_loss/perplexity = 6.39354277/597.9713135 secs/batch = 0.6130s, grad.norm=0.27398121\n",
      "   880: 0 [  880/ 1327], train_loss/perplexity = 6.59999371/735.0905762 secs/batch = 0.6164s, grad.norm=0.36896011\n",
      "   885: 0 [  885/ 1327], train_loss/perplexity = 6.45963192/638.8258667 secs/batch = 0.6190s, grad.norm=0.37792104\n",
      "   890: 0 [  890/ 1327], train_loss/perplexity = 6.61414957/745.5704346 secs/batch = 0.6133s, grad.norm=0.48147005\n",
      "   895: 0 [  895/ 1327], train_loss/perplexity = 6.60660553/739.9669800 secs/batch = 0.6064s, grad.norm=0.38151044\n",
      "   900: 0 [  900/ 1327], train_loss/perplexity = 6.57544994/717.2683105 secs/batch = 0.6099s, grad.norm=0.43445763\n",
      "   905: 0 [  905/ 1327], train_loss/perplexity = 6.47826433/650.8403320 secs/batch = 0.6214s, grad.norm=0.32880095\n",
      "   910: 0 [  910/ 1327], train_loss/perplexity = 6.46481323/642.1444092 secs/batch = 0.6155s, grad.norm=0.41453293\n",
      "   915: 0 [  915/ 1327], train_loss/perplexity = 6.73882389/844.5668335 secs/batch = 0.6175s, grad.norm=0.38301426\n",
      "   920: 0 [  920/ 1327], train_loss/perplexity = 6.72039557/829.1454468 secs/batch = 0.6198s, grad.norm=0.42121395\n",
      "   925: 0 [  925/ 1327], train_loss/perplexity = 6.54186249/693.5771484 secs/batch = 0.6151s, grad.norm=0.40449041\n",
      "   930: 0 [  930/ 1327], train_loss/perplexity = 6.47188759/646.7033081 secs/batch = 0.6181s, grad.norm=0.49581873\n",
      "   935: 0 [  935/ 1327], train_loss/perplexity = 6.57547712/717.2877808 secs/batch = 0.6073s, grad.norm=0.27355421\n",
      "   940: 0 [  940/ 1327], train_loss/perplexity = 6.54324150/694.5342407 secs/batch = 0.6284s, grad.norm=0.41227371\n",
      "   945: 0 [  945/ 1327], train_loss/perplexity = 6.64287806/767.3001709 secs/batch = 0.6102s, grad.norm=0.29986596\n",
      "   950: 0 [  950/ 1327], train_loss/perplexity = 6.50056410/665.5169678 secs/batch = 0.6137s, grad.norm=0.31074357\n",
      "   955: 0 [  955/ 1327], train_loss/perplexity = 6.63936090/764.6062012 secs/batch = 0.6162s, grad.norm=0.32430780\n",
      "   960: 0 [  960/ 1327], train_loss/perplexity = 6.70238829/814.3483887 secs/batch = 0.6145s, grad.norm=0.34551591\n",
      "   965: 0 [  965/ 1327], train_loss/perplexity = 6.58212185/722.0698242 secs/batch = 0.6151s, grad.norm=0.38146895\n",
      "   970: 0 [  970/ 1327], train_loss/perplexity = 6.60412741/738.1354980 secs/batch = 0.6117s, grad.norm=0.38387913\n",
      "   975: 0 [  975/ 1327], train_loss/perplexity = 6.55592966/703.4027710 secs/batch = 0.6160s, grad.norm=0.34121606\n",
      "   980: 0 [  980/ 1327], train_loss/perplexity = 6.46106243/639.7403564 secs/batch = 0.6144s, grad.norm=0.26228395\n",
      "   985: 0 [  985/ 1327], train_loss/perplexity = 6.64211273/766.7131348 secs/batch = 0.6177s, grad.norm=0.32516760\n",
      "   990: 0 [  990/ 1327], train_loss/perplexity = 6.69764805/810.4973145 secs/batch = 0.6633s, grad.norm=0.33944806\n",
      "   995: 0 [  995/ 1327], train_loss/perplexity = 6.67655563/793.5809937 secs/batch = 0.6103s, grad.norm=0.35018364\n",
      "  1000: 0 [ 1000/ 1327], train_loss/perplexity = 6.40597439/605.4514771 secs/batch = 0.6078s, grad.norm=0.38443279\n",
      "  1005: 0 [ 1005/ 1327], train_loss/perplexity = 6.64635468/769.9724121 secs/batch = 0.6097s, grad.norm=0.31523257\n",
      "  1010: 0 [ 1010/ 1327], train_loss/perplexity = 6.38616991/593.5787354 secs/batch = 0.6115s, grad.norm=0.34491912\n",
      "  1015: 0 [ 1015/ 1327], train_loss/perplexity = 6.59540462/731.7249146 secs/batch = 0.6100s, grad.norm=0.35390356\n",
      "  1020: 0 [ 1020/ 1327], train_loss/perplexity = 6.69759083/810.4509277 secs/batch = 0.6140s, grad.norm=0.38598594\n",
      "  1025: 0 [ 1025/ 1327], train_loss/perplexity = 6.56398153/709.0893555 secs/batch = 0.6128s, grad.norm=0.26462770\n",
      "  1030: 0 [ 1030/ 1327], train_loss/perplexity = 6.54333210/694.5971680 secs/batch = 0.6152s, grad.norm=0.27844837\n",
      "  1035: 0 [ 1035/ 1327], train_loss/perplexity = 6.50845432/670.7888184 secs/batch = 0.6137s, grad.norm=0.40283704\n",
      "  1040: 0 [ 1040/ 1327], train_loss/perplexity = 6.49800730/663.8175049 secs/batch = 0.6255s, grad.norm=0.24341343\n",
      "  1045: 0 [ 1045/ 1327], train_loss/perplexity = 6.32758808/559.8047485 secs/batch = 0.6162s, grad.norm=0.30590719\n",
      "  1050: 0 [ 1050/ 1327], train_loss/perplexity = 6.55809450/704.9271851 secs/batch = 0.6164s, grad.norm=0.31620628\n",
      "  1055: 0 [ 1055/ 1327], train_loss/perplexity = 6.72048712/829.2213135 secs/batch = 0.6148s, grad.norm=0.42490834\n",
      "  1060: 0 [ 1060/ 1327], train_loss/perplexity = 6.45382881/635.1294556 secs/batch = 0.6107s, grad.norm=0.39920381\n",
      "  1065: 0 [ 1065/ 1327], train_loss/perplexity = 6.51593208/675.8236084 secs/batch = 0.6097s, grad.norm=0.32332361\n",
      "  1070: 0 [ 1070/ 1327], train_loss/perplexity = 6.67259693/790.4456787 secs/batch = 0.6166s, grad.norm=0.32894933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1075: 0 [ 1075/ 1327], train_loss/perplexity = 6.55407906/702.1022339 secs/batch = 0.6192s, grad.norm=0.31029329\n",
      "  1080: 0 [ 1080/ 1327], train_loss/perplexity = 6.46852350/644.5313721 secs/batch = 0.6274s, grad.norm=0.31953108\n",
      "  1085: 0 [ 1085/ 1327], train_loss/perplexity = 6.51386738/674.4296265 secs/batch = 0.6151s, grad.norm=0.41981539\n",
      "  1090: 0 [ 1090/ 1327], train_loss/perplexity = 6.59781122/733.4879761 secs/batch = 0.6121s, grad.norm=0.25253704\n",
      "  1095: 0 [ 1095/ 1327], train_loss/perplexity = 6.57855415/719.4982910 secs/batch = 0.6093s, grad.norm=0.43054444\n",
      "  1100: 0 [ 1100/ 1327], train_loss/perplexity = 6.64365864/767.8993530 secs/batch = 0.6096s, grad.norm=0.43237484\n",
      "  1105: 0 [ 1105/ 1327], train_loss/perplexity = 6.45963526/638.8280029 secs/batch = 0.6088s, grad.norm=0.38816988\n",
      "  1110: 0 [ 1110/ 1327], train_loss/perplexity = 6.77248955/873.4837646 secs/batch = 0.6179s, grad.norm=0.32356152\n",
      "  1115: 0 [ 1115/ 1327], train_loss/perplexity = 6.52636671/682.9124756 secs/batch = 0.6088s, grad.norm=0.35647312\n",
      "  1120: 0 [ 1120/ 1327], train_loss/perplexity = 6.51376343/674.3595581 secs/batch = 0.6185s, grad.norm=0.33303979\n",
      "  1125: 0 [ 1125/ 1327], train_loss/perplexity = 6.71207666/822.2764893 secs/batch = 0.6092s, grad.norm=0.30854025\n",
      "  1130: 0 [ 1130/ 1327], train_loss/perplexity = 6.54255629/694.0585327 secs/batch = 0.6157s, grad.norm=0.26461050\n",
      "  1135: 0 [ 1135/ 1327], train_loss/perplexity = 6.57284689/715.4036255 secs/batch = 0.6146s, grad.norm=0.31139889\n",
      "  1140: 0 [ 1140/ 1327], train_loss/perplexity = 6.64305019/767.4322510 secs/batch = 0.6169s, grad.norm=0.33782855\n",
      "  1145: 0 [ 1145/ 1327], train_loss/perplexity = 6.49875975/664.3171997 secs/batch = 0.6146s, grad.norm=0.28545973\n",
      "  1150: 0 [ 1150/ 1327], train_loss/perplexity = 6.45712900/637.2289429 secs/batch = 0.6170s, grad.norm=0.31116351\n",
      "  1155: 0 [ 1155/ 1327], train_loss/perplexity = 6.55158043/700.3501587 secs/batch = 0.6229s, grad.norm=0.26158440\n",
      "  1160: 0 [ 1160/ 1327], train_loss/perplexity = 6.56398153/709.0893555 secs/batch = 0.6147s, grad.norm=0.35489264\n",
      "  1165: 0 [ 1165/ 1327], train_loss/perplexity = 6.67974377/796.1151123 secs/batch = 0.6188s, grad.norm=0.34577259\n",
      "  1170: 0 [ 1170/ 1327], train_loss/perplexity = 6.55021715/699.3960571 secs/batch = 0.6081s, grad.norm=0.41824475\n",
      "  1175: 0 [ 1175/ 1327], train_loss/perplexity = 6.40943289/607.5490112 secs/batch = 0.6115s, grad.norm=0.37175515\n",
      "  1180: 0 [ 1180/ 1327], train_loss/perplexity = 6.32335997/557.4428101 secs/batch = 0.6128s, grad.norm=0.29685813\n",
      "  1185: 0 [ 1185/ 1327], train_loss/perplexity = 6.56759548/711.6566162 secs/batch = 0.6271s, grad.norm=0.30625361\n",
      "  1190: 0 [ 1190/ 1327], train_loss/perplexity = 6.60126591/736.0263672 secs/batch = 0.6235s, grad.norm=0.27992740\n",
      "  1195: 0 [ 1195/ 1327], train_loss/perplexity = 6.48564100/655.6591187 secs/batch = 0.6131s, grad.norm=0.43100643\n",
      "  1200: 0 [ 1200/ 1327], train_loss/perplexity = 6.34721279/570.8992920 secs/batch = 0.6091s, grad.norm=0.33025467\n",
      "  1205: 0 [ 1205/ 1327], train_loss/perplexity = 6.56493950/709.7689819 secs/batch = 0.6065s, grad.norm=0.36430582\n",
      "  1210: 0 [ 1210/ 1327], train_loss/perplexity = 6.41034269/608.1020508 secs/batch = 0.6214s, grad.norm=0.36716551\n",
      "  1215: 0 [ 1215/ 1327], train_loss/perplexity = 6.36484957/581.0574341 secs/batch = 0.6109s, grad.norm=0.29871178\n",
      "  1220: 0 [ 1220/ 1327], train_loss/perplexity = 6.43426418/622.8241577 secs/batch = 0.6175s, grad.norm=0.33790660\n",
      "  1225: 0 [ 1225/ 1327], train_loss/perplexity = 6.42236900/615.4594116 secs/batch = 0.6066s, grad.norm=0.27875501\n",
      "  1230: 0 [ 1230/ 1327], train_loss/perplexity = 6.53741407/690.4987183 secs/batch = 0.6103s, grad.norm=0.30401400\n",
      "  1235: 0 [ 1235/ 1327], train_loss/perplexity = 6.52737188/683.5992432 secs/batch = 0.6587s, grad.norm=0.27703652\n",
      "  1240: 0 [ 1240/ 1327], train_loss/perplexity = 6.54301691/694.3782959 secs/batch = 0.6180s, grad.norm=0.26038268\n",
      "  1245: 0 [ 1245/ 1327], train_loss/perplexity = 6.39950943/601.5498657 secs/batch = 0.6132s, grad.norm=0.29306099\n",
      "  1250: 0 [ 1250/ 1327], train_loss/perplexity = 6.54686451/697.0551147 secs/batch = 0.6122s, grad.norm=0.36776671\n",
      "  1255: 0 [ 1255/ 1327], train_loss/perplexity = 6.52955341/685.0921631 secs/batch = 0.6314s, grad.norm=0.50926936\n",
      "  1260: 0 [ 1260/ 1327], train_loss/perplexity = 6.60811615/741.0856323 secs/batch = 0.6067s, grad.norm=0.33805150\n",
      "  1265: 0 [ 1265/ 1327], train_loss/perplexity = 6.48145008/652.9170532 secs/batch = 0.6127s, grad.norm=0.28403389\n",
      "  1270: 0 [ 1270/ 1327], train_loss/perplexity = 6.51300859/673.8507080 secs/batch = 0.6182s, grad.norm=0.31172431\n",
      "  1275: 0 [ 1275/ 1327], train_loss/perplexity = 6.70873690/819.5348511 secs/batch = 0.6107s, grad.norm=0.38180289\n",
      "  1280: 0 [ 1280/ 1327], train_loss/perplexity = 6.49746799/663.4595947 secs/batch = 0.6166s, grad.norm=0.37676373\n",
      "  1285: 0 [ 1285/ 1327], train_loss/perplexity = 6.40717793/606.1806030 secs/batch = 0.6137s, grad.norm=0.42190930\n",
      "  1290: 0 [ 1290/ 1327], train_loss/perplexity = 6.47194529/646.7406006 secs/batch = 0.6132s, grad.norm=0.28923705\n",
      "  1295: 0 [ 1295/ 1327], train_loss/perplexity = 6.63072062/758.0281982 secs/batch = 0.6094s, grad.norm=0.28105387\n",
      "  1300: 0 [ 1300/ 1327], train_loss/perplexity = 6.56989384/713.2941284 secs/batch = 0.6162s, grad.norm=0.35149676\n",
      "  1305: 0 [ 1305/ 1327], train_loss/perplexity = 6.69431496/807.8003540 secs/batch = 0.6177s, grad.norm=0.36374861\n",
      "  1310: 0 [ 1310/ 1327], train_loss/perplexity = 6.71510458/824.7700195 secs/batch = 0.6163s, grad.norm=0.29369846\n",
      "  1315: 0 [ 1315/ 1327], train_loss/perplexity = 6.66482067/784.3228149 secs/batch = 0.6159s, grad.norm=0.25170955\n",
      "  1320: 0 [ 1320/ 1327], train_loss/perplexity = 6.69368029/807.2878418 secs/batch = 0.6190s, grad.norm=0.27021241\n",
      "  1325: 0 [ 1325/ 1327], train_loss/perplexity = 6.60124111/736.0081177 secs/batch = 0.6175s, grad.norm=0.32070348\n",
      "Epoch training time: 839.8915412425995\n",
      "Saved char model cv/epoch000_6.5724.model\n",
      "  1332: 1 [    5/ 1327], train_loss/perplexity = 6.72998810/837.1373291 secs/batch = 0.6496s, grad.norm=0.35979894\n",
      "  1337: 1 [   10/ 1327], train_loss/perplexity = 6.39668608/599.8538818 secs/batch = 0.6152s, grad.norm=0.39804158\n",
      "  1342: 1 [   15/ 1327], train_loss/perplexity = 6.39068079/596.2623901 secs/batch = 0.6055s, grad.norm=0.27387142\n",
      "  1347: 1 [   20/ 1327], train_loss/perplexity = 6.80015755/897.9887695 secs/batch = 0.6156s, grad.norm=0.51729798\n",
      "  1352: 1 [   25/ 1327], train_loss/perplexity = 6.64273643/767.1914673 secs/batch = 0.6078s, grad.norm=0.26836053\n",
      "  1357: 1 [   30/ 1327], train_loss/perplexity = 6.46606636/642.9495850 secs/batch = 0.6152s, grad.norm=0.30742362\n",
      "  1362: 1 [   35/ 1327], train_loss/perplexity = 6.44747639/631.1076050 secs/batch = 0.6281s, grad.norm=0.28765997\n",
      "  1367: 1 [   40/ 1327], train_loss/perplexity = 6.52325201/680.7886963 secs/batch = 0.6120s, grad.norm=0.31104833\n",
      "  1372: 1 [   45/ 1327], train_loss/perplexity = 6.32887411/560.5251465 secs/batch = 0.6103s, grad.norm=0.30607745\n",
      "  1377: 1 [   50/ 1327], train_loss/perplexity = 6.59685135/732.7842407 secs/batch = 0.6099s, grad.norm=0.36700767\n",
      "  1382: 1 [   55/ 1327], train_loss/perplexity = 6.53919029/691.7262573 secs/batch = 0.6078s, grad.norm=0.34205553\n",
      "  1387: 1 [   60/ 1327], train_loss/perplexity = 6.53513241/688.9249878 secs/batch = 0.6151s, grad.norm=0.33458945\n",
      "  1392: 1 [   65/ 1327], train_loss/perplexity = 6.43206215/621.4541626 secs/batch = 0.6075s, grad.norm=0.45587721\n",
      "  1397: 1 [   70/ 1327], train_loss/perplexity = 6.32023239/555.7020874 secs/batch = 0.6140s, grad.norm=0.40975025\n",
      "  1402: 1 [   75/ 1327], train_loss/perplexity = 6.41275883/609.5730591 secs/batch = 0.6179s, grad.norm=0.39913791\n",
      "  1407: 1 [   80/ 1327], train_loss/perplexity = 6.56413317/709.1968994 secs/batch = 0.6172s, grad.norm=0.55336988\n",
      "  1412: 1 [   85/ 1327], train_loss/perplexity = 6.80984259/906.7280884 secs/batch = 0.6182s, grad.norm=1.69477630\n",
      "  1417: 1 [   90/ 1327], train_loss/perplexity = 6.59341192/730.2682495 secs/batch = 0.6164s, grad.norm=0.28320763\n",
      "  1422: 1 [   95/ 1327], train_loss/perplexity = 6.48889351/657.7951050 secs/batch = 0.6091s, grad.norm=0.59671509\n",
      "  1427: 1 [  100/ 1327], train_loss/perplexity = 6.58952761/727.4371338 secs/batch = 0.6050s, grad.norm=0.35817781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1432: 1 [  105/ 1327], train_loss/perplexity = 6.63615036/762.1553345 secs/batch = 0.6131s, grad.norm=0.32633582\n",
      "  1437: 1 [  110/ 1327], train_loss/perplexity = 6.44774961/631.2800903 secs/batch = 0.6181s, grad.norm=0.34873429\n",
      "  1442: 1 [  115/ 1327], train_loss/perplexity = 6.27517796/531.2208862 secs/batch = 0.6096s, grad.norm=0.40628150\n",
      "  1447: 1 [  120/ 1327], train_loss/perplexity = 6.55820513/705.0051880 secs/batch = 0.6134s, grad.norm=0.38063699\n",
      "  1452: 1 [  125/ 1327], train_loss/perplexity = 6.64219093/766.7731323 secs/batch = 0.6144s, grad.norm=0.37956536\n",
      "  1457: 1 [  130/ 1327], train_loss/perplexity = 6.61789131/748.3653564 secs/batch = 0.6186s, grad.norm=0.48556170\n",
      "  1462: 1 [  135/ 1327], train_loss/perplexity = 6.49723959/663.3081055 secs/batch = 0.6116s, grad.norm=0.40482077\n",
      "  1467: 1 [  140/ 1327], train_loss/perplexity = 6.59050989/728.1520386 secs/batch = 0.6161s, grad.norm=0.29561308\n",
      "  1472: 1 [  145/ 1327], train_loss/perplexity = 6.67261648/790.4611206 secs/batch = 0.6145s, grad.norm=0.34490710\n",
      "  1477: 1 [  150/ 1327], train_loss/perplexity = 6.50235271/666.7083740 secs/batch = 0.6192s, grad.norm=0.34190646\n",
      "  1482: 1 [  155/ 1327], train_loss/perplexity = 6.58887625/726.9635010 secs/batch = 0.6144s, grad.norm=0.37288737\n",
      "  1487: 1 [  160/ 1327], train_loss/perplexity = 6.39423418/598.3848877 secs/batch = 0.6130s, grad.norm=0.54085451\n",
      "  1492: 1 [  165/ 1327], train_loss/perplexity = 6.53984880/692.1819458 secs/batch = 0.6160s, grad.norm=0.51217288\n",
      "  1497: 1 [  170/ 1327], train_loss/perplexity = 6.58879375/726.9035034 secs/batch = 0.6215s, grad.norm=0.59667784\n",
      "  1502: 1 [  175/ 1327], train_loss/perplexity = 6.60624170/739.6977539 secs/batch = 0.6150s, grad.norm=0.26999527\n",
      "  1507: 1 [  180/ 1327], train_loss/perplexity = 6.58566427/724.6322632 secs/batch = 0.6102s, grad.norm=0.39101073\n",
      "  1512: 1 [  185/ 1327], train_loss/perplexity = 6.66485405/784.3489990 secs/batch = 0.6093s, grad.norm=0.33733463\n",
      "  1517: 1 [  190/ 1327], train_loss/perplexity = 6.52128744/679.4525757 secs/batch = 0.6095s, grad.norm=0.34069756\n",
      "  1522: 1 [  195/ 1327], train_loss/perplexity = 6.36415052/580.6513672 secs/batch = 0.6145s, grad.norm=0.29423100\n",
      "  1527: 1 [  200/ 1327], train_loss/perplexity = 6.58311224/722.7853394 secs/batch = 0.6225s, grad.norm=0.34721908\n",
      "  1532: 1 [  205/ 1327], train_loss/perplexity = 6.50142527/666.0903320 secs/batch = 0.6132s, grad.norm=0.35129267\n",
      "  1537: 1 [  210/ 1327], train_loss/perplexity = 6.41120768/608.6282349 secs/batch = 0.6074s, grad.norm=0.31254455\n",
      "  1542: 1 [  215/ 1327], train_loss/perplexity = 6.47054195/645.8336182 secs/batch = 0.6208s, grad.norm=0.30297646\n",
      "  1547: 1 [  220/ 1327], train_loss/perplexity = 6.53801394/690.9130249 secs/batch = 0.6195s, grad.norm=0.37966934\n",
      "  1552: 1 [  225/ 1327], train_loss/perplexity = 6.69962549/812.1016235 secs/batch = 0.6134s, grad.norm=0.37103179\n",
      "  1557: 1 [  230/ 1327], train_loss/perplexity = 6.53518200/688.9591675 secs/batch = 0.6116s, grad.norm=0.30985212\n",
      "  1562: 1 [  235/ 1327], train_loss/perplexity = 6.49737930/663.4007568 secs/batch = 0.6167s, grad.norm=0.50662196\n",
      "  1567: 1 [  240/ 1327], train_loss/perplexity = 6.30103731/545.1370850 secs/batch = 0.6258s, grad.norm=0.35503909\n",
      "  1572: 1 [  245/ 1327], train_loss/perplexity = 6.57195568/714.7663574 secs/batch = 0.6276s, grad.norm=0.45746055\n",
      "  1577: 1 [  250/ 1327], train_loss/perplexity = 6.40507460/604.9069214 secs/batch = 0.6456s, grad.norm=0.31006414\n",
      "  1582: 1 [  255/ 1327], train_loss/perplexity = 6.51857758/677.6138306 secs/batch = 0.6064s, grad.norm=0.34275052\n",
      "  1587: 1 [  260/ 1327], train_loss/perplexity = 6.69576168/808.9698486 secs/batch = 0.6125s, grad.norm=0.31798235\n",
      "  1592: 1 [  265/ 1327], train_loss/perplexity = 6.50418472/667.9309082 secs/batch = 0.6072s, grad.norm=0.30057019\n",
      "  1597: 1 [  270/ 1327], train_loss/perplexity = 6.65294361/775.0624390 secs/batch = 0.6265s, grad.norm=0.90681720\n",
      "  1602: 1 [  275/ 1327], train_loss/perplexity = 6.79101610/889.8172607 secs/batch = 0.6205s, grad.norm=0.50133944\n",
      "  1607: 1 [  280/ 1327], train_loss/perplexity = 6.42136383/614.8410645 secs/batch = 0.6049s, grad.norm=0.32130581\n",
      "  1612: 1 [  285/ 1327], train_loss/perplexity = 6.60242558/736.8803711 secs/batch = 0.6107s, grad.norm=0.34482014\n",
      "  1617: 1 [  290/ 1327], train_loss/perplexity = 6.54594994/696.4179077 secs/batch = 0.6130s, grad.norm=0.32502353\n",
      "  1622: 1 [  295/ 1327], train_loss/perplexity = 6.35864305/577.4622192 secs/batch = 0.6146s, grad.norm=0.38318533\n",
      "  1627: 1 [  300/ 1327], train_loss/perplexity = 6.26840258/527.6338501 secs/batch = 0.6046s, grad.norm=0.42549798\n",
      "  1632: 1 [  305/ 1327], train_loss/perplexity = 6.37749910/588.4542236 secs/batch = 0.6075s, grad.norm=0.44087714\n",
      "  1637: 1 [  310/ 1327], train_loss/perplexity = 6.40083361/602.3469238 secs/batch = 0.6053s, grad.norm=0.38232371\n",
      "  1642: 1 [  315/ 1327], train_loss/perplexity = 6.28807354/538.1156616 secs/batch = 0.6129s, grad.norm=0.49856916\n",
      "  1647: 1 [  320/ 1327], train_loss/perplexity = 6.42009544/614.0617065 secs/batch = 0.6171s, grad.norm=0.94368172\n",
      "  1652: 1 [  325/ 1327], train_loss/perplexity = 6.23810530/511.8877258 secs/batch = 0.6088s, grad.norm=0.42967600\n",
      "  1657: 1 [  330/ 1327], train_loss/perplexity = 6.55725670/704.3368530 secs/batch = 0.6086s, grad.norm=0.35312694\n",
      "  1662: 1 [  335/ 1327], train_loss/perplexity = 5.96290112/388.7362671 secs/batch = 0.6177s, grad.norm=0.37715986\n",
      "  1667: 1 [  340/ 1327], train_loss/perplexity = 6.50051737/665.4858398 secs/batch = 0.6117s, grad.norm=0.34633076\n",
      "  1672: 1 [  345/ 1327], train_loss/perplexity = 6.44996929/632.6828613 secs/batch = 0.6082s, grad.norm=0.29062611\n",
      "  1677: 1 [  350/ 1327], train_loss/perplexity = 6.41751957/612.4819946 secs/batch = 0.6084s, grad.norm=0.37026384\n",
      "  1682: 1 [  355/ 1327], train_loss/perplexity = 6.55667877/703.9298706 secs/batch = 0.6057s, grad.norm=0.38453022\n",
      "  1687: 1 [  360/ 1327], train_loss/perplexity = 6.59332752/730.2066040 secs/batch = 0.6147s, grad.norm=0.32464707\n",
      "  1692: 1 [  365/ 1327], train_loss/perplexity = 6.48238468/653.5275269 secs/batch = 0.6069s, grad.norm=0.24707761\n",
      "  1697: 1 [  370/ 1327], train_loss/perplexity = 6.52088881/679.1817627 secs/batch = 0.6118s, grad.norm=0.39046353\n",
      "  1702: 1 [  375/ 1327], train_loss/perplexity = 6.28825426/538.2129517 secs/batch = 0.6157s, grad.norm=0.63652664\n",
      "  1707: 1 [  380/ 1327], train_loss/perplexity = 6.41646671/611.8374634 secs/batch = 0.6121s, grad.norm=0.30626452\n",
      "  1712: 1 [  385/ 1327], train_loss/perplexity = 6.49503708/661.8487549 secs/batch = 0.6120s, grad.norm=0.42974833\n",
      "  1717: 1 [  390/ 1327], train_loss/perplexity = 6.45638609/636.7557373 secs/batch = 0.6213s, grad.norm=0.32260743\n",
      "  1722: 1 [  395/ 1327], train_loss/perplexity = 6.68247890/798.2955322 secs/batch = 0.6134s, grad.norm=0.42312267\n",
      "  1727: 1 [  400/ 1327], train_loss/perplexity = 6.32444668/558.0489502 secs/batch = 0.6123s, grad.norm=0.34664047\n",
      "  1732: 1 [  405/ 1327], train_loss/perplexity = 6.56343555/708.7023315 secs/batch = 0.6172s, grad.norm=0.41568613\n",
      "  1737: 1 [  410/ 1327], train_loss/perplexity = 6.54151154/693.3338013 secs/batch = 0.6142s, grad.norm=0.50590420\n",
      "  1742: 1 [  415/ 1327], train_loss/perplexity = 6.27942085/533.4796143 secs/batch = 0.6150s, grad.norm=0.30876029\n",
      "  1747: 1 [  420/ 1327], train_loss/perplexity = 6.32764244/559.8352051 secs/batch = 0.6115s, grad.norm=0.32342508\n",
      "  1752: 1 [  425/ 1327], train_loss/perplexity = 6.59666443/732.6473389 secs/batch = 0.6107s, grad.norm=0.29861662\n",
      "  1757: 1 [  430/ 1327], train_loss/perplexity = 6.47959328/651.7058105 secs/batch = 0.6144s, grad.norm=0.33427870\n",
      "  1762: 1 [  435/ 1327], train_loss/perplexity = 6.54848433/698.1851196 secs/batch = 0.6166s, grad.norm=0.40048796\n",
      "  1767: 1 [  440/ 1327], train_loss/perplexity = 6.38673353/593.9133911 secs/batch = 0.6186s, grad.norm=0.63438725\n",
      "  1772: 1 [  445/ 1327], train_loss/perplexity = 6.40511847/604.9334717 secs/batch = 0.6212s, grad.norm=0.65068603\n",
      "  1777: 1 [  450/ 1327], train_loss/perplexity = 6.35391808/574.7401733 secs/batch = 0.6127s, grad.norm=0.30023071\n",
      "  1782: 1 [  455/ 1327], train_loss/perplexity = 6.12261915/456.0576172 secs/batch = 0.6134s, grad.norm=0.32963213\n",
      "  1787: 1 [  460/ 1327], train_loss/perplexity = 6.45802641/637.8010254 secs/batch = 0.6091s, grad.norm=0.39162087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1792: 1 [  465/ 1327], train_loss/perplexity = 6.35114527/573.1487427 secs/batch = 0.6126s, grad.norm=0.50329798\n",
      "  1797: 1 [  470/ 1327], train_loss/perplexity = 6.58251762/722.3556519 secs/batch = 0.6184s, grad.norm=0.46115971\n",
      "  1802: 1 [  475/ 1327], train_loss/perplexity = 6.54741621/697.4398193 secs/batch = 0.6161s, grad.norm=0.29729715\n",
      "  1807: 1 [  480/ 1327], train_loss/perplexity = 6.40624428/605.6148682 secs/batch = 0.6165s, grad.norm=0.30963373\n",
      "  1812: 1 [  485/ 1327], train_loss/perplexity = 6.37445641/586.6664429 secs/batch = 0.6109s, grad.norm=0.38642746\n",
      "  1817: 1 [  490/ 1327], train_loss/perplexity = 6.37958288/589.6817017 secs/batch = 0.6150s, grad.norm=0.38039529\n",
      "  1822: 1 [  495/ 1327], train_loss/perplexity = 6.14679432/467.2172241 secs/batch = 0.6567s, grad.norm=0.90451193\n",
      "  1827: 1 [  500/ 1327], train_loss/perplexity = 6.51399899/674.5184326 secs/batch = 0.6161s, grad.norm=0.30808425\n",
      "  1832: 1 [  505/ 1327], train_loss/perplexity = 6.36082458/578.7233887 secs/batch = 0.6106s, grad.norm=0.41906330\n",
      "  1837: 1 [  510/ 1327], train_loss/perplexity = 6.48094511/652.5874023 secs/batch = 0.6171s, grad.norm=0.30110914\n",
      "  1842: 1 [  515/ 1327], train_loss/perplexity = 6.31323528/551.8273926 secs/batch = 0.6112s, grad.norm=0.38666782\n",
      "  1847: 1 [  520/ 1327], train_loss/perplexity = 6.48358727/654.3139648 secs/batch = 0.6138s, grad.norm=0.40224594\n",
      "  1852: 1 [  525/ 1327], train_loss/perplexity = 6.34884262/571.8305054 secs/batch = 0.6037s, grad.norm=0.43516937\n",
      "  1857: 1 [  530/ 1327], train_loss/perplexity = 6.32522821/558.4852295 secs/batch = 0.6114s, grad.norm=0.39536363\n",
      "  1862: 1 [  535/ 1327], train_loss/perplexity = 6.44801664/631.4486694 secs/batch = 0.6193s, grad.norm=0.35227272\n",
      "  1867: 1 [  540/ 1327], train_loss/perplexity = 6.43925762/625.9419556 secs/batch = 0.6119s, grad.norm=0.31269944\n",
      "  1872: 1 [  545/ 1327], train_loss/perplexity = 6.52549601/682.3181152 secs/batch = 0.6110s, grad.norm=0.39656323\n",
      "  1877: 1 [  550/ 1327], train_loss/perplexity = 6.44315767/628.3879395 secs/batch = 0.6096s, grad.norm=0.32959971\n",
      "  1882: 1 [  555/ 1327], train_loss/perplexity = 6.35879326/577.5490112 secs/batch = 0.6106s, grad.norm=0.39832771\n",
      "  1887: 1 [  560/ 1327], train_loss/perplexity = 6.51549578/675.5288086 secs/batch = 0.6128s, grad.norm=0.36741066\n",
      "  1892: 1 [  565/ 1327], train_loss/perplexity = 6.40696144/606.0493774 secs/batch = 0.6116s, grad.norm=0.46049693\n",
      "  1897: 1 [  570/ 1327], train_loss/perplexity = 6.28659439/537.3203125 secs/batch = 0.6110s, grad.norm=0.34611985\n",
      "  1902: 1 [  575/ 1327], train_loss/perplexity = 6.40213251/603.1298218 secs/batch = 0.6142s, grad.norm=0.56179821\n",
      "  1907: 1 [  580/ 1327], train_loss/perplexity = 6.43723345/624.6762085 secs/batch = 0.6086s, grad.norm=0.39331722\n",
      "  1912: 1 [  585/ 1327], train_loss/perplexity = 6.26206398/524.2999878 secs/batch = 0.6084s, grad.norm=0.36702320\n",
      "  1917: 1 [  590/ 1327], train_loss/perplexity = 6.48487282/655.1556396 secs/batch = 0.6152s, grad.norm=0.33985999\n",
      "  1922: 1 [  595/ 1327], train_loss/perplexity = 6.39857769/600.9896240 secs/batch = 0.6154s, grad.norm=0.33629945\n",
      "  1927: 1 [  600/ 1327], train_loss/perplexity = 6.55450487/702.4013062 secs/batch = 0.6109s, grad.norm=0.32490525\n",
      "  1932: 1 [  605/ 1327], train_loss/perplexity = 6.44953775/632.4099121 secs/batch = 0.6122s, grad.norm=0.37101734\n",
      "  1937: 1 [  610/ 1327], train_loss/perplexity = 6.61515713/746.3219604 secs/batch = 0.6104s, grad.norm=0.49580368\n",
      "  1942: 1 [  615/ 1327], train_loss/perplexity = 6.12579441/457.5080261 secs/batch = 0.6104s, grad.norm=0.36262113\n",
      "  1947: 1 [  620/ 1327], train_loss/perplexity = 6.34914780/572.0050659 secs/batch = 0.6114s, grad.norm=0.34836739\n",
      "  1952: 1 [  625/ 1327], train_loss/perplexity = 6.50490522/668.4122925 secs/batch = 0.6135s, grad.norm=0.33313277\n",
      "  1957: 1 [  630/ 1327], train_loss/perplexity = 6.45574760/636.3493042 secs/batch = 0.6142s, grad.norm=0.32376382\n",
      "  1962: 1 [  635/ 1327], train_loss/perplexity = 6.39040327/596.0969238 secs/batch = 0.6217s, grad.norm=0.59960699\n",
      "  1967: 1 [  640/ 1327], train_loss/perplexity = 6.34441280/569.3029785 secs/batch = 0.6124s, grad.norm=0.59005964\n",
      "  1972: 1 [  645/ 1327], train_loss/perplexity = 6.42980766/620.0546875 secs/batch = 0.6093s, grad.norm=0.37446487\n",
      "  1977: 1 [  650/ 1327], train_loss/perplexity = 6.19953394/492.5194397 secs/batch = 0.6151s, grad.norm=0.43747020\n",
      "  1982: 1 [  655/ 1327], train_loss/perplexity = 6.23510742/510.3554382 secs/batch = 0.6054s, grad.norm=0.40157515\n",
      "  1987: 1 [  660/ 1327], train_loss/perplexity = 6.31067467/550.4161987 secs/batch = 0.6127s, grad.norm=0.29765752\n",
      "  1992: 1 [  665/ 1327], train_loss/perplexity = 6.42168665/615.0396118 secs/batch = 0.6127s, grad.norm=0.33369035\n",
      "  1997: 1 [  670/ 1327], train_loss/perplexity = 6.35012484/572.5642090 secs/batch = 0.6123s, grad.norm=0.34282109\n",
      "  2002: 1 [  675/ 1327], train_loss/perplexity = 6.11481380/452.5117798 secs/batch = 0.6148s, grad.norm=0.43134764\n",
      "  2007: 1 [  680/ 1327], train_loss/perplexity = 6.48772717/657.0283203 secs/batch = 0.6102s, grad.norm=0.41667205\n",
      "  2012: 1 [  685/ 1327], train_loss/perplexity = 6.44969177/632.5073242 secs/batch = 0.6136s, grad.norm=0.47529128\n",
      "  2017: 1 [  690/ 1327], train_loss/perplexity = 6.45054817/633.0491943 secs/batch = 0.6253s, grad.norm=0.60969293\n",
      "  2022: 1 [  695/ 1327], train_loss/perplexity = 6.31361103/552.0347900 secs/batch = 0.6027s, grad.norm=0.30864432\n",
      "  2027: 1 [  700/ 1327], train_loss/perplexity = 6.42633295/617.9039307 secs/batch = 0.6127s, grad.norm=0.34450287\n",
      "  2032: 1 [  705/ 1327], train_loss/perplexity = 6.16181087/474.2861633 secs/batch = 0.6119s, grad.norm=0.47861460\n",
      "  2037: 1 [  710/ 1327], train_loss/perplexity = 6.35554266/575.6746216 secs/batch = 0.6201s, grad.norm=0.49579203\n",
      "  2042: 1 [  715/ 1327], train_loss/perplexity = 6.31890917/554.9672852 secs/batch = 0.6172s, grad.norm=0.38509426\n",
      "  2047: 1 [  720/ 1327], train_loss/perplexity = 6.34194899/567.9020386 secs/batch = 0.6150s, grad.norm=0.42914760\n",
      "  2052: 1 [  725/ 1327], train_loss/perplexity = 6.11133766/450.9415283 secs/batch = 0.6116s, grad.norm=0.43881169\n",
      "  2057: 1 [  730/ 1327], train_loss/perplexity = 6.13153934/460.1439209 secs/batch = 0.6132s, grad.norm=0.35327145\n",
      "  2062: 1 [  735/ 1327], train_loss/perplexity = 6.31090593/550.5434570 secs/batch = 0.6143s, grad.norm=0.38552585\n",
      "  2067: 1 [  740/ 1327], train_loss/perplexity = 6.00703049/406.2750854 secs/batch = 0.6538s, grad.norm=0.37711683\n",
      "  2072: 1 [  745/ 1327], train_loss/perplexity = 6.32240534/556.9109497 secs/batch = 0.6186s, grad.norm=0.56479222\n",
      "  2077: 1 [  750/ 1327], train_loss/perplexity = 6.19032383/488.0041199 secs/batch = 0.6118s, grad.norm=0.40095770\n",
      "  2082: 1 [  755/ 1327], train_loss/perplexity = 6.20629025/495.8583069 secs/batch = 0.6041s, grad.norm=0.37212464\n",
      "  2087: 1 [  760/ 1327], train_loss/perplexity = 6.13294554/460.7914429 secs/batch = 0.6061s, grad.norm=0.54919404\n",
      "  2092: 1 [  765/ 1327], train_loss/perplexity = 6.14181423/464.8962402 secs/batch = 0.6141s, grad.norm=0.37538540\n",
      "  2097: 1 [  770/ 1327], train_loss/perplexity = 6.16736126/476.9259644 secs/batch = 0.6170s, grad.norm=0.37925401\n",
      "  2102: 1 [  775/ 1327], train_loss/perplexity = 6.23156404/508.5502625 secs/batch = 0.6131s, grad.norm=0.30931187\n",
      "  2107: 1 [  780/ 1327], train_loss/perplexity = 6.40603590/605.4887085 secs/batch = 0.6110s, grad.norm=0.36982328\n",
      "  2112: 1 [  785/ 1327], train_loss/perplexity = 6.22874355/507.1179199 secs/batch = 0.6119s, grad.norm=0.40108696\n",
      "  2117: 1 [  790/ 1327], train_loss/perplexity = 6.03104305/416.1488647 secs/batch = 0.6076s, grad.norm=0.33668971\n",
      "  2122: 1 [  795/ 1327], train_loss/perplexity = 6.37243700/585.4829102 secs/batch = 0.6161s, grad.norm=0.37085250\n",
      "  2127: 1 [  800/ 1327], train_loss/perplexity = 6.31916094/555.1070557 secs/batch = 0.6147s, grad.norm=0.36471477\n",
      "  2132: 1 [  805/ 1327], train_loss/perplexity = 6.46544552/642.5505981 secs/batch = 0.6263s, grad.norm=0.42362863\n",
      "  2137: 1 [  810/ 1327], train_loss/perplexity = 6.37978029/589.7980957 secs/batch = 0.6084s, grad.norm=0.39787784\n",
      "  2142: 1 [  815/ 1327], train_loss/perplexity = 6.14588118/466.7908020 secs/batch = 0.6065s, grad.norm=0.43021470\n",
      "  2147: 1 [  820/ 1327], train_loss/perplexity = 5.97142506/392.0639954 secs/batch = 0.6128s, grad.norm=0.54392737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2152: 1 [  825/ 1327], train_loss/perplexity = 6.11309624/451.7352295 secs/batch = 0.6116s, grad.norm=0.47009477\n",
      "  2157: 1 [  830/ 1327], train_loss/perplexity = 5.97387648/393.0262756 secs/batch = 0.6106s, grad.norm=0.51945227\n",
      "  2162: 1 [  835/ 1327], train_loss/perplexity = 6.30420494/546.8666382 secs/batch = 0.6133s, grad.norm=0.49557158\n",
      "  2167: 1 [  840/ 1327], train_loss/perplexity = 6.33696270/565.0773926 secs/batch = 0.6148s, grad.norm=0.35528308\n",
      "  2172: 1 [  845/ 1327], train_loss/perplexity = 6.17782784/481.9439697 secs/batch = 0.6106s, grad.norm=0.34990704\n",
      "  2177: 1 [  850/ 1327], train_loss/perplexity = 6.21630239/500.8478699 secs/batch = 0.6244s, grad.norm=0.47241876\n",
      "  2182: 1 [  855/ 1327], train_loss/perplexity = 6.22903061/507.2635193 secs/batch = 0.6135s, grad.norm=0.35679573\n",
      "  2187: 1 [  860/ 1327], train_loss/perplexity = 6.01335716/408.8536072 secs/batch = 0.6112s, grad.norm=0.49601361\n",
      "  2192: 1 [  865/ 1327], train_loss/perplexity = 6.42826891/619.1013184 secs/batch = 0.6065s, grad.norm=0.39804170\n",
      "  2197: 1 [  870/ 1327], train_loss/perplexity = 6.44869566/631.8775635 secs/batch = 0.6170s, grad.norm=0.45498765\n",
      "  2202: 1 [  875/ 1327], train_loss/perplexity = 6.04582882/422.3476562 secs/batch = 0.6102s, grad.norm=0.43665117\n",
      "  2207: 1 [  880/ 1327], train_loss/perplexity = 6.19672203/491.1364746 secs/batch = 0.6105s, grad.norm=0.46834394\n",
      "  2212: 1 [  885/ 1327], train_loss/perplexity = 6.10368490/447.5037537 secs/batch = 0.6119s, grad.norm=0.46951050\n",
      "  2217: 1 [  890/ 1327], train_loss/perplexity = 6.30796576/548.9271851 secs/batch = 0.6180s, grad.norm=0.54004478\n",
      "  2222: 1 [  895/ 1327], train_loss/perplexity = 6.37186575/585.1485596 secs/batch = 0.6159s, grad.norm=0.78328550\n",
      "  2227: 1 [  900/ 1327], train_loss/perplexity = 6.20595503/495.6921387 secs/batch = 0.6093s, grad.norm=0.50029171\n",
      "  2232: 1 [  905/ 1327], train_loss/perplexity = 6.14415884/465.9875183 secs/batch = 0.6111s, grad.norm=0.36309779\n",
      "  2237: 1 [  910/ 1327], train_loss/perplexity = 6.17879963/482.4125366 secs/batch = 0.6202s, grad.norm=0.44081527\n",
      "  2242: 1 [  915/ 1327], train_loss/perplexity = 6.44336128/628.5158691 secs/batch = 0.6166s, grad.norm=0.55020255\n",
      "  2247: 1 [  920/ 1327], train_loss/perplexity = 6.46061659/639.4552002 secs/batch = 0.6133s, grad.norm=0.41289151\n",
      "  2252: 1 [  925/ 1327], train_loss/perplexity = 6.20240593/493.9359741 secs/batch = 0.6106s, grad.norm=0.43203175\n",
      "  2257: 1 [  930/ 1327], train_loss/perplexity = 6.18558931/485.6990967 secs/batch = 0.6189s, grad.norm=0.70694029\n",
      "  2262: 1 [  935/ 1327], train_loss/perplexity = 6.26814651/527.4987793 secs/batch = 0.6196s, grad.norm=0.33809844\n",
      "  2267: 1 [  940/ 1327], train_loss/perplexity = 6.22896481/507.2301331 secs/batch = 0.6148s, grad.norm=0.58822036\n",
      "  2272: 1 [  945/ 1327], train_loss/perplexity = 6.42599249/617.6935425 secs/batch = 0.6182s, grad.norm=0.34127396\n",
      "  2277: 1 [  950/ 1327], train_loss/perplexity = 6.17863417/482.3327332 secs/batch = 0.6222s, grad.norm=0.36844930\n",
      "  2282: 1 [  955/ 1327], train_loss/perplexity = 6.34569883/570.0355835 secs/batch = 0.6121s, grad.norm=0.35547137\n",
      "  2287: 1 [  960/ 1327], train_loss/perplexity = 6.44520092/629.6731567 secs/batch = 0.6164s, grad.norm=0.47572598\n",
      "  2292: 1 [  965/ 1327], train_loss/perplexity = 6.24803591/516.9963989 secs/batch = 0.6114s, grad.norm=0.40187052\n",
      "  2297: 1 [  970/ 1327], train_loss/perplexity = 6.36029625/578.4176636 secs/batch = 0.6144s, grad.norm=0.42576388\n",
      "  2302: 1 [  975/ 1327], train_loss/perplexity = 6.16083717/473.8245850 secs/batch = 0.6147s, grad.norm=0.40115345\n",
      "  2307: 1 [  980/ 1327], train_loss/perplexity = 6.02684212/414.4043274 secs/batch = 0.6120s, grad.norm=0.33805603\n",
      "  2312: 1 [  985/ 1327], train_loss/perplexity = 6.30566978/547.6682739 secs/batch = 0.6637s, grad.norm=0.41778499\n",
      "  2317: 1 [  990/ 1327], train_loss/perplexity = 6.36021328/578.3696899 secs/batch = 0.6109s, grad.norm=0.41463646\n",
      "  2322: 1 [  995/ 1327], train_loss/perplexity = 6.33786678/565.5885010 secs/batch = 0.6213s, grad.norm=0.41665721\n",
      "  2327: 1 [ 1000/ 1327], train_loss/perplexity = 5.96840763/390.8827515 secs/batch = 0.6135s, grad.norm=0.50579858\n",
      "  2332: 1 [ 1005/ 1327], train_loss/perplexity = 6.37734938/588.3660889 secs/batch = 0.6127s, grad.norm=0.43613032\n",
      "  2337: 1 [ 1010/ 1327], train_loss/perplexity = 5.92700911/375.0311584 secs/batch = 0.6116s, grad.norm=0.36201349\n",
      "  2342: 1 [ 1015/ 1327], train_loss/perplexity = 6.30443478/546.9923096 secs/batch = 0.6107s, grad.norm=0.44349808\n",
      "  2347: 1 [ 1020/ 1327], train_loss/perplexity = 6.44854927/631.7850952 secs/batch = 0.6074s, grad.norm=0.48237833\n",
      "  2352: 1 [ 1025/ 1327], train_loss/perplexity = 6.25333548/519.7435303 secs/batch = 0.6204s, grad.norm=0.33708140\n",
      "  2357: 1 [ 1030/ 1327], train_loss/perplexity = 6.16555023/476.0630188 secs/batch = 0.6107s, grad.norm=0.34187785\n",
      "  2362: 1 [ 1035/ 1327], train_loss/perplexity = 6.16359234/475.1318359 secs/batch = 0.6168s, grad.norm=0.53062677\n",
      "  2367: 1 [ 1040/ 1327], train_loss/perplexity = 6.19310760/489.3645020 secs/batch = 0.6098s, grad.norm=0.33351791\n",
      "  2372: 1 [ 1045/ 1327], train_loss/perplexity = 5.98453617/397.2382202 secs/batch = 0.6188s, grad.norm=0.47964245\n",
      "  2377: 1 [ 1050/ 1327], train_loss/perplexity = 6.16518641/475.8898315 secs/batch = 0.6130s, grad.norm=0.38803759\n",
      "  2382: 1 [ 1055/ 1327], train_loss/perplexity = 6.38556719/593.2211304 secs/batch = 0.6153s, grad.norm=0.49374560\n",
      "  2387: 1 [ 1060/ 1327], train_loss/perplexity = 6.05792046/427.4855347 secs/batch = 0.6162s, grad.norm=0.46022242\n",
      "  2392: 1 [ 1065/ 1327], train_loss/perplexity = 6.08743668/440.2913513 secs/batch = 0.6089s, grad.norm=0.39490670\n",
      "  2397: 1 [ 1070/ 1327], train_loss/perplexity = 6.37082052/584.5372314 secs/batch = 0.6187s, grad.norm=0.42476514\n",
      "  2402: 1 [ 1075/ 1327], train_loss/perplexity = 6.15952349/473.2025452 secs/batch = 0.6099s, grad.norm=0.40127018\n",
      "  2407: 1 [ 1080/ 1327], train_loss/perplexity = 6.03560829/418.0530396 secs/batch = 0.6109s, grad.norm=0.82142282\n",
      "  2412: 1 [ 1085/ 1327], train_loss/perplexity = 6.06521559/430.6155090 secs/batch = 0.6092s, grad.norm=0.46221188\n",
      "  2417: 1 [ 1090/ 1327], train_loss/perplexity = 6.26536226/526.0321045 secs/batch = 0.6074s, grad.norm=0.37282661\n",
      "  2422: 1 [ 1095/ 1327], train_loss/perplexity = 6.22853041/507.0098267 secs/batch = 0.6166s, grad.norm=0.48972744\n",
      "  2427: 1 [ 1100/ 1327], train_loss/perplexity = 6.27706242/532.2229004 secs/batch = 0.6210s, grad.norm=0.48534563\n",
      "  2432: 1 [ 1105/ 1327], train_loss/perplexity = 6.05001593/424.1197815 secs/batch = 0.6191s, grad.norm=0.51109874\n",
      "  2437: 1 [ 1110/ 1327], train_loss/perplexity = 6.47320747/647.5574341 secs/batch = 0.6100s, grad.norm=0.45960215\n",
      "  2442: 1 [ 1115/ 1327], train_loss/perplexity = 6.07604933/435.3060303 secs/batch = 0.6169s, grad.norm=0.41510946\n",
      "  2447: 1 [ 1120/ 1327], train_loss/perplexity = 6.11391115/452.1035156 secs/batch = 0.6134s, grad.norm=0.41770276\n",
      "  2452: 1 [ 1125/ 1327], train_loss/perplexity = 6.37483168/586.8866577 secs/batch = 0.6127s, grad.norm=0.39429778\n",
      "  2457: 1 [ 1130/ 1327], train_loss/perplexity = 6.20989037/497.6466980 secs/batch = 0.6188s, grad.norm=0.38566458\n",
      "  2462: 1 [ 1135/ 1327], train_loss/perplexity = 6.12404490/456.7083130 secs/batch = 0.6171s, grad.norm=0.33995479\n",
      "  2467: 1 [ 1140/ 1327], train_loss/perplexity = 6.33664560/564.8982544 secs/batch = 0.6061s, grad.norm=0.38106245\n",
      "  2472: 1 [ 1145/ 1327], train_loss/perplexity = 6.15209484/469.7003174 secs/batch = 0.6215s, grad.norm=0.55323654\n",
      "  2477: 1 [ 1150/ 1327], train_loss/perplexity = 6.03301811/416.9715881 secs/batch = 0.6129s, grad.norm=0.45959923\n",
      "  2482: 1 [ 1155/ 1327], train_loss/perplexity = 6.18926525/487.4877930 secs/batch = 0.6097s, grad.norm=0.35634407\n",
      "  2487: 1 [ 1160/ 1327], train_loss/perplexity = 6.25299454/519.5663452 secs/batch = 0.6168s, grad.norm=0.48728552\n",
      "  2492: 1 [ 1165/ 1327], train_loss/perplexity = 6.23116541/508.3475647 secs/batch = 0.6154s, grad.norm=0.39363512\n",
      "  2497: 1 [ 1170/ 1327], train_loss/perplexity = 6.15398073/470.5869446 secs/batch = 0.6089s, grad.norm=0.42678273\n",
      "  2502: 1 [ 1175/ 1327], train_loss/perplexity = 5.96949148/391.3066406 secs/batch = 0.6075s, grad.norm=0.48261482\n",
      "  2507: 1 [ 1180/ 1327], train_loss/perplexity = 5.81840754/336.4358521 secs/batch = 0.6195s, grad.norm=0.41056845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2512: 1 [ 1185/ 1327], train_loss/perplexity = 6.21062517/498.0125122 secs/batch = 0.6193s, grad.norm=0.50994080\n",
      "  2517: 1 [ 1190/ 1327], train_loss/perplexity = 6.15963888/473.2571411 secs/batch = 0.6111s, grad.norm=0.32194468\n",
      "  2522: 1 [ 1195/ 1327], train_loss/perplexity = 5.97461700/393.3174438 secs/batch = 0.6174s, grad.norm=0.45590094\n",
      "  2527: 1 [ 1200/ 1327], train_loss/perplexity = 5.87791729/357.0648193 secs/batch = 0.6138s, grad.norm=0.40301856\n",
      "  2532: 1 [ 1205/ 1327], train_loss/perplexity = 6.10973072/450.2174683 secs/batch = 0.6130s, grad.norm=0.43781853\n",
      "  2537: 1 [ 1210/ 1327], train_loss/perplexity = 5.86401844/352.1363525 secs/batch = 0.6157s, grad.norm=0.41581738\n",
      "  2542: 1 [ 1215/ 1327], train_loss/perplexity = 5.86265564/351.6567688 secs/batch = 0.6165s, grad.norm=0.49482828\n",
      "  2547: 1 [ 1220/ 1327], train_loss/perplexity = 5.91768122/371.5491638 secs/batch = 0.6247s, grad.norm=0.34145108\n",
      "  2552: 1 [ 1225/ 1327], train_loss/perplexity = 5.93794155/379.1536560 secs/batch = 0.6152s, grad.norm=0.36187640\n",
      "  2557: 1 [ 1230/ 1327], train_loss/perplexity = 6.03313684/417.0211182 secs/batch = 0.6540s, grad.norm=0.37496480\n",
      "  2562: 1 [ 1235/ 1327], train_loss/perplexity = 6.12976837/459.3297424 secs/batch = 0.6182s, grad.norm=0.40784207\n",
      "  2567: 1 [ 1240/ 1327], train_loss/perplexity = 6.15013552/468.7809143 secs/batch = 0.6155s, grad.norm=0.44948676\n",
      "  2572: 1 [ 1245/ 1327], train_loss/perplexity = 5.96748114/390.5207520 secs/batch = 0.6197s, grad.norm=0.37853047\n",
      "  2577: 1 [ 1250/ 1327], train_loss/perplexity = 6.15516663/471.1453552 secs/batch = 0.6161s, grad.norm=0.39051744\n",
      "  2582: 1 [ 1255/ 1327], train_loss/perplexity = 6.07803631/436.1718445 secs/batch = 0.6135s, grad.norm=0.52269226\n",
      "  2587: 1 [ 1260/ 1327], train_loss/perplexity = 6.18343067/484.6517944 secs/batch = 0.6074s, grad.norm=0.49050871\n",
      "  2592: 1 [ 1265/ 1327], train_loss/perplexity = 6.12201929/455.7841187 secs/batch = 0.6087s, grad.norm=0.46316937\n",
      "  2597: 1 [ 1270/ 1327], train_loss/perplexity = 6.05113983/424.5967102 secs/batch = 0.6176s, grad.norm=0.41812462\n",
      "  2602: 1 [ 1275/ 1327], train_loss/perplexity = 6.32448149/558.0683594 secs/batch = 0.6116s, grad.norm=0.42844233\n",
      "  2607: 1 [ 1280/ 1327], train_loss/perplexity = 6.05176640/424.8628540 secs/batch = 0.6167s, grad.norm=0.42443249\n",
      "  2612: 1 [ 1285/ 1327], train_loss/perplexity = 5.95200062/384.5218506 secs/batch = 0.6155s, grad.norm=0.44675559\n",
      "  2617: 1 [ 1290/ 1327], train_loss/perplexity = 6.07249594/433.7619629 secs/batch = 0.6112s, grad.norm=0.53446710\n",
      "  2622: 1 [ 1295/ 1327], train_loss/perplexity = 6.23669434/511.1659851 secs/batch = 0.6232s, grad.norm=0.40376014\n",
      "  2627: 1 [ 1300/ 1327], train_loss/perplexity = 6.19926548/492.3872375 secs/batch = 0.6192s, grad.norm=0.51401341\n",
      "  2632: 1 [ 1305/ 1327], train_loss/perplexity = 6.34847355/571.6195068 secs/batch = 0.6242s, grad.norm=0.44227347\n",
      "  2637: 1 [ 1310/ 1327], train_loss/perplexity = 6.44583654/630.0735474 secs/batch = 0.6135s, grad.norm=0.45918432\n",
      "  2642: 1 [ 1315/ 1327], train_loss/perplexity = 6.31924200/555.1520386 secs/batch = 0.6145s, grad.norm=0.34627968\n",
      "  2647: 1 [ 1320/ 1327], train_loss/perplexity = 6.35181618/573.5333862 secs/batch = 0.6154s, grad.norm=0.39117107\n",
      "  2652: 1 [ 1325/ 1327], train_loss/perplexity = 6.24612427/516.0090332 secs/batch = 0.6145s, grad.norm=0.53671253\n",
      "Epoch training time: 816.1133284568787\n",
      "Saved char model cv/epoch001_6.1962.model\n",
      "  2659: 2 [    5/ 1327], train_loss/perplexity = 6.31678486/553.7896118 secs/batch = 0.6112s, grad.norm=0.39821637\n",
      "  2664: 2 [   10/ 1327], train_loss/perplexity = 5.94346380/381.2532349 secs/batch = 0.6128s, grad.norm=0.42025667\n",
      "  2669: 2 [   15/ 1327], train_loss/perplexity = 5.94401312/381.4627075 secs/batch = 0.6152s, grad.norm=0.38205913\n",
      "  2674: 2 [   20/ 1327], train_loss/perplexity = 6.42331409/616.0413818 secs/batch = 0.6128s, grad.norm=0.55977988\n",
      "  2679: 2 [   25/ 1327], train_loss/perplexity = 6.28178978/534.7448730 secs/batch = 0.6115s, grad.norm=0.37392753\n",
      "  2684: 2 [   30/ 1327], train_loss/perplexity = 6.06771469/431.6929932 secs/batch = 0.6156s, grad.norm=0.40517628\n",
      "  2689: 2 [   35/ 1327], train_loss/perplexity = 5.96970367/391.3896790 secs/batch = 0.6165s, grad.norm=0.41211289\n",
      "  2694: 2 [   40/ 1327], train_loss/perplexity = 6.08736610/440.2602844 secs/batch = 0.6131s, grad.norm=0.41090250\n",
      "  2699: 2 [   45/ 1327], train_loss/perplexity = 5.83543301/342.2128906 secs/batch = 0.6167s, grad.norm=0.49279642\n",
      "  2704: 2 [   50/ 1327], train_loss/perplexity = 6.24375010/514.7854004 secs/batch = 0.6173s, grad.norm=0.39898893\n",
      "  2709: 2 [   55/ 1327], train_loss/perplexity = 6.06816626/431.8879700 secs/batch = 0.6137s, grad.norm=0.40755031\n",
      "  2714: 2 [   60/ 1327], train_loss/perplexity = 6.18627167/486.0306396 secs/batch = 0.6131s, grad.norm=0.38618848\n",
      "  2719: 2 [   65/ 1327], train_loss/perplexity = 5.88624716/360.0515442 secs/batch = 0.6186s, grad.norm=0.48922941\n",
      "  2724: 2 [   70/ 1327], train_loss/perplexity = 5.84435320/345.2791443 secs/batch = 0.6146s, grad.norm=0.55831432\n",
      "  2729: 2 [   75/ 1327], train_loss/perplexity = 5.81606102/335.6473389 secs/batch = 0.6065s, grad.norm=0.42818800\n",
      "  2734: 2 [   80/ 1327], train_loss/perplexity = 6.09236050/442.4646301 secs/batch = 0.6165s, grad.norm=0.39034474\n",
      "  2739: 2 [   85/ 1327], train_loss/perplexity = 6.26131535/523.9075928 secs/batch = 0.6060s, grad.norm=1.30894625\n",
      "  2744: 2 [   90/ 1327], train_loss/perplexity = 6.18859816/487.1626892 secs/batch = 0.6442s, grad.norm=0.34862134\n",
      "  2749: 2 [   95/ 1327], train_loss/perplexity = 6.04132128/420.4482117 secs/batch = 0.6045s, grad.norm=0.46835431\n",
      "  2754: 2 [  100/ 1327], train_loss/perplexity = 6.14343119/465.6485596 secs/batch = 0.6131s, grad.norm=0.45734507\n",
      "  2759: 2 [  105/ 1327], train_loss/perplexity = 6.24307394/514.4374390 secs/batch = 0.6058s, grad.norm=0.48095626\n",
      "  2764: 2 [  110/ 1327], train_loss/perplexity = 6.00265646/404.5019226 secs/batch = 0.6227s, grad.norm=0.38393322\n",
      "  2769: 2 [  115/ 1327], train_loss/perplexity = 5.80636740/332.4094238 secs/batch = 0.6127s, grad.norm=0.46133354\n",
      "  2774: 2 [  120/ 1327], train_loss/perplexity = 6.08406925/438.8111877 secs/batch = 0.6138s, grad.norm=0.42918348\n",
      "  2779: 2 [  125/ 1327], train_loss/perplexity = 6.17201662/479.1513977 secs/batch = 0.6118s, grad.norm=0.40247527\n",
      "  2784: 2 [  130/ 1327], train_loss/perplexity = 6.16500711/475.8045349 secs/batch = 0.6165s, grad.norm=0.50521308\n",
      "  2789: 2 [  135/ 1327], train_loss/perplexity = 6.04308319/421.1896362 secs/batch = 0.6112s, grad.norm=0.51744199\n",
      "  2794: 2 [  140/ 1327], train_loss/perplexity = 6.26572037/526.2205200 secs/batch = 0.6113s, grad.norm=0.48939419\n",
      "  2799: 2 [  145/ 1327], train_loss/perplexity = 6.28300619/535.3957520 secs/batch = 0.6134s, grad.norm=0.41055924\n",
      "  2804: 2 [  150/ 1327], train_loss/perplexity = 6.14005280/464.0780640 secs/batch = 0.6068s, grad.norm=0.43106720\n",
      "  2809: 2 [  155/ 1327], train_loss/perplexity = 6.24855328/517.2639160 secs/batch = 0.6085s, grad.norm=0.40059710\n",
      "  2814: 2 [  160/ 1327], train_loss/perplexity = 5.97685194/394.1974487 secs/batch = 0.6197s, grad.norm=0.52406454\n",
      "  2819: 2 [  165/ 1327], train_loss/perplexity = 6.14828539/467.9143982 secs/batch = 0.6135s, grad.norm=0.54993832\n",
      "  2824: 2 [  170/ 1327], train_loss/perplexity = 6.10525274/448.2059021 secs/batch = 0.6158s, grad.norm=0.38260406\n",
      "  2829: 2 [  175/ 1327], train_loss/perplexity = 6.22996569/507.7380676 secs/batch = 0.6108s, grad.norm=0.45637316\n",
      "  2834: 2 [  180/ 1327], train_loss/perplexity = 6.13541842/461.9323425 secs/batch = 0.6164s, grad.norm=0.37714720\n",
      "  2839: 2 [  185/ 1327], train_loss/perplexity = 6.35014009/572.5729370 secs/batch = 0.6112s, grad.norm=0.43855277\n",
      "  2844: 2 [  190/ 1327], train_loss/perplexity = 6.07413912/434.4753113 secs/batch = 0.6148s, grad.norm=0.43556347\n",
      "  2849: 2 [  195/ 1327], train_loss/perplexity = 6.03920126/419.5577698 secs/batch = 0.6152s, grad.norm=0.49847540\n",
      "  2854: 2 [  200/ 1327], train_loss/perplexity = 6.16320658/474.9486084 secs/batch = 0.6116s, grad.norm=0.41106728\n",
      "  2859: 2 [  205/ 1327], train_loss/perplexity = 6.00953531/407.2940063 secs/batch = 0.6096s, grad.norm=0.39451182\n",
      "  2864: 2 [  210/ 1327], train_loss/perplexity = 6.03101206/416.1359558 secs/batch = 0.6117s, grad.norm=0.48640454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2869: 2 [  215/ 1327], train_loss/perplexity = 6.05532455/426.3772583 secs/batch = 0.6140s, grad.norm=0.35346660\n",
      "  2874: 2 [  220/ 1327], train_loss/perplexity = 6.16462135/475.6210022 secs/batch = 0.6151s, grad.norm=0.38395756\n",
      "  2879: 2 [  225/ 1327], train_loss/perplexity = 6.38152122/590.8258057 secs/batch = 0.6197s, grad.norm=0.52773833\n",
      "  2884: 2 [  230/ 1327], train_loss/perplexity = 6.10050869/446.0846252 secs/batch = 0.6058s, grad.norm=0.37313989\n",
      "  2889: 2 [  235/ 1327], train_loss/perplexity = 6.06995106/432.6595154 secs/batch = 0.6134s, grad.norm=0.51250458\n",
      "  2894: 2 [  240/ 1327], train_loss/perplexity = 5.86868238/353.7825317 secs/batch = 0.6118s, grad.norm=0.52506840\n",
      "  2899: 2 [  245/ 1327], train_loss/perplexity = 6.22870064/507.0961609 secs/batch = 0.6431s, grad.norm=0.43481451\n",
      "  2904: 2 [  250/ 1327], train_loss/perplexity = 5.97049236/391.6984863 secs/batch = 0.6093s, grad.norm=0.39652377\n",
      "  2909: 2 [  255/ 1327], train_loss/perplexity = 6.07455444/434.6557922 secs/batch = 0.6082s, grad.norm=0.40197310\n",
      "  2914: 2 [  260/ 1327], train_loss/perplexity = 6.34092474/567.3206787 secs/batch = 0.6095s, grad.norm=0.39011794\n",
      "  2919: 2 [  265/ 1327], train_loss/perplexity = 6.14608955/466.8880615 secs/batch = 0.6069s, grad.norm=0.36863297\n",
      "  2924: 2 [  270/ 1327], train_loss/perplexity = 6.22081327/503.1122437 secs/batch = 0.6166s, grad.norm=0.45917284\n",
      "  2929: 2 [  275/ 1327], train_loss/perplexity = 6.40896606/607.2655029 secs/batch = 0.6165s, grad.norm=0.43210277\n",
      "  2934: 2 [  280/ 1327], train_loss/perplexity = 6.13978100/463.9519653 secs/batch = 0.6144s, grad.norm=0.40397063\n",
      "  2939: 2 [  285/ 1327], train_loss/perplexity = 6.27721405/532.3036499 secs/batch = 0.6387s, grad.norm=0.51106471\n",
      "  2944: 2 [  290/ 1327], train_loss/perplexity = 6.16156864/474.1712952 secs/batch = 0.6097s, grad.norm=0.44468692\n",
      "  2949: 2 [  295/ 1327], train_loss/perplexity = 5.86691141/353.1565247 secs/batch = 0.6179s, grad.norm=0.42300996\n",
      "  2954: 2 [  300/ 1327], train_loss/perplexity = 5.73021984/308.0369873 secs/batch = 0.6117s, grad.norm=0.46827587\n",
      "  2959: 2 [  305/ 1327], train_loss/perplexity = 5.96951056/391.3140869 secs/batch = 0.6132s, grad.norm=0.54077625\n",
      "  2964: 2 [  310/ 1327], train_loss/perplexity = 5.97633028/393.9918823 secs/batch = 0.6096s, grad.norm=0.46305439\n",
      "  2969: 2 [  315/ 1327], train_loss/perplexity = 5.78313875/324.7769775 secs/batch = 0.6125s, grad.norm=0.41349119\n",
      "  2974: 2 [  320/ 1327], train_loss/perplexity = 5.96714020/390.3876343 secs/batch = 0.6128s, grad.norm=0.42435879\n",
      "  2979: 2 [  325/ 1327], train_loss/perplexity = 5.79154587/327.5189209 secs/batch = 0.6157s, grad.norm=0.52404535\n",
      "  2984: 2 [  330/ 1327], train_loss/perplexity = 6.15711212/472.0628357 secs/batch = 0.6105s, grad.norm=0.48777518\n",
      "  2989: 2 [  335/ 1327], train_loss/perplexity = 5.35977697/212.6775055 secs/batch = 0.6146s, grad.norm=0.50798041\n",
      "  2994: 2 [  340/ 1327], train_loss/perplexity = 6.12891340/458.9371948 secs/batch = 0.6120s, grad.norm=0.38218004\n",
      "  2999: 2 [  345/ 1327], train_loss/perplexity = 6.05890274/427.9056702 secs/batch = 0.6161s, grad.norm=0.49436727\n",
      "  3004: 2 [  350/ 1327], train_loss/perplexity = 6.10060263/446.1265259 secs/batch = 0.6156s, grad.norm=0.46528807\n",
      "  3009: 2 [  355/ 1327], train_loss/perplexity = 6.21662378/501.0088501 secs/batch = 0.6143s, grad.norm=0.46739888\n",
      "  3014: 2 [  360/ 1327], train_loss/perplexity = 6.29824972/543.6195679 secs/batch = 0.6140s, grad.norm=0.41037822\n",
      "  3019: 2 [  365/ 1327], train_loss/perplexity = 6.15415239/470.6677246 secs/batch = 0.6165s, grad.norm=0.40370473\n",
      "  3024: 2 [  370/ 1327], train_loss/perplexity = 6.19956493/492.5346985 secs/batch = 0.6078s, grad.norm=0.58741099\n",
      "  3029: 2 [  375/ 1327], train_loss/perplexity = 5.68257856/293.7057800 secs/batch = 0.6119s, grad.norm=0.35914746\n",
      "  3034: 2 [  380/ 1327], train_loss/perplexity = 6.00484514/405.3882141 secs/batch = 0.6102s, grad.norm=0.49471140\n",
      "  3039: 2 [  385/ 1327], train_loss/perplexity = 6.07501745/434.8570862 secs/batch = 0.6152s, grad.norm=0.43199414\n",
      "  3044: 2 [  390/ 1327], train_loss/perplexity = 6.09544134/443.8298950 secs/batch = 0.6090s, grad.norm=0.50917202\n",
      "  3049: 2 [  395/ 1327], train_loss/perplexity = 6.31749010/554.1802979 secs/batch = 0.6177s, grad.norm=0.48274350\n",
      "  3054: 2 [  400/ 1327], train_loss/perplexity = 5.95698738/386.4441528 secs/batch = 0.6092s, grad.norm=0.43215615\n",
      "  3059: 2 [  405/ 1327], train_loss/perplexity = 6.17359924/479.9103088 secs/batch = 0.6174s, grad.norm=0.41591090\n",
      "  3064: 2 [  410/ 1327], train_loss/perplexity = 6.07600117/435.2850647 secs/batch = 0.6138s, grad.norm=0.46000510\n",
      "  3069: 2 [  415/ 1327], train_loss/perplexity = 5.93147945/376.7114258 secs/batch = 0.6179s, grad.norm=0.45277038\n",
      "  3074: 2 [  420/ 1327], train_loss/perplexity = 5.88360071/359.0999451 secs/batch = 0.6229s, grad.norm=0.45587447\n",
      "  3079: 2 [  425/ 1327], train_loss/perplexity = 6.28466749/536.2859497 secs/batch = 0.6082s, grad.norm=0.46073198\n",
      "  3084: 2 [  430/ 1327], train_loss/perplexity = 6.09646416/444.2840576 secs/batch = 0.6089s, grad.norm=0.41275528\n",
      "  3089: 2 [  435/ 1327], train_loss/perplexity = 6.19608736/490.8248596 secs/batch = 0.6129s, grad.norm=0.42008173\n",
      "  3094: 2 [  440/ 1327], train_loss/perplexity = 6.08416700/438.8540955 secs/batch = 0.6176s, grad.norm=0.65006965\n",
      "  3099: 2 [  445/ 1327], train_loss/perplexity = 5.99575186/401.7185974 secs/batch = 0.6128s, grad.norm=0.48285758\n",
      "  3104: 2 [  450/ 1327], train_loss/perplexity = 5.95405769/385.3136597 secs/batch = 0.6142s, grad.norm=0.41395772\n",
      "  3109: 2 [  455/ 1327], train_loss/perplexity = 5.77755594/322.9688721 secs/batch = 0.6063s, grad.norm=0.50081283\n",
      "  3114: 2 [  460/ 1327], train_loss/perplexity = 5.99297428/400.6043396 secs/batch = 0.6041s, grad.norm=0.46462438\n",
      "  3119: 2 [  465/ 1327], train_loss/perplexity = 5.97661400/394.1036682 secs/batch = 0.6091s, grad.norm=0.49278459\n",
      "  3124: 2 [  470/ 1327], train_loss/perplexity = 6.22719955/506.3355408 secs/batch = 0.6243s, grad.norm=0.46392810\n",
      "  3129: 2 [  475/ 1327], train_loss/perplexity = 6.14982653/468.6360779 secs/batch = 0.6147s, grad.norm=0.47897086\n",
      "  3134: 2 [  480/ 1327], train_loss/perplexity = 6.06573820/430.8406067 secs/batch = 0.6181s, grad.norm=0.45639306\n",
      "  3139: 2 [  485/ 1327], train_loss/perplexity = 5.91256618/369.6535339 secs/batch = 0.6129s, grad.norm=0.40315303\n",
      "  3144: 2 [  490/ 1327], train_loss/perplexity = 5.99549675/401.6161499 secs/batch = 0.6535s, grad.norm=0.52862686\n",
      "  3149: 2 [  495/ 1327], train_loss/perplexity = 5.82310295/338.0192871 secs/batch = 0.6069s, grad.norm=1.01240158\n",
      "  3154: 2 [  500/ 1327], train_loss/perplexity = 6.16579580/476.1799316 secs/batch = 0.6134s, grad.norm=0.40434122\n",
      "  3159: 2 [  505/ 1327], train_loss/perplexity = 5.99146891/400.0017395 secs/batch = 0.6113s, grad.norm=0.53328121\n",
      "  3164: 2 [  510/ 1327], train_loss/perplexity = 6.20047665/492.9839783 secs/batch = 0.6131s, grad.norm=0.38887167\n",
      "  3169: 2 [  515/ 1327], train_loss/perplexity = 5.97162437/392.1421509 secs/batch = 0.6158s, grad.norm=0.48179990\n",
      "  3174: 2 [  520/ 1327], train_loss/perplexity = 6.18891525/487.3171997 secs/batch = 0.6171s, grad.norm=0.42655277\n",
      "  3179: 2 [  525/ 1327], train_loss/perplexity = 5.96171808/388.2766418 secs/batch = 0.6140s, grad.norm=0.49802601\n",
      "  3184: 2 [  530/ 1327], train_loss/perplexity = 5.89589024/363.5403137 secs/batch = 0.6126s, grad.norm=0.36892003\n",
      "  3189: 2 [  535/ 1327], train_loss/perplexity = 6.04068136/420.1792297 secs/batch = 0.6115s, grad.norm=0.39918062\n",
      "  3194: 2 [  540/ 1327], train_loss/perplexity = 6.07761145/435.9865723 secs/batch = 0.6145s, grad.norm=0.42461738\n",
      "  3199: 2 [  545/ 1327], train_loss/perplexity = 6.16727400/476.8843384 secs/batch = 0.6140s, grad.norm=0.40715787\n",
      "  3204: 2 [  550/ 1327], train_loss/perplexity = 6.07789183/436.1088257 secs/batch = 0.6188s, grad.norm=0.45440522\n",
      "  3209: 2 [  555/ 1327], train_loss/perplexity = 5.96179199/388.3053284 secs/batch = 0.6099s, grad.norm=0.47413641\n",
      "  3214: 2 [  560/ 1327], train_loss/perplexity = 6.08841228/440.7211304 secs/batch = 0.6149s, grad.norm=0.44959512\n",
      "  3219: 2 [  565/ 1327], train_loss/perplexity = 6.07376814/434.3141479 secs/batch = 0.6105s, grad.norm=0.50076467\n",
      "  3224: 2 [  570/ 1327], train_loss/perplexity = 5.91296196/369.7998657 secs/batch = 0.6082s, grad.norm=0.50232500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3229: 2 [  575/ 1327], train_loss/perplexity = 5.95413589/385.3437805 secs/batch = 0.6077s, grad.norm=0.63888305\n",
      "  3234: 2 [  580/ 1327], train_loss/perplexity = 6.10233307/446.8992004 secs/batch = 0.6093s, grad.norm=0.58770132\n",
      "  3239: 2 [  585/ 1327], train_loss/perplexity = 5.90405464/366.5205688 secs/batch = 0.6112s, grad.norm=0.46313217\n",
      "  3244: 2 [  590/ 1327], train_loss/perplexity = 6.09177113/442.2039185 secs/batch = 0.6147s, grad.norm=0.37987912\n",
      "  3249: 2 [  595/ 1327], train_loss/perplexity = 6.04609632/422.4606628 secs/batch = 0.6128s, grad.norm=0.46728548\n",
      "  3254: 2 [  600/ 1327], train_loss/perplexity = 6.25407171/520.1263428 secs/batch = 0.6187s, grad.norm=0.44474322\n",
      "  3259: 2 [  605/ 1327], train_loss/perplexity = 6.17453003/480.3572083 secs/batch = 0.6099s, grad.norm=0.47113091\n",
      "  3264: 2 [  610/ 1327], train_loss/perplexity = 6.26148033/523.9940796 secs/batch = 0.6152s, grad.norm=0.47941098\n",
      "  3269: 2 [  615/ 1327], train_loss/perplexity = 5.68793821/295.2841797 secs/batch = 0.6221s, grad.norm=0.40078911\n",
      "  3274: 2 [  620/ 1327], train_loss/perplexity = 6.01611471/409.9826050 secs/batch = 0.6150s, grad.norm=0.48543677\n",
      "  3279: 2 [  625/ 1327], train_loss/perplexity = 6.21389532/499.6437378 secs/batch = 0.6149s, grad.norm=0.54207015\n",
      "  3284: 2 [  630/ 1327], train_loss/perplexity = 6.12807846/458.5541687 secs/batch = 0.6164s, grad.norm=0.52857912\n",
      "  3289: 2 [  635/ 1327], train_loss/perplexity = 5.95498276/385.6702576 secs/batch = 0.6078s, grad.norm=0.49826777\n",
      "  3294: 2 [  640/ 1327], train_loss/perplexity = 6.02941179/415.4705811 secs/batch = 0.6175s, grad.norm=0.43298510\n",
      "  3299: 2 [  645/ 1327], train_loss/perplexity = 6.12059498/455.1354065 secs/batch = 0.6114s, grad.norm=0.36602709\n",
      "  3304: 2 [  650/ 1327], train_loss/perplexity = 5.81902266/336.6428833 secs/batch = 0.6160s, grad.norm=0.56130564\n",
      "  3309: 2 [  655/ 1327], train_loss/perplexity = 5.86946869/354.0608215 secs/batch = 0.6114s, grad.norm=0.47136918\n",
      "  3314: 2 [  660/ 1327], train_loss/perplexity = 5.87887621/357.4073792 secs/batch = 0.6104s, grad.norm=0.43052056\n",
      "  3319: 2 [  665/ 1327], train_loss/perplexity = 6.06632233/431.0923462 secs/batch = 0.6051s, grad.norm=0.51609355\n",
      "  3324: 2 [  670/ 1327], train_loss/perplexity = 5.94811821/383.0318604 secs/batch = 0.6046s, grad.norm=0.45140240\n",
      "  3329: 2 [  675/ 1327], train_loss/perplexity = 5.69374132/297.0027161 secs/batch = 0.6144s, grad.norm=0.47463197\n",
      "  3334: 2 [  680/ 1327], train_loss/perplexity = 6.08562565/439.4946899 secs/batch = 0.6119s, grad.norm=0.54115260\n",
      "  3339: 2 [  685/ 1327], train_loss/perplexity = 6.07055092/432.9191284 secs/batch = 0.6155s, grad.norm=0.59581882\n",
      "  3344: 2 [  690/ 1327], train_loss/perplexity = 6.03279734/416.8795471 secs/batch = 0.6212s, grad.norm=0.40926605\n",
      "  3349: 2 [  695/ 1327], train_loss/perplexity = 5.97759151/394.4891052 secs/batch = 0.6024s, grad.norm=0.38556337\n",
      "  3354: 2 [  700/ 1327], train_loss/perplexity = 6.07383776/434.3443909 secs/batch = 0.6113s, grad.norm=0.44570711\n",
      "  3359: 2 [  705/ 1327], train_loss/perplexity = 5.78843164/326.5005493 secs/batch = 0.6103s, grad.norm=0.47001138\n",
      "  3364: 2 [  710/ 1327], train_loss/perplexity = 5.97178221/392.2040405 secs/batch = 0.6132s, grad.norm=0.43077126\n",
      "  3369: 2 [  715/ 1327], train_loss/perplexity = 5.91993284/372.3867188 secs/batch = 0.6108s, grad.norm=0.58559644\n",
      "  3374: 2 [  720/ 1327], train_loss/perplexity = 5.99679470/402.1377563 secs/batch = 0.6142s, grad.norm=0.54786140\n",
      "  3379: 2 [  725/ 1327], train_loss/perplexity = 5.64991188/284.2664185 secs/batch = 0.6123s, grad.norm=0.44484636\n",
      "  3384: 2 [  730/ 1327], train_loss/perplexity = 5.82249594/337.8141785 secs/batch = 0.6128s, grad.norm=0.46637431\n",
      "  3389: 2 [  735/ 1327], train_loss/perplexity = 5.97696781/394.2431335 secs/batch = 0.6518s, grad.norm=0.43739313\n",
      "  3394: 2 [  740/ 1327], train_loss/perplexity = 5.51870060/249.3108673 secs/batch = 0.6157s, grad.norm=0.51958442\n",
      "  3399: 2 [  745/ 1327], train_loss/perplexity = 5.94251347/380.8910828 secs/batch = 0.6147s, grad.norm=0.43303058\n",
      "  3404: 2 [  750/ 1327], train_loss/perplexity = 5.79036045/327.1309204 secs/batch = 0.6101s, grad.norm=0.46071565\n",
      "  3409: 2 [  755/ 1327], train_loss/perplexity = 5.87151003/354.7843018 secs/batch = 0.6178s, grad.norm=0.46454275\n",
      "  3414: 2 [  760/ 1327], train_loss/perplexity = 5.72119474/305.2694092 secs/batch = 0.6063s, grad.norm=0.45899370\n",
      "  3419: 2 [  765/ 1327], train_loss/perplexity = 5.81444979/335.1069641 secs/batch = 0.6064s, grad.norm=0.59773690\n",
      "  3424: 2 [  770/ 1327], train_loss/perplexity = 5.82952261/340.1962280 secs/batch = 0.6096s, grad.norm=0.54491878\n",
      "  3429: 2 [  775/ 1327], train_loss/perplexity = 5.88567638/359.8460693 secs/batch = 0.6130s, grad.norm=0.40419149\n",
      "  3434: 2 [  780/ 1327], train_loss/perplexity = 6.04234219/420.8776550 secs/batch = 0.6133s, grad.norm=0.43661767\n",
      "  3439: 2 [  785/ 1327], train_loss/perplexity = 5.84542990/345.6510925 secs/batch = 0.6161s, grad.norm=0.52775908\n",
      "  3444: 2 [  790/ 1327], train_loss/perplexity = 5.71257591/302.6496582 secs/batch = 0.6165s, grad.norm=0.47550482\n",
      "  3449: 2 [  795/ 1327], train_loss/perplexity = 5.99911356/403.0713501 secs/batch = 0.6251s, grad.norm=0.40384468\n",
      "  3454: 2 [  800/ 1327], train_loss/perplexity = 5.98648214/398.0119934 secs/batch = 0.6090s, grad.norm=0.42000040\n",
      "  3459: 2 [  805/ 1327], train_loss/perplexity = 6.17465401/480.4167786 secs/batch = 0.6067s, grad.norm=0.41739148\n",
      "  3464: 2 [  810/ 1327], train_loss/perplexity = 5.99448299/401.2091980 secs/batch = 0.6090s, grad.norm=0.51267338\n",
      "  3469: 2 [  815/ 1327], train_loss/perplexity = 5.79406595/328.3453369 secs/batch = 0.6122s, grad.norm=0.48074174\n",
      "  3474: 2 [  820/ 1327], train_loss/perplexity = 5.53366709/253.0702362 secs/batch = 0.6139s, grad.norm=0.56339413\n",
      "  3479: 2 [  825/ 1327], train_loss/perplexity = 5.68329859/293.9173584 secs/batch = 0.6089s, grad.norm=0.41565928\n",
      "  3484: 2 [  830/ 1327], train_loss/perplexity = 5.57685947/264.2404480 secs/batch = 0.6059s, grad.norm=0.54925102\n",
      "  3489: 2 [  835/ 1327], train_loss/perplexity = 5.83706427/342.7715759 secs/batch = 0.6087s, grad.norm=0.52258706\n",
      "  3494: 2 [  840/ 1327], train_loss/perplexity = 5.97539330/393.6228943 secs/batch = 0.6102s, grad.norm=0.39050683\n",
      "  3499: 2 [  845/ 1327], train_loss/perplexity = 5.82729864/339.4404907 secs/batch = 0.6114s, grad.norm=0.40299743\n",
      "  3504: 2 [  850/ 1327], train_loss/perplexity = 5.78370905/324.9622498 secs/batch = 0.6078s, grad.norm=0.47099349\n",
      "  3509: 2 [  855/ 1327], train_loss/perplexity = 5.81669903/335.8615417 secs/batch = 0.6061s, grad.norm=0.45079005\n",
      "  3514: 2 [  860/ 1327], train_loss/perplexity = 5.63566971/280.2465515 secs/batch = 0.6105s, grad.norm=0.53623068\n",
      "  3519: 2 [  865/ 1327], train_loss/perplexity = 6.09222031/442.4025879 secs/batch = 0.6105s, grad.norm=0.62875485\n",
      "  3524: 2 [  870/ 1327], train_loss/perplexity = 6.05140686/424.7101135 secs/batch = 0.6118s, grad.norm=0.46288103\n",
      "  3529: 2 [  875/ 1327], train_loss/perplexity = 5.67810154/292.3937988 secs/batch = 0.6110s, grad.norm=0.40849537\n",
      "  3534: 2 [  880/ 1327], train_loss/perplexity = 5.77149868/321.0184631 secs/batch = 0.6157s, grad.norm=0.44710070\n",
      "  3539: 2 [  885/ 1327], train_loss/perplexity = 5.75015783/314.2402649 secs/batch = 0.6197s, grad.norm=0.49457291\n",
      "  3544: 2 [  890/ 1327], train_loss/perplexity = 5.93663645/378.6591492 secs/batch = 0.6112s, grad.norm=0.42359963\n",
      "  3549: 2 [  895/ 1327], train_loss/perplexity = 5.97301674/392.6885376 secs/batch = 0.6121s, grad.norm=0.51730913\n",
      "  3554: 2 [  900/ 1327], train_loss/perplexity = 5.89559698/363.4337463 secs/batch = 0.6123s, grad.norm=0.57250452\n",
      "  3559: 2 [  905/ 1327], train_loss/perplexity = 5.76305962/318.3207703 secs/batch = 0.6225s, grad.norm=0.42282376\n",
      "  3564: 2 [  910/ 1327], train_loss/perplexity = 5.87716436/356.7960510 secs/batch = 0.6129s, grad.norm=0.52798229\n",
      "  3569: 2 [  915/ 1327], train_loss/perplexity = 6.06554222/430.7561646 secs/batch = 0.6145s, grad.norm=0.44465563\n",
      "  3574: 2 [  920/ 1327], train_loss/perplexity = 6.12887144/458.9179382 secs/batch = 0.6204s, grad.norm=0.53141922\n",
      "  3579: 2 [  925/ 1327], train_loss/perplexity = 5.83623266/342.4866333 secs/batch = 0.6125s, grad.norm=0.41924283\n",
      "  3584: 2 [  930/ 1327], train_loss/perplexity = 5.83266926/341.2684021 secs/batch = 0.6193s, grad.norm=0.58420694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3589: 2 [  935/ 1327], train_loss/perplexity = 5.89241838/362.2803650 secs/batch = 0.6179s, grad.norm=0.49184757\n",
      "  3594: 2 [  940/ 1327], train_loss/perplexity = 5.85464430/348.8507996 secs/batch = 0.6241s, grad.norm=0.67369437\n",
      "  3599: 2 [  945/ 1327], train_loss/perplexity = 6.06723690/431.4867859 secs/batch = 0.6071s, grad.norm=0.38621593\n",
      "  3604: 2 [  950/ 1327], train_loss/perplexity = 5.97751665/394.4595642 secs/batch = 0.6059s, grad.norm=0.80569339\n",
      "  3609: 2 [  955/ 1327], train_loss/perplexity = 6.00418758/405.1217346 secs/batch = 0.6192s, grad.norm=0.40882725\n",
      "  3614: 2 [  960/ 1327], train_loss/perplexity = 6.13281870/460.7330017 secs/batch = 0.6186s, grad.norm=0.39456660\n",
      "  3619: 2 [  965/ 1327], train_loss/perplexity = 5.90754557/367.8023071 secs/batch = 0.6148s, grad.norm=0.41422519\n",
      "  3624: 2 [  970/ 1327], train_loss/perplexity = 6.05344391/425.5761414 secs/batch = 0.6177s, grad.norm=0.36721727\n",
      "  3629: 2 [  975/ 1327], train_loss/perplexity = 5.84103012/344.1336670 secs/batch = 0.6163s, grad.norm=0.40622038\n",
      "  3634: 2 [  980/ 1327], train_loss/perplexity = 5.69246674/296.6244202 secs/batch = 0.6584s, grad.norm=0.43424457\n",
      "  3639: 2 [  985/ 1327], train_loss/perplexity = 5.95093727/384.1131897 secs/batch = 0.6180s, grad.norm=0.50611544\n",
      "  3644: 2 [  990/ 1327], train_loss/perplexity = 6.13228321/460.4863586 secs/batch = 0.6079s, grad.norm=0.65995425\n",
      "  3649: 2 [  995/ 1327], train_loss/perplexity = 5.95478725/385.5948792 secs/batch = 0.6081s, grad.norm=0.49302319\n",
      "  3654: 2 [ 1000/ 1327], train_loss/perplexity = 5.57749319/264.4079590 secs/batch = 0.6093s, grad.norm=0.42578062\n",
      "  3659: 2 [ 1005/ 1327], train_loss/perplexity = 6.00575256/405.7562256 secs/batch = 0.6078s, grad.norm=0.46569875\n",
      "  3664: 2 [ 1010/ 1327], train_loss/perplexity = 5.56586742/261.3518066 secs/batch = 0.6106s, grad.norm=0.47957391\n",
      "  3669: 2 [ 1015/ 1327], train_loss/perplexity = 5.93731880/378.9176025 secs/batch = 0.6136s, grad.norm=0.45332661\n",
      "  3674: 2 [ 1020/ 1327], train_loss/perplexity = 6.13506556/461.7693787 secs/batch = 0.6099s, grad.norm=0.55551934\n",
      "  3679: 2 [ 1025/ 1327], train_loss/perplexity = 5.94051361/380.1301270 secs/batch = 0.6167s, grad.norm=0.39348260\n",
      "  3684: 2 [ 1030/ 1327], train_loss/perplexity = 5.84196949/344.4570923 secs/batch = 0.6137s, grad.norm=0.40245897\n",
      "  3689: 2 [ 1035/ 1327], train_loss/perplexity = 5.69875336/298.4950562 secs/batch = 0.6106s, grad.norm=0.46985167\n",
      "  3694: 2 [ 1040/ 1327], train_loss/perplexity = 5.91134787/369.2034607 secs/batch = 0.6092s, grad.norm=0.39844617\n",
      "  3699: 2 [ 1045/ 1327], train_loss/perplexity = 5.61926222/275.6859131 secs/batch = 0.6139s, grad.norm=0.45391071\n",
      "  3704: 2 [ 1050/ 1327], train_loss/perplexity = 5.77782583/323.0560608 secs/batch = 0.6149s, grad.norm=0.54354465\n",
      "  3709: 2 [ 1055/ 1327], train_loss/perplexity = 5.96941042/391.2749023 secs/batch = 0.6121s, grad.norm=0.47193620\n",
      "  3714: 2 [ 1060/ 1327], train_loss/perplexity = 5.65002394/284.2982788 secs/batch = 0.6139s, grad.norm=0.56363058\n",
      "  3719: 2 [ 1065/ 1327], train_loss/perplexity = 5.76724815/319.6568604 secs/batch = 0.6182s, grad.norm=0.64510846\n",
      "  3724: 2 [ 1070/ 1327], train_loss/perplexity = 6.04260254/420.9872437 secs/batch = 0.6073s, grad.norm=0.53187281\n",
      "  3729: 2 [ 1075/ 1327], train_loss/perplexity = 5.79456949/328.5107422 secs/batch = 0.6182s, grad.norm=0.47883019\n",
      "  3734: 2 [ 1080/ 1327], train_loss/perplexity = 5.65183258/284.8129272 secs/batch = 0.6101s, grad.norm=0.56145406\n",
      "  3739: 2 [ 1085/ 1327], train_loss/perplexity = 5.59654522/269.4937439 secs/batch = 0.6261s, grad.norm=0.55097204\n",
      "  3744: 2 [ 1090/ 1327], train_loss/perplexity = 5.86629343/352.9383545 secs/batch = 0.6070s, grad.norm=0.44925901\n",
      "  3749: 2 [ 1095/ 1327], train_loss/perplexity = 5.88947201/361.2145081 secs/batch = 0.6157s, grad.norm=0.51216060\n",
      "  3754: 2 [ 1100/ 1327], train_loss/perplexity = 5.97018051/391.5763550 secs/batch = 0.6064s, grad.norm=0.57090145\n",
      "  3759: 2 [ 1105/ 1327], train_loss/perplexity = 5.68780994/295.2463074 secs/batch = 0.6114s, grad.norm=0.50989479\n",
      "  3764: 2 [ 1110/ 1327], train_loss/perplexity = 6.20345497/494.4544067 secs/batch = 0.6135s, grad.norm=0.53387839\n",
      "  3769: 2 [ 1115/ 1327], train_loss/perplexity = 5.72296190/305.8093567 secs/batch = 0.6080s, grad.norm=0.43408594\n",
      "  3774: 2 [ 1120/ 1327], train_loss/perplexity = 5.83356285/341.5734863 secs/batch = 0.6134s, grad.norm=0.49227637\n",
      "  3779: 2 [ 1125/ 1327], train_loss/perplexity = 6.07485819/434.7878418 secs/batch = 0.6112s, grad.norm=0.41971996\n",
      "  3784: 2 [ 1130/ 1327], train_loss/perplexity = 5.81490707/335.2602539 secs/batch = 0.6100s, grad.norm=0.41290605\n",
      "  3789: 2 [ 1135/ 1327], train_loss/perplexity = 5.79004622/327.0281372 secs/batch = 0.6062s, grad.norm=0.40511239\n",
      "  3794: 2 [ 1140/ 1327], train_loss/perplexity = 5.99802732/402.6337585 secs/batch = 0.6142s, grad.norm=0.44179887\n",
      "  3799: 2 [ 1145/ 1327], train_loss/perplexity = 5.83997297/343.7700500 secs/batch = 0.6140s, grad.norm=0.71179599\n",
      "  3804: 2 [ 1150/ 1327], train_loss/perplexity = 5.70725536/301.0436707 secs/batch = 0.6139s, grad.norm=0.43833447\n",
      "  3809: 2 [ 1155/ 1327], train_loss/perplexity = 5.86516809/352.5414124 secs/batch = 0.6160s, grad.norm=0.47447670\n",
      "  3814: 2 [ 1160/ 1327], train_loss/perplexity = 5.86259413/351.6351624 secs/batch = 0.6180s, grad.norm=0.53311658\n",
      "  3819: 2 [ 1165/ 1327], train_loss/perplexity = 5.88301325/358.8890381 secs/batch = 0.6165s, grad.norm=0.50737768\n",
      "  3824: 2 [ 1170/ 1327], train_loss/perplexity = 5.78546095/325.5320740 secs/batch = 0.6073s, grad.norm=0.51458246\n",
      "  3829: 2 [ 1175/ 1327], train_loss/perplexity = 5.60593748/272.0368347 secs/batch = 0.6381s, grad.norm=0.51487911\n",
      "  3834: 2 [ 1180/ 1327], train_loss/perplexity = 5.47681379/239.0837250 secs/batch = 0.6112s, grad.norm=0.54864866\n",
      "  3839: 2 [ 1185/ 1327], train_loss/perplexity = 5.77667189/322.6834717 secs/batch = 0.6138s, grad.norm=0.52144998\n",
      "  3844: 2 [ 1190/ 1327], train_loss/perplexity = 5.82129049/337.4071960 secs/batch = 0.6108s, grad.norm=0.61671340\n",
      "  3849: 2 [ 1195/ 1327], train_loss/perplexity = 5.63916302/281.2272339 secs/batch = 0.6091s, grad.norm=0.56293881\n",
      "  3854: 2 [ 1200/ 1327], train_loss/perplexity = 5.55686140/259.0086365 secs/batch = 0.6169s, grad.norm=0.47466877\n",
      "  3859: 2 [ 1205/ 1327], train_loss/perplexity = 5.69343805/296.9126587 secs/batch = 0.6153s, grad.norm=0.52514392\n",
      "  3864: 2 [ 1210/ 1327], train_loss/perplexity = 5.49886417/244.4141693 secs/batch = 0.6186s, grad.norm=0.60860282\n",
      "  3869: 2 [ 1215/ 1327], train_loss/perplexity = 5.46461296/236.1844177 secs/batch = 0.6147s, grad.norm=0.54232103\n",
      "  3874: 2 [ 1220/ 1327], train_loss/perplexity = 5.56609488/261.4112549 secs/batch = 0.6121s, grad.norm=0.42319497\n",
      "  3879: 2 [ 1225/ 1327], train_loss/perplexity = 5.61817503/275.3863525 secs/batch = 0.6556s, grad.norm=0.44339448\n",
      "  3884: 2 [ 1230/ 1327], train_loss/perplexity = 5.68605709/294.7292480 secs/batch = 0.6103s, grad.norm=0.43969929\n",
      "  3889: 2 [ 1235/ 1327], train_loss/perplexity = 5.74585676/312.8916016 secs/batch = 0.6168s, grad.norm=0.43974012\n",
      "  3894: 2 [ 1240/ 1327], train_loss/perplexity = 5.82625151/339.0852356 secs/batch = 0.6119s, grad.norm=0.48451921\n",
      "  3899: 2 [ 1245/ 1327], train_loss/perplexity = 5.58858299/267.3565063 secs/batch = 0.6180s, grad.norm=0.44569543\n",
      "  3904: 2 [ 1250/ 1327], train_loss/perplexity = 5.84273911/344.7222900 secs/batch = 0.6117s, grad.norm=0.48571545\n",
      "  3909: 2 [ 1255/ 1327], train_loss/perplexity = 5.70659685/300.8454895 secs/batch = 0.6119s, grad.norm=0.53755617\n",
      "  3914: 2 [ 1260/ 1327], train_loss/perplexity = 5.84549332/345.6730347 secs/batch = 0.6273s, grad.norm=0.43798676\n",
      "  3919: 2 [ 1265/ 1327], train_loss/perplexity = 5.83379841/341.6539612 secs/batch = 0.6203s, grad.norm=0.69123000\n",
      "  3924: 2 [ 1270/ 1327], train_loss/perplexity = 5.66861868/289.6341858 secs/batch = 0.6212s, grad.norm=0.41601300\n",
      "  3929: 2 [ 1275/ 1327], train_loss/perplexity = 6.00297213/404.6296082 secs/batch = 0.6058s, grad.norm=0.48761567\n",
      "  3934: 2 [ 1280/ 1327], train_loss/perplexity = 5.75013590/314.2333679 secs/batch = 0.6177s, grad.norm=0.54222512\n",
      "  3939: 2 [ 1285/ 1327], train_loss/perplexity = 5.64646530/283.2883606 secs/batch = 0.6097s, grad.norm=0.41300511\n",
      "  3944: 2 [ 1290/ 1327], train_loss/perplexity = 5.75130320/314.6003723 secs/batch = 0.6190s, grad.norm=0.47035417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3949: 2 [ 1295/ 1327], train_loss/perplexity = 5.91597986/370.9175720 secs/batch = 0.6169s, grad.norm=0.54318219\n",
      "  3954: 2 [ 1300/ 1327], train_loss/perplexity = 5.87137079/354.7349243 secs/batch = 0.6063s, grad.norm=0.51040274\n",
      "  3959: 2 [ 1305/ 1327], train_loss/perplexity = 6.05441141/425.9880981 secs/batch = 0.6163s, grad.norm=0.52178538\n",
      "  3964: 2 [ 1310/ 1327], train_loss/perplexity = 6.15452337/470.8423767 secs/batch = 0.6181s, grad.norm=0.40934575\n",
      "  3969: 2 [ 1315/ 1327], train_loss/perplexity = 6.04130077/420.4395752 secs/batch = 0.6150s, grad.norm=0.53662229\n",
      "  3974: 2 [ 1320/ 1327], train_loss/perplexity = 6.05935192/428.0979004 secs/batch = 0.6143s, grad.norm=0.46626619\n",
      "  3979: 2 [ 1325/ 1327], train_loss/perplexity = 5.84908915/346.9182434 secs/batch = 0.6075s, grad.norm=0.48710349\n",
      "Epoch training time: 815.5400528907776\n",
      "Saved char model cv/epoch002_5.8711.model\n",
      "  3986: 3 [    5/ 1327], train_loss/perplexity = 6.01457453/409.3516235 secs/batch = 0.6133s, grad.norm=0.43234113\n",
      "  3991: 3 [   10/ 1327], train_loss/perplexity = 5.61618328/274.8384094 secs/batch = 0.6199s, grad.norm=0.54013151\n",
      "  3996: 3 [   15/ 1327], train_loss/perplexity = 5.62538338/277.3786011 secs/batch = 0.6252s, grad.norm=0.44915742\n",
      "  4001: 3 [   20/ 1327], train_loss/perplexity = 6.07632113/435.4243774 secs/batch = 0.6134s, grad.norm=0.58365762\n",
      "  4006: 3 [   25/ 1327], train_loss/perplexity = 5.96626472/390.0460205 secs/batch = 0.6107s, grad.norm=0.41037583\n",
      "  4011: 3 [   30/ 1327], train_loss/perplexity = 5.78945255/326.8340454 secs/batch = 0.6139s, grad.norm=0.40326265\n",
      "  4016: 3 [   35/ 1327], train_loss/perplexity = 5.68626833/294.7915039 secs/batch = 0.6156s, grad.norm=0.59194756\n",
      "  4021: 3 [   40/ 1327], train_loss/perplexity = 5.75096130/314.4928284 secs/batch = 0.6121s, grad.norm=0.47658753\n",
      "  4026: 3 [   45/ 1327], train_loss/perplexity = 5.44428062/231.4307404 secs/batch = 0.6537s, grad.norm=0.47989196\n",
      "  4031: 3 [   50/ 1327], train_loss/perplexity = 5.90048885/365.2159729 secs/batch = 0.6163s, grad.norm=0.50475937\n",
      "  4036: 3 [   55/ 1327], train_loss/perplexity = 5.71963692/304.7942505 secs/batch = 0.6195s, grad.norm=0.40929365\n",
      "  4041: 3 [   60/ 1327], train_loss/perplexity = 5.92628908/374.7612305 secs/batch = 0.6040s, grad.norm=0.49380621\n",
      "  4046: 3 [   65/ 1327], train_loss/perplexity = 5.53843451/254.2796173 secs/batch = 0.6145s, grad.norm=0.54111820\n",
      "  4051: 3 [   70/ 1327], train_loss/perplexity = 5.42295599/226.5478058 secs/batch = 0.6082s, grad.norm=0.47637016\n",
      "  4056: 3 [   75/ 1327], train_loss/perplexity = 5.50079966/244.8876801 secs/batch = 0.6116s, grad.norm=0.59193754\n",
      "  4061: 3 [   80/ 1327], train_loss/perplexity = 5.78225851/324.4912415 secs/batch = 0.6075s, grad.norm=0.52880657\n",
      "  4066: 3 [   85/ 1327], train_loss/perplexity = 5.85106754/347.6052551 secs/batch = 0.6197s, grad.norm=0.71462321\n",
      "  4071: 3 [   90/ 1327], train_loss/perplexity = 5.80666733/332.5091248 secs/batch = 0.6131s, grad.norm=0.48263881\n",
      "  4076: 3 [   95/ 1327], train_loss/perplexity = 5.68140364/293.3609009 secs/batch = 0.6116s, grad.norm=0.52947193\n",
      "  4081: 3 [  100/ 1327], train_loss/perplexity = 5.81786776/336.2543030 secs/batch = 0.6076s, grad.norm=0.47089061\n",
      "  4086: 3 [  105/ 1327], train_loss/perplexity = 5.93451786/377.8577576 secs/batch = 0.6089s, grad.norm=0.74633420\n",
      "  4091: 3 [  110/ 1327], train_loss/perplexity = 5.69609261/297.7018738 secs/batch = 0.6118s, grad.norm=0.42089227\n",
      "  4096: 3 [  115/ 1327], train_loss/perplexity = 5.48978472/242.2050629 secs/batch = 0.6125s, grad.norm=0.52680767\n",
      "  4101: 3 [  120/ 1327], train_loss/perplexity = 5.70922232/301.6364136 secs/batch = 0.6085s, grad.norm=0.45277995\n",
      "  4106: 3 [  125/ 1327], train_loss/perplexity = 5.83848858/343.2601318 secs/batch = 0.6128s, grad.norm=0.57258862\n",
      "  4111: 3 [  130/ 1327], train_loss/perplexity = 5.82638931/339.1319580 secs/batch = 0.6581s, grad.norm=0.50884020\n",
      "  4116: 3 [  135/ 1327], train_loss/perplexity = 5.71085358/302.1288452 secs/batch = 0.6136s, grad.norm=0.43718407\n",
      "  4121: 3 [  140/ 1327], train_loss/perplexity = 5.94105101/380.3344421 secs/batch = 0.6065s, grad.norm=0.49427035\n",
      "  4126: 3 [  145/ 1327], train_loss/perplexity = 5.97604847/393.8808594 secs/batch = 0.6081s, grad.norm=0.55460870\n",
      "  4131: 3 [  150/ 1327], train_loss/perplexity = 5.79016066/327.0655518 secs/batch = 0.6118s, grad.norm=0.44574648\n",
      "  4136: 3 [  155/ 1327], train_loss/perplexity = 6.01105595/407.9138184 secs/batch = 0.6072s, grad.norm=0.53432792\n",
      "  4141: 3 [  160/ 1327], train_loss/perplexity = 5.67305279/290.9212952 secs/batch = 0.6085s, grad.norm=0.53039676\n",
      "  4146: 3 [  165/ 1327], train_loss/perplexity = 5.86971903/354.1494751 secs/batch = 0.6077s, grad.norm=0.46896392\n",
      "  4151: 3 [  170/ 1327], train_loss/perplexity = 5.83622026/342.4823914 secs/batch = 0.6152s, grad.norm=0.57128358\n",
      "  4156: 3 [  175/ 1327], train_loss/perplexity = 5.89788914/364.2677307 secs/batch = 0.6163s, grad.norm=0.45395130\n",
      "  4161: 3 [  180/ 1327], train_loss/perplexity = 5.89538574/363.3569641 secs/batch = 0.6045s, grad.norm=0.50066024\n",
      "  4166: 3 [  185/ 1327], train_loss/perplexity = 6.05281830/425.3099976 secs/batch = 0.6149s, grad.norm=0.50980294\n",
      "  4171: 3 [  190/ 1327], train_loss/perplexity = 5.71803093/304.3051453 secs/batch = 0.6219s, grad.norm=0.44820753\n",
      "  4176: 3 [  195/ 1327], train_loss/perplexity = 5.72253656/305.6793213 secs/batch = 0.6170s, grad.norm=0.43143457\n",
      "  4181: 3 [  200/ 1327], train_loss/perplexity = 5.79723120/329.3862915 secs/batch = 0.6178s, grad.norm=0.43919438\n",
      "  4186: 3 [  205/ 1327], train_loss/perplexity = 5.74241924/311.8178711 secs/batch = 0.6129s, grad.norm=0.49413064\n",
      "  4191: 3 [  210/ 1327], train_loss/perplexity = 5.75348568/315.2877502 secs/batch = 0.6090s, grad.norm=0.60099280\n",
      "  4196: 3 [  215/ 1327], train_loss/perplexity = 5.75914145/317.0759888 secs/batch = 0.6098s, grad.norm=0.42411238\n",
      "  4201: 3 [  220/ 1327], train_loss/perplexity = 5.88756704/360.5270691 secs/batch = 0.6133s, grad.norm=0.47649431\n",
      "  4206: 3 [  225/ 1327], train_loss/perplexity = 6.03722239/418.7283630 secs/batch = 0.6108s, grad.norm=0.52710414\n",
      "  4211: 3 [  230/ 1327], train_loss/perplexity = 5.84098864/344.1193848 secs/batch = 0.6082s, grad.norm=0.51586324\n",
      "  4216: 3 [  235/ 1327], train_loss/perplexity = 5.75564575/315.9695129 secs/batch = 0.6152s, grad.norm=0.46287683\n",
      "  4221: 3 [  240/ 1327], train_loss/perplexity = 5.62430239/277.0789185 secs/batch = 0.6547s, grad.norm=0.64875197\n",
      "  4226: 3 [  245/ 1327], train_loss/perplexity = 5.89763308/364.1744690 secs/batch = 0.6146s, grad.norm=0.41358006\n",
      "  4231: 3 [  250/ 1327], train_loss/perplexity = 5.58869076/267.3853149 secs/batch = 0.6235s, grad.norm=0.43177211\n",
      "  4236: 3 [  255/ 1327], train_loss/perplexity = 5.71278620/302.7133179 secs/batch = 0.6121s, grad.norm=0.48354763\n",
      "  4241: 3 [  260/ 1327], train_loss/perplexity = 6.02374649/413.1234741 secs/batch = 0.6120s, grad.norm=0.53350645\n",
      "  4246: 3 [  265/ 1327], train_loss/perplexity = 5.86813211/353.5878906 secs/batch = 0.6152s, grad.norm=0.41501719\n",
      "  4251: 3 [  270/ 1327], train_loss/perplexity = 5.94775677/382.8934631 secs/batch = 0.6136s, grad.norm=0.49607560\n",
      "  4256: 3 [  275/ 1327], train_loss/perplexity = 6.14704609/467.3348694 secs/batch = 0.6047s, grad.norm=0.49545208\n",
      "  4261: 3 [  280/ 1327], train_loss/perplexity = 5.90717697/367.6667480 secs/batch = 0.6152s, grad.norm=0.59299731\n",
      "  4266: 3 [  285/ 1327], train_loss/perplexity = 5.99435329/401.1571655 secs/batch = 0.6068s, grad.norm=0.49023682\n",
      "  4271: 3 [  290/ 1327], train_loss/perplexity = 5.87037802/354.3829041 secs/batch = 0.6149s, grad.norm=0.48073107\n",
      "  4276: 3 [  295/ 1327], train_loss/perplexity = 5.57273245/263.1521606 secs/batch = 0.6136s, grad.norm=0.42690936\n",
      "  4281: 3 [  300/ 1327], train_loss/perplexity = 5.36388540/213.5530701 secs/batch = 0.6141s, grad.norm=0.56649989\n",
      "  4286: 3 [  305/ 1327], train_loss/perplexity = 5.68382931/294.0733643 secs/batch = 0.6120s, grad.norm=0.58182770\n",
      "  4291: 3 [  310/ 1327], train_loss/perplexity = 5.72583580/306.6894836 secs/batch = 0.6179s, grad.norm=0.51509768\n",
      "  4296: 3 [  315/ 1327], train_loss/perplexity = 5.46061707/235.2425385 secs/batch = 0.6139s, grad.norm=0.42913988\n",
      "  4301: 3 [  320/ 1327], train_loss/perplexity = 5.68908501/295.6230164 secs/batch = 0.6171s, grad.norm=0.54094529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4306: 3 [  325/ 1327], train_loss/perplexity = 5.46968508/237.3854218 secs/batch = 0.6581s, grad.norm=0.65885693\n",
      "  4311: 3 [  330/ 1327], train_loss/perplexity = 5.82988882/340.3208313 secs/batch = 0.6170s, grad.norm=0.43001580\n",
      "  4316: 3 [  335/ 1327], train_loss/perplexity = 5.04349184/155.0103455 secs/batch = 0.6119s, grad.norm=0.45311448\n",
      "  4321: 3 [  340/ 1327], train_loss/perplexity = 5.85678911/349.5998230 secs/batch = 0.6129s, grad.norm=0.43052310\n",
      "  4326: 3 [  345/ 1327], train_loss/perplexity = 5.75925398/317.1116638 secs/batch = 0.6158s, grad.norm=0.51129740\n",
      "  4331: 3 [  350/ 1327], train_loss/perplexity = 5.83636999/342.5336914 secs/batch = 0.6078s, grad.norm=0.50469661\n",
      "  4336: 3 [  355/ 1327], train_loss/perplexity = 5.97391319/393.0407104 secs/batch = 0.6118s, grad.norm=0.60697478\n",
      "  4341: 3 [  360/ 1327], train_loss/perplexity = 6.02526093/413.7495728 secs/batch = 0.6121s, grad.norm=0.44693524\n",
      "  4346: 3 [  365/ 1327], train_loss/perplexity = 5.88532734/359.7204895 secs/batch = 0.6159s, grad.norm=0.41537952\n",
      "  4351: 3 [  370/ 1327], train_loss/perplexity = 5.88683748/360.2641296 secs/batch = 0.6141s, grad.norm=0.53059590\n",
      "  4356: 3 [  375/ 1327], train_loss/perplexity = 5.36121702/212.9839935 secs/batch = 0.6100s, grad.norm=0.49035752\n",
      "  4361: 3 [  380/ 1327], train_loss/perplexity = 5.65867615/286.7687378 secs/batch = 0.6085s, grad.norm=0.48378491\n",
      "  4366: 3 [  385/ 1327], train_loss/perplexity = 5.76880980/320.1564636 secs/batch = 0.6083s, grad.norm=0.50427669\n",
      "  4371: 3 [  390/ 1327], train_loss/perplexity = 5.74507093/312.6458130 secs/batch = 0.6110s, grad.norm=0.42590442\n",
      "  4376: 3 [  395/ 1327], train_loss/perplexity = 6.01533365/409.6625061 secs/batch = 0.6108s, grad.norm=0.51498783\n",
      "  4381: 3 [  400/ 1327], train_loss/perplexity = 5.63556290/280.2166138 secs/batch = 0.6141s, grad.norm=0.44077682\n",
      "  4386: 3 [  405/ 1327], train_loss/perplexity = 5.90910864/368.3776550 secs/batch = 0.6110s, grad.norm=0.47865903\n",
      "  4391: 3 [  410/ 1327], train_loss/perplexity = 5.77157974/321.0444946 secs/batch = 0.6139s, grad.norm=0.43894419\n",
      "  4396: 3 [  415/ 1327], train_loss/perplexity = 5.61496782/274.5045471 secs/batch = 0.6082s, grad.norm=0.51575655\n",
      "  4401: 3 [  420/ 1327], train_loss/perplexity = 5.63569689/280.2541504 secs/batch = 0.6100s, grad.norm=0.66549468\n",
      "  4406: 3 [  425/ 1327], train_loss/perplexity = 5.91668367/371.1787109 secs/batch = 0.6150s, grad.norm=0.46289146\n",
      "  4411: 3 [  430/ 1327], train_loss/perplexity = 5.84899330/346.8850098 secs/batch = 0.6144s, grad.norm=0.44445288\n",
      "  4416: 3 [  435/ 1327], train_loss/perplexity = 5.90872335/368.2357483 secs/batch = 0.6261s, grad.norm=0.44737723\n",
      "  4421: 3 [  440/ 1327], train_loss/perplexity = 5.76505804/318.9575500 secs/batch = 0.6116s, grad.norm=0.59248561\n",
      "  4426: 3 [  445/ 1327], train_loss/perplexity = 5.69902563/298.5763245 secs/batch = 0.6170s, grad.norm=0.48321494\n",
      "  4431: 3 [  450/ 1327], train_loss/perplexity = 5.64057255/281.6239014 secs/batch = 0.6158s, grad.norm=0.51395959\n",
      "  4436: 3 [  455/ 1327], train_loss/perplexity = 5.46359634/235.9444427 secs/batch = 0.6130s, grad.norm=0.50657678\n",
      "  4441: 3 [  460/ 1327], train_loss/perplexity = 5.68540621/294.5374756 secs/batch = 0.6098s, grad.norm=0.51753026\n",
      "  4446: 3 [  465/ 1327], train_loss/perplexity = 5.68920946/295.6597900 secs/batch = 0.6132s, grad.norm=0.47684425\n",
      "  4451: 3 [  470/ 1327], train_loss/perplexity = 5.93573856/378.3193054 secs/batch = 0.6203s, grad.norm=0.43466875\n",
      "  4456: 3 [  475/ 1327], train_loss/perplexity = 5.77600956/322.4698181 secs/batch = 0.6099s, grad.norm=0.46455672\n",
      "  4461: 3 [  480/ 1327], train_loss/perplexity = 5.73442936/309.3363953 secs/batch = 0.6123s, grad.norm=0.45234132\n",
      "  4466: 3 [  485/ 1327], train_loss/perplexity = 5.62998199/278.6571045 secs/batch = 0.6504s, grad.norm=0.45586297\n",
      "  4471: 3 [  490/ 1327], train_loss/perplexity = 5.67289257/290.8746948 secs/batch = 0.6134s, grad.norm=0.54999298\n",
      "  4476: 3 [  495/ 1327], train_loss/perplexity = 5.56960154/262.3295593 secs/batch = 0.6175s, grad.norm=0.84408867\n",
      "  4481: 3 [  500/ 1327], train_loss/perplexity = 5.90202141/365.7760925 secs/batch = 0.6112s, grad.norm=0.48960552\n",
      "  4486: 3 [  505/ 1327], train_loss/perplexity = 5.66044807/287.2773438 secs/batch = 0.6113s, grad.norm=0.47525164\n",
      "  4491: 3 [  510/ 1327], train_loss/perplexity = 5.96293879/388.7509155 secs/batch = 0.6118s, grad.norm=0.45263007\n",
      "  4496: 3 [  515/ 1327], train_loss/perplexity = 5.66812229/289.4904480 secs/batch = 0.6150s, grad.norm=0.42901710\n",
      "  4501: 3 [  520/ 1327], train_loss/perplexity = 5.93798113/379.1686707 secs/batch = 0.6450s, grad.norm=0.47978941\n",
      "  4506: 3 [  525/ 1327], train_loss/perplexity = 5.64868021/283.9165039 secs/batch = 0.6133s, grad.norm=0.53697294\n",
      "  4511: 3 [  530/ 1327], train_loss/perplexity = 5.61978817/275.8309326 secs/batch = 0.6066s, grad.norm=0.50489211\n",
      "  4516: 3 [  535/ 1327], train_loss/perplexity = 5.72485828/306.3898315 secs/batch = 0.6113s, grad.norm=0.47813785\n",
      "  4521: 3 [  540/ 1327], train_loss/perplexity = 5.77656651/322.6494751 secs/batch = 0.6131s, grad.norm=0.46334386\n",
      "  4526: 3 [  545/ 1327], train_loss/perplexity = 5.93558264/378.2603149 secs/batch = 0.6164s, grad.norm=0.42669925\n",
      "  4531: 3 [  550/ 1327], train_loss/perplexity = 5.79768705/329.5364685 secs/batch = 0.6300s, grad.norm=0.46735954\n",
      "  4536: 3 [  555/ 1327], train_loss/perplexity = 5.72064257/305.1009216 secs/batch = 0.6142s, grad.norm=0.54706812\n",
      "  4541: 3 [  560/ 1327], train_loss/perplexity = 5.78721952/326.1050415 secs/batch = 0.6130s, grad.norm=0.48036170\n",
      "  4546: 3 [  565/ 1327], train_loss/perplexity = 5.79549646/328.8153992 secs/batch = 0.6195s, grad.norm=0.57515621\n",
      "  4551: 3 [  570/ 1327], train_loss/perplexity = 5.63679028/280.5607605 secs/batch = 0.6183s, grad.norm=0.52141315\n",
      "  4556: 3 [  575/ 1327], train_loss/perplexity = 5.61703014/275.0712585 secs/batch = 0.6112s, grad.norm=0.48661971\n",
      "  4561: 3 [  580/ 1327], train_loss/perplexity = 5.92391682/373.8732300 secs/batch = 0.6191s, grad.norm=0.75478452\n",
      "  4566: 3 [  585/ 1327], train_loss/perplexity = 5.58288383/265.8371277 secs/batch = 0.6051s, grad.norm=0.51565230\n",
      "  4571: 3 [  590/ 1327], train_loss/perplexity = 5.78136730/324.2021790 secs/batch = 0.6088s, grad.norm=0.42154971\n",
      "  4576: 3 [  595/ 1327], train_loss/perplexity = 5.78588390/325.6697693 secs/batch = 0.6120s, grad.norm=0.57544619\n",
      "  4581: 3 [  600/ 1327], train_loss/perplexity = 5.96563959/389.8022766 secs/batch = 0.6091s, grad.norm=0.45708650\n",
      "  4586: 3 [  605/ 1327], train_loss/perplexity = 5.90096951/365.3915405 secs/batch = 0.6181s, grad.norm=0.46447095\n",
      "  4591: 3 [  610/ 1327], train_loss/perplexity = 5.99083519/399.7483215 secs/batch = 0.6198s, grad.norm=0.49795496\n",
      "  4596: 3 [  615/ 1327], train_loss/perplexity = 5.38460493/218.0239563 secs/batch = 0.6126s, grad.norm=0.43849212\n",
      "  4601: 3 [  620/ 1327], train_loss/perplexity = 5.78690004/326.0008545 secs/batch = 0.6147s, grad.norm=0.56299013\n",
      "  4606: 3 [  625/ 1327], train_loss/perplexity = 5.92915535/375.8369141 secs/batch = 0.6108s, grad.norm=0.55344403\n",
      "  4611: 3 [  630/ 1327], train_loss/perplexity = 5.84167194/344.3546143 secs/batch = 0.6184s, grad.norm=0.41946808\n",
      "  4616: 3 [  635/ 1327], train_loss/perplexity = 5.66456079/288.4612732 secs/batch = 0.6167s, grad.norm=0.48413312\n",
      "  4621: 3 [  640/ 1327], train_loss/perplexity = 5.79991722/330.2722168 secs/batch = 0.6095s, grad.norm=0.59738702\n",
      "  4626: 3 [  645/ 1327], train_loss/perplexity = 5.88358879/359.0956421 secs/batch = 0.6222s, grad.norm=0.42625371\n",
      "  4631: 3 [  650/ 1327], train_loss/perplexity = 5.57331419/263.3052979 secs/batch = 0.6199s, grad.norm=0.65382844\n",
      "  4636: 3 [  655/ 1327], train_loss/perplexity = 5.64049292/281.6015015 secs/batch = 0.6199s, grad.norm=0.56220251\n",
      "  4641: 3 [  660/ 1327], train_loss/perplexity = 5.59808540/269.9091492 secs/batch = 0.6275s, grad.norm=0.48446506\n",
      "  4646: 3 [  665/ 1327], train_loss/perplexity = 5.79571819/328.8883057 secs/batch = 0.6099s, grad.norm=0.53958499\n",
      "  4651: 3 [  670/ 1327], train_loss/perplexity = 5.66013479/287.1873474 secs/batch = 0.6115s, grad.norm=0.50869149\n",
      "  4656: 3 [  675/ 1327], train_loss/perplexity = 5.42076778/226.0526123 secs/batch = 0.6078s, grad.norm=0.51538646\n",
      "  4661: 3 [  680/ 1327], train_loss/perplexity = 5.78773975/326.2747192 secs/batch = 0.6221s, grad.norm=0.51506662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4666: 3 [  685/ 1327], train_loss/perplexity = 5.79889488/329.9347534 secs/batch = 0.6081s, grad.norm=0.61598617\n",
      "  4671: 3 [  690/ 1327], train_loss/perplexity = 5.77218866/321.2400513 secs/batch = 0.6129s, grad.norm=0.47443038\n",
      "  4676: 3 [  695/ 1327], train_loss/perplexity = 5.69654942/297.8379211 secs/batch = 0.6135s, grad.norm=0.44430164\n",
      "  4681: 3 [  700/ 1327], train_loss/perplexity = 5.83720303/342.8191528 secs/batch = 0.6192s, grad.norm=0.46327636\n",
      "  4686: 3 [  705/ 1327], train_loss/perplexity = 5.54107332/254.9514923 secs/batch = 0.6152s, grad.norm=0.49857035\n",
      "  4691: 3 [  710/ 1327], train_loss/perplexity = 5.69353008/296.9400024 secs/batch = 0.6139s, grad.norm=0.45681837\n",
      "  4696: 3 [  715/ 1327], train_loss/perplexity = 5.62189627/276.4130249 secs/batch = 0.6333s, grad.norm=0.48238832\n",
      "  4701: 3 [  720/ 1327], train_loss/perplexity = 5.70650816/300.8188171 secs/batch = 0.6176s, grad.norm=0.57189083\n",
      "  4706: 3 [  725/ 1327], train_loss/perplexity = 5.40640354/222.8287506 secs/batch = 0.6169s, grad.norm=0.50753224\n",
      "  4711: 3 [  730/ 1327], train_loss/perplexity = 5.52695465/251.3772125 secs/batch = 0.6535s, grad.norm=0.43207330\n",
      "  4716: 3 [  735/ 1327], train_loss/perplexity = 5.72723436/307.1187134 secs/batch = 0.6170s, grad.norm=0.44404498\n",
      "  4721: 3 [  740/ 1327], train_loss/perplexity = 5.23681879/188.0708618 secs/batch = 0.6082s, grad.norm=0.51745182\n",
      "  4726: 3 [  745/ 1327], train_loss/perplexity = 5.68679428/294.9465942 secs/batch = 0.6084s, grad.norm=0.52881014\n",
      "  4731: 3 [  750/ 1327], train_loss/perplexity = 5.54828167/256.7959290 secs/batch = 0.6080s, grad.norm=0.49791989\n",
      "  4736: 3 [  755/ 1327], train_loss/perplexity = 5.59462929/268.9779053 secs/batch = 0.6082s, grad.norm=0.52225876\n",
      "  4741: 3 [  760/ 1327], train_loss/perplexity = 5.45907307/234.8796082 secs/batch = 0.6125s, grad.norm=0.55845702\n",
      "  4746: 3 [  765/ 1327], train_loss/perplexity = 5.52810144/251.6656494 secs/batch = 0.6058s, grad.norm=0.56702906\n",
      "  4751: 3 [  770/ 1327], train_loss/perplexity = 5.53682232/253.8699951 secs/batch = 0.6192s, grad.norm=0.49709550\n",
      "  4756: 3 [  775/ 1327], train_loss/perplexity = 5.59306049/268.5562744 secs/batch = 0.6084s, grad.norm=0.45506728\n",
      "  4761: 3 [  780/ 1327], train_loss/perplexity = 5.75304508/315.1488647 secs/batch = 0.6144s, grad.norm=0.45221770\n",
      "  4766: 3 [  785/ 1327], train_loss/perplexity = 5.57983875/265.0288696 secs/batch = 0.6227s, grad.norm=0.49918306\n",
      "  4771: 3 [  790/ 1327], train_loss/perplexity = 5.50413179/245.7050476 secs/batch = 0.6174s, grad.norm=0.72822642\n",
      "  4776: 3 [  795/ 1327], train_loss/perplexity = 5.73480797/309.4535522 secs/batch = 0.6221s, grad.norm=0.51798719\n",
      "  4781: 3 [  800/ 1327], train_loss/perplexity = 5.71040869/301.9944763 secs/batch = 0.6105s, grad.norm=0.47076622\n",
      "  4786: 3 [  805/ 1327], train_loss/perplexity = 5.97052193/391.7100525 secs/batch = 0.6138s, grad.norm=0.44433171\n",
      "  4791: 3 [  810/ 1327], train_loss/perplexity = 5.74729729/313.3426514 secs/batch = 0.6158s, grad.norm=0.53878814\n",
      "  4796: 3 [  815/ 1327], train_loss/perplexity = 5.54869890/256.9030762 secs/batch = 0.6156s, grad.norm=0.48498616\n",
      "  4801: 3 [  820/ 1327], train_loss/perplexity = 5.23558617/187.8391724 secs/batch = 0.6156s, grad.norm=0.57773435\n",
      "  4806: 3 [  825/ 1327], train_loss/perplexity = 5.41071892/223.7924194 secs/batch = 0.6149s, grad.norm=0.43189469\n",
      "  4811: 3 [  830/ 1327], train_loss/perplexity = 5.28852415/198.0509186 secs/batch = 0.6097s, grad.norm=0.46800324\n",
      "  4816: 3 [  835/ 1327], train_loss/perplexity = 5.55762959/259.2076721 secs/batch = 0.6091s, grad.norm=0.54875249\n",
      "  4821: 3 [  840/ 1327], train_loss/perplexity = 5.73269796/308.8012695 secs/batch = 0.6177s, grad.norm=0.60354805\n",
      "  4826: 3 [  845/ 1327], train_loss/perplexity = 5.55844593/259.4193726 secs/batch = 0.6172s, grad.norm=0.50524062\n",
      "  4831: 3 [  850/ 1327], train_loss/perplexity = 5.53505421/253.4215240 secs/batch = 0.6140s, grad.norm=0.47248200\n",
      "  4836: 3 [  855/ 1327], train_loss/perplexity = 5.57761383/264.4398499 secs/batch = 0.6159s, grad.norm=0.49717763\n",
      "  4841: 3 [  860/ 1327], train_loss/perplexity = 5.32786846/205.9984131 secs/batch = 0.6128s, grad.norm=0.51187694\n",
      "  4846: 3 [  865/ 1327], train_loss/perplexity = 5.74964476/314.0790710 secs/batch = 0.6167s, grad.norm=0.46996808\n",
      "  4851: 3 [  870/ 1327], train_loss/perplexity = 5.77958870/323.6260681 secs/batch = 0.6114s, grad.norm=0.49993160\n",
      "  4856: 3 [  875/ 1327], train_loss/perplexity = 5.40238857/221.9358978 secs/batch = 0.6185s, grad.norm=0.57648808\n",
      "  4861: 3 [  880/ 1327], train_loss/perplexity = 5.51457167/248.2835999 secs/batch = 0.6099s, grad.norm=0.64398700\n",
      "  4866: 3 [  885/ 1327], train_loss/perplexity = 5.49363852/243.1402740 secs/batch = 0.6154s, grad.norm=0.47491461\n",
      "  4871: 3 [  890/ 1327], train_loss/perplexity = 5.69465876/297.2753296 secs/batch = 0.6210s, grad.norm=0.46084100\n",
      "  4876: 3 [  895/ 1327], train_loss/perplexity = 5.69320011/296.8420410 secs/batch = 0.6104s, grad.norm=0.49598932\n",
      "  4881: 3 [  900/ 1327], train_loss/perplexity = 5.67105722/290.3413391 secs/batch = 0.6115s, grad.norm=0.52285570\n",
      "  4886: 3 [  905/ 1327], train_loss/perplexity = 5.48894310/242.0012970 secs/batch = 0.6098s, grad.norm=0.54634267\n",
      "  4891: 3 [  910/ 1327], train_loss/perplexity = 5.58539486/266.5054932 secs/batch = 0.6156s, grad.norm=0.59726173\n",
      "  4896: 3 [  915/ 1327], train_loss/perplexity = 5.83637953/342.5369568 secs/batch = 0.6121s, grad.norm=0.52521443\n",
      "  4901: 3 [  920/ 1327], train_loss/perplexity = 5.89461279/363.0762329 secs/batch = 0.6114s, grad.norm=0.56618124\n",
      "  4906: 3 [  925/ 1327], train_loss/perplexity = 5.60850954/272.7374268 secs/batch = 0.6195s, grad.norm=0.48280373\n",
      "  4911: 3 [  930/ 1327], train_loss/perplexity = 5.58751726/267.0717163 secs/batch = 0.6213s, grad.norm=0.56761813\n",
      "  4916: 3 [  935/ 1327], train_loss/perplexity = 5.62383509/276.9494629 secs/batch = 0.6197s, grad.norm=0.43812788\n",
      "  4921: 3 [  940/ 1327], train_loss/perplexity = 5.60726786/272.3989868 secs/batch = 0.6111s, grad.norm=0.65908730\n",
      "  4926: 3 [  945/ 1327], train_loss/perplexity = 5.80895758/333.2715454 secs/batch = 0.6126s, grad.norm=0.45135108\n",
      "  4931: 3 [  950/ 1327], train_loss/perplexity = 5.52606773/251.1543579 secs/batch = 0.6129s, grad.norm=0.51411694\n",
      "  4936: 3 [  955/ 1327], train_loss/perplexity = 5.76873302/320.1318665 secs/batch = 0.6107s, grad.norm=0.57758838\n",
      "  4941: 3 [  960/ 1327], train_loss/perplexity = 5.96439791/389.3185425 secs/batch = 0.6096s, grad.norm=0.45432442\n",
      "  4946: 3 [  965/ 1327], train_loss/perplexity = 5.67659998/291.9550781 secs/batch = 0.6081s, grad.norm=0.45638382\n",
      "  4951: 3 [  970/ 1327], train_loss/perplexity = 5.83613396/342.4528503 secs/batch = 0.6149s, grad.norm=0.42114922\n",
      "  4956: 3 [  975/ 1327], train_loss/perplexity = 5.61140156/273.5273438 secs/batch = 0.6493s, grad.norm=0.47736537\n",
      "  4961: 3 [  980/ 1327], train_loss/perplexity = 5.42907763/227.9389038 secs/batch = 0.6070s, grad.norm=0.53351307\n",
      "  4966: 3 [  985/ 1327], train_loss/perplexity = 5.77839613/323.2403259 secs/batch = 0.6140s, grad.norm=0.75781882\n",
      "  4971: 3 [  990/ 1327], train_loss/perplexity = 5.82160664/337.5138855 secs/batch = 0.6101s, grad.norm=0.47359368\n",
      "  4976: 3 [  995/ 1327], train_loss/perplexity = 5.69603109/297.6835632 secs/batch = 0.6136s, grad.norm=0.51532483\n",
      "  4981: 3 [ 1000/ 1327], train_loss/perplexity = 5.32140875/204.6720123 secs/batch = 0.6181s, grad.norm=0.49766573\n",
      "  4986: 3 [ 1005/ 1327], train_loss/perplexity = 5.73395872/309.1908569 secs/batch = 0.6128s, grad.norm=0.52784318\n",
      "  4991: 3 [ 1010/ 1327], train_loss/perplexity = 5.33234119/206.9218445 secs/batch = 0.6144s, grad.norm=0.51634908\n",
      "  4996: 3 [ 1015/ 1327], train_loss/perplexity = 5.70624161/300.7386475 secs/batch = 0.6095s, grad.norm=0.53557712\n",
      "  5001: 3 [ 1020/ 1327], train_loss/perplexity = 5.92770243/375.2912598 secs/batch = 0.6119s, grad.norm=0.64561218\n",
      "  5006: 3 [ 1025/ 1327], train_loss/perplexity = 5.72646046/306.8811340 secs/batch = 0.6075s, grad.norm=0.48482803\n",
      "  5011: 3 [ 1030/ 1327], train_loss/perplexity = 5.63362360/279.6737061 secs/batch = 0.6116s, grad.norm=0.48745424\n",
      "  5016: 3 [ 1035/ 1327], train_loss/perplexity = 5.46342897/235.9049530 secs/batch = 0.6088s, grad.norm=0.52272177\n",
      "  5021: 3 [ 1040/ 1327], train_loss/perplexity = 5.69225645/296.5620422 secs/batch = 0.6203s, grad.norm=0.44861403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5026: 3 [ 1045/ 1327], train_loss/perplexity = 5.36386871/213.5495148 secs/batch = 0.6119s, grad.norm=0.44814005\n",
      "  5031: 3 [ 1050/ 1327], train_loss/perplexity = 5.49933052/244.5281677 secs/batch = 0.6127s, grad.norm=0.58023810\n",
      "  5036: 3 [ 1055/ 1327], train_loss/perplexity = 5.73552799/309.6764221 secs/batch = 0.6143s, grad.norm=0.55076772\n",
      "  5041: 3 [ 1060/ 1327], train_loss/perplexity = 5.36224651/213.2033691 secs/batch = 0.6059s, grad.norm=0.50742602\n",
      "  5046: 3 [ 1065/ 1327], train_loss/perplexity = 5.38654566/218.4474945 secs/batch = 0.6126s, grad.norm=0.47086105\n",
      "  5051: 3 [ 1070/ 1327], train_loss/perplexity = 5.79855585/329.8229065 secs/batch = 0.6136s, grad.norm=0.58631879\n",
      "  5056: 3 [ 1075/ 1327], train_loss/perplexity = 5.50166416/245.0994720 secs/batch = 0.6074s, grad.norm=0.46893013\n",
      "  5061: 3 [ 1080/ 1327], train_loss/perplexity = 5.38726139/218.6038971 secs/batch = 0.6131s, grad.norm=0.55022067\n",
      "  5066: 3 [ 1085/ 1327], train_loss/perplexity = 5.30954838/202.2588654 secs/batch = 0.6096s, grad.norm=0.56224537\n",
      "  5071: 3 [ 1090/ 1327], train_loss/perplexity = 5.53909779/254.4483337 secs/batch = 0.6087s, grad.norm=0.51663989\n",
      "  5076: 3 [ 1095/ 1327], train_loss/perplexity = 5.63501835/280.0640564 secs/batch = 0.6074s, grad.norm=0.56106693\n",
      "  5081: 3 [ 1100/ 1327], train_loss/perplexity = 5.63871956/281.1025391 secs/batch = 0.6228s, grad.norm=0.52952743\n",
      "  5086: 3 [ 1105/ 1327], train_loss/perplexity = 5.45532942/234.0019379 secs/batch = 0.6139s, grad.norm=0.54381287\n",
      "  5091: 3 [ 1110/ 1327], train_loss/perplexity = 5.97142839/392.0653076 secs/batch = 0.6097s, grad.norm=0.50128752\n",
      "  5096: 3 [ 1115/ 1327], train_loss/perplexity = 5.51514626/248.4263153 secs/batch = 0.6141s, grad.norm=0.48735726\n",
      "  5101: 3 [ 1120/ 1327], train_loss/perplexity = 5.60888720/272.8404541 secs/batch = 0.6261s, grad.norm=0.45114067\n",
      "  5106: 3 [ 1125/ 1327], train_loss/perplexity = 5.86514807/352.5343628 secs/batch = 0.6159s, grad.norm=0.61925614\n",
      "  5111: 3 [ 1130/ 1327], train_loss/perplexity = 5.58127928/265.4109192 secs/batch = 0.6116s, grad.norm=0.45163831\n",
      "  5116: 3 [ 1135/ 1327], train_loss/perplexity = 5.55708218/259.0658264 secs/batch = 0.6195s, grad.norm=0.49121156\n",
      "  5121: 3 [ 1140/ 1327], train_loss/perplexity = 5.77527905/322.2343445 secs/batch = 0.6182s, grad.norm=0.50325865\n",
      "  5126: 3 [ 1145/ 1327], train_loss/perplexity = 5.53702545/253.9215698 secs/batch = 0.6167s, grad.norm=0.60872853\n",
      "  5131: 3 [ 1150/ 1327], train_loss/perplexity = 5.47115564/237.7347717 secs/batch = 0.6108s, grad.norm=0.44691592\n",
      "  5136: 3 [ 1155/ 1327], train_loss/perplexity = 5.59594107/269.3309937 secs/batch = 0.6117s, grad.norm=0.47803238\n",
      "  5141: 3 [ 1160/ 1327], train_loss/perplexity = 5.62920952/278.4419250 secs/batch = 0.6156s, grad.norm=0.65187675\n",
      "  5146: 3 [ 1165/ 1327], train_loss/perplexity = 5.66295147/287.9974060 secs/batch = 0.6136s, grad.norm=0.53002650\n",
      "  5151: 3 [ 1170/ 1327], train_loss/perplexity = 5.53320503/252.9533386 secs/batch = 0.6139s, grad.norm=0.52673668\n",
      "  5156: 3 [ 1175/ 1327], train_loss/perplexity = 5.36234188/213.2237091 secs/batch = 0.6166s, grad.norm=0.60029954\n",
      "  5161: 3 [ 1180/ 1327], train_loss/perplexity = 5.25354767/191.2435303 secs/batch = 0.6154s, grad.norm=0.46554068\n",
      "  5166: 3 [ 1185/ 1327], train_loss/perplexity = 5.52453184/250.7689056 secs/batch = 0.6097s, grad.norm=0.49046642\n",
      "  5171: 3 [ 1190/ 1327], train_loss/perplexity = 5.53712749/253.9474792 secs/batch = 0.6169s, grad.norm=0.53881210\n",
      "  5176: 3 [ 1195/ 1327], train_loss/perplexity = 5.39483070/220.2648468 secs/batch = 0.6115s, grad.norm=0.57931370\n",
      "  5181: 3 [ 1200/ 1327], train_loss/perplexity = 5.29407501/199.1533203 secs/batch = 0.6168s, grad.norm=0.49775520\n",
      "  5186: 3 [ 1205/ 1327], train_loss/perplexity = 5.43897104/230.2051849 secs/batch = 0.6184s, grad.norm=0.56469840\n",
      "  5191: 3 [ 1210/ 1327], train_loss/perplexity = 5.21393538/183.8160248 secs/batch = 0.6167s, grad.norm=0.57855487\n",
      "  5196: 3 [ 1215/ 1327], train_loss/perplexity = 5.21421194/183.8668671 secs/batch = 0.6149s, grad.norm=0.51717597\n",
      "  5201: 3 [ 1220/ 1327], train_loss/perplexity = 5.35329103/211.3025513 secs/batch = 0.6599s, grad.norm=0.55408466\n",
      "  5206: 3 [ 1225/ 1327], train_loss/perplexity = 5.37939548/216.8911285 secs/batch = 0.6165s, grad.norm=0.58352089\n",
      "  5211: 3 [ 1230/ 1327], train_loss/perplexity = 5.45162821/233.1374512 secs/batch = 0.6128s, grad.norm=0.52816010\n",
      "  5216: 3 [ 1235/ 1327], train_loss/perplexity = 5.49251223/242.8665771 secs/batch = 0.6189s, grad.norm=0.47229028\n",
      "  5221: 3 [ 1240/ 1327], train_loss/perplexity = 5.60260582/271.1320190 secs/batch = 0.6123s, grad.norm=0.50343567\n",
      "  5226: 3 [ 1245/ 1327], train_loss/perplexity = 5.34404516/209.3578796 secs/batch = 0.6063s, grad.norm=0.47217962\n",
      "  5231: 3 [ 1250/ 1327], train_loss/perplexity = 5.60904980/272.8848267 secs/batch = 0.6086s, grad.norm=0.56706393\n",
      "  5236: 3 [ 1255/ 1327], train_loss/perplexity = 5.46677399/236.6953735 secs/batch = 0.6058s, grad.norm=0.53271669\n",
      "  5241: 3 [ 1260/ 1327], train_loss/perplexity = 5.57886076/264.7698059 secs/batch = 0.6220s, grad.norm=0.53958410\n",
      "  5246: 3 [ 1265/ 1327], train_loss/perplexity = 5.58349133/265.9986877 secs/batch = 0.6128s, grad.norm=0.57713628\n",
      "  5251: 3 [ 1270/ 1327], train_loss/perplexity = 5.44500923/231.5994110 secs/batch = 0.6039s, grad.norm=0.52721184\n",
      "  5256: 3 [ 1275/ 1327], train_loss/perplexity = 5.72962236/307.8529968 secs/batch = 0.6047s, grad.norm=0.52358860\n",
      "  5261: 3 [ 1280/ 1327], train_loss/perplexity = 5.51446724/248.2576752 secs/batch = 0.6248s, grad.norm=0.66374862\n",
      "  5266: 3 [ 1285/ 1327], train_loss/perplexity = 5.41450453/224.6412201 secs/batch = 0.6191s, grad.norm=0.51527047\n",
      "  5271: 3 [ 1290/ 1327], train_loss/perplexity = 5.53455257/253.2944336 secs/batch = 0.6116s, grad.norm=0.51675308\n",
      "  5276: 3 [ 1295/ 1327], train_loss/perplexity = 5.68730497/295.0972595 secs/batch = 0.6087s, grad.norm=0.49841836\n",
      "  5281: 3 [ 1300/ 1327], train_loss/perplexity = 5.59886837/270.1205444 secs/batch = 0.6136s, grad.norm=0.49095064\n",
      "  5286: 3 [ 1305/ 1327], train_loss/perplexity = 5.79890728/329.9388428 secs/batch = 0.6105s, grad.norm=0.56970465\n",
      "  5291: 3 [ 1310/ 1327], train_loss/perplexity = 5.97948360/395.2362061 secs/batch = 0.6124s, grad.norm=0.45673594\n",
      "  5296: 3 [ 1315/ 1327], train_loss/perplexity = 5.86388445/352.0891724 secs/batch = 0.6141s, grad.norm=0.65154308\n",
      "  5301: 3 [ 1320/ 1327], train_loss/perplexity = 5.77347565/321.6537476 secs/batch = 0.6092s, grad.norm=0.45265096\n",
      "  5306: 3 [ 1325/ 1327], train_loss/perplexity = 5.65980101/287.0915222 secs/batch = 0.6142s, grad.norm=0.49848276\n",
      "Epoch training time: 815.3142948150635\n",
      "Saved char model cv/epoch003_5.6453.model\n",
      "  5313: 4 [    5/ 1327], train_loss/perplexity = 5.83424902/341.8079529 secs/batch = 0.6107s, grad.norm=0.49125573\n",
      "  5318: 4 [   10/ 1327], train_loss/perplexity = 5.41597986/224.9728851 secs/batch = 0.6081s, grad.norm=0.52061224\n",
      "  5323: 4 [   15/ 1327], train_loss/perplexity = 5.40068531/221.5581970 secs/batch = 0.6140s, grad.norm=0.46846271\n",
      "  5328: 4 [   20/ 1327], train_loss/perplexity = 5.84621668/345.9231567 secs/batch = 0.6073s, grad.norm=0.57467532\n",
      "  5333: 4 [   25/ 1327], train_loss/perplexity = 5.74538946/312.7453918 secs/batch = 0.6099s, grad.norm=0.48793826\n",
      "  5338: 4 [   30/ 1327], train_loss/perplexity = 5.57752180/264.4155273 secs/batch = 0.6156s, grad.norm=0.47396356\n",
      "  5343: 4 [   35/ 1327], train_loss/perplexity = 5.49695826/243.9487762 secs/batch = 0.6179s, grad.norm=0.69366187\n",
      "  5348: 4 [   40/ 1327], train_loss/perplexity = 5.53604507/253.6727600 secs/batch = 0.6644s, grad.norm=0.51038754\n",
      "  5353: 4 [   45/ 1327], train_loss/perplexity = 5.20286989/181.7932281 secs/batch = 0.6238s, grad.norm=0.50391346\n",
      "  5358: 4 [   50/ 1327], train_loss/perplexity = 5.67679405/292.0117493 secs/batch = 0.6126s, grad.norm=0.56475085\n",
      "  5363: 4 [   55/ 1327], train_loss/perplexity = 5.49328899/243.0552979 secs/batch = 0.6122s, grad.norm=0.45158118\n",
      "  5368: 4 [   60/ 1327], train_loss/perplexity = 5.72096491/305.1992798 secs/batch = 0.6089s, grad.norm=0.48972619\n",
      "  5373: 4 [   65/ 1327], train_loss/perplexity = 5.32673120/205.7642670 secs/batch = 0.6144s, grad.norm=0.49929154\n",
      "  5378: 4 [   70/ 1327], train_loss/perplexity = 5.21859598/184.6747131 secs/batch = 0.6063s, grad.norm=0.63977379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5383: 4 [   75/ 1327], train_loss/perplexity = 5.24051046/188.7664337 secs/batch = 0.6112s, grad.norm=0.56104445\n",
      "  5388: 4 [   80/ 1327], train_loss/perplexity = 5.51377535/248.0859680 secs/batch = 0.6124s, grad.norm=0.46878296\n",
      "  5393: 4 [   85/ 1327], train_loss/perplexity = 5.62146139/276.2928467 secs/batch = 0.6082s, grad.norm=0.63840741\n",
      "  5398: 4 [   90/ 1327], train_loss/perplexity = 5.57598019/264.0082092 secs/batch = 0.6135s, grad.norm=0.50947481\n",
      "  5403: 4 [   95/ 1327], train_loss/perplexity = 5.43243837/228.7062378 secs/batch = 0.6069s, grad.norm=0.55942231\n",
      "  5408: 4 [  100/ 1327], train_loss/perplexity = 5.63951969/281.3275757 secs/batch = 0.6135s, grad.norm=0.62366778\n",
      "  5413: 4 [  105/ 1327], train_loss/perplexity = 5.70314121/299.8076782 secs/batch = 0.6118s, grad.norm=0.61698580\n",
      "  5418: 4 [  110/ 1327], train_loss/perplexity = 5.45888519/234.8354797 secs/batch = 0.6149s, grad.norm=0.44861370\n",
      "  5423: 4 [  115/ 1327], train_loss/perplexity = 5.31438971/203.2404327 secs/batch = 0.6144s, grad.norm=0.54412407\n",
      "  5428: 4 [  120/ 1327], train_loss/perplexity = 5.45180273/233.1781464 secs/batch = 0.6139s, grad.norm=0.55908078\n",
      "  5433: 4 [  125/ 1327], train_loss/perplexity = 5.71255732/302.6440430 secs/batch = 0.6143s, grad.norm=0.77613443\n",
      "  5438: 4 [  130/ 1327], train_loss/perplexity = 5.52168369/250.0556946 secs/batch = 0.6078s, grad.norm=0.51016390\n",
      "  5443: 4 [  135/ 1327], train_loss/perplexity = 5.47537994/238.7411652 secs/batch = 0.6154s, grad.norm=0.46597227\n",
      "  5448: 4 [  140/ 1327], train_loss/perplexity = 5.74807310/313.5858154 secs/batch = 0.6191s, grad.norm=0.54061538\n",
      "  5453: 4 [  145/ 1327], train_loss/perplexity = 5.72717381/307.1001282 secs/batch = 0.6142s, grad.norm=0.48249757\n",
      "  5458: 4 [  150/ 1327], train_loss/perplexity = 5.61435747/274.3370667 secs/batch = 0.6105s, grad.norm=0.50719965\n",
      "  5463: 4 [  155/ 1327], train_loss/perplexity = 5.81727791/336.0560303 secs/batch = 0.6298s, grad.norm=0.53317285\n",
      "  5468: 4 [  160/ 1327], train_loss/perplexity = 5.43338251/228.9222717 secs/batch = 0.6144s, grad.norm=0.51643729\n",
      "  5473: 4 [  165/ 1327], train_loss/perplexity = 5.68795729/295.2898254 secs/batch = 0.6148s, grad.norm=0.51179379\n",
      "  5478: 4 [  170/ 1327], train_loss/perplexity = 5.59367943/268.7225342 secs/batch = 0.6452s, grad.norm=0.53018218\n",
      "  5483: 4 [  175/ 1327], train_loss/perplexity = 5.68130589/293.3322449 secs/batch = 0.6145s, grad.norm=0.48381919\n",
      "  5488: 4 [  180/ 1327], train_loss/perplexity = 5.67914152/292.6980591 secs/batch = 0.6116s, grad.norm=0.56664276\n",
      "  5493: 4 [  185/ 1327], train_loss/perplexity = 5.85226202/348.0207214 secs/batch = 0.6245s, grad.norm=0.52929258\n",
      "  5498: 4 [  190/ 1327], train_loss/perplexity = 5.42327309/226.6196594 secs/batch = 0.6068s, grad.norm=0.45813739\n",
      "  5503: 4 [  195/ 1327], train_loss/perplexity = 5.52020884/249.6871796 secs/batch = 0.6059s, grad.norm=0.47855619\n",
      "  5508: 4 [  200/ 1327], train_loss/perplexity = 5.57797337/264.5349426 secs/batch = 0.6169s, grad.norm=0.60401809\n",
      "  5513: 4 [  205/ 1327], train_loss/perplexity = 5.53147888/252.5170746 secs/batch = 0.6119s, grad.norm=0.46771464\n",
      "  5518: 4 [  210/ 1327], train_loss/perplexity = 5.50142002/245.0396423 secs/batch = 0.6100s, grad.norm=0.49601269\n",
      "  5523: 4 [  215/ 1327], train_loss/perplexity = 5.56647921/261.5117493 secs/batch = 0.6117s, grad.norm=0.43650427\n",
      "  5528: 4 [  220/ 1327], train_loss/perplexity = 5.66513634/288.6273193 secs/batch = 0.6108s, grad.norm=0.47436112\n",
      "  5533: 4 [  225/ 1327], train_loss/perplexity = 5.84016228/343.8351440 secs/batch = 0.6137s, grad.norm=0.61293650\n",
      "  5538: 4 [  230/ 1327], train_loss/perplexity = 5.63637543/280.4443970 secs/batch = 0.6110s, grad.norm=0.57276529\n",
      "  5543: 4 [  235/ 1327], train_loss/perplexity = 5.53258562/252.7967072 secs/batch = 0.6480s, grad.norm=0.51640576\n",
      "  5548: 4 [  240/ 1327], train_loss/perplexity = 5.39400291/220.0825958 secs/batch = 0.6117s, grad.norm=0.71954185\n",
      "  5553: 4 [  245/ 1327], train_loss/perplexity = 5.66996813/290.0252991 secs/batch = 0.6122s, grad.norm=0.48206711\n",
      "  5558: 4 [  250/ 1327], train_loss/perplexity = 5.33728218/207.9467773 secs/batch = 0.6139s, grad.norm=0.45055029\n",
      "  5563: 4 [  255/ 1327], train_loss/perplexity = 5.49265194/242.9005127 secs/batch = 0.6177s, grad.norm=0.45419791\n",
      "  5568: 4 [  260/ 1327], train_loss/perplexity = 5.78945971/326.8363953 secs/batch = 0.6188s, grad.norm=0.50771463\n",
      "  5573: 4 [  265/ 1327], train_loss/perplexity = 5.68052053/293.1019592 secs/batch = 0.6164s, grad.norm=0.54575658\n",
      "  5578: 4 [  270/ 1327], train_loss/perplexity = 5.74574757/312.8574219 secs/batch = 0.6153s, grad.norm=0.49280399\n",
      "  5583: 4 [  275/ 1327], train_loss/perplexity = 5.93683815/378.7355347 secs/batch = 0.6177s, grad.norm=0.57687503\n",
      "  5588: 4 [  280/ 1327], train_loss/perplexity = 5.70451975/300.2212524 secs/batch = 0.6164s, grad.norm=0.59933758\n",
      "  5593: 4 [  285/ 1327], train_loss/perplexity = 5.81119823/334.0191345 secs/batch = 0.6157s, grad.norm=0.59458464\n",
      "  5598: 4 [  290/ 1327], train_loss/perplexity = 5.68827629/295.3840332 secs/batch = 0.6119s, grad.norm=0.53735352\n",
      "  5603: 4 [  295/ 1327], train_loss/perplexity = 5.40317726/222.1110077 secs/batch = 0.6186s, grad.norm=0.52144068\n",
      "  5608: 4 [  300/ 1327], train_loss/perplexity = 5.07362604/159.7525482 secs/batch = 0.6146s, grad.norm=0.51874781\n",
      "  5613: 4 [  305/ 1327], train_loss/perplexity = 5.49686289/243.9255066 secs/batch = 0.6129s, grad.norm=0.62150794\n",
      "  5618: 4 [  310/ 1327], train_loss/perplexity = 5.55426216/258.3362732 secs/batch = 0.6128s, grad.norm=0.58206618\n",
      "  5623: 4 [  315/ 1327], train_loss/perplexity = 5.27558613/195.5050354 secs/batch = 0.6153s, grad.norm=0.50111514\n",
      "  5628: 4 [  320/ 1327], train_loss/perplexity = 5.46831036/237.0593109 secs/batch = 0.6169s, grad.norm=0.57339931\n",
      "  5633: 4 [  325/ 1327], train_loss/perplexity = 5.20644188/182.4437408 secs/batch = 0.6121s, grad.norm=0.58195508\n",
      "  5638: 4 [  330/ 1327], train_loss/perplexity = 5.61914015/275.6522522 secs/batch = 0.6129s, grad.norm=0.56660730\n",
      "  5643: 4 [  335/ 1327], train_loss/perplexity = 4.84589577/127.2171860 secs/batch = 0.6097s, grad.norm=0.47574529\n",
      "  5648: 4 [  340/ 1327], train_loss/perplexity = 5.67088652/290.2917786 secs/batch = 0.6186s, grad.norm=0.49550846\n",
      "  5653: 4 [  345/ 1327], train_loss/perplexity = 5.54056931/254.8230286 secs/batch = 0.6130s, grad.norm=0.54100591\n",
      "  5658: 4 [  350/ 1327], train_loss/perplexity = 5.64571905/283.0770264 secs/batch = 0.6116s, grad.norm=0.53491592\n",
      "  5663: 4 [  355/ 1327], train_loss/perplexity = 5.77758884/322.9794922 secs/batch = 0.6092s, grad.norm=0.68996596\n",
      "  5668: 4 [  360/ 1327], train_loss/perplexity = 5.81635714/335.7467346 secs/batch = 0.6094s, grad.norm=0.49482095\n",
      "  5673: 4 [  365/ 1327], train_loss/perplexity = 5.68940687/295.7181702 secs/batch = 0.6608s, grad.norm=0.49554843\n",
      "  5678: 4 [  370/ 1327], train_loss/perplexity = 5.69161749/296.3726196 secs/batch = 0.6122s, grad.norm=0.54371023\n",
      "  5683: 4 [  375/ 1327], train_loss/perplexity = 5.13658571/170.1338959 secs/batch = 0.6132s, grad.norm=0.53142083\n",
      "  5688: 4 [  380/ 1327], train_loss/perplexity = 5.43948603/230.3237762 secs/batch = 0.6126s, grad.norm=0.48059577\n",
      "  5693: 4 [  385/ 1327], train_loss/perplexity = 5.54197216/255.1807556 secs/batch = 0.6160s, grad.norm=0.50039357\n",
      "  5698: 4 [  390/ 1327], train_loss/perplexity = 5.51463604/248.2995911 secs/batch = 0.6115s, grad.norm=0.48146716\n",
      "  5703: 4 [  395/ 1327], train_loss/perplexity = 5.79900837/329.9721985 secs/batch = 0.6159s, grad.norm=0.57808697\n",
      "  5708: 4 [  400/ 1327], train_loss/perplexity = 5.42111349/226.1307831 secs/batch = 0.6081s, grad.norm=0.52304512\n",
      "  5713: 4 [  405/ 1327], train_loss/perplexity = 5.76751471/319.7420959 secs/batch = 0.6087s, grad.norm=0.64038187\n",
      "  5718: 4 [  410/ 1327], train_loss/perplexity = 5.56969118/262.3530579 secs/batch = 0.6152s, grad.norm=0.49383035\n",
      "  5723: 4 [  415/ 1327], train_loss/perplexity = 5.40621805/222.7874146 secs/batch = 0.6092s, grad.norm=0.52467799\n",
      "  5728: 4 [  420/ 1327], train_loss/perplexity = 5.32354641/205.1100006 secs/batch = 0.6173s, grad.norm=0.51795661\n",
      "  5733: 4 [  425/ 1327], train_loss/perplexity = 5.66759872/289.3389282 secs/batch = 0.6143s, grad.norm=0.52802199\n",
      "  5738: 4 [  430/ 1327], train_loss/perplexity = 5.65081978/284.5246277 secs/batch = 0.6150s, grad.norm=0.49440384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5743: 4 [  435/ 1327], train_loss/perplexity = 5.70648432/300.8116455 secs/batch = 0.6086s, grad.norm=0.52403021\n",
      "  5748: 4 [  440/ 1327], train_loss/perplexity = 5.52943516/252.0015259 secs/batch = 0.6130s, grad.norm=0.59991646\n",
      "  5753: 4 [  445/ 1327], train_loss/perplexity = 5.54089832/254.9068909 secs/batch = 0.6163s, grad.norm=0.57099855\n",
      "  5758: 4 [  450/ 1327], train_loss/perplexity = 5.43235683/228.6875916 secs/batch = 0.6151s, grad.norm=0.48527333\n",
      "  5763: 4 [  455/ 1327], train_loss/perplexity = 5.28917980/198.1808167 secs/batch = 0.6116s, grad.norm=0.62239444\n",
      "  5768: 4 [  460/ 1327], train_loss/perplexity = 5.50812626/246.6884613 secs/batch = 0.6096s, grad.norm=0.55327982\n",
      "  5773: 4 [  465/ 1327], train_loss/perplexity = 5.45371151/233.6236572 secs/batch = 0.6047s, grad.norm=0.57136822\n",
      "  5778: 4 [  470/ 1327], train_loss/perplexity = 5.76584101/319.2073975 secs/batch = 0.6170s, grad.norm=0.48808366\n",
      "  5783: 4 [  475/ 1327], train_loss/perplexity = 5.54332924/255.5272980 secs/batch = 0.6131s, grad.norm=0.56544632\n",
      "  5788: 4 [  480/ 1327], train_loss/perplexity = 5.53105307/252.4095764 secs/batch = 0.6557s, grad.norm=0.48734638\n",
      "  5793: 4 [  485/ 1327], train_loss/perplexity = 5.46842623/237.0867767 secs/batch = 0.6071s, grad.norm=0.63549286\n",
      "  5798: 4 [  490/ 1327], train_loss/perplexity = 5.43340063/228.9264221 secs/batch = 0.6124s, grad.norm=0.51493698\n",
      "  5803: 4 [  495/ 1327], train_loss/perplexity = 5.28109932/196.5858612 secs/batch = 0.6095s, grad.norm=0.51438314\n",
      "  5808: 4 [  500/ 1327], train_loss/perplexity = 5.72584820/306.6932983 secs/batch = 0.6133s, grad.norm=0.57599133\n",
      "  5813: 4 [  505/ 1327], train_loss/perplexity = 5.46039438/235.1901550 secs/batch = 0.6099s, grad.norm=0.56229180\n",
      "  5818: 4 [  510/ 1327], train_loss/perplexity = 5.83127403/340.7925720 secs/batch = 0.6060s, grad.norm=0.54959971\n",
      "  5823: 4 [  515/ 1327], train_loss/perplexity = 5.52667284/251.3063812 secs/batch = 0.6094s, grad.norm=0.51743448\n",
      "  5828: 4 [  520/ 1327], train_loss/perplexity = 5.75789309/316.6804199 secs/batch = 0.6057s, grad.norm=0.56969798\n",
      "  5833: 4 [  525/ 1327], train_loss/perplexity = 5.38475180/218.0559692 secs/batch = 0.6154s, grad.norm=0.47876370\n",
      "  5838: 4 [  530/ 1327], train_loss/perplexity = 5.44659233/231.9663544 secs/batch = 0.6143s, grad.norm=0.53742105\n",
      "  5843: 4 [  535/ 1327], train_loss/perplexity = 5.54551458/256.0863342 secs/batch = 0.6121s, grad.norm=0.57940352\n",
      "  5848: 4 [  540/ 1327], train_loss/perplexity = 5.60193205/270.9494019 secs/batch = 0.6123s, grad.norm=0.46197250\n",
      "  5853: 4 [  545/ 1327], train_loss/perplexity = 5.73702526/310.1404419 secs/batch = 0.6122s, grad.norm=0.46753848\n",
      "  5858: 4 [  550/ 1327], train_loss/perplexity = 5.61621904/274.8482361 secs/batch = 0.6222s, grad.norm=0.52093726\n",
      "  5863: 4 [  555/ 1327], train_loss/perplexity = 5.51480484/248.3415070 secs/batch = 0.6186s, grad.norm=0.57913935\n",
      "  5868: 4 [  560/ 1327], train_loss/perplexity = 5.53050137/252.2703552 secs/batch = 0.6520s, grad.norm=0.51786548\n",
      "  5873: 4 [  565/ 1327], train_loss/perplexity = 5.56288624/260.5738220 secs/batch = 0.6233s, grad.norm=0.54296559\n",
      "  5878: 4 [  570/ 1327], train_loss/perplexity = 5.42312384/226.5858307 secs/batch = 0.6142s, grad.norm=0.53343755\n",
      "  5883: 4 [  575/ 1327], train_loss/perplexity = 5.36278820/213.3188934 secs/batch = 0.6073s, grad.norm=0.49260080\n",
      "  5888: 4 [  580/ 1327], train_loss/perplexity = 5.74562883/312.8202820 secs/batch = 0.6029s, grad.norm=0.79136807\n",
      "  5893: 4 [  585/ 1327], train_loss/perplexity = 5.34709072/209.9964752 secs/batch = 0.6130s, grad.norm=0.51720989\n",
      "  5898: 4 [  590/ 1327], train_loss/perplexity = 5.60692644/272.3059998 secs/batch = 0.6124s, grad.norm=0.51440161\n",
      "  5903: 4 [  595/ 1327], train_loss/perplexity = 5.59009123/267.7600403 secs/batch = 0.6151s, grad.norm=0.75231898\n",
      "  5908: 4 [  600/ 1327], train_loss/perplexity = 5.76455879/318.7983704 secs/batch = 0.6146s, grad.norm=0.46004465\n",
      "  5913: 4 [  605/ 1327], train_loss/perplexity = 5.71757841/304.1674500 secs/batch = 0.6129s, grad.norm=0.51926988\n",
      "  5918: 4 [  610/ 1327], train_loss/perplexity = 5.82391787/338.2948608 secs/batch = 0.6165s, grad.norm=0.57145578\n",
      "  5923: 4 [  615/ 1327], train_loss/perplexity = 5.21326876/183.6935272 secs/batch = 0.6117s, grad.norm=0.49799171\n",
      "  5928: 4 [  620/ 1327], train_loss/perplexity = 5.57284641/263.1821594 secs/batch = 0.6098s, grad.norm=0.54607826\n",
      "  5933: 4 [  625/ 1327], train_loss/perplexity = 5.72119427/305.2692871 secs/batch = 0.6105s, grad.norm=0.47914353\n",
      "  5938: 4 [  630/ 1327], train_loss/perplexity = 5.67355776/291.0682373 secs/batch = 0.6138s, grad.norm=0.47388011\n",
      "  5943: 4 [  635/ 1327], train_loss/perplexity = 5.45215797/233.2610016 secs/batch = 0.6187s, grad.norm=0.50642502\n",
      "  5948: 4 [  640/ 1327], train_loss/perplexity = 5.56650782/261.5192261 secs/batch = 0.6145s, grad.norm=0.60581064\n",
      "  5953: 4 [  645/ 1327], train_loss/perplexity = 5.72943449/307.7951660 secs/batch = 0.6097s, grad.norm=0.49272814\n",
      "  5958: 4 [  650/ 1327], train_loss/perplexity = 5.41113853/223.8863525 secs/batch = 0.6129s, grad.norm=0.65981221\n",
      "  5963: 4 [  655/ 1327], train_loss/perplexity = 5.48008871/239.8679810 secs/batch = 0.6149s, grad.norm=0.52767813\n",
      "  5968: 4 [  660/ 1327], train_loss/perplexity = 5.34324646/209.1907349 secs/batch = 0.6122s, grad.norm=0.48491892\n",
      "  5973: 4 [  665/ 1327], train_loss/perplexity = 5.61004353/273.1561279 secs/batch = 0.6144s, grad.norm=0.55889946\n",
      "  5978: 4 [  670/ 1327], train_loss/perplexity = 5.42619610/227.2830353 secs/batch = 0.6119s, grad.norm=0.48008192\n",
      "  5983: 4 [  675/ 1327], train_loss/perplexity = 5.21337175/183.7124481 secs/batch = 0.6251s, grad.norm=0.59114504\n",
      "  5988: 4 [  680/ 1327], train_loss/perplexity = 5.58530712/266.4821167 secs/batch = 0.6176s, grad.norm=0.52306813\n",
      "  5993: 4 [  685/ 1327], train_loss/perplexity = 5.59889460/270.1276550 secs/batch = 0.6100s, grad.norm=0.62602836\n",
      "  5998: 4 [  690/ 1327], train_loss/perplexity = 5.61206865/273.7098694 secs/batch = 0.6112s, grad.norm=0.46633139\n",
      "  6003: 4 [  695/ 1327], train_loss/perplexity = 5.49754667/244.0923615 secs/batch = 0.6079s, grad.norm=0.49668324\n",
      "  6008: 4 [  700/ 1327], train_loss/perplexity = 5.62761736/277.9989624 secs/batch = 0.6110s, grad.norm=0.52684605\n",
      "  6013: 4 [  705/ 1327], train_loss/perplexity = 5.36334562/213.4378357 secs/batch = 0.6104s, grad.norm=0.53110415\n",
      "  6018: 4 [  710/ 1327], train_loss/perplexity = 5.45683002/234.3533478 secs/batch = 0.6156s, grad.norm=0.48433700\n",
      "  6023: 4 [  715/ 1327], train_loss/perplexity = 5.41447735/224.6351166 secs/batch = 0.6107s, grad.norm=0.54538804\n",
      "  6028: 4 [  720/ 1327], train_loss/perplexity = 5.51236773/247.7369995 secs/batch = 0.6185s, grad.norm=0.53908837\n",
      "  6033: 4 [  725/ 1327], train_loss/perplexity = 5.21788216/184.5429382 secs/batch = 0.6425s, grad.norm=0.52845061\n",
      "  6038: 4 [  730/ 1327], train_loss/perplexity = 5.41729212/225.2682953 secs/batch = 0.6139s, grad.norm=0.60678345\n",
      "  6043: 4 [  735/ 1327], train_loss/perplexity = 5.54013157/254.7115021 secs/batch = 0.6147s, grad.norm=0.50754833\n",
      "  6048: 4 [  740/ 1327], train_loss/perplexity = 5.02102232/151.5661774 secs/batch = 0.6157s, grad.norm=0.52538580\n",
      "  6053: 4 [  745/ 1327], train_loss/perplexity = 5.46764803/236.9023438 secs/batch = 0.6188s, grad.norm=0.51277232\n",
      "  6058: 4 [  750/ 1327], train_loss/perplexity = 5.37987328/216.9947815 secs/batch = 0.6063s, grad.norm=0.65405285\n",
      "  6063: 4 [  755/ 1327], train_loss/perplexity = 5.38928509/219.0467377 secs/batch = 0.6458s, grad.norm=0.49559602\n",
      "  6068: 4 [  760/ 1327], train_loss/perplexity = 5.25161600/190.8744659 secs/batch = 0.6075s, grad.norm=0.53365785\n",
      "  6073: 4 [  765/ 1327], train_loss/perplexity = 5.28661823/197.6738129 secs/batch = 0.6147s, grad.norm=0.49836263\n",
      "  6078: 4 [  770/ 1327], train_loss/perplexity = 5.34346437/209.2363281 secs/batch = 0.6117s, grad.norm=0.68794233\n",
      "  6083: 4 [  775/ 1327], train_loss/perplexity = 5.41996098/225.8703003 secs/batch = 0.6124s, grad.norm=0.54479533\n",
      "  6088: 4 [  780/ 1327], train_loss/perplexity = 5.56833410/261.9972839 secs/batch = 0.6077s, grad.norm=0.48489392\n",
      "  6093: 4 [  785/ 1327], train_loss/perplexity = 5.43777704/229.9304962 secs/batch = 0.6085s, grad.norm=0.69714171\n",
      "  6098: 4 [  790/ 1327], train_loss/perplexity = 5.31010389/202.3712463 secs/batch = 0.6088s, grad.norm=0.63713902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6103: 4 [  795/ 1327], train_loss/perplexity = 5.53575563/253.5993500 secs/batch = 0.6135s, grad.norm=0.48121077\n",
      "  6108: 4 [  800/ 1327], train_loss/perplexity = 5.52590084/251.1124420 secs/batch = 0.6190s, grad.norm=0.50415021\n",
      "  6113: 4 [  805/ 1327], train_loss/perplexity = 5.78924227/326.7653198 secs/batch = 0.6168s, grad.norm=0.49381351\n",
      "  6118: 4 [  810/ 1327], train_loss/perplexity = 5.52709103/251.4114990 secs/batch = 0.6159s, grad.norm=0.54432112\n",
      "  6123: 4 [  815/ 1327], train_loss/perplexity = 5.37596321/216.1479645 secs/batch = 0.6108s, grad.norm=0.51608461\n",
      "  6128: 4 [  820/ 1327], train_loss/perplexity = 5.04940557/155.9297485 secs/batch = 0.6165s, grad.norm=0.51648396\n",
      "  6133: 4 [  825/ 1327], train_loss/perplexity = 5.24279833/189.1988068 secs/batch = 0.6165s, grad.norm=0.52405494\n",
      "  6138: 4 [  830/ 1327], train_loss/perplexity = 5.12398624/168.0037384 secs/batch = 0.6141s, grad.norm=0.49134493\n",
      "  6143: 4 [  835/ 1327], train_loss/perplexity = 5.38627052/218.3873901 secs/batch = 0.6179s, grad.norm=0.58616412\n",
      "  6148: 4 [  840/ 1327], train_loss/perplexity = 5.56210661/260.3707581 secs/batch = 0.6189s, grad.norm=0.56558794\n",
      "  6153: 4 [  845/ 1327], train_loss/perplexity = 5.33596373/207.6727905 secs/batch = 0.6149s, grad.norm=0.52008951\n",
      "  6158: 4 [  850/ 1327], train_loss/perplexity = 5.38574171/218.2719421 secs/batch = 0.6137s, grad.norm=0.49356785\n",
      "  6163: 4 [  855/ 1327], train_loss/perplexity = 5.36940956/214.7360382 secs/batch = 0.6173s, grad.norm=0.56673503\n",
      "  6168: 4 [  860/ 1327], train_loss/perplexity = 5.14179611/171.0226746 secs/batch = 0.6090s, grad.norm=0.50630772\n",
      "  6173: 4 [  865/ 1327], train_loss/perplexity = 5.55444813/258.3843384 secs/batch = 0.6099s, grad.norm=0.49918225\n",
      "  6178: 4 [  870/ 1327], train_loss/perplexity = 5.58857155/267.3534546 secs/batch = 0.6115s, grad.norm=0.54542303\n",
      "  6183: 4 [  875/ 1327], train_loss/perplexity = 5.19197035/179.8225098 secs/batch = 0.6111s, grad.norm=0.55994302\n",
      "  6188: 4 [  880/ 1327], train_loss/perplexity = 5.27928925/196.2303467 secs/batch = 0.6166s, grad.norm=0.54450327\n",
      "  6193: 4 [  885/ 1327], train_loss/perplexity = 5.32660246/205.7377777 secs/batch = 0.6099s, grad.norm=0.54738379\n",
      "  6198: 4 [  890/ 1327], train_loss/perplexity = 5.51979303/249.5833740 secs/batch = 0.6092s, grad.norm=0.52527553\n",
      "  6203: 4 [  895/ 1327], train_loss/perplexity = 5.53905153/254.4365540 secs/batch = 0.6143s, grad.norm=0.53889823\n",
      "  6208: 4 [  900/ 1327], train_loss/perplexity = 5.49785519/244.1676788 secs/batch = 0.6119s, grad.norm=0.59315872\n",
      "  6213: 4 [  905/ 1327], train_loss/perplexity = 5.28670311/197.6905823 secs/batch = 0.6127s, grad.norm=0.54401106\n",
      "  6218: 4 [  910/ 1327], train_loss/perplexity = 5.37697315/216.3663788 secs/batch = 0.6088s, grad.norm=0.63581997\n",
      "  6223: 4 [  915/ 1327], train_loss/perplexity = 5.68499708/294.4169922 secs/batch = 0.6159s, grad.norm=0.56463170\n",
      "  6228: 4 [  920/ 1327], train_loss/perplexity = 5.69481707/297.3223877 secs/batch = 0.6249s, grad.norm=0.55208355\n",
      "  6233: 4 [  925/ 1327], train_loss/perplexity = 5.41423035/224.5796356 secs/batch = 0.6157s, grad.norm=0.49364018\n",
      "  6238: 4 [  930/ 1327], train_loss/perplexity = 5.36585379/213.9738464 secs/batch = 0.6142s, grad.norm=0.55673915\n",
      "  6243: 4 [  935/ 1327], train_loss/perplexity = 5.47363091/238.3239594 secs/batch = 0.6125s, grad.norm=0.48170772\n",
      "  6248: 4 [  940/ 1327], train_loss/perplexity = 5.46346664/235.9138336 secs/batch = 0.6134s, grad.norm=0.70666915\n",
      "  6253: 4 [  945/ 1327], train_loss/perplexity = 5.62704515/277.8399353 secs/batch = 0.6133s, grad.norm=0.51702255\n",
      "  6258: 4 [  950/ 1327], train_loss/perplexity = 5.39124298/219.4760132 secs/batch = 0.6321s, grad.norm=0.53416163\n",
      "  6263: 4 [  955/ 1327], train_loss/perplexity = 5.51816463/249.1772919 secs/batch = 0.6146s, grad.norm=0.50166225\n",
      "  6268: 4 [  960/ 1327], train_loss/perplexity = 5.79942703/330.1103516 secs/batch = 0.6144s, grad.norm=0.50918669\n",
      "  6273: 4 [  965/ 1327], train_loss/perplexity = 5.53817749/254.2142639 secs/batch = 0.6073s, grad.norm=0.58308667\n",
      "  6278: 4 [  970/ 1327], train_loss/perplexity = 5.67337465/291.0149536 secs/batch = 0.6463s, grad.norm=0.46892715\n",
      "  6283: 4 [  975/ 1327], train_loss/perplexity = 5.44922781/232.5785065 secs/batch = 0.6008s, grad.norm=0.55343926\n",
      "  6288: 4 [  980/ 1327], train_loss/perplexity = 5.25565481/191.6469421 secs/batch = 0.6117s, grad.norm=0.48588112\n",
      "  6293: 4 [  985/ 1327], train_loss/perplexity = 5.48616123/241.3290253 secs/batch = 0.6106s, grad.norm=0.60901517\n",
      "  6298: 4 [  990/ 1327], train_loss/perplexity = 5.67366123/291.0983582 secs/batch = 0.6090s, grad.norm=0.58713377\n",
      "  6303: 4 [  995/ 1327], train_loss/perplexity = 5.56078959/260.0280762 secs/batch = 0.6149s, grad.norm=0.60229886\n",
      "  6308: 4 [ 1000/ 1327], train_loss/perplexity = 5.12884808/168.8225403 secs/batch = 0.6107s, grad.norm=0.50013196\n",
      "  6313: 4 [ 1005/ 1327], train_loss/perplexity = 5.55269718/257.9323120 secs/batch = 0.6082s, grad.norm=0.51058954\n",
      "  6318: 4 [ 1010/ 1327], train_loss/perplexity = 5.12994862/169.0084381 secs/batch = 0.6116s, grad.norm=0.52955300\n",
      "  6323: 4 [ 1015/ 1327], train_loss/perplexity = 5.52833462/251.7243500 secs/batch = 0.6238s, grad.norm=0.55817038\n",
      "  6328: 4 [ 1020/ 1327], train_loss/perplexity = 5.76280785/318.2406311 secs/batch = 0.6060s, grad.norm=0.57452738\n",
      "  6333: 4 [ 1025/ 1327], train_loss/perplexity = 5.54936743/257.0748901 secs/batch = 0.6120s, grad.norm=0.51897800\n",
      "  6338: 4 [ 1030/ 1327], train_loss/perplexity = 5.44729090/232.1284485 secs/batch = 0.6089s, grad.norm=0.51260191\n",
      "  6343: 4 [ 1035/ 1327], train_loss/perplexity = 5.28736210/197.8209076 secs/batch = 0.6047s, grad.norm=0.53912377\n",
      "  6348: 4 [ 1040/ 1327], train_loss/perplexity = 5.53786707/254.1353607 secs/batch = 0.6092s, grad.norm=0.51059258\n",
      "  6353: 4 [ 1045/ 1327], train_loss/perplexity = 5.19129944/179.7019196 secs/batch = 0.6114s, grad.norm=0.53348505\n",
      "  6358: 4 [ 1050/ 1327], train_loss/perplexity = 5.28321600/197.0024109 secs/batch = 0.6093s, grad.norm=0.59330970\n",
      "  6363: 4 [ 1055/ 1327], train_loss/perplexity = 5.53389359/253.1275635 secs/batch = 0.6055s, grad.norm=0.59717393\n",
      "  6368: 4 [ 1060/ 1327], train_loss/perplexity = 5.13554192/169.9563904 secs/batch = 0.6043s, grad.norm=0.56847715\n",
      "  6373: 4 [ 1065/ 1327], train_loss/perplexity = 5.21360302/183.7549438 secs/batch = 0.6152s, grad.norm=0.50530946\n",
      "  6378: 4 [ 1070/ 1327], train_loss/perplexity = 5.55367374/258.1843262 secs/batch = 0.6085s, grad.norm=0.49845484\n",
      "  6383: 4 [ 1075/ 1327], train_loss/perplexity = 5.31760693/203.8953552 secs/batch = 0.6132s, grad.norm=0.59379274\n",
      "  6388: 4 [ 1080/ 1327], train_loss/perplexity = 5.18379831/178.3589935 secs/batch = 0.6106s, grad.norm=0.53149551\n",
      "  6393: 4 [ 1085/ 1327], train_loss/perplexity = 5.15067339/172.5476379 secs/batch = 0.6119s, grad.norm=0.63582295\n",
      "  6398: 4 [ 1090/ 1327], train_loss/perplexity = 5.33848572/208.1972046 secs/batch = 0.6073s, grad.norm=0.52110815\n",
      "  6403: 4 [ 1095/ 1327], train_loss/perplexity = 5.43626547/229.5831909 secs/batch = 0.6107s, grad.norm=0.58857685\n",
      "  6408: 4 [ 1100/ 1327], train_loss/perplexity = 5.44487190/231.5676117 secs/batch = 0.6174s, grad.norm=0.52714330\n",
      "  6413: 4 [ 1105/ 1327], train_loss/perplexity = 5.23476219/187.6844635 secs/batch = 0.6064s, grad.norm=0.55083519\n",
      "  6418: 4 [ 1110/ 1327], train_loss/perplexity = 5.76021051/317.4151306 secs/batch = 0.6131s, grad.norm=0.55077457\n",
      "  6423: 4 [ 1115/ 1327], train_loss/perplexity = 5.29696751/199.7302094 secs/batch = 0.6133s, grad.norm=0.51783872\n",
      "  6428: 4 [ 1120/ 1327], train_loss/perplexity = 5.43742990/229.8506927 secs/batch = 0.6043s, grad.norm=0.49413741\n",
      "  6433: 4 [ 1125/ 1327], train_loss/perplexity = 5.66310453/288.0415039 secs/batch = 0.6061s, grad.norm=0.62074214\n",
      "  6438: 4 [ 1130/ 1327], train_loss/perplexity = 5.34924507/210.4493561 secs/batch = 0.6125s, grad.norm=0.48717922\n",
      "  6443: 4 [ 1135/ 1327], train_loss/perplexity = 5.38094473/217.2274017 secs/batch = 0.6054s, grad.norm=0.51842350\n",
      "  6448: 4 [ 1140/ 1327], train_loss/perplexity = 5.63555861/280.2153931 secs/batch = 0.6172s, grad.norm=0.59704244\n",
      "  6453: 4 [ 1145/ 1327], train_loss/perplexity = 5.39859104/221.0946808 secs/batch = 0.6172s, grad.norm=0.69471508\n",
      "  6458: 4 [ 1150/ 1327], train_loss/perplexity = 5.33073616/206.5899963 secs/batch = 0.6123s, grad.norm=0.49749029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6463: 4 [ 1155/ 1327], train_loss/perplexity = 5.44198990/230.9011993 secs/batch = 0.6097s, grad.norm=0.49633390\n",
      "  6468: 4 [ 1160/ 1327], train_loss/perplexity = 5.42118549/226.1470642 secs/batch = 0.6138s, grad.norm=0.54904509\n",
      "  6473: 4 [ 1165/ 1327], train_loss/perplexity = 5.49651527/243.8407288 secs/batch = 0.6211s, grad.norm=0.55713212\n",
      "  6478: 4 [ 1170/ 1327], train_loss/perplexity = 5.35519028/211.7042542 secs/batch = 0.6139s, grad.norm=0.54359239\n",
      "  6483: 4 [ 1175/ 1327], train_loss/perplexity = 5.14836025/172.1489716 secs/batch = 0.6124s, grad.norm=0.53501976\n",
      "  6488: 4 [ 1180/ 1327], train_loss/perplexity = 5.11197519/165.9979095 secs/batch = 0.6119s, grad.norm=0.53484714\n",
      "  6493: 4 [ 1185/ 1327], train_loss/perplexity = 5.38818407/218.8056946 secs/batch = 0.6098s, grad.norm=0.55093688\n",
      "  6498: 4 [ 1190/ 1327], train_loss/perplexity = 5.35909128/212.5317230 secs/batch = 0.6099s, grad.norm=0.50959021\n",
      "  6503: 4 [ 1195/ 1327], train_loss/perplexity = 5.20286036/181.7914886 secs/batch = 0.6121s, grad.norm=0.53402019\n",
      "  6508: 4 [ 1200/ 1327], train_loss/perplexity = 5.14790821/172.0711823 secs/batch = 0.6092s, grad.norm=0.64904732\n",
      "  6513: 4 [ 1205/ 1327], train_loss/perplexity = 5.26841640/194.1083374 secs/batch = 0.6106s, grad.norm=0.61064398\n",
      "  6518: 4 [ 1210/ 1327], train_loss/perplexity = 5.01731300/151.0050049 secs/batch = 0.6109s, grad.norm=0.64275002\n",
      "  6523: 4 [ 1215/ 1327], train_loss/perplexity = 5.06061125/157.6868744 secs/batch = 0.6424s, grad.norm=0.59194350\n",
      "  6528: 4 [ 1220/ 1327], train_loss/perplexity = 5.18929291/179.3416901 secs/batch = 0.6124s, grad.norm=0.48647374\n",
      "  6533: 4 [ 1225/ 1327], train_loss/perplexity = 5.18799973/179.1099243 secs/batch = 0.6065s, grad.norm=0.63386923\n",
      "  6538: 4 [ 1230/ 1327], train_loss/perplexity = 5.25788069/192.0739899 secs/batch = 0.6155s, grad.norm=0.58284736\n",
      "  6543: 4 [ 1235/ 1327], train_loss/perplexity = 5.28117514/196.6007690 secs/batch = 0.6168s, grad.norm=0.51338577\n",
      "  6548: 4 [ 1240/ 1327], train_loss/perplexity = 5.44074011/230.6127930 secs/batch = 0.6118s, grad.norm=0.55467033\n",
      "  6553: 4 [ 1245/ 1327], train_loss/perplexity = 5.18391228/178.3793182 secs/batch = 0.6090s, grad.norm=0.50172698\n",
      "  6558: 4 [ 1250/ 1327], train_loss/perplexity = 5.36841393/214.5223541 secs/batch = 0.6126s, grad.norm=0.49381956\n",
      "  6563: 4 [ 1255/ 1327], train_loss/perplexity = 5.28589010/197.5299225 secs/batch = 0.6076s, grad.norm=0.59196973\n",
      "  6568: 4 [ 1260/ 1327], train_loss/perplexity = 5.39712667/220.7711639 secs/batch = 0.6146s, grad.norm=0.54517692\n",
      "  6573: 4 [ 1265/ 1327], train_loss/perplexity = 5.43363237/228.9794769 secs/batch = 0.6122s, grad.norm=0.62102908\n",
      "  6578: 4 [ 1270/ 1327], train_loss/perplexity = 5.26345444/193.1475525 secs/batch = 0.6121s, grad.norm=0.49623051\n",
      "  6583: 4 [ 1275/ 1327], train_loss/perplexity = 5.51164627/247.5583344 secs/batch = 0.6065s, grad.norm=0.58778352\n",
      "  6588: 4 [ 1280/ 1327], train_loss/perplexity = 5.37735224/216.4484100 secs/batch = 0.6129s, grad.norm=0.78565747\n",
      "  6593: 4 [ 1285/ 1327], train_loss/perplexity = 5.30380535/201.1006165 secs/batch = 0.6123s, grad.norm=0.58977073\n",
      "  6598: 4 [ 1290/ 1327], train_loss/perplexity = 5.41613674/225.0081787 secs/batch = 0.6119s, grad.norm=0.63397026\n",
      "  6603: 4 [ 1295/ 1327], train_loss/perplexity = 5.47943687/239.7116852 secs/batch = 0.6128s, grad.norm=0.48878253\n",
      "  6608: 4 [ 1300/ 1327], train_loss/perplexity = 5.46157312/235.4675598 secs/batch = 0.6045s, grad.norm=0.56200397\n",
      "  6613: 4 [ 1305/ 1327], train_loss/perplexity = 5.62437868/277.1000671 secs/batch = 0.6071s, grad.norm=0.55508250\n",
      "  6618: 4 [ 1310/ 1327], train_loss/perplexity = 5.83327341/341.4746399 secs/batch = 0.6032s, grad.norm=0.51229346\n",
      "  6623: 4 [ 1315/ 1327], train_loss/perplexity = 5.63991165/281.4378662 secs/batch = 0.6129s, grad.norm=0.57653648\n",
      "  6628: 4 [ 1320/ 1327], train_loss/perplexity = 5.61034203/273.2376709 secs/batch = 0.6134s, grad.norm=0.51022553\n",
      "  6633: 4 [ 1325/ 1327], train_loss/perplexity = 5.51246643/247.7614594 secs/batch = 0.6124s, grad.norm=0.51863164\n",
      "Epoch training time: 814.6000032424927\n",
      "Saved char model cv/epoch004_5.4776.model\n",
      "  6640: 5 [    5/ 1327], train_loss/perplexity = 5.66180611/287.6677246 secs/batch = 0.6131s, grad.norm=0.53721815\n",
      "  6645: 5 [   10/ 1327], train_loss/perplexity = 5.25492096/191.5063477 secs/batch = 0.6140s, grad.norm=0.57822531\n",
      "  6650: 5 [   15/ 1327], train_loss/perplexity = 5.23909235/188.4989319 secs/batch = 0.6587s, grad.norm=0.50778890\n",
      "  6655: 5 [   20/ 1327], train_loss/perplexity = 5.65761757/286.4653320 secs/batch = 0.6102s, grad.norm=0.59583205\n",
      "  6660: 5 [   25/ 1327], train_loss/perplexity = 5.52877903/251.8362427 secs/batch = 0.6138s, grad.norm=0.48969653\n",
      "  6665: 5 [   30/ 1327], train_loss/perplexity = 5.40620661/222.7848663 secs/batch = 0.6085s, grad.norm=0.50537032\n",
      "  6670: 5 [   35/ 1327], train_loss/perplexity = 5.33056974/206.5556183 secs/batch = 0.6532s, grad.norm=0.63788617\n",
      "  6675: 5 [   40/ 1327], train_loss/perplexity = 5.35937023/212.5910187 secs/batch = 0.6108s, grad.norm=0.53161550\n",
      "  6680: 5 [   45/ 1327], train_loss/perplexity = 5.04220819/154.8114929 secs/batch = 0.6136s, grad.norm=0.54464519\n",
      "  6685: 5 [   50/ 1327], train_loss/perplexity = 5.46901941/237.2274628 secs/batch = 0.6093s, grad.norm=0.52503759\n",
      "  6690: 5 [   55/ 1327], train_loss/perplexity = 5.33260918/206.9773102 secs/batch = 0.6047s, grad.norm=0.50076234\n",
      "  6695: 5 [   60/ 1327], train_loss/perplexity = 5.55117416/257.5397644 secs/batch = 0.6125s, grad.norm=0.58352625\n",
      "  6700: 5 [   65/ 1327], train_loss/perplexity = 5.13590097/170.0174255 secs/batch = 0.6182s, grad.norm=0.49115703\n",
      "  6705: 5 [   70/ 1327], train_loss/perplexity = 4.96933937/143.9317780 secs/batch = 0.6128s, grad.norm=0.53725284\n",
      "  6710: 5 [   75/ 1327], train_loss/perplexity = 5.04751205/155.6347656 secs/batch = 0.6109s, grad.norm=0.66172856\n",
      "  6715: 5 [   80/ 1327], train_loss/perplexity = 5.33928728/208.3641510 secs/batch = 0.6127s, grad.norm=0.58717877\n",
      "  6720: 5 [   85/ 1327], train_loss/perplexity = 5.38748407/218.6525726 secs/batch = 0.6184s, grad.norm=0.58066797\n",
      "  6725: 5 [   90/ 1327], train_loss/perplexity = 5.41337252/224.3870697 secs/batch = 0.6096s, grad.norm=0.58171916\n",
      "  6730: 5 [   95/ 1327], train_loss/perplexity = 5.23913097/188.5062103 secs/batch = 0.6284s, grad.norm=0.54597098\n",
      "  6735: 5 [  100/ 1327], train_loss/perplexity = 5.46061373/235.2417603 secs/batch = 0.6109s, grad.norm=0.59431946\n",
      "  6740: 5 [  105/ 1327], train_loss/perplexity = 5.51830006/249.2110291 secs/batch = 0.6123s, grad.norm=0.62317568\n",
      "  6745: 5 [  110/ 1327], train_loss/perplexity = 5.25994349/192.4706116 secs/batch = 0.6109s, grad.norm=0.49142742\n",
      "  6750: 5 [  115/ 1327], train_loss/perplexity = 5.14111662/170.9064941 secs/batch = 0.6129s, grad.norm=0.53565311\n",
      "  6755: 5 [  120/ 1327], train_loss/perplexity = 5.30239010/200.8162079 secs/batch = 0.6155s, grad.norm=0.72749484\n",
      "  6760: 5 [  125/ 1327], train_loss/perplexity = 5.41841841/225.5221558 secs/batch = 0.6145s, grad.norm=0.55436391\n",
      "  6765: 5 [  130/ 1327], train_loss/perplexity = 5.32624054/205.6633301 secs/batch = 0.6092s, grad.norm=0.54935294\n",
      "  6770: 5 [  135/ 1327], train_loss/perplexity = 5.36751127/214.3287964 secs/batch = 0.6060s, grad.norm=0.57813889\n",
      "  6775: 5 [  140/ 1327], train_loss/perplexity = 5.55988932/259.7940674 secs/batch = 0.6094s, grad.norm=0.54109222\n",
      "  6780: 5 [  145/ 1327], train_loss/perplexity = 5.55676365/258.9833069 secs/batch = 0.6114s, grad.norm=0.50113153\n",
      "  6785: 5 [  150/ 1327], train_loss/perplexity = 5.41718197/225.2434845 secs/batch = 0.6085s, grad.norm=0.51596278\n",
      "  6790: 5 [  155/ 1327], train_loss/perplexity = 5.65212250/284.8955078 secs/batch = 0.6131s, grad.norm=0.56160527\n",
      "  6795: 5 [  160/ 1327], train_loss/perplexity = 5.32768440/205.9604950 secs/batch = 0.6059s, grad.norm=0.58886552\n",
      "  6800: 5 [  165/ 1327], train_loss/perplexity = 5.54944324/257.0943909 secs/batch = 0.6061s, grad.norm=0.52616954\n",
      "  6805: 5 [  170/ 1327], train_loss/perplexity = 5.39383745/220.0461884 secs/batch = 0.6065s, grad.norm=0.54842013\n",
      "  6810: 5 [  175/ 1327], train_loss/perplexity = 5.52408218/250.6561737 secs/batch = 0.6106s, grad.norm=0.49395955\n",
      "  6815: 5 [  180/ 1327], train_loss/perplexity = 5.49985838/244.6572876 secs/batch = 0.6031s, grad.norm=0.53298634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6820: 5 [  185/ 1327], train_loss/perplexity = 5.68083763/293.1949158 secs/batch = 0.6037s, grad.norm=0.57301444\n",
      "  6825: 5 [  190/ 1327], train_loss/perplexity = 5.22820425/186.4576721 secs/batch = 0.6138s, grad.norm=0.53145939\n",
      "  6830: 5 [  195/ 1327], train_loss/perplexity = 5.41877127/225.6017456 secs/batch = 0.6060s, grad.norm=0.53543985\n",
      "  6835: 5 [  200/ 1327], train_loss/perplexity = 5.39516497/220.3384857 secs/batch = 0.6124s, grad.norm=0.54944283\n",
      "  6840: 5 [  205/ 1327], train_loss/perplexity = 5.40401983/222.2982178 secs/batch = 0.6035s, grad.norm=0.50458699\n",
      "  6845: 5 [  210/ 1327], train_loss/perplexity = 5.35452032/211.5624695 secs/batch = 0.6031s, grad.norm=0.50705779\n",
      "  6850: 5 [  215/ 1327], train_loss/perplexity = 5.40914345/223.4401245 secs/batch = 0.6109s, grad.norm=0.48604995\n",
      "  6855: 5 [  220/ 1327], train_loss/perplexity = 5.51064396/247.3103333 secs/batch = 0.6152s, grad.norm=0.49382201\n",
      "  6860: 5 [  225/ 1327], train_loss/perplexity = 5.62878990/278.3251038 secs/batch = 0.6091s, grad.norm=0.52433008\n",
      "  6865: 5 [  230/ 1327], train_loss/perplexity = 5.46890402/237.2000885 secs/batch = 0.6264s, grad.norm=0.54364878\n",
      "  6870: 5 [  235/ 1327], train_loss/perplexity = 5.37914610/216.8370361 secs/batch = 0.6066s, grad.norm=0.51862425\n",
      "  6875: 5 [  240/ 1327], train_loss/perplexity = 5.16461229/174.9696045 secs/batch = 0.6072s, grad.norm=0.56545633\n",
      "  6880: 5 [  245/ 1327], train_loss/perplexity = 5.50027370/244.7589111 secs/batch = 0.6144s, grad.norm=0.61983913\n",
      "  6885: 5 [  250/ 1327], train_loss/perplexity = 5.22564173/185.9804840 secs/batch = 0.6151s, grad.norm=0.54140133\n",
      "  6890: 5 [  255/ 1327], train_loss/perplexity = 5.33424902/207.3170013 secs/batch = 0.6103s, grad.norm=0.52472067\n",
      "  6895: 5 [  260/ 1327], train_loss/perplexity = 5.65135956/284.6782532 secs/batch = 0.6011s, grad.norm=0.55318666\n",
      "  6900: 5 [  265/ 1327], train_loss/perplexity = 5.61087370/273.3829956 secs/batch = 0.6108s, grad.norm=0.58771789\n",
      "  6905: 5 [  270/ 1327], train_loss/perplexity = 5.59041739/267.8473816 secs/batch = 0.6077s, grad.norm=0.53696865\n",
      "  6910: 5 [  275/ 1327], train_loss/perplexity = 5.76672316/319.4891052 secs/batch = 0.6062s, grad.norm=0.53903115\n",
      "  6915: 5 [  280/ 1327], train_loss/perplexity = 5.51006985/247.1683960 secs/batch = 0.6471s, grad.norm=0.53709340\n",
      "  6920: 5 [  285/ 1327], train_loss/perplexity = 5.65112448/284.6113281 secs/batch = 0.6099s, grad.norm=0.66628212\n",
      "  6925: 5 [  290/ 1327], train_loss/perplexity = 5.54654169/256.3494873 secs/batch = 0.6107s, grad.norm=0.52586663\n",
      "  6930: 5 [  295/ 1327], train_loss/perplexity = 5.26217508/192.9006042 secs/batch = 0.6035s, grad.norm=0.55143958\n",
      "  6935: 5 [  300/ 1327], train_loss/perplexity = 4.89690399/133.8746643 secs/batch = 0.6182s, grad.norm=0.52067214\n",
      "  6940: 5 [  305/ 1327], train_loss/perplexity = 5.28795052/197.9373474 secs/batch = 0.6146s, grad.norm=0.55049521\n",
      "  6945: 5 [  310/ 1327], train_loss/perplexity = 5.40424299/222.3478394 secs/batch = 0.6132s, grad.norm=0.64994174\n",
      "  6950: 5 [  315/ 1327], train_loss/perplexity = 5.05532312/156.8552094 secs/batch = 0.6147s, grad.norm=0.50930858\n",
      "  6955: 5 [  320/ 1327], train_loss/perplexity = 5.27348900/195.0954590 secs/batch = 0.6098s, grad.norm=0.54224449\n",
      "  6960: 5 [  325/ 1327], train_loss/perplexity = 5.03005266/152.9410706 secs/batch = 0.6135s, grad.norm=0.61800593\n",
      "  6965: 5 [  330/ 1327], train_loss/perplexity = 5.47675133/239.0687866 secs/batch = 0.6156s, grad.norm=0.63126981\n",
      "  6970: 5 [  335/ 1327], train_loss/perplexity = 4.68757057/108.5890503 secs/batch = 0.6103s, grad.norm=0.52409875\n",
      "  6975: 5 [  340/ 1327], train_loss/perplexity = 5.53744602/254.0283813 secs/batch = 0.6087s, grad.norm=0.53021538\n",
      "  6980: 5 [  345/ 1327], train_loss/perplexity = 5.35474586/211.6101990 secs/batch = 0.6131s, grad.norm=0.54281658\n",
      "  6985: 5 [  350/ 1327], train_loss/perplexity = 5.47863674/239.5199585 secs/batch = 0.6073s, grad.norm=0.55833036\n",
      "  6990: 5 [  355/ 1327], train_loss/perplexity = 5.55863190/259.4676208 secs/batch = 0.6145s, grad.norm=0.65025169\n",
      "  6995: 5 [  360/ 1327], train_loss/perplexity = 5.66168022/287.6315308 secs/batch = 0.6130s, grad.norm=0.53032988\n",
      "  7000: 5 [  365/ 1327], train_loss/perplexity = 5.52010870/249.6621704 secs/batch = 0.6131s, grad.norm=0.53025359\n",
      "  7005: 5 [  370/ 1327], train_loss/perplexity = 5.51702452/248.8933563 secs/batch = 0.6059s, grad.norm=0.54112482\n",
      "  7010: 5 [  375/ 1327], train_loss/perplexity = 4.91213608/135.9294586 secs/batch = 0.6157s, grad.norm=0.49574810\n",
      "  7015: 5 [  380/ 1327], train_loss/perplexity = 5.21727896/184.4316559 secs/batch = 0.6053s, grad.norm=0.53067005\n",
      "  7020: 5 [  385/ 1327], train_loss/perplexity = 5.36871243/214.5863953 secs/batch = 0.6160s, grad.norm=0.56166977\n",
      "  7025: 5 [  390/ 1327], train_loss/perplexity = 5.39221954/219.6904602 secs/batch = 0.6139s, grad.norm=0.57871246\n",
      "  7030: 5 [  395/ 1327], train_loss/perplexity = 5.66897154/289.7363892 secs/batch = 0.6155s, grad.norm=0.55988324\n",
      "  7035: 5 [  400/ 1327], train_loss/perplexity = 5.29256296/198.8524170 secs/batch = 0.6170s, grad.norm=0.56048506\n",
      "  7040: 5 [  405/ 1327], train_loss/perplexity = 5.61689043/275.0328064 secs/batch = 0.6251s, grad.norm=0.50162774\n",
      "  7045: 5 [  410/ 1327], train_loss/perplexity = 5.40812588/223.2128601 secs/batch = 0.6124s, grad.norm=0.50043470\n",
      "  7050: 5 [  415/ 1327], train_loss/perplexity = 5.21965408/184.8702240 secs/batch = 0.6071s, grad.norm=0.55730766\n",
      "  7055: 5 [  420/ 1327], train_loss/perplexity = 5.12589121/168.3240814 secs/batch = 0.6118s, grad.norm=0.52789456\n",
      "  7060: 5 [  425/ 1327], train_loss/perplexity = 5.43706179/229.7660828 secs/batch = 0.6132s, grad.norm=0.55495852\n",
      "  7065: 5 [  430/ 1327], train_loss/perplexity = 5.48077297/240.0321808 secs/batch = 0.6148s, grad.norm=0.52691591\n",
      "  7070: 5 [  435/ 1327], train_loss/perplexity = 5.54444981/255.8137970 secs/batch = 0.6163s, grad.norm=0.59626913\n",
      "  7075: 5 [  440/ 1327], train_loss/perplexity = 5.33885384/208.2738647 secs/batch = 0.6059s, grad.norm=0.62276304\n",
      "  7080: 5 [  445/ 1327], train_loss/perplexity = 5.40413475/222.3237762 secs/batch = 0.6149s, grad.norm=0.57025385\n",
      "  7085: 5 [  450/ 1327], train_loss/perplexity = 5.24814558/190.2132111 secs/batch = 0.6098s, grad.norm=0.55178457\n",
      "  7090: 5 [  455/ 1327], train_loss/perplexity = 5.10206366/164.3607483 secs/batch = 0.6154s, grad.norm=0.58221811\n",
      "  7095: 5 [  460/ 1327], train_loss/perplexity = 5.35807180/212.3151703 secs/batch = 0.6160s, grad.norm=0.59187096\n",
      "  7100: 5 [  465/ 1327], train_loss/perplexity = 5.27715826/195.8126373 secs/batch = 0.6022s, grad.norm=0.59251928\n",
      "  7105: 5 [  470/ 1327], train_loss/perplexity = 5.64109325/281.7705994 secs/batch = 0.6071s, grad.norm=0.59703678\n",
      "  7110: 5 [  475/ 1327], train_loss/perplexity = 5.31505728/203.3761597 secs/batch = 0.6065s, grad.norm=0.50720686\n",
      "  7115: 5 [  480/ 1327], train_loss/perplexity = 5.38887358/218.9566040 secs/batch = 0.6170s, grad.norm=0.53114378\n",
      "  7120: 5 [  485/ 1327], train_loss/perplexity = 5.31664991/203.7003174 secs/batch = 0.6091s, grad.norm=0.53573549\n",
      "  7125: 5 [  490/ 1327], train_loss/perplexity = 5.24554682/189.7195282 secs/batch = 0.6151s, grad.norm=0.57632571\n",
      "  7130: 5 [  495/ 1327], train_loss/perplexity = 5.14497375/171.5669861 secs/batch = 0.6087s, grad.norm=0.52722996\n",
      "  7135: 5 [  500/ 1327], train_loss/perplexity = 5.53828430/254.2414246 secs/batch = 0.6082s, grad.norm=0.57296234\n",
      "  7140: 5 [  505/ 1327], train_loss/perplexity = 5.30192757/200.7233429 secs/batch = 0.6115s, grad.norm=0.55872101\n",
      "  7145: 5 [  510/ 1327], train_loss/perplexity = 5.71495628/303.3709412 secs/batch = 0.6046s, grad.norm=0.71763748\n",
      "  7150: 5 [  515/ 1327], train_loss/perplexity = 5.41163111/223.9966583 secs/batch = 0.6073s, grad.norm=0.55018133\n",
      "  7155: 5 [  520/ 1327], train_loss/perplexity = 5.56938362/262.2723999 secs/batch = 0.6051s, grad.norm=0.57404792\n",
      "  7160: 5 [  525/ 1327], train_loss/perplexity = 5.24572563/189.7534637 secs/batch = 0.6217s, grad.norm=0.51776987\n",
      "  7165: 5 [  530/ 1327], train_loss/perplexity = 5.28691339/197.7321625 secs/batch = 0.5988s, grad.norm=0.55508667\n",
      "  7170: 5 [  535/ 1327], train_loss/perplexity = 5.35672665/212.0297546 secs/batch = 0.6103s, grad.norm=0.54597789\n",
      "  7175: 5 [  540/ 1327], train_loss/perplexity = 5.41482925/224.7141724 secs/batch = 0.6137s, grad.norm=0.46681678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7180: 5 [  545/ 1327], train_loss/perplexity = 5.55306816/258.0280151 secs/batch = 0.6100s, grad.norm=0.49900010\n",
      "  7185: 5 [  550/ 1327], train_loss/perplexity = 5.42581272/227.1959229 secs/batch = 0.6052s, grad.norm=0.52589935\n",
      "  7190: 5 [  555/ 1327], train_loss/perplexity = 5.31501770/203.3681183 secs/batch = 0.6102s, grad.norm=0.53114361\n",
      "  7195: 5 [  560/ 1327], train_loss/perplexity = 5.37315226/215.5412445 secs/batch = 0.6125s, grad.norm=0.59113055\n",
      "  7200: 5 [  565/ 1327], train_loss/perplexity = 5.37206602/215.3072357 secs/batch = 0.6137s, grad.norm=0.54324269\n",
      "  7205: 5 [  570/ 1327], train_loss/perplexity = 5.25592756/191.6992188 secs/batch = 0.6056s, grad.norm=0.56713325\n",
      "  7210: 5 [  575/ 1327], train_loss/perplexity = 5.18876076/179.2462921 secs/batch = 0.6209s, grad.norm=0.56001592\n",
      "  7215: 5 [  580/ 1327], train_loss/perplexity = 5.52095795/249.8742828 secs/batch = 0.6113s, grad.norm=0.64387965\n",
      "  7220: 5 [  585/ 1327], train_loss/perplexity = 5.15746546/173.7235870 secs/batch = 0.6098s, grad.norm=0.53279012\n",
      "  7225: 5 [  590/ 1327], train_loss/perplexity = 5.43255711/228.7333984 secs/batch = 0.6136s, grad.norm=0.50405562\n",
      "  7230: 5 [  595/ 1327], train_loss/perplexity = 5.35215330/211.0622864 secs/batch = 0.6039s, grad.norm=0.61201519\n",
      "  7235: 5 [  600/ 1327], train_loss/perplexity = 5.60738277/272.4302979 secs/batch = 0.6080s, grad.norm=0.55651182\n",
      "  7240: 5 [  605/ 1327], train_loss/perplexity = 5.53915310/254.4624023 secs/batch = 0.6135s, grad.norm=0.52014273\n",
      "  7245: 5 [  610/ 1327], train_loss/perplexity = 5.68184233/293.4896240 secs/batch = 0.6087s, grad.norm=0.57312703\n",
      "  7250: 5 [  615/ 1327], train_loss/perplexity = 5.06429005/158.2680359 secs/batch = 0.6083s, grad.norm=0.52173811\n",
      "  7255: 5 [  620/ 1327], train_loss/perplexity = 5.41427612/224.5899048 secs/batch = 0.6095s, grad.norm=0.51227057\n",
      "  7260: 5 [  625/ 1327], train_loss/perplexity = 5.56550550/261.2572327 secs/batch = 0.6578s, grad.norm=0.55335093\n",
      "  7265: 5 [  630/ 1327], train_loss/perplexity = 5.56708288/261.6696777 secs/batch = 0.6112s, grad.norm=0.56282508\n",
      "  7270: 5 [  635/ 1327], train_loss/perplexity = 5.30813360/201.9729156 secs/batch = 0.6023s, grad.norm=0.55257332\n",
      "  7275: 5 [  640/ 1327], train_loss/perplexity = 5.42846680/227.7997131 secs/batch = 0.6134s, grad.norm=0.57483697\n",
      "  7280: 5 [  645/ 1327], train_loss/perplexity = 5.56033039/259.9086914 secs/batch = 0.6142s, grad.norm=0.51454127\n",
      "  7285: 5 [  650/ 1327], train_loss/perplexity = 5.33917141/208.3400116 secs/batch = 0.6133s, grad.norm=0.77137893\n",
      "  7290: 5 [  655/ 1327], train_loss/perplexity = 5.32506561/205.4218445 secs/batch = 0.6068s, grad.norm=0.59848869\n",
      "  7295: 5 [  660/ 1327], train_loss/perplexity = 5.21705818/184.3909454 secs/batch = 0.6152s, grad.norm=0.50386536\n",
      "  7300: 5 [  665/ 1327], train_loss/perplexity = 5.41263342/224.2212830 secs/batch = 0.6143s, grad.norm=0.55540138\n",
      "  7305: 5 [  670/ 1327], train_loss/perplexity = 5.28652334/197.6550446 secs/batch = 0.6151s, grad.norm=0.51244640\n",
      "  7310: 5 [  675/ 1327], train_loss/perplexity = 5.06703281/158.7027283 secs/batch = 0.6133s, grad.norm=0.59395105\n",
      "  7315: 5 [  680/ 1327], train_loss/perplexity = 5.39523315/220.3535156 secs/batch = 0.6081s, grad.norm=0.56296295\n",
      "  7320: 5 [  685/ 1327], train_loss/perplexity = 5.42821455/227.7422638 secs/batch = 0.6132s, grad.norm=0.59486109\n",
      "  7325: 5 [  690/ 1327], train_loss/perplexity = 5.45025969/232.8186188 secs/batch = 0.6110s, grad.norm=0.54095060\n",
      "  7330: 5 [  695/ 1327], train_loss/perplexity = 5.35697269/212.0819397 secs/batch = 0.6088s, grad.norm=0.58494920\n",
      "  7335: 5 [  700/ 1327], train_loss/perplexity = 5.48796511/241.7647400 secs/batch = 0.6165s, grad.norm=0.53288853\n",
      "  7340: 5 [  705/ 1327], train_loss/perplexity = 5.23000145/186.7930756 secs/batch = 0.6159s, grad.norm=0.54812312\n",
      "  7345: 5 [  710/ 1327], train_loss/perplexity = 5.30479479/201.2996826 secs/batch = 0.6150s, grad.norm=0.59886551\n",
      "  7350: 5 [  715/ 1327], train_loss/perplexity = 5.24290752/189.2194672 secs/batch = 0.6111s, grad.norm=0.51209325\n",
      "  7355: 5 [  720/ 1327], train_loss/perplexity = 5.33776903/208.0480499 secs/batch = 0.6111s, grad.norm=0.56113660\n",
      "  7360: 5 [  725/ 1327], train_loss/perplexity = 5.05184269/156.3102264 secs/batch = 0.6091s, grad.norm=0.52820808\n",
      "  7365: 5 [  730/ 1327], train_loss/perplexity = 5.24041271/188.7479858 secs/batch = 0.6154s, grad.norm=0.58091605\n",
      "  7370: 5 [  735/ 1327], train_loss/perplexity = 5.43476725/229.2394867 secs/batch = 0.6115s, grad.norm=0.51991683\n",
      "  7375: 5 [  740/ 1327], train_loss/perplexity = 4.83588791/125.9503632 secs/batch = 0.6183s, grad.norm=0.54993725\n",
      "  7380: 5 [  745/ 1327], train_loss/perplexity = 5.32729721/205.8807678 secs/batch = 0.6089s, grad.norm=0.59465206\n",
      "  7385: 5 [  750/ 1327], train_loss/perplexity = 5.14986420/172.4080811 secs/batch = 0.6122s, grad.norm=0.55493444\n",
      "  7390: 5 [  755/ 1327], train_loss/perplexity = 5.20154905/181.5532532 secs/batch = 0.6089s, grad.norm=0.51446390\n",
      "  7395: 5 [  760/ 1327], train_loss/perplexity = 5.08205652/161.1050262 secs/batch = 0.6141s, grad.norm=0.53877693\n",
      "  7400: 5 [  765/ 1327], train_loss/perplexity = 5.16907930/175.7529449 secs/batch = 0.6148s, grad.norm=0.54943025\n",
      "  7405: 5 [  770/ 1327], train_loss/perplexity = 5.12579393/168.3077087 secs/batch = 0.6106s, grad.norm=0.66211033\n",
      "  7410: 5 [  775/ 1327], train_loss/perplexity = 5.25725889/191.9546051 secs/batch = 0.6081s, grad.norm=0.59115654\n",
      "  7415: 5 [  780/ 1327], train_loss/perplexity = 5.42677784/227.4152985 secs/batch = 0.6125s, grad.norm=0.52454454\n",
      "  7420: 5 [  785/ 1327], train_loss/perplexity = 5.35238791/211.1118164 secs/batch = 0.6086s, grad.norm=0.70997363\n",
      "  7425: 5 [  790/ 1327], train_loss/perplexity = 5.11674929/166.7922974 secs/batch = 0.6124s, grad.norm=0.58508247\n",
      "  7430: 5 [  795/ 1327], train_loss/perplexity = 5.42015314/225.9137115 secs/batch = 0.6168s, grad.norm=0.55551648\n",
      "  7435: 5 [  800/ 1327], train_loss/perplexity = 5.40393686/222.2797852 secs/batch = 0.6134s, grad.norm=0.53074855\n",
      "  7440: 5 [  805/ 1327], train_loss/perplexity = 5.65057850/284.4559631 secs/batch = 0.6161s, grad.norm=0.51480222\n",
      "  7445: 5 [  810/ 1327], train_loss/perplexity = 5.35288429/211.2166290 secs/batch = 0.6078s, grad.norm=0.57009923\n",
      "  7450: 5 [  815/ 1327], train_loss/perplexity = 5.21614504/184.2226410 secs/batch = 0.6078s, grad.norm=0.54611826\n",
      "  7455: 5 [  820/ 1327], train_loss/perplexity = 4.89220619/133.2472229 secs/batch = 0.6196s, grad.norm=0.51811492\n",
      "  7460: 5 [  825/ 1327], train_loss/perplexity = 5.04447842/155.1633453 secs/batch = 0.6115s, grad.norm=0.49759486\n",
      "  7465: 5 [  830/ 1327], train_loss/perplexity = 5.00838566/149.6629333 secs/batch = 0.6101s, grad.norm=0.63997203\n",
      "  7470: 5 [  835/ 1327], train_loss/perplexity = 5.19111204/179.6682434 secs/batch = 0.6152s, grad.norm=0.55550414\n",
      "  7475: 5 [  840/ 1327], train_loss/perplexity = 5.38245392/217.5554810 secs/batch = 0.6133s, grad.norm=0.58092409\n",
      "  7480: 5 [  845/ 1327], train_loss/perplexity = 5.18450451/178.4849854 secs/batch = 0.6117s, grad.norm=0.54806238\n",
      "  7485: 5 [  850/ 1327], train_loss/perplexity = 5.24227810/189.1004028 secs/batch = 0.6060s, grad.norm=0.56543696\n",
      "  7490: 5 [  855/ 1327], train_loss/perplexity = 5.21992207/184.9197693 secs/batch = 0.6028s, grad.norm=0.53991783\n",
      "  7495: 5 [  860/ 1327], train_loss/perplexity = 4.99771595/148.0745697 secs/batch = 0.6141s, grad.norm=0.53593075\n",
      "  7500: 5 [  865/ 1327], train_loss/perplexity = 5.42606831/227.2539978 secs/batch = 0.6133s, grad.norm=0.58066785\n",
      "  7505: 5 [  870/ 1327], train_loss/perplexity = 5.40763521/223.1033783 secs/batch = 0.6467s, grad.norm=0.53531373\n",
      "  7510: 5 [  875/ 1327], train_loss/perplexity = 5.03598738/153.8514252 secs/batch = 0.6109s, grad.norm=0.55758679\n",
      "  7515: 5 [  880/ 1327], train_loss/perplexity = 5.12331724/167.8913879 secs/batch = 0.6078s, grad.norm=0.51475954\n",
      "  7520: 5 [  885/ 1327], train_loss/perplexity = 5.17183924/176.2386780 secs/batch = 0.6143s, grad.norm=0.60784560\n",
      "  7525: 5 [  890/ 1327], train_loss/perplexity = 5.40221834/221.8981171 secs/batch = 0.6088s, grad.norm=0.56946617\n",
      "  7530: 5 [  895/ 1327], train_loss/perplexity = 5.44003773/230.4508820 secs/batch = 0.6155s, grad.norm=0.53161287\n",
      "  7535: 5 [  900/ 1327], train_loss/perplexity = 5.36036825/212.8032990 secs/batch = 0.6093s, grad.norm=0.56592768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7540: 5 [  905/ 1327], train_loss/perplexity = 5.14209890/171.0744629 secs/batch = 0.6118s, grad.norm=0.58955467\n",
      "  7545: 5 [  910/ 1327], train_loss/perplexity = 5.21556997/184.1167297 secs/batch = 0.6088s, grad.norm=0.56387413\n",
      "  7550: 5 [  915/ 1327], train_loss/perplexity = 5.54061222/254.8339691 secs/batch = 0.6105s, grad.norm=0.57963318\n",
      "  7555: 5 [  920/ 1327], train_loss/perplexity = 5.56741238/261.7558899 secs/batch = 0.6061s, grad.norm=0.56165701\n",
      "  7560: 5 [  925/ 1327], train_loss/perplexity = 5.28303051/196.9658813 secs/batch = 0.6106s, grad.norm=0.53627497\n",
      "  7565: 5 [  930/ 1327], train_loss/perplexity = 5.24939394/190.4508057 secs/batch = 0.6120s, grad.norm=0.55930537\n",
      "  7570: 5 [  935/ 1327], train_loss/perplexity = 5.33339930/207.1409149 secs/batch = 0.6139s, grad.norm=0.53020978\n",
      "  7575: 5 [  940/ 1327], train_loss/perplexity = 5.31197357/202.7499695 secs/batch = 0.6028s, grad.norm=0.60203797\n",
      "  7580: 5 [  945/ 1327], train_loss/perplexity = 5.47717381/239.1698151 secs/batch = 0.6125s, grad.norm=0.55325651\n",
      "  7585: 5 [  950/ 1327], train_loss/perplexity = 5.25379753/191.2913208 secs/batch = 0.6111s, grad.norm=0.55007648\n",
      "  7590: 5 [  955/ 1327], train_loss/perplexity = 5.38728809/218.6097260 secs/batch = 0.6041s, grad.norm=0.53111678\n",
      "  7595: 5 [  960/ 1327], train_loss/perplexity = 5.65655565/286.1613159 secs/batch = 0.6068s, grad.norm=0.56736779\n",
      "  7600: 5 [  965/ 1327], train_loss/perplexity = 5.36529112/213.8534851 secs/batch = 0.6087s, grad.norm=0.53729206\n",
      "  7605: 5 [  970/ 1327], train_loss/perplexity = 5.52951050/252.0205231 secs/batch = 0.6121s, grad.norm=0.48902500\n",
      "  7610: 5 [  975/ 1327], train_loss/perplexity = 5.35364676/211.3777313 secs/batch = 0.6107s, grad.norm=0.60378879\n",
      "  7615: 5 [  980/ 1327], train_loss/perplexity = 5.09497929/163.2004700 secs/batch = 0.6111s, grad.norm=0.52695733\n",
      "  7620: 5 [  985/ 1327], train_loss/perplexity = 5.35267019/211.1714172 secs/batch = 0.6071s, grad.norm=0.58752984\n",
      "  7625: 5 [  990/ 1327], train_loss/perplexity = 5.48807621/241.7915955 secs/batch = 0.6137s, grad.norm=0.54014325\n",
      "  7630: 5 [  995/ 1327], train_loss/perplexity = 5.45163631/233.1393433 secs/batch = 0.6430s, grad.norm=0.63566583\n",
      "  7635: 5 [ 1000/ 1327], train_loss/perplexity = 4.94243717/140.1113129 secs/batch = 0.6056s, grad.norm=0.50475037\n",
      "  7640: 5 [ 1005/ 1327], train_loss/perplexity = 5.39409637/220.1031647 secs/batch = 0.6105s, grad.norm=0.55624163\n",
      "  7645: 5 [ 1010/ 1327], train_loss/perplexity = 4.96309376/143.0356293 secs/batch = 0.6107s, grad.norm=0.52872992\n",
      "  7650: 5 [ 1015/ 1327], train_loss/perplexity = 5.37981319/216.9817352 secs/batch = 0.6107s, grad.norm=0.56022537\n",
      "  7655: 5 [ 1020/ 1327], train_loss/perplexity = 5.60849380/272.7331543 secs/batch = 0.6180s, grad.norm=0.59441787\n",
      "  7660: 5 [ 1025/ 1327], train_loss/perplexity = 5.41791964/225.4096985 secs/batch = 0.6106s, grad.norm=0.51601911\n",
      "  7665: 5 [ 1030/ 1327], train_loss/perplexity = 5.27743959/195.8677368 secs/batch = 0.6150s, grad.norm=0.52417254\n",
      "  7670: 5 [ 1035/ 1327], train_loss/perplexity = 5.14391422/171.3852997 secs/batch = 0.6050s, grad.norm=0.57483071\n",
      "  7675: 5 [ 1040/ 1327], train_loss/perplexity = 5.44307232/231.1512604 secs/batch = 0.6174s, grad.norm=0.55299461\n",
      "  7680: 5 [ 1045/ 1327], train_loss/perplexity = 5.08523417/161.6177826 secs/batch = 0.6107s, grad.norm=0.58256066\n",
      "  7685: 5 [ 1050/ 1327], train_loss/perplexity = 5.10217047/164.3782959 secs/batch = 0.6130s, grad.norm=0.55405128\n",
      "  7690: 5 [ 1055/ 1327], train_loss/perplexity = 5.37356520/215.6302643 secs/batch = 0.6119s, grad.norm=0.56297803\n",
      "  7695: 5 [ 1060/ 1327], train_loss/perplexity = 4.93880463/139.6032715 secs/batch = 0.6195s, grad.norm=0.54636180\n",
      "  7700: 5 [ 1065/ 1327], train_loss/perplexity = 5.04998207/156.0196686 secs/batch = 0.6302s, grad.norm=0.57638544\n",
      "  7705: 5 [ 1070/ 1327], train_loss/perplexity = 5.39795732/220.9546204 secs/batch = 0.6094s, grad.norm=0.57379633\n",
      "  7710: 5 [ 1075/ 1327], train_loss/perplexity = 5.17092514/176.0776520 secs/batch = 0.6127s, grad.norm=0.56433731\n",
      "  7715: 5 [ 1080/ 1327], train_loss/perplexity = 5.03269291/153.3453979 secs/batch = 0.6134s, grad.norm=0.56801564\n",
      "  7720: 5 [ 1085/ 1327], train_loss/perplexity = 4.99357128/147.4621124 secs/batch = 0.6052s, grad.norm=0.66492844\n",
      "  7725: 5 [ 1090/ 1327], train_loss/perplexity = 5.17427111/176.6678009 secs/batch = 0.6119s, grad.norm=0.61984885\n",
      "  7730: 5 [ 1095/ 1327], train_loss/perplexity = 5.31246758/202.8501587 secs/batch = 0.6134s, grad.norm=0.58353639\n",
      "  7735: 5 [ 1100/ 1327], train_loss/perplexity = 5.25482082/191.4871674 secs/batch = 0.6155s, grad.norm=0.53399998\n",
      "  7740: 5 [ 1105/ 1327], train_loss/perplexity = 5.10320854/164.5490265 secs/batch = 0.6080s, grad.norm=0.56265670\n",
      "  7745: 5 [ 1110/ 1327], train_loss/perplexity = 5.65352488/285.2953186 secs/batch = 0.6140s, grad.norm=0.56942987\n",
      "  7750: 5 [ 1115/ 1327], train_loss/perplexity = 5.13382006/169.6640015 secs/batch = 0.6537s, grad.norm=0.53001863\n",
      "  7755: 5 [ 1120/ 1327], train_loss/perplexity = 5.28673601/197.6970825 secs/batch = 0.6124s, grad.norm=0.56831944\n",
      "  7760: 5 [ 1125/ 1327], train_loss/perplexity = 5.50190163/245.1576843 secs/batch = 0.6134s, grad.norm=0.55819362\n",
      "  7765: 5 [ 1130/ 1327], train_loss/perplexity = 5.21022797/183.1358032 secs/batch = 0.6154s, grad.norm=0.54272425\n",
      "  7770: 5 [ 1135/ 1327], train_loss/perplexity = 5.20782042/182.6954193 secs/batch = 0.6175s, grad.norm=0.52964133\n",
      "  7775: 5 [ 1140/ 1327], train_loss/perplexity = 5.46856546/237.1197968 secs/batch = 0.6099s, grad.norm=0.54417419\n",
      "  7780: 5 [ 1145/ 1327], train_loss/perplexity = 5.22797537/186.4149933 secs/batch = 0.6127s, grad.norm=0.62006867\n",
      "  7785: 5 [ 1150/ 1327], train_loss/perplexity = 5.21980619/184.8983459 secs/batch = 0.6098s, grad.norm=0.50888097\n",
      "  7790: 5 [ 1155/ 1327], train_loss/perplexity = 5.27700806/195.7832184 secs/batch = 0.6192s, grad.norm=0.53874189\n",
      "  7795: 5 [ 1160/ 1327], train_loss/perplexity = 5.24449301/189.5196991 secs/batch = 0.6163s, grad.norm=0.56592071\n",
      "  7800: 5 [ 1165/ 1327], train_loss/perplexity = 5.35454988/211.5687256 secs/batch = 0.6112s, grad.norm=0.61321580\n",
      "  7805: 5 [ 1170/ 1327], train_loss/perplexity = 5.20366573/181.9379578 secs/batch = 0.6031s, grad.norm=0.60373592\n",
      "  7810: 5 [ 1175/ 1327], train_loss/perplexity = 5.00677061/149.4214172 secs/batch = 0.6102s, grad.norm=0.58278567\n",
      "  7815: 5 [ 1180/ 1327], train_loss/perplexity = 4.98074770/145.5831909 secs/batch = 0.6094s, grad.norm=0.55795044\n",
      "  7820: 5 [ 1185/ 1327], train_loss/perplexity = 5.21873045/184.6995544 secs/batch = 0.6076s, grad.norm=0.57081902\n",
      "  7825: 5 [ 1190/ 1327], train_loss/perplexity = 5.20639801/182.4357452 secs/batch = 0.6294s, grad.norm=0.51686019\n",
      "  7830: 5 [ 1195/ 1327], train_loss/perplexity = 5.06168556/157.8563690 secs/batch = 0.6080s, grad.norm=0.53928500\n",
      "  7835: 5 [ 1200/ 1327], train_loss/perplexity = 5.02595758/152.3160400 secs/batch = 0.6126s, grad.norm=0.72616673\n",
      "  7840: 5 [ 1205/ 1327], train_loss/perplexity = 5.08100557/160.9358063 secs/batch = 0.6090s, grad.norm=0.56798619\n",
      "  7845: 5 [ 1210/ 1327], train_loss/perplexity = 4.85240555/128.0480499 secs/batch = 0.6144s, grad.norm=0.55523920\n",
      "  7850: 5 [ 1215/ 1327], train_loss/perplexity = 4.93094206/138.5099335 secs/batch = 0.6077s, grad.norm=0.68732953\n",
      "  7855: 5 [ 1220/ 1327], train_loss/perplexity = 5.08396578/161.4129181 secs/batch = 0.6103s, grad.norm=0.60854101\n",
      "  7860: 5 [ 1225/ 1327], train_loss/perplexity = 5.01167631/150.1562347 secs/batch = 0.6110s, grad.norm=0.61791164\n",
      "  7865: 5 [ 1230/ 1327], train_loss/perplexity = 5.13322115/169.5624237 secs/batch = 0.6085s, grad.norm=0.59423691\n",
      "  7870: 5 [ 1235/ 1327], train_loss/perplexity = 5.15673780/173.5972290 secs/batch = 0.6091s, grad.norm=0.52725577\n",
      "  7875: 5 [ 1240/ 1327], train_loss/perplexity = 5.26593590/193.6274414 secs/batch = 0.6096s, grad.norm=0.55736494\n",
      "  7880: 5 [ 1245/ 1327], train_loss/perplexity = 5.06381321/158.1925964 secs/batch = 0.6084s, grad.norm=0.56417173\n",
      "  7885: 5 [ 1250/ 1327], train_loss/perplexity = 5.26218176/192.9019012 secs/batch = 0.6042s, grad.norm=0.52946639\n",
      "  7890: 5 [ 1255/ 1327], train_loss/perplexity = 5.17332792/176.5012512 secs/batch = 0.6125s, grad.norm=0.59447062\n",
      "  7895: 5 [ 1260/ 1327], train_loss/perplexity = 5.22474432/185.8136597 secs/batch = 0.6110s, grad.norm=0.57472038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7900: 5 [ 1265/ 1327], train_loss/perplexity = 5.27243328/194.8896027 secs/batch = 0.6090s, grad.norm=0.60589302\n",
      "  7905: 5 [ 1270/ 1327], train_loss/perplexity = 5.08135271/160.9916840 secs/batch = 0.6116s, grad.norm=0.54296762\n",
      "  7910: 5 [ 1275/ 1327], train_loss/perplexity = 5.35877705/212.4649506 secs/batch = 0.6141s, grad.norm=0.61032277\n",
      "  7915: 5 [ 1280/ 1327], train_loss/perplexity = 5.16312838/174.7101593 secs/batch = 0.6086s, grad.norm=0.64898771\n",
      "  7920: 5 [ 1285/ 1327], train_loss/perplexity = 5.09539175/163.2677917 secs/batch = 0.6111s, grad.norm=0.53610522\n",
      "  7925: 5 [ 1290/ 1327], train_loss/perplexity = 5.23152018/187.0769806 secs/batch = 0.6074s, grad.norm=0.51896101\n",
      "  7930: 5 [ 1295/ 1327], train_loss/perplexity = 5.32998133/206.4341125 secs/batch = 0.6132s, grad.norm=0.53509027\n",
      "  7935: 5 [ 1300/ 1327], train_loss/perplexity = 5.34762430/210.1085510 secs/batch = 0.6130s, grad.norm=0.54114962\n",
      "  7940: 5 [ 1305/ 1327], train_loss/perplexity = 5.50454998/245.8078156 secs/batch = 0.6099s, grad.norm=0.59370720\n",
      "  7945: 5 [ 1310/ 1327], train_loss/perplexity = 5.70672655/300.8845215 secs/batch = 0.6079s, grad.norm=0.55132681\n",
      "  7950: 5 [ 1315/ 1327], train_loss/perplexity = 5.50052023/244.8192596 secs/batch = 0.6072s, grad.norm=0.54153436\n",
      "  7955: 5 [ 1320/ 1327], train_loss/perplexity = 5.47396135/238.4027252 secs/batch = 0.6054s, grad.norm=0.53837556\n",
      "  7960: 5 [ 1325/ 1327], train_loss/perplexity = 5.38367701/217.8217316 secs/batch = 0.6156s, grad.norm=0.54449046\n",
      "Epoch training time: 812.7702896595001\n",
      "Saved char model cv/epoch005_5.3389.model\n",
      "  7967: 6 [    5/ 1327], train_loss/perplexity = 5.54301882/255.4479828 secs/batch = 0.6079s, grad.norm=0.58239222\n",
      "  7972: 6 [   10/ 1327], train_loss/perplexity = 5.07421303/159.8463440 secs/batch = 0.6057s, grad.norm=0.54694790\n",
      "  7977: 6 [   15/ 1327], train_loss/perplexity = 5.16136456/174.4022675 secs/batch = 0.6052s, grad.norm=0.56183887\n",
      "  7982: 6 [   20/ 1327], train_loss/perplexity = 5.49230528/242.8163147 secs/batch = 0.6120s, grad.norm=0.57395625\n",
      "  7987: 6 [   25/ 1327], train_loss/perplexity = 5.35753679/212.2015991 secs/batch = 0.6061s, grad.norm=0.52737790\n",
      "  7992: 6 [   30/ 1327], train_loss/perplexity = 5.30135536/200.6085205 secs/batch = 0.6107s, grad.norm=0.53868139\n",
      "  7997: 6 [   35/ 1327], train_loss/perplexity = 5.22143269/185.1993256 secs/batch = 0.6095s, grad.norm=0.76212943\n",
      "  8002: 6 [   40/ 1327], train_loss/perplexity = 5.22394514/185.6652222 secs/batch = 0.6098s, grad.norm=0.55559796\n",
      "  8007: 6 [   45/ 1327], train_loss/perplexity = 4.88739204/132.6072845 secs/batch = 0.6142s, grad.norm=0.51925302\n",
      "  8012: 6 [   50/ 1327], train_loss/perplexity = 5.30399752/201.1392670 secs/batch = 0.6058s, grad.norm=0.55380362\n",
      "  8017: 6 [   55/ 1327], train_loss/perplexity = 5.18878222/179.2501373 secs/batch = 0.6101s, grad.norm=0.55771041\n",
      "  8022: 6 [   60/ 1327], train_loss/perplexity = 5.44711208/232.0869446 secs/batch = 0.6079s, grad.norm=0.57242757\n",
      "  8027: 6 [   65/ 1327], train_loss/perplexity = 4.99526978/147.7127838 secs/batch = 0.6127s, grad.norm=0.53835708\n",
      "  8032: 6 [   70/ 1327], train_loss/perplexity = 4.85010052/127.7532272 secs/batch = 0.6069s, grad.norm=0.56437653\n",
      "  8037: 6 [   75/ 1327], train_loss/perplexity = 4.90108585/134.4356842 secs/batch = 0.6081s, grad.norm=0.60063022\n",
      "  8042: 6 [   80/ 1327], train_loss/perplexity = 5.19450283/180.2784882 secs/batch = 0.6060s, grad.norm=0.52935785\n",
      "  8047: 6 [   85/ 1327], train_loss/perplexity = 5.25003576/190.5730896 secs/batch = 0.6132s, grad.norm=0.56643116\n",
      "  8052: 6 [   90/ 1327], train_loss/perplexity = 5.27477455/195.3464355 secs/batch = 0.6013s, grad.norm=0.56560385\n",
      "  8057: 6 [   95/ 1327], train_loss/perplexity = 5.10749149/165.2552948 secs/batch = 0.6058s, grad.norm=0.55158377\n",
      "  8062: 6 [  100/ 1327], train_loss/perplexity = 5.31845999/204.0693665 secs/batch = 0.6157s, grad.norm=0.57474750\n",
      "  8067: 6 [  105/ 1327], train_loss/perplexity = 5.40631437/222.8088837 secs/batch = 0.6065s, grad.norm=0.61839342\n",
      "  8072: 6 [  110/ 1327], train_loss/perplexity = 5.14625502/171.7869415 secs/batch = 0.6093s, grad.norm=0.53210467\n",
      "  8077: 6 [  115/ 1327], train_loss/perplexity = 5.00581694/149.2789917 secs/batch = 0.6057s, grad.norm=0.58750939\n",
      "  8082: 6 [  120/ 1327], train_loss/perplexity = 5.16627455/175.2606964 secs/batch = 0.6094s, grad.norm=0.62401313\n",
      "  8087: 6 [  125/ 1327], train_loss/perplexity = 5.30491877/201.3246460 secs/batch = 0.6134s, grad.norm=0.64957380\n",
      "  8092: 6 [  130/ 1327], train_loss/perplexity = 5.16876650/175.6979828 secs/batch = 0.6176s, grad.norm=0.58891398\n",
      "  8097: 6 [  135/ 1327], train_loss/perplexity = 5.18139744/177.9312897 secs/batch = 0.6056s, grad.norm=0.55939984\n",
      "  8102: 6 [  140/ 1327], train_loss/perplexity = 5.42751551/227.5831146 secs/batch = 0.6075s, grad.norm=0.56237161\n",
      "  8107: 6 [  145/ 1327], train_loss/perplexity = 5.40016937/221.4439240 secs/batch = 0.6114s, grad.norm=0.54703069\n",
      "  8112: 6 [  150/ 1327], train_loss/perplexity = 5.34236526/209.0064850 secs/batch = 0.6112s, grad.norm=0.59700823\n",
      "  8117: 6 [  155/ 1327], train_loss/perplexity = 5.58597946/266.6613464 secs/batch = 0.6149s, grad.norm=0.59683049\n",
      "  8122: 6 [  160/ 1327], train_loss/perplexity = 5.21063185/183.2097778 secs/batch = 0.6161s, grad.norm=0.60109043\n",
      "  8127: 6 [  165/ 1327], train_loss/perplexity = 5.45888948/234.8364868 secs/batch = 0.6096s, grad.norm=0.53882575\n",
      "  8132: 6 [  170/ 1327], train_loss/perplexity = 5.23959160/188.5930634 secs/batch = 0.6127s, grad.norm=0.56831056\n",
      "  8137: 6 [  175/ 1327], train_loss/perplexity = 5.38750696/218.6575775 secs/batch = 0.6069s, grad.norm=0.54839313\n",
      "  8142: 6 [  180/ 1327], train_loss/perplexity = 5.37037325/214.9430847 secs/batch = 0.6383s, grad.norm=0.54225767\n",
      "  8147: 6 [  185/ 1327], train_loss/perplexity = 5.55420113/258.3205261 secs/batch = 0.6082s, grad.norm=0.54369116\n",
      "  8152: 6 [  190/ 1327], train_loss/perplexity = 5.05942869/157.5005035 secs/batch = 0.6090s, grad.norm=0.50958884\n",
      "  8157: 6 [  195/ 1327], train_loss/perplexity = 5.29753017/199.8426208 secs/batch = 0.6084s, grad.norm=0.53860646\n",
      "  8162: 6 [  200/ 1327], train_loss/perplexity = 5.26978207/194.3735962 secs/batch = 0.6203s, grad.norm=0.65750545\n",
      "  8167: 6 [  205/ 1327], train_loss/perplexity = 5.30601263/201.5449829 secs/batch = 0.6082s, grad.norm=0.55055249\n",
      "  8172: 6 [  210/ 1327], train_loss/perplexity = 5.23200655/187.1679840 secs/batch = 0.6113s, grad.norm=0.51131284\n",
      "  8177: 6 [  215/ 1327], train_loss/perplexity = 5.31679535/203.7299500 secs/batch = 0.6118s, grad.norm=0.50524938\n",
      "  8182: 6 [  220/ 1327], train_loss/perplexity = 5.39864922/221.1075439 secs/batch = 0.6085s, grad.norm=0.54136544\n",
      "  8187: 6 [  225/ 1327], train_loss/perplexity = 5.52216625/250.1763916 secs/batch = 0.6105s, grad.norm=0.57163227\n",
      "  8192: 6 [  230/ 1327], train_loss/perplexity = 5.35515404/211.6965790 secs/batch = 0.6120s, grad.norm=0.57303447\n",
      "  8197: 6 [  235/ 1327], train_loss/perplexity = 5.23918009/188.5154724 secs/batch = 0.6023s, grad.norm=0.56193304\n",
      "  8202: 6 [  240/ 1327], train_loss/perplexity = 5.07246161/159.5666351 secs/batch = 0.6043s, grad.norm=0.57820076\n",
      "  8207: 6 [  245/ 1327], train_loss/perplexity = 5.38518572/218.1506195 secs/batch = 0.6128s, grad.norm=0.57764506\n",
      "  8212: 6 [  250/ 1327], train_loss/perplexity = 5.08399391/161.4174500 secs/batch = 0.6022s, grad.norm=0.52154112\n",
      "  8217: 6 [  255/ 1327], train_loss/perplexity = 5.17315817/176.4712830 secs/batch = 0.6456s, grad.norm=0.53341705\n",
      "  8222: 6 [  260/ 1327], train_loss/perplexity = 5.48583412/241.2500916 secs/batch = 0.6144s, grad.norm=0.57953531\n",
      "  8227: 6 [  265/ 1327], train_loss/perplexity = 5.45951366/234.9831085 secs/batch = 0.6170s, grad.norm=0.56327510\n",
      "  8232: 6 [  270/ 1327], train_loss/perplexity = 5.48595476/241.2792053 secs/batch = 0.6094s, grad.norm=0.52902478\n",
      "  8237: 6 [  275/ 1327], train_loss/perplexity = 5.66079187/287.3761292 secs/batch = 0.6134s, grad.norm=0.56247568\n",
      "  8242: 6 [  280/ 1327], train_loss/perplexity = 5.36300278/213.3646698 secs/batch = 0.6101s, grad.norm=0.59519672\n",
      "  8247: 6 [  285/ 1327], train_loss/perplexity = 5.56669521/261.5682373 secs/batch = 0.6093s, grad.norm=0.67009509\n",
      "  8252: 6 [  290/ 1327], train_loss/perplexity = 5.42739725/227.5561981 secs/batch = 0.6080s, grad.norm=0.55188334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8257: 6 [  295/ 1327], train_loss/perplexity = 5.15447044/173.2040558 secs/batch = 0.6079s, grad.norm=0.59243637\n",
      "  8262: 6 [  300/ 1327], train_loss/perplexity = 4.74513769/115.0236435 secs/batch = 0.6060s, grad.norm=0.58297604\n",
      "  8267: 6 [  305/ 1327], train_loss/perplexity = 5.20867682/182.8519440 secs/batch = 0.6120s, grad.norm=0.55545843\n",
      "  8272: 6 [  310/ 1327], train_loss/perplexity = 5.23733568/188.1680908 secs/batch = 0.6134s, grad.norm=0.60482007\n",
      "  8277: 6 [  315/ 1327], train_loss/perplexity = 4.91779661/136.7010803 secs/batch = 0.6188s, grad.norm=0.53261423\n",
      "  8282: 6 [  320/ 1327], train_loss/perplexity = 5.07404423/159.8193665 secs/batch = 0.6125s, grad.norm=0.54552019\n",
      "  8287: 6 [  325/ 1327], train_loss/perplexity = 4.88184834/131.8741913 secs/batch = 0.6104s, grad.norm=0.63217562\n",
      "  8292: 6 [  330/ 1327], train_loss/perplexity = 5.32922029/206.2770691 secs/batch = 0.6121s, grad.norm=0.66025716\n",
      "  8297: 6 [  335/ 1327], train_loss/perplexity = 4.57751656/97.2725220 secs/batch = 0.6064s, grad.norm=0.54272288\n",
      "  8302: 6 [  340/ 1327], train_loss/perplexity = 5.40267706/221.9999237 secs/batch = 0.6137s, grad.norm=0.57018137\n",
      "  8307: 6 [  345/ 1327], train_loss/perplexity = 5.24205875/189.0589294 secs/batch = 0.6088s, grad.norm=0.56548381\n",
      "  8312: 6 [  350/ 1327], train_loss/perplexity = 5.33416414/207.2994080 secs/batch = 0.6095s, grad.norm=0.57386923\n",
      "  8317: 6 [  355/ 1327], train_loss/perplexity = 5.38218355/217.4966736 secs/batch = 0.6134s, grad.norm=0.58207285\n",
      "  8322: 6 [  360/ 1327], train_loss/perplexity = 5.50580072/246.1154480 secs/batch = 0.6177s, grad.norm=0.56158906\n",
      "  8327: 6 [  365/ 1327], train_loss/perplexity = 5.40076113/221.5749969 secs/batch = 0.6102s, grad.norm=0.54477417\n",
      "  8332: 6 [  370/ 1327], train_loss/perplexity = 5.39747858/220.8488617 secs/batch = 0.6100s, grad.norm=0.59636009\n",
      "  8337: 6 [  375/ 1327], train_loss/perplexity = 4.80773735/122.4542313 secs/batch = 0.6257s, grad.norm=0.51038450\n",
      "  8342: 6 [  380/ 1327], train_loss/perplexity = 5.08923435/162.2655792 secs/batch = 0.6117s, grad.norm=0.57488060\n",
      "  8347: 6 [  385/ 1327], train_loss/perplexity = 5.24555779/189.7216034 secs/batch = 0.6120s, grad.norm=0.56561148\n",
      "  8352: 6 [  390/ 1327], train_loss/perplexity = 5.24803019/190.1912537 secs/batch = 0.6122s, grad.norm=0.54028201\n",
      "  8357: 6 [  395/ 1327], train_loss/perplexity = 5.51400280/248.1424103 secs/batch = 0.6086s, grad.norm=0.57266337\n",
      "  8362: 6 [  400/ 1327], train_loss/perplexity = 5.15004539/172.4393158 secs/batch = 0.6122s, grad.norm=0.51990807\n",
      "  8367: 6 [  405/ 1327], train_loss/perplexity = 5.50673628/246.3458099 secs/batch = 0.6197s, grad.norm=0.58947009\n",
      "  8372: 6 [  410/ 1327], train_loss/perplexity = 5.27070045/194.5521851 secs/batch = 0.6121s, grad.norm=0.55748641\n",
      "  8377: 6 [  415/ 1327], train_loss/perplexity = 5.08872747/162.1833496 secs/batch = 0.6088s, grad.norm=0.55940145\n",
      "  8382: 6 [  420/ 1327], train_loss/perplexity = 5.03742218/154.0723267 secs/batch = 0.6112s, grad.norm=0.57631236\n",
      "  8387: 6 [  425/ 1327], train_loss/perplexity = 5.27563000/195.5136108 secs/batch = 0.6217s, grad.norm=0.56622696\n",
      "  8392: 6 [  430/ 1327], train_loss/perplexity = 5.37914705/216.8372498 secs/batch = 0.6098s, grad.norm=0.58004528\n",
      "  8397: 6 [  435/ 1327], train_loss/perplexity = 5.41948748/225.7633820 secs/batch = 0.6146s, grad.norm=0.61093968\n",
      "  8402: 6 [  440/ 1327], train_loss/perplexity = 5.18964863/179.4055023 secs/batch = 0.6175s, grad.norm=0.59985906\n",
      "  8407: 6 [  445/ 1327], train_loss/perplexity = 5.29364204/199.0671234 secs/batch = 0.6057s, grad.norm=0.60500324\n",
      "  8412: 6 [  450/ 1327], train_loss/perplexity = 5.15118265/172.6355438 secs/batch = 0.6117s, grad.norm=0.56967217\n",
      "  8417: 6 [  455/ 1327], train_loss/perplexity = 4.95540619/141.9402466 secs/batch = 0.6098s, grad.norm=0.52413207\n",
      "  8422: 6 [  460/ 1327], train_loss/perplexity = 5.23281956/187.3202209 secs/batch = 0.6078s, grad.norm=0.57509601\n",
      "  8427: 6 [  465/ 1327], train_loss/perplexity = 5.10408974/164.6940918 secs/batch = 0.6132s, grad.norm=0.64821613\n",
      "  8432: 6 [  470/ 1327], train_loss/perplexity = 5.53243923/252.7597046 secs/batch = 0.6106s, grad.norm=0.60026050\n",
      "  8437: 6 [  475/ 1327], train_loss/perplexity = 5.16119719/174.3730927 secs/batch = 0.6496s, grad.norm=0.50855231\n",
      "  8442: 6 [  480/ 1327], train_loss/perplexity = 5.25400829/191.3316498 secs/batch = 0.6080s, grad.norm=0.60643256\n",
      "  8447: 6 [  485/ 1327], train_loss/perplexity = 5.17718649/177.1835938 secs/batch = 0.6082s, grad.norm=0.53439927\n",
      "  8452: 6 [  490/ 1327], train_loss/perplexity = 5.13880205/170.5113831 secs/batch = 0.6073s, grad.norm=0.58709890\n",
      "  8457: 6 [  495/ 1327], train_loss/perplexity = 5.00945711/149.8233795 secs/batch = 0.6101s, grad.norm=0.54801553\n",
      "  8462: 6 [  500/ 1327], train_loss/perplexity = 5.36513376/213.8198395 secs/batch = 0.6124s, grad.norm=0.62008625\n",
      "  8467: 6 [  505/ 1327], train_loss/perplexity = 5.16887140/175.7164154 secs/batch = 0.6112s, grad.norm=0.57369053\n",
      "  8472: 6 [  510/ 1327], train_loss/perplexity = 5.58055258/265.2181091 secs/batch = 0.6128s, grad.norm=0.56537437\n",
      "  8477: 6 [  515/ 1327], train_loss/perplexity = 5.27778769/195.9359283 secs/batch = 0.6089s, grad.norm=0.57016480\n",
      "  8482: 6 [  520/ 1327], train_loss/perplexity = 5.45550680/234.0434570 secs/batch = 0.6058s, grad.norm=0.58760965\n",
      "  8487: 6 [  525/ 1327], train_loss/perplexity = 5.09016037/162.4159088 secs/batch = 0.6151s, grad.norm=0.57304668\n",
      "  8492: 6 [  530/ 1327], train_loss/perplexity = 5.15651751/173.5589905 secs/batch = 0.6098s, grad.norm=0.61755258\n",
      "  8497: 6 [  535/ 1327], train_loss/perplexity = 5.23447752/187.6310425 secs/batch = 0.6096s, grad.norm=0.60002738\n",
      "  8502: 6 [  540/ 1327], train_loss/perplexity = 5.30669308/201.6821747 secs/batch = 0.6124s, grad.norm=0.51931077\n",
      "  8507: 6 [  545/ 1327], train_loss/perplexity = 5.42371035/226.7187653 secs/batch = 0.6072s, grad.norm=0.54551375\n",
      "  8512: 6 [  550/ 1327], train_loss/perplexity = 5.31236029/202.8283997 secs/batch = 0.6091s, grad.norm=0.52894264\n",
      "  8517: 6 [  555/ 1327], train_loss/perplexity = 5.17376757/176.5788574 secs/batch = 0.6140s, grad.norm=0.53459269\n",
      "  8522: 6 [  560/ 1327], train_loss/perplexity = 5.21045637/183.1776428 secs/batch = 0.6095s, grad.norm=0.56815404\n",
      "  8527: 6 [  565/ 1327], train_loss/perplexity = 5.25851536/192.1959381 secs/batch = 0.6140s, grad.norm=0.56945592\n",
      "  8532: 6 [  570/ 1327], train_loss/perplexity = 5.09011602/162.4087067 secs/batch = 0.6225s, grad.norm=0.58239138\n",
      "  8537: 6 [  575/ 1327], train_loss/perplexity = 5.06816149/158.8819580 secs/batch = 0.6206s, grad.norm=0.63318324\n",
      "  8542: 6 [  580/ 1327], train_loss/perplexity = 5.39489794/220.2796631 secs/batch = 0.6110s, grad.norm=0.66165352\n",
      "  8547: 6 [  585/ 1327], train_loss/perplexity = 5.04462910/155.1867218 secs/batch = 0.6071s, grad.norm=0.58126950\n",
      "  8552: 6 [  590/ 1327], train_loss/perplexity = 5.36129475/213.0005493 secs/batch = 0.6098s, grad.norm=0.58452731\n",
      "  8557: 6 [  595/ 1327], train_loss/perplexity = 5.19986486/181.2477417 secs/batch = 0.6157s, grad.norm=0.63705623\n",
      "  8562: 6 [  600/ 1327], train_loss/perplexity = 5.46004200/235.1072998 secs/batch = 0.6130s, grad.norm=0.51743484\n",
      "  8567: 6 [  605/ 1327], train_loss/perplexity = 5.43105173/228.3893280 secs/batch = 0.6103s, grad.norm=0.54466486\n",
      "  8572: 6 [  610/ 1327], train_loss/perplexity = 5.52143002/249.9922791 secs/batch = 0.6114s, grad.norm=0.55529761\n",
      "  8577: 6 [  615/ 1327], train_loss/perplexity = 4.92169428/137.2349243 secs/batch = 0.6098s, grad.norm=0.53516489\n",
      "  8582: 6 [  620/ 1327], train_loss/perplexity = 5.33553076/207.5829010 secs/batch = 0.6126s, grad.norm=0.53806996\n",
      "  8587: 6 [  625/ 1327], train_loss/perplexity = 5.46750498/236.8684692 secs/batch = 0.6122s, grad.norm=0.57090563\n",
      "  8592: 6 [  630/ 1327], train_loss/perplexity = 5.47270250/238.1027985 secs/batch = 0.6061s, grad.norm=0.55891800\n",
      "  8597: 6 [  635/ 1327], train_loss/perplexity = 5.17376518/176.5784302 secs/batch = 0.6137s, grad.norm=0.55692816\n",
      "  8602: 6 [  640/ 1327], train_loss/perplexity = 5.29359531/199.0578156 secs/batch = 0.6067s, grad.norm=0.56312573\n",
      "  8607: 6 [  645/ 1327], train_loss/perplexity = 5.44939041/232.6163177 secs/batch = 0.6190s, grad.norm=0.57280666\n",
      "  8612: 6 [  650/ 1327], train_loss/perplexity = 5.15432024/173.1780548 secs/batch = 0.6138s, grad.norm=0.64257276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8617: 6 [  655/ 1327], train_loss/perplexity = 5.19249010/179.9160004 secs/batch = 0.6087s, grad.norm=0.53730059\n",
      "  8622: 6 [  660/ 1327], train_loss/perplexity = 5.09863138/163.7975769 secs/batch = 0.6123s, grad.norm=0.56193095\n",
      "  8627: 6 [  665/ 1327], train_loss/perplexity = 5.24739885/190.0712280 secs/batch = 0.6121s, grad.norm=0.53390592\n",
      "  8632: 6 [  670/ 1327], train_loss/perplexity = 5.16757250/175.4883270 secs/batch = 0.6161s, grad.norm=0.55194575\n",
      "  8637: 6 [  675/ 1327], train_loss/perplexity = 4.94534111/140.5187683 secs/batch = 0.6155s, grad.norm=0.60220706\n",
      "  8642: 6 [  680/ 1327], train_loss/perplexity = 5.28880644/198.1068268 secs/batch = 0.6141s, grad.norm=0.58292645\n",
      "  8647: 6 [  685/ 1327], train_loss/perplexity = 5.28955460/198.2550964 secs/batch = 0.6034s, grad.norm=0.55342972\n",
      "  8652: 6 [  690/ 1327], train_loss/perplexity = 5.35660696/212.0043793 secs/batch = 0.6141s, grad.norm=0.55073613\n",
      "  8657: 6 [  695/ 1327], train_loss/perplexity = 5.18696833/178.9252930 secs/batch = 0.6134s, grad.norm=0.53127712\n",
      "  8662: 6 [  700/ 1327], train_loss/perplexity = 5.39735746/220.8221130 secs/batch = 0.6094s, grad.norm=0.53275597\n",
      "  8667: 6 [  705/ 1327], train_loss/perplexity = 5.11684322/166.8079681 secs/batch = 0.6146s, grad.norm=0.57346851\n",
      "  8672: 6 [  710/ 1327], train_loss/perplexity = 5.13843393/170.4486237 secs/batch = 0.6123s, grad.norm=0.51837122\n",
      "  8677: 6 [  715/ 1327], train_loss/perplexity = 5.14071083/170.8371582 secs/batch = 0.6081s, grad.norm=0.56145048\n",
      "  8682: 6 [  720/ 1327], train_loss/perplexity = 5.21996975/184.9285889 secs/batch = 0.6550s, grad.norm=0.54694146\n",
      "  8687: 6 [  725/ 1327], train_loss/perplexity = 4.95056438/141.2546692 secs/batch = 0.6056s, grad.norm=0.55678707\n",
      "  8692: 6 [  730/ 1327], train_loss/perplexity = 5.11309052/166.1831512 secs/batch = 0.6069s, grad.norm=0.60646886\n",
      "  8697: 6 [  735/ 1327], train_loss/perplexity = 5.29854345/200.0452271 secs/batch = 0.6074s, grad.norm=0.56960964\n",
      "  8702: 6 [  740/ 1327], train_loss/perplexity = 4.68507051/108.3179092 secs/batch = 0.6101s, grad.norm=0.59626108\n",
      "  8707: 6 [  745/ 1327], train_loss/perplexity = 5.18310165/178.2347717 secs/batch = 0.6118s, grad.norm=0.54884726\n",
      "  8712: 6 [  750/ 1327], train_loss/perplexity = 5.01551342/150.7335052 secs/batch = 0.6095s, grad.norm=0.57435894\n",
      "  8717: 6 [  755/ 1327], train_loss/perplexity = 5.05593061/156.9505157 secs/batch = 0.6106s, grad.norm=0.51705748\n",
      "  8722: 6 [  760/ 1327], train_loss/perplexity = 5.02257013/151.8009491 secs/batch = 0.6017s, grad.norm=0.63882297\n",
      "  8727: 6 [  765/ 1327], train_loss/perplexity = 5.06542158/158.4472198 secs/batch = 0.6107s, grad.norm=0.59286577\n",
      "  8732: 6 [  770/ 1327], train_loss/perplexity = 5.03517771/153.7269135 secs/batch = 0.6084s, grad.norm=0.59967625\n",
      "  8737: 6 [  775/ 1327], train_loss/perplexity = 5.11135817/165.8955231 secs/batch = 0.6112s, grad.norm=0.58379471\n",
      "  8742: 6 [  780/ 1327], train_loss/perplexity = 5.32610559/205.6355896 secs/batch = 0.6125s, grad.norm=0.54873890\n",
      "  8747: 6 [  785/ 1327], train_loss/perplexity = 5.17332840/176.5013275 secs/batch = 0.6109s, grad.norm=0.57932156\n",
      "  8752: 6 [  790/ 1327], train_loss/perplexity = 5.03072214/153.0434875 secs/batch = 0.6134s, grad.norm=0.60992360\n",
      "  8757: 6 [  795/ 1327], train_loss/perplexity = 5.31839371/204.0558472 secs/batch = 0.6090s, grad.norm=0.54426622\n",
      "  8762: 6 [  800/ 1327], train_loss/perplexity = 5.27106667/194.6234436 secs/batch = 0.6119s, grad.norm=0.56124556\n",
      "  8767: 6 [  805/ 1327], train_loss/perplexity = 5.52586937/251.1045532 secs/batch = 0.6060s, grad.norm=0.55523753\n",
      "  8772: 6 [  810/ 1327], train_loss/perplexity = 5.21084547/183.2489319 secs/batch = 0.6118s, grad.norm=0.58884037\n",
      "  8777: 6 [  815/ 1327], train_loss/perplexity = 5.10840368/165.4060974 secs/batch = 0.6105s, grad.norm=0.56276721\n",
      "  8782: 6 [  820/ 1327], train_loss/perplexity = 4.79055405/120.3680420 secs/batch = 0.6136s, grad.norm=0.57948530\n",
      "  8787: 6 [  825/ 1327], train_loss/perplexity = 4.94672108/140.7128143 secs/batch = 0.6081s, grad.norm=0.53560060\n",
      "  8792: 6 [  830/ 1327], train_loss/perplexity = 4.84437513/127.0238876 secs/batch = 0.6125s, grad.norm=0.57509661\n",
      "  8797: 6 [  835/ 1327], train_loss/perplexity = 5.09177160/162.6778107 secs/batch = 0.6168s, grad.norm=0.57509971\n",
      "  8802: 6 [  840/ 1327], train_loss/perplexity = 5.26293039/193.0463715 secs/batch = 0.6142s, grad.norm=0.57883406\n",
      "  8807: 6 [  845/ 1327], train_loss/perplexity = 5.03985405/154.4474792 secs/batch = 0.6087s, grad.norm=0.56696504\n",
      "  8812: 6 [  850/ 1327], train_loss/perplexity = 5.12844992/168.7553253 secs/batch = 0.6126s, grad.norm=0.57623535\n",
      "  8817: 6 [  855/ 1327], train_loss/perplexity = 5.09844971/163.7678223 secs/batch = 0.6136s, grad.norm=0.59208959\n",
      "  8822: 6 [  860/ 1327], train_loss/perplexity = 4.84273863/126.8161774 secs/batch = 0.6144s, grad.norm=0.57256448\n",
      "  8827: 6 [  865/ 1327], train_loss/perplexity = 5.29605961/199.5489502 secs/batch = 0.6110s, grad.norm=0.57062662\n",
      "  8832: 6 [  870/ 1327], train_loss/perplexity = 5.27698612/195.7789307 secs/batch = 0.6089s, grad.norm=0.58862561\n",
      "  8837: 6 [  875/ 1327], train_loss/perplexity = 4.91295910/136.0413818 secs/batch = 0.6055s, grad.norm=0.66502464\n",
      "  8842: 6 [  880/ 1327], train_loss/perplexity = 4.99547005/147.7423706 secs/batch = 0.6094s, grad.norm=0.50955474\n",
      "  8847: 6 [  885/ 1327], train_loss/perplexity = 5.05326939/156.5334015 secs/batch = 0.6105s, grad.norm=0.59464109\n",
      "  8852: 6 [  890/ 1327], train_loss/perplexity = 5.26829004/194.0838013 secs/batch = 0.6059s, grad.norm=0.56309295\n",
      "  8857: 6 [  895/ 1327], train_loss/perplexity = 5.31904840/204.1894836 secs/batch = 0.6109s, grad.norm=0.55782270\n",
      "  8862: 6 [  900/ 1327], train_loss/perplexity = 5.22006559/184.9463196 secs/batch = 0.6019s, grad.norm=0.63514262\n",
      "  8867: 6 [  905/ 1327], train_loss/perplexity = 5.01929712/151.3049164 secs/batch = 0.6166s, grad.norm=0.59065503\n",
      "  8872: 6 [  910/ 1327], train_loss/perplexity = 5.11147594/165.9150543 secs/batch = 0.6093s, grad.norm=0.61985642\n",
      "  8877: 6 [  915/ 1327], train_loss/perplexity = 5.41230869/224.1484833 secs/batch = 0.6111s, grad.norm=0.60594130\n",
      "  8882: 6 [  920/ 1327], train_loss/perplexity = 5.44159126/230.8091736 secs/batch = 0.6114s, grad.norm=0.59950805\n",
      "  8887: 6 [  925/ 1327], train_loss/perplexity = 5.19473267/180.3199310 secs/batch = 0.6082s, grad.norm=0.56007248\n",
      "  8892: 6 [  930/ 1327], train_loss/perplexity = 5.11944771/167.2429810 secs/batch = 0.6141s, grad.norm=0.56994915\n",
      "  8897: 6 [  935/ 1327], train_loss/perplexity = 5.23211193/187.1877136 secs/batch = 0.6077s, grad.norm=0.54781389\n",
      "  8902: 6 [  940/ 1327], train_loss/perplexity = 5.16660166/175.3180389 secs/batch = 0.6129s, grad.norm=0.55996370\n",
      "  8907: 6 [  945/ 1327], train_loss/perplexity = 5.39562273/220.4393768 secs/batch = 0.6074s, grad.norm=0.57060391\n",
      "  8912: 6 [  950/ 1327], train_loss/perplexity = 5.14815426/172.1135254 secs/batch = 0.6151s, grad.norm=0.57258940\n",
      "  8917: 6 [  955/ 1327], train_loss/perplexity = 5.23784065/188.2631378 secs/batch = 0.6075s, grad.norm=0.56089664\n",
      "  8922: 6 [  960/ 1327], train_loss/perplexity = 5.52506590/250.9028778 secs/batch = 0.6139s, grad.norm=0.55014902\n",
      "  8927: 6 [  965/ 1327], train_loss/perplexity = 5.28516912/197.3875580 secs/batch = 0.6264s, grad.norm=0.59056252\n",
      "  8932: 6 [  970/ 1327], train_loss/perplexity = 5.43159103/228.5125275 secs/batch = 0.6163s, grad.norm=0.49523473\n",
      "  8937: 6 [  975/ 1327], train_loss/perplexity = 5.22655058/186.1495819 secs/batch = 0.6079s, grad.norm=0.62322545\n",
      "  8942: 6 [  980/ 1327], train_loss/perplexity = 4.94262743/140.1379700 secs/batch = 0.6136s, grad.norm=0.50869256\n",
      "  8947: 6 [  985/ 1327], train_loss/perplexity = 5.20386362/181.9739685 secs/batch = 0.6148s, grad.norm=0.58809155\n",
      "  8952: 6 [  990/ 1327], train_loss/perplexity = 5.36837435/214.5138550 secs/batch = 0.6209s, grad.norm=0.61709648\n",
      "  8957: 6 [  995/ 1327], train_loss/perplexity = 5.30996609/202.3433685 secs/batch = 0.6121s, grad.norm=0.59598535\n",
      "  8962: 6 [ 1000/ 1327], train_loss/perplexity = 4.85471106/128.3435974 secs/batch = 0.6131s, grad.norm=0.54490089\n",
      "  8967: 6 [ 1005/ 1327], train_loss/perplexity = 5.29274845/198.8893127 secs/batch = 0.6109s, grad.norm=0.58475107\n",
      "  8972: 6 [ 1010/ 1327], train_loss/perplexity = 4.84887457/127.5967102 secs/batch = 0.6102s, grad.norm=0.53425503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8977: 6 [ 1015/ 1327], train_loss/perplexity = 5.31061316/202.4743347 secs/batch = 0.6526s, grad.norm=0.56053561\n",
      "  8982: 6 [ 1020/ 1327], train_loss/perplexity = 5.48528194/241.1169128 secs/batch = 0.6117s, grad.norm=0.61306727\n",
      "  8987: 6 [ 1025/ 1327], train_loss/perplexity = 5.31436062/203.2345276 secs/batch = 0.6170s, grad.norm=0.56537282\n",
      "  8992: 6 [ 1030/ 1327], train_loss/perplexity = 5.16183853/174.4849548 secs/batch = 0.6214s, grad.norm=0.53831470\n",
      "  8997: 6 [ 1035/ 1327], train_loss/perplexity = 5.00261688/148.8020477 secs/batch = 0.6112s, grad.norm=0.56214643\n",
      "  9002: 6 [ 1040/ 1327], train_loss/perplexity = 5.32679319/205.7770233 secs/batch = 0.6108s, grad.norm=0.52493852\n",
      "  9007: 6 [ 1045/ 1327], train_loss/perplexity = 4.95226526/141.4951172 secs/batch = 0.6097s, grad.norm=0.56528670\n",
      "  9012: 6 [ 1050/ 1327], train_loss/perplexity = 4.95901585/142.4535370 secs/batch = 0.6062s, grad.norm=0.58117938\n",
      "  9017: 6 [ 1055/ 1327], train_loss/perplexity = 5.23197746/187.1625519 secs/batch = 0.6078s, grad.norm=0.59054261\n",
      "  9022: 6 [ 1060/ 1327], train_loss/perplexity = 4.80099964/121.6319427 secs/batch = 0.6098s, grad.norm=0.61479437\n",
      "  9027: 6 [ 1065/ 1327], train_loss/perplexity = 4.89897013/134.1515503 secs/batch = 0.6100s, grad.norm=0.53466159\n",
      "  9032: 6 [ 1070/ 1327], train_loss/perplexity = 5.28148079/196.6608734 secs/batch = 0.6128s, grad.norm=0.59916770\n",
      "  9037: 6 [ 1075/ 1327], train_loss/perplexity = 5.05164957/156.2800446 secs/batch = 0.6135s, grad.norm=0.62268698\n",
      "  9042: 6 [ 1080/ 1327], train_loss/perplexity = 4.92997694/138.3763275 secs/batch = 0.6064s, grad.norm=0.54550219\n",
      "  9047: 6 [ 1085/ 1327], train_loss/perplexity = 4.81658602/123.5425949 secs/batch = 0.6136s, grad.norm=0.61351842\n",
      "  9052: 6 [ 1090/ 1327], train_loss/perplexity = 5.04207468/154.7908173 secs/batch = 0.6147s, grad.norm=0.59407437\n",
      "  9057: 6 [ 1095/ 1327], train_loss/perplexity = 5.19211197/179.8479919 secs/batch = 0.6075s, grad.norm=0.64970213\n",
      "  9062: 6 [ 1100/ 1327], train_loss/perplexity = 5.12812757/168.7009430 secs/batch = 0.6108s, grad.norm=0.67136818\n",
      "  9067: 6 [ 1105/ 1327], train_loss/perplexity = 4.96977520/143.9945068 secs/batch = 0.6068s, grad.norm=0.58715546\n",
      "  9072: 6 [ 1110/ 1327], train_loss/perplexity = 5.50645494/246.2765198 secs/batch = 0.6097s, grad.norm=0.62092686\n",
      "  9077: 6 [ 1115/ 1327], train_loss/perplexity = 5.03773355/154.1203156 secs/batch = 0.6089s, grad.norm=0.61154312\n",
      "  9082: 6 [ 1120/ 1327], train_loss/perplexity = 5.18632698/178.8105774 secs/batch = 0.6063s, grad.norm=0.54728878\n",
      "  9087: 6 [ 1125/ 1327], train_loss/perplexity = 5.41987610/225.8511353 secs/batch = 0.6071s, grad.norm=0.62453103\n",
      "  9092: 6 [ 1130/ 1327], train_loss/perplexity = 5.08901453/162.2299042 secs/batch = 0.6151s, grad.norm=0.58231676\n",
      "  9097: 6 [ 1135/ 1327], train_loss/perplexity = 5.09595060/163.3590546 secs/batch = 0.6122s, grad.norm=0.53520477\n",
      "  9102: 6 [ 1140/ 1327], train_loss/perplexity = 5.38858604/218.8936615 secs/batch = 0.6148s, grad.norm=0.59371585\n",
      "  9107: 6 [ 1145/ 1327], train_loss/perplexity = 5.11159945/165.9355469 secs/batch = 0.6095s, grad.norm=0.62950116\n",
      "  9112: 6 [ 1150/ 1327], train_loss/perplexity = 5.07726574/160.3350525 secs/batch = 0.6105s, grad.norm=0.54870719\n",
      "  9117: 6 [ 1155/ 1327], train_loss/perplexity = 5.21341562/183.7205048 secs/batch = 0.6125s, grad.norm=0.58323973\n",
      "  9122: 6 [ 1160/ 1327], train_loss/perplexity = 5.11793089/166.9894867 secs/batch = 0.6102s, grad.norm=0.55779105\n",
      "  9127: 6 [ 1165/ 1327], train_loss/perplexity = 5.21980381/184.8979034 secs/batch = 0.6126s, grad.norm=0.58036482\n",
      "  9132: 6 [ 1170/ 1327], train_loss/perplexity = 5.11380863/166.3025360 secs/batch = 0.6128s, grad.norm=0.60282040\n",
      "  9137: 6 [ 1175/ 1327], train_loss/perplexity = 4.89041281/133.0084686 secs/batch = 0.6133s, grad.norm=0.68863893\n",
      "  9142: 6 [ 1180/ 1327], train_loss/perplexity = 4.84544849/127.1603012 secs/batch = 0.6068s, grad.norm=0.60120559\n",
      "  9147: 6 [ 1185/ 1327], train_loss/perplexity = 5.08701992/161.9066467 secs/batch = 0.6130s, grad.norm=0.55571872\n",
      "  9152: 6 [ 1190/ 1327], train_loss/perplexity = 5.11211681/166.0214233 secs/batch = 0.6081s, grad.norm=0.53649241\n",
      "  9157: 6 [ 1195/ 1327], train_loss/perplexity = 4.93782282/139.4662781 secs/batch = 0.6068s, grad.norm=0.61264384\n",
      "  9162: 6 [ 1200/ 1327], train_loss/perplexity = 4.88001633/131.6328125 secs/batch = 0.6039s, grad.norm=0.62106371\n",
      "  9167: 6 [ 1205/ 1327], train_loss/perplexity = 4.95636177/142.0759430 secs/batch = 0.6111s, grad.norm=0.62202466\n",
      "  9172: 6 [ 1210/ 1327], train_loss/perplexity = 4.71992779/112.1601562 secs/batch = 0.6217s, grad.norm=0.58351272\n",
      "  9177: 6 [ 1215/ 1327], train_loss/perplexity = 4.81945229/123.8972092 secs/batch = 0.6075s, grad.norm=0.58380735\n",
      "  9182: 6 [ 1220/ 1327], train_loss/perplexity = 4.94971657/141.1349487 secs/batch = 0.6093s, grad.norm=0.57522780\n",
      "  9187: 6 [ 1225/ 1327], train_loss/perplexity = 4.90253639/134.6308289 secs/batch = 0.6055s, grad.norm=0.63285077\n",
      "  9192: 6 [ 1230/ 1327], train_loss/perplexity = 4.96333265/143.0698090 secs/batch = 0.6112s, grad.norm=0.56051904\n",
      "  9197: 6 [ 1235/ 1327], train_loss/perplexity = 5.03208447/153.2521362 secs/batch = 0.6632s, grad.norm=0.55539596\n",
      "  9202: 6 [ 1240/ 1327], train_loss/perplexity = 5.15657520/173.5690002 secs/batch = 0.6169s, grad.norm=0.56287229\n",
      "  9207: 6 [ 1245/ 1327], train_loss/perplexity = 4.97558880/144.8340759 secs/batch = 0.6124s, grad.norm=0.57470638\n",
      "  9212: 6 [ 1250/ 1327], train_loss/perplexity = 5.10781193/165.3082581 secs/batch = 0.6069s, grad.norm=0.54171050\n",
      "  9217: 6 [ 1255/ 1327], train_loss/perplexity = 5.10152674/164.2725220 secs/batch = 0.6077s, grad.norm=0.62263614\n",
      "  9222: 6 [ 1260/ 1327], train_loss/perplexity = 5.08533812/161.6345825 secs/batch = 0.6539s, grad.norm=0.62928540\n",
      "  9227: 6 [ 1265/ 1327], train_loss/perplexity = 5.14931965/172.3142090 secs/batch = 0.6060s, grad.norm=0.58615506\n",
      "  9232: 6 [ 1270/ 1327], train_loss/perplexity = 4.97642851/144.9557495 secs/batch = 0.6187s, grad.norm=0.55502611\n",
      "  9237: 6 [ 1275/ 1327], train_loss/perplexity = 5.22395945/185.6678772 secs/batch = 0.6066s, grad.norm=0.61684489\n",
      "  9242: 6 [ 1280/ 1327], train_loss/perplexity = 5.02513933/152.1914673 secs/batch = 0.6117s, grad.norm=0.64174074\n",
      "  9247: 6 [ 1285/ 1327], train_loss/perplexity = 4.99593544/147.8111420 secs/batch = 0.6083s, grad.norm=0.60551053\n",
      "  9252: 6 [ 1290/ 1327], train_loss/perplexity = 5.14452934/171.4907532 secs/batch = 0.6101s, grad.norm=0.56615019\n",
      "  9257: 6 [ 1295/ 1327], train_loss/perplexity = 5.18227673/178.0878143 secs/batch = 0.6186s, grad.norm=0.53478193\n",
      "  9262: 6 [ 1300/ 1327], train_loss/perplexity = 5.23144245/187.0624390 secs/batch = 0.6176s, grad.norm=0.60739702\n",
      "  9267: 6 [ 1305/ 1327], train_loss/perplexity = 5.36970758/214.8000488 secs/batch = 0.6170s, grad.norm=0.59181815\n",
      "  9272: 6 [ 1310/ 1327], train_loss/perplexity = 5.58927679/267.5420532 secs/batch = 0.6103s, grad.norm=0.56447464\n",
      "  9277: 6 [ 1315/ 1327], train_loss/perplexity = 5.41387653/224.5001831 secs/batch = 0.6064s, grad.norm=0.63331604\n",
      "  9282: 6 [ 1320/ 1327], train_loss/perplexity = 5.34966469/210.5376892 secs/batch = 0.6156s, grad.norm=0.55383414\n",
      "  9287: 6 [ 1325/ 1327], train_loss/perplexity = 5.27935934/196.2441101 secs/batch = 0.6095s, grad.norm=0.55993390\n",
      "Epoch training time: 812.6736283302307\n",
      "Saved char model cv/epoch006_5.2286.model\n",
      "  9294: 7 [    5/ 1327], train_loss/perplexity = 5.42426968/226.8456116 secs/batch = 0.6157s, grad.norm=0.56292832\n",
      "  9299: 7 [   10/ 1327], train_loss/perplexity = 5.00225067/148.7475586 secs/batch = 0.6102s, grad.norm=0.58543533\n",
      "  9304: 7 [   15/ 1327], train_loss/perplexity = 5.09989071/164.0039825 secs/batch = 0.6105s, grad.norm=0.56053579\n",
      "  9309: 7 [   20/ 1327], train_loss/perplexity = 5.39011669/219.2289734 secs/batch = 0.6060s, grad.norm=0.60797900\n",
      "  9314: 7 [   25/ 1327], train_loss/perplexity = 5.25228024/191.0012970 secs/batch = 0.6133s, grad.norm=0.59427965\n",
      "  9319: 7 [   30/ 1327], train_loss/perplexity = 5.16449404/174.9489136 secs/batch = 0.6156s, grad.norm=0.55428880\n",
      "  9324: 7 [   35/ 1327], train_loss/perplexity = 5.02761984/152.5694427 secs/batch = 0.6100s, grad.norm=0.61773556\n",
      "  9329: 7 [   40/ 1327], train_loss/perplexity = 5.11480141/166.4677124 secs/batch = 0.6076s, grad.norm=0.57636034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9334: 7 [   45/ 1327], train_loss/perplexity = 4.80904293/122.6142120 secs/batch = 0.6122s, grad.norm=0.58439070\n",
      "  9339: 7 [   50/ 1327], train_loss/perplexity = 5.17383242/176.5903168 secs/batch = 0.6088s, grad.norm=0.55175817\n",
      "  9344: 7 [   55/ 1327], train_loss/perplexity = 5.05928659/157.4781342 secs/batch = 0.6079s, grad.norm=0.57119364\n",
      "  9349: 7 [   60/ 1327], train_loss/perplexity = 5.34332705/209.2075958 secs/batch = 0.6052s, grad.norm=0.60469395\n",
      "  9354: 7 [   65/ 1327], train_loss/perplexity = 4.88515997/132.3116302 secs/batch = 0.6093s, grad.norm=0.53662843\n",
      "  9359: 7 [   70/ 1327], train_loss/perplexity = 4.74500608/115.0085068 secs/batch = 0.6009s, grad.norm=0.59752649\n",
      "  9364: 7 [   75/ 1327], train_loss/perplexity = 4.78937244/120.2258987 secs/batch = 0.6097s, grad.norm=0.59764552\n",
      "  9369: 7 [   80/ 1327], train_loss/perplexity = 5.10174370/164.3081665 secs/batch = 0.6628s, grad.norm=0.59752655\n",
      "  9374: 7 [   85/ 1327], train_loss/perplexity = 5.07449722/159.8917847 secs/batch = 0.6095s, grad.norm=0.55193710\n",
      "  9379: 7 [   90/ 1327], train_loss/perplexity = 5.15960884/174.0963440 secs/batch = 0.6063s, grad.norm=0.63170099\n",
      "  9384: 7 [   95/ 1327], train_loss/perplexity = 4.98457813/146.1419067 secs/batch = 0.6144s, grad.norm=0.56047326\n",
      "  9389: 7 [  100/ 1327], train_loss/perplexity = 5.21608973/184.2124481 secs/batch = 0.6140s, grad.norm=0.57337862\n",
      "  9394: 7 [  105/ 1327], train_loss/perplexity = 5.24764919/190.1188049 secs/batch = 0.6109s, grad.norm=0.66180259\n",
      "  9399: 7 [  110/ 1327], train_loss/perplexity = 4.99519300/147.7014465 secs/batch = 0.6090s, grad.norm=0.54986614\n",
      "  9404: 7 [  115/ 1327], train_loss/perplexity = 4.91418123/136.2077484 secs/batch = 0.6048s, grad.norm=0.56086230\n",
      "  9409: 7 [  120/ 1327], train_loss/perplexity = 5.01470518/150.6117249 secs/batch = 0.6092s, grad.norm=0.62466711\n",
      "  9414: 7 [  125/ 1327], train_loss/perplexity = 5.17443228/176.6962738 secs/batch = 0.6082s, grad.norm=0.64291936\n",
      "  9419: 7 [  130/ 1327], train_loss/perplexity = 5.00840235/149.6654358 secs/batch = 0.6160s, grad.norm=0.58425522\n",
      "  9424: 7 [  135/ 1327], train_loss/perplexity = 5.03733253/154.0585175 secs/batch = 0.6118s, grad.norm=0.58033419\n",
      "  9429: 7 [  140/ 1327], train_loss/perplexity = 5.34695959/209.9689331 secs/batch = 0.6144s, grad.norm=0.61914533\n",
      "  9434: 7 [  145/ 1327], train_loss/perplexity = 5.32002354/204.3886871 secs/batch = 0.6065s, grad.norm=0.56687605\n",
      "  9439: 7 [  150/ 1327], train_loss/perplexity = 5.21901321/184.7517853 secs/batch = 0.6104s, grad.norm=0.63503283\n",
      "  9444: 7 [  155/ 1327], train_loss/perplexity = 5.43400621/229.0650940 secs/batch = 0.6075s, grad.norm=0.61836320\n",
      "  9449: 7 [  160/ 1327], train_loss/perplexity = 5.12478495/168.1379852 secs/batch = 0.6149s, grad.norm=0.62722653\n",
      "  9454: 7 [  165/ 1327], train_loss/perplexity = 5.35417747/211.4899445 secs/batch = 0.6051s, grad.norm=0.55349839\n",
      "  9459: 7 [  170/ 1327], train_loss/perplexity = 5.08465719/161.5245514 secs/batch = 0.6085s, grad.norm=0.57495099\n",
      "  9464: 7 [  175/ 1327], train_loss/perplexity = 5.30136490/200.6104431 secs/batch = 0.6076s, grad.norm=0.54672480\n",
      "  9469: 7 [  180/ 1327], train_loss/perplexity = 5.25619698/191.7508698 secs/batch = 0.6150s, grad.norm=0.55350173\n",
      "  9474: 7 [  185/ 1327], train_loss/perplexity = 5.42206192/226.3453522 secs/batch = 0.6118s, grad.norm=0.54196751\n",
      "  9479: 7 [  190/ 1327], train_loss/perplexity = 4.94628048/140.6508331 secs/batch = 0.6171s, grad.norm=0.52972525\n",
      "  9484: 7 [  195/ 1327], train_loss/perplexity = 5.20490074/182.1627960 secs/batch = 0.6115s, grad.norm=0.54603982\n",
      "  9489: 7 [  200/ 1327], train_loss/perplexity = 5.15240383/172.8464813 secs/batch = 0.6057s, grad.norm=0.57136780\n",
      "  9494: 7 [  205/ 1327], train_loss/perplexity = 5.17949772/177.5935822 secs/batch = 0.6084s, grad.norm=0.56009012\n",
      "  9499: 7 [  210/ 1327], train_loss/perplexity = 5.15292215/172.9360962 secs/batch = 0.6104s, grad.norm=0.56185901\n",
      "  9504: 7 [  215/ 1327], train_loss/perplexity = 5.23176813/187.1233673 secs/batch = 0.6135s, grad.norm=0.54125583\n",
      "  9509: 7 [  220/ 1327], train_loss/perplexity = 5.29417706/199.1736450 secs/batch = 0.6156s, grad.norm=0.56153101\n",
      "  9514: 7 [  225/ 1327], train_loss/perplexity = 5.42230320/226.3999634 secs/batch = 0.6107s, grad.norm=0.60284287\n",
      "  9519: 7 [  230/ 1327], train_loss/perplexity = 5.22260284/185.4161682 secs/batch = 0.6091s, grad.norm=0.57973880\n",
      "  9524: 7 [  235/ 1327], train_loss/perplexity = 5.14812756/172.1089172 secs/batch = 0.6090s, grad.norm=0.57033187\n",
      "  9529: 7 [  240/ 1327], train_loss/perplexity = 4.95345545/141.6636353 secs/batch = 0.6068s, grad.norm=0.60048652\n",
      "  9534: 7 [  245/ 1327], train_loss/perplexity = 5.29025221/198.3934631 secs/batch = 0.6112s, grad.norm=0.54632562\n",
      "  9539: 7 [  250/ 1327], train_loss/perplexity = 4.97607756/144.9048767 secs/batch = 0.6045s, grad.norm=0.55057746\n",
      "  9544: 7 [  255/ 1327], train_loss/perplexity = 5.05053616/156.1061401 secs/batch = 0.6045s, grad.norm=0.53837824\n",
      "  9549: 7 [  260/ 1327], train_loss/perplexity = 5.42417336/226.8237610 secs/batch = 0.6106s, grad.norm=0.61686897\n",
      "  9554: 7 [  265/ 1327], train_loss/perplexity = 5.39049482/219.3118744 secs/batch = 0.6094s, grad.norm=0.56111670\n",
      "  9559: 7 [  270/ 1327], train_loss/perplexity = 5.37997293/217.0164032 secs/batch = 0.6116s, grad.norm=0.54641956\n",
      "  9564: 7 [  275/ 1327], train_loss/perplexity = 5.54470301/255.8785706 secs/batch = 0.6069s, grad.norm=0.58709693\n",
      "  9569: 7 [  280/ 1327], train_loss/perplexity = 5.25636625/191.7833252 secs/batch = 0.6076s, grad.norm=0.60390234\n",
      "  9574: 7 [  285/ 1327], train_loss/perplexity = 5.43959904/230.3498077 secs/batch = 0.6158s, grad.norm=0.63883650\n",
      "  9579: 7 [  290/ 1327], train_loss/perplexity = 5.31425571/203.2132111 secs/batch = 0.6038s, grad.norm=0.56089151\n",
      "  9584: 7 [  295/ 1327], train_loss/perplexity = 5.02679873/152.4442139 secs/batch = 0.6162s, grad.norm=0.54703486\n",
      "  9589: 7 [  300/ 1327], train_loss/perplexity = 4.64779568/104.3546982 secs/batch = 0.6383s, grad.norm=0.59976107\n",
      "  9594: 7 [  305/ 1327], train_loss/perplexity = 5.09755945/163.6220856 secs/batch = 0.6113s, grad.norm=0.59168589\n",
      "  9599: 7 [  310/ 1327], train_loss/perplexity = 5.12925768/168.8916931 secs/batch = 0.6069s, grad.norm=0.56452066\n",
      "  9604: 7 [  315/ 1327], train_loss/perplexity = 4.82465744/124.5438004 secs/batch = 0.6016s, grad.norm=0.54835075\n",
      "  9609: 7 [  320/ 1327], train_loss/perplexity = 4.89358282/133.4307709 secs/batch = 0.6083s, grad.norm=0.60665441\n",
      "  9614: 7 [  325/ 1327], train_loss/perplexity = 4.76653719/117.5116196 secs/batch = 0.6184s, grad.norm=0.61548030\n",
      "  9619: 7 [  330/ 1327], train_loss/perplexity = 5.22262001/185.4193420 secs/batch = 0.6090s, grad.norm=0.62759358\n",
      "  9624: 7 [  335/ 1327], train_loss/perplexity = 4.47608423/87.8898392 secs/batch = 0.6067s, grad.norm=0.56414080\n",
      "  9629: 7 [  340/ 1327], train_loss/perplexity = 5.31038857/202.4288635 secs/batch = 0.6025s, grad.norm=0.59799504\n",
      "  9634: 7 [  345/ 1327], train_loss/perplexity = 5.14538193/171.6370239 secs/batch = 0.6034s, grad.norm=0.56885350\n",
      "  9639: 7 [  350/ 1327], train_loss/perplexity = 5.24846363/190.2737122 secs/batch = 0.6103s, grad.norm=0.59668827\n",
      "  9644: 7 [  355/ 1327], train_loss/perplexity = 5.25486517/191.4956665 secs/batch = 0.6111s, grad.norm=0.56890363\n",
      "  9649: 7 [  360/ 1327], train_loss/perplexity = 5.38858414/218.8932495 secs/batch = 0.6140s, grad.norm=0.55051237\n",
      "  9654: 7 [  365/ 1327], train_loss/perplexity = 5.31360435/203.0808868 secs/batch = 0.6026s, grad.norm=0.54697508\n",
      "  9659: 7 [  370/ 1327], train_loss/perplexity = 5.32548857/205.5087433 secs/batch = 0.6013s, grad.norm=0.62054032\n",
      "  9664: 7 [  375/ 1327], train_loss/perplexity = 4.69586086/109.4930267 secs/batch = 0.6453s, grad.norm=0.53865093\n",
      "  9669: 7 [  380/ 1327], train_loss/perplexity = 4.95566416/141.9768677 secs/batch = 0.6060s, grad.norm=0.62709862\n",
      "  9674: 7 [  385/ 1327], train_loss/perplexity = 5.12911320/168.8672943 secs/batch = 0.6158s, grad.norm=0.58337563\n",
      "  9679: 7 [  390/ 1327], train_loss/perplexity = 5.11386061/166.3111725 secs/batch = 0.6074s, grad.norm=0.55820060\n",
      "  9684: 7 [  395/ 1327], train_loss/perplexity = 5.39514208/220.3334503 secs/batch = 0.6092s, grad.norm=0.64189583\n",
      "  9689: 7 [  400/ 1327], train_loss/perplexity = 5.04319715/154.9646759 secs/batch = 0.6050s, grad.norm=0.54054695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9694: 7 [  405/ 1327], train_loss/perplexity = 5.42586088/227.2068634 secs/batch = 0.6146s, grad.norm=0.61827385\n",
      "  9699: 7 [  410/ 1327], train_loss/perplexity = 5.12023067/167.3739777 secs/batch = 0.6147s, grad.norm=0.57640964\n",
      "  9704: 7 [  415/ 1327], train_loss/perplexity = 4.94183826/140.0274200 secs/batch = 0.6071s, grad.norm=0.58805346\n",
      "  9709: 7 [  420/ 1327], train_loss/perplexity = 4.89196682/133.2153320 secs/batch = 0.6072s, grad.norm=0.57533282\n",
      "  9714: 7 [  425/ 1327], train_loss/perplexity = 5.16266680/174.6295319 secs/batch = 0.6103s, grad.norm=0.65004468\n",
      "  9719: 7 [  430/ 1327], train_loss/perplexity = 5.27287149/194.9750366 secs/batch = 0.6164s, grad.norm=0.59295422\n",
      "  9724: 7 [  435/ 1327], train_loss/perplexity = 5.32994747/206.4271240 secs/batch = 0.6083s, grad.norm=0.59159595\n",
      "  9729: 7 [  440/ 1327], train_loss/perplexity = 5.03590155/153.8382263 secs/batch = 0.6111s, grad.norm=0.62835962\n",
      "  9734: 7 [  445/ 1327], train_loss/perplexity = 5.16433477/174.9210510 secs/batch = 0.6096s, grad.norm=0.63881963\n",
      "  9739: 7 [  450/ 1327], train_loss/perplexity = 5.04077482/154.5897522 secs/batch = 0.6077s, grad.norm=0.58588582\n",
      "  9744: 7 [  455/ 1327], train_loss/perplexity = 4.87936258/131.5467834 secs/batch = 0.6144s, grad.norm=0.57392305\n",
      "  9749: 7 [  460/ 1327], train_loss/perplexity = 5.13360691/169.6278534 secs/batch = 0.6084s, grad.norm=0.57161504\n",
      "  9754: 7 [  465/ 1327], train_loss/perplexity = 4.95063114/141.2640991 secs/batch = 0.6089s, grad.norm=0.59564543\n",
      "  9759: 7 [  470/ 1327], train_loss/perplexity = 5.40596104/222.7301636 secs/batch = 0.6137s, grad.norm=0.60602367\n",
      "  9764: 7 [  475/ 1327], train_loss/perplexity = 4.97706175/145.0475616 secs/batch = 0.6147s, grad.norm=0.51858449\n",
      "  9769: 7 [  480/ 1327], train_loss/perplexity = 5.17010880/175.9339752 secs/batch = 0.6103s, grad.norm=0.61277455\n",
      "  9774: 7 [  485/ 1327], train_loss/perplexity = 5.05108404/156.1916962 secs/batch = 0.6080s, grad.norm=0.55930293\n",
      "  9779: 7 [  490/ 1327], train_loss/perplexity = 5.03369856/153.4996948 secs/batch = 0.6086s, grad.norm=0.60308594\n",
      "  9784: 7 [  495/ 1327], train_loss/perplexity = 4.95145416/141.3804016 secs/batch = 0.6169s, grad.norm=0.56866211\n",
      "  9789: 7 [  500/ 1327], train_loss/perplexity = 5.24694681/189.9853210 secs/batch = 0.6103s, grad.norm=0.58619916\n",
      "  9794: 7 [  505/ 1327], train_loss/perplexity = 5.11127043/165.8809662 secs/batch = 0.6140s, grad.norm=0.59952968\n",
      "  9799: 7 [  510/ 1327], train_loss/perplexity = 5.50811195/246.6849365 secs/batch = 0.6114s, grad.norm=0.55394405\n",
      "  9804: 7 [  515/ 1327], train_loss/perplexity = 5.18514299/178.5989838 secs/batch = 0.6091s, grad.norm=0.58113593\n",
      "  9809: 7 [  520/ 1327], train_loss/perplexity = 5.34266949/209.0700684 secs/batch = 0.6096s, grad.norm=0.54988867\n",
      "  9814: 7 [  525/ 1327], train_loss/perplexity = 4.99758244/148.0547943 secs/batch = 0.6139s, grad.norm=0.59902328\n",
      "  9819: 7 [  530/ 1327], train_loss/perplexity = 5.05148554/156.2544098 secs/batch = 0.6141s, grad.norm=0.61388350\n",
      "  9824: 7 [  535/ 1327], train_loss/perplexity = 5.13867378/170.4895172 secs/batch = 0.6126s, grad.norm=0.64135587\n",
      "  9829: 7 [  540/ 1327], train_loss/perplexity = 5.21512508/184.0348358 secs/batch = 0.6098s, grad.norm=0.52160585\n",
      "  9834: 7 [  545/ 1327], train_loss/perplexity = 5.32927799/206.2889709 secs/batch = 0.6117s, grad.norm=0.57087290\n",
      "  9839: 7 [  550/ 1327], train_loss/perplexity = 5.20863485/182.8442841 secs/batch = 0.6118s, grad.norm=0.57468748\n",
      "  9844: 7 [  555/ 1327], train_loss/perplexity = 5.07424021/159.8506927 secs/batch = 0.6143s, grad.norm=0.57277685\n",
      "  9849: 7 [  560/ 1327], train_loss/perplexity = 5.09710169/163.5472107 secs/batch = 0.6126s, grad.norm=0.62783903\n",
      "  9854: 7 [  565/ 1327], train_loss/perplexity = 5.12113953/167.5261688 secs/batch = 0.6084s, grad.norm=0.60051489\n",
      "  9859: 7 [  570/ 1327], train_loss/perplexity = 5.01388454/150.4881744 secs/batch = 0.6104s, grad.norm=0.66485918\n",
      "  9864: 7 [  575/ 1327], train_loss/perplexity = 4.93206692/138.6658325 secs/batch = 0.6146s, grad.norm=0.61130112\n",
      "  9869: 7 [  580/ 1327], train_loss/perplexity = 5.29452229/199.2424164 secs/batch = 0.6126s, grad.norm=0.58253747\n",
      "  9874: 7 [  585/ 1327], train_loss/perplexity = 4.95864677/142.4009705 secs/batch = 0.6170s, grad.norm=0.60567105\n",
      "  9879: 7 [  590/ 1327], train_loss/perplexity = 5.18778372/179.0712433 secs/batch = 0.6043s, grad.norm=0.57287169\n",
      "  9884: 7 [  595/ 1327], train_loss/perplexity = 5.10496521/164.8383331 secs/batch = 0.6149s, grad.norm=0.65080404\n",
      "  9889: 7 [  600/ 1327], train_loss/perplexity = 5.35314083/211.2708282 secs/batch = 0.6149s, grad.norm=0.55024624\n",
      "  9894: 7 [  605/ 1327], train_loss/perplexity = 5.32496500/205.4011688 secs/batch = 0.6077s, grad.norm=0.54388875\n",
      "  9899: 7 [  610/ 1327], train_loss/perplexity = 5.42809057/227.7140198 secs/batch = 0.6108s, grad.norm=0.56731898\n",
      "  9904: 7 [  615/ 1327], train_loss/perplexity = 4.82371569/124.4265594 secs/batch = 0.6172s, grad.norm=0.54360765\n",
      "  9909: 7 [  620/ 1327], train_loss/perplexity = 5.22399426/185.6743317 secs/batch = 0.6204s, grad.norm=0.55864650\n",
      "  9914: 7 [  625/ 1327], train_loss/perplexity = 5.35650349/211.9824524 secs/batch = 0.6114s, grad.norm=0.58324784\n",
      "  9919: 7 [  630/ 1327], train_loss/perplexity = 5.31852961/204.0835724 secs/batch = 0.6069s, grad.norm=0.55782878\n",
      "  9924: 7 [  635/ 1327], train_loss/perplexity = 5.06321716/158.0983276 secs/batch = 0.6083s, grad.norm=0.57806695\n",
      "  9929: 7 [  640/ 1327], train_loss/perplexity = 5.17724609/177.1941681 secs/batch = 0.6109s, grad.norm=0.54345435\n",
      "  9934: 7 [  645/ 1327], train_loss/perplexity = 5.36526060/213.8469543 secs/batch = 0.6074s, grad.norm=0.60268414\n",
      "  9939: 7 [  650/ 1327], train_loss/perplexity = 5.03948450/154.3904114 secs/batch = 0.6109s, grad.norm=0.70191139\n",
      "  9944: 7 [  655/ 1327], train_loss/perplexity = 5.09379864/163.0078888 secs/batch = 0.6146s, grad.norm=0.54671967\n",
      "  9949: 7 [  660/ 1327], train_loss/perplexity = 4.98293352/145.9017639 secs/batch = 0.6126s, grad.norm=0.58643252\n",
      "  9954: 7 [  665/ 1327], train_loss/perplexity = 5.16185284/174.4874573 secs/batch = 0.6126s, grad.norm=0.53974026\n",
      "  9959: 7 [  670/ 1327], train_loss/perplexity = 5.05949593/157.5110931 secs/batch = 0.6158s, grad.norm=0.56250215\n",
      "  9964: 7 [  675/ 1327], train_loss/perplexity = 4.84155083/126.6656342 secs/batch = 0.6128s, grad.norm=0.61995566\n",
      "  9969: 7 [  680/ 1327], train_loss/perplexity = 5.15330839/173.0028992 secs/batch = 0.6129s, grad.norm=0.55977845\n",
      "  9974: 7 [  685/ 1327], train_loss/perplexity = 5.16915560/175.7663574 secs/batch = 0.6088s, grad.norm=0.59599090\n",
      "  9979: 7 [  690/ 1327], train_loss/perplexity = 5.28867722/198.0812378 secs/batch = 0.6104s, grad.norm=0.54772621\n",
      "  9984: 7 [  695/ 1327], train_loss/perplexity = 5.09290314/162.8619843 secs/batch = 0.6081s, grad.norm=0.55524504\n",
      "  9989: 7 [  700/ 1327], train_loss/perplexity = 5.31274748/202.9069519 secs/batch = 0.6130s, grad.norm=0.58695406\n",
      "  9994: 7 [  705/ 1327], train_loss/perplexity = 5.04769897/155.6638641 secs/batch = 0.6093s, grad.norm=0.57861990\n",
      "  9999: 7 [  710/ 1327], train_loss/perplexity = 5.03288460/153.3748016 secs/batch = 0.6093s, grad.norm=0.58314306\n",
      " 10004: 7 [  715/ 1327], train_loss/perplexity = 5.02307987/151.8783569 secs/batch = 0.6088s, grad.norm=0.55104536\n",
      " 10009: 7 [  720/ 1327], train_loss/perplexity = 5.09595156/163.3592224 secs/batch = 0.6484s, grad.norm=0.59120005\n",
      " 10014: 7 [  725/ 1327], train_loss/perplexity = 4.83610296/125.9774551 secs/batch = 0.6105s, grad.norm=0.57624865\n",
      " 10019: 7 [  730/ 1327], train_loss/perplexity = 5.00459814/149.0971527 secs/batch = 0.6068s, grad.norm=0.56814528\n",
      " 10024: 7 [  735/ 1327], train_loss/perplexity = 5.18496227/178.5667114 secs/batch = 0.6082s, grad.norm=0.55756855\n",
      " 10029: 7 [  740/ 1327], train_loss/perplexity = 4.59135485/98.6279678 secs/batch = 0.6089s, grad.norm=0.54320759\n",
      " 10034: 7 [  745/ 1327], train_loss/perplexity = 5.07401180/159.8141785 secs/batch = 0.6093s, grad.norm=0.56828958\n",
      " 10039: 7 [  750/ 1327], train_loss/perplexity = 4.93762875/139.4392090 secs/batch = 0.6096s, grad.norm=0.56505895\n",
      " 10044: 7 [  755/ 1327], train_loss/perplexity = 5.01764393/151.0549927 secs/batch = 0.6083s, grad.norm=0.55370450\n",
      " 10049: 7 [  760/ 1327], train_loss/perplexity = 4.87048054/130.3835602 secs/batch = 0.6052s, grad.norm=0.57916117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10054: 7 [  765/ 1327], train_loss/perplexity = 4.94302082/140.1931152 secs/batch = 0.6076s, grad.norm=0.59389722\n",
      " 10059: 7 [  770/ 1327], train_loss/perplexity = 4.91264343/135.9984436 secs/batch = 0.6090s, grad.norm=0.64939511\n",
      " 10064: 7 [  775/ 1327], train_loss/perplexity = 4.96786594/143.7198486 secs/batch = 0.6174s, grad.norm=0.56555271\n",
      " 10069: 7 [  780/ 1327], train_loss/perplexity = 5.20213175/181.6590881 secs/batch = 0.6114s, grad.norm=0.56589264\n",
      " 10074: 7 [  785/ 1327], train_loss/perplexity = 5.10223150/164.3883362 secs/batch = 0.6083s, grad.norm=0.63086963\n",
      " 10079: 7 [  790/ 1327], train_loss/perplexity = 4.92144299/137.2004547 secs/batch = 0.6116s, grad.norm=0.64893824\n",
      " 10084: 7 [  795/ 1327], train_loss/perplexity = 5.19361115/180.1178131 secs/batch = 0.6086s, grad.norm=0.56260031\n",
      " 10089: 7 [  800/ 1327], train_loss/perplexity = 5.20235395/181.6994476 secs/batch = 0.6128s, grad.norm=0.58349365\n",
      " 10094: 7 [  805/ 1327], train_loss/perplexity = 5.43986130/230.4102173 secs/batch = 0.6109s, grad.norm=0.54899853\n",
      " 10099: 7 [  810/ 1327], train_loss/perplexity = 5.09651518/163.4513092 secs/batch = 0.6120s, grad.norm=0.61261088\n",
      " 10104: 7 [  815/ 1327], train_loss/perplexity = 5.02163267/151.6587067 secs/batch = 0.6167s, grad.norm=0.58000273\n",
      " 10109: 7 [  820/ 1327], train_loss/perplexity = 4.70560169/110.5647888 secs/batch = 0.6157s, grad.norm=0.53950793\n",
      " 10114: 7 [  825/ 1327], train_loss/perplexity = 4.89294481/133.3456726 secs/batch = 0.6105s, grad.norm=0.60080856\n",
      " 10119: 7 [  830/ 1327], train_loss/perplexity = 4.77353525/118.3368530 secs/batch = 0.6095s, grad.norm=0.57338041\n",
      " 10124: 7 [  835/ 1327], train_loss/perplexity = 4.97595406/144.8869934 secs/batch = 0.6110s, grad.norm=0.58209592\n",
      " 10129: 7 [  840/ 1327], train_loss/perplexity = 5.17782688/177.2971039 secs/batch = 0.6090s, grad.norm=0.63973391\n",
      " 10134: 7 [  845/ 1327], train_loss/perplexity = 4.94788456/140.8766327 secs/batch = 0.6100s, grad.norm=0.60254300\n",
      " 10139: 7 [  850/ 1327], train_loss/perplexity = 5.06175900/157.8679657 secs/batch = 0.6042s, grad.norm=0.55907947\n",
      " 10144: 7 [  855/ 1327], train_loss/perplexity = 5.04852867/155.7930756 secs/batch = 0.6097s, grad.norm=0.66596097\n",
      " 10149: 7 [  860/ 1327], train_loss/perplexity = 4.74471283/114.9747849 secs/batch = 0.6114s, grad.norm=0.55646652\n",
      " 10154: 7 [  865/ 1327], train_loss/perplexity = 5.23906946/188.4946136 secs/batch = 0.6034s, grad.norm=0.59827197\n",
      " 10159: 7 [  870/ 1327], train_loss/perplexity = 5.17325878/176.4890442 secs/batch = 0.6095s, grad.norm=0.60692728\n",
      " 10164: 7 [  875/ 1327], train_loss/perplexity = 4.77299643/118.2731094 secs/batch = 0.6077s, grad.norm=0.57226568\n",
      " 10169: 7 [  880/ 1327], train_loss/perplexity = 4.93811798/139.5074463 secs/batch = 0.6126s, grad.norm=0.55731702\n",
      " 10174: 7 [  885/ 1327], train_loss/perplexity = 4.97808743/145.1964111 secs/batch = 0.6110s, grad.norm=0.57943279\n",
      " 10179: 7 [  890/ 1327], train_loss/perplexity = 5.21111298/183.2979584 secs/batch = 0.6102s, grad.norm=0.56225699\n",
      " 10184: 7 [  895/ 1327], train_loss/perplexity = 5.21413374/183.8524933 secs/batch = 0.6059s, grad.norm=0.56573176\n",
      " 10189: 7 [  900/ 1327], train_loss/perplexity = 5.09805536/163.7032471 secs/batch = 0.6106s, grad.norm=0.59164661\n",
      " 10194: 7 [  905/ 1327], train_loss/perplexity = 4.93611813/139.2287292 secs/batch = 0.6127s, grad.norm=0.58475196\n",
      " 10199: 7 [  910/ 1327], train_loss/perplexity = 5.01094675/150.0467224 secs/batch = 0.6143s, grad.norm=0.59445429\n",
      " 10204: 7 [  915/ 1327], train_loss/perplexity = 5.28971672/198.2872467 secs/batch = 0.6095s, grad.norm=0.59283507\n",
      " 10209: 7 [  920/ 1327], train_loss/perplexity = 5.33943605/208.3951569 secs/batch = 0.6101s, grad.norm=0.60466331\n",
      " 10214: 7 [  925/ 1327], train_loss/perplexity = 5.08624792/161.7817078 secs/batch = 0.6093s, grad.norm=0.58744252\n",
      " 10219: 7 [  930/ 1327], train_loss/perplexity = 5.03783703/154.1362610 secs/batch = 0.6205s, grad.norm=0.60522568\n",
      " 10224: 7 [  935/ 1327], train_loss/perplexity = 5.14669228/171.8620758 secs/batch = 0.5977s, grad.norm=0.55958378\n",
      " 10229: 7 [  940/ 1327], train_loss/perplexity = 5.06918001/159.0438538 secs/batch = 0.6105s, grad.norm=0.52915937\n",
      " 10234: 7 [  945/ 1327], train_loss/perplexity = 5.29892588/200.1217346 secs/batch = 0.6087s, grad.norm=0.61748123\n",
      " 10239: 7 [  950/ 1327], train_loss/perplexity = 5.01741219/151.0199890 secs/batch = 0.6014s, grad.norm=0.58527166\n",
      " 10244: 7 [  955/ 1327], train_loss/perplexity = 5.14702415/171.9191284 secs/batch = 0.6099s, grad.norm=0.57206732\n",
      " 10249: 7 [  960/ 1327], train_loss/perplexity = 5.41878080/225.6038971 secs/batch = 0.6131s, grad.norm=0.55388987\n",
      " 10254: 7 [  965/ 1327], train_loss/perplexity = 5.18317699/178.2481995 secs/batch = 0.6201s, grad.norm=0.61865205\n",
      " 10259: 7 [  970/ 1327], train_loss/perplexity = 5.32918119/206.2690125 secs/batch = 0.6102s, grad.norm=0.52636331\n",
      " 10264: 7 [  975/ 1327], train_loss/perplexity = 5.10921383/165.5401611 secs/batch = 0.6133s, grad.norm=0.62271780\n",
      " 10269: 7 [  980/ 1327], train_loss/perplexity = 4.87446594/130.9042206 secs/batch = 0.6166s, grad.norm=0.54502141\n",
      " 10274: 7 [  985/ 1327], train_loss/perplexity = 5.13528824/169.9132843 secs/batch = 0.6126s, grad.norm=0.58653486\n",
      " 10279: 7 [  990/ 1327], train_loss/perplexity = 5.24074411/188.8105469 secs/batch = 0.6145s, grad.norm=0.58677351\n",
      " 10284: 7 [  995/ 1327], train_loss/perplexity = 5.21600819/184.1974335 secs/batch = 0.6146s, grad.norm=0.57581997\n",
      " 10289: 7 [ 1000/ 1327], train_loss/perplexity = 4.74894905/115.4628754 secs/batch = 0.6167s, grad.norm=0.58127809\n",
      " 10294: 7 [ 1005/ 1327], train_loss/perplexity = 5.18426561/178.4423523 secs/batch = 0.6134s, grad.norm=0.63213450\n",
      " 10299: 7 [ 1010/ 1327], train_loss/perplexity = 4.79941177/121.4389648 secs/batch = 0.6085s, grad.norm=0.55956507\n",
      " 10304: 7 [ 1015/ 1327], train_loss/perplexity = 5.25613165/191.7383423 secs/batch = 0.6491s, grad.norm=0.58606893\n",
      " 10309: 7 [ 1020/ 1327], train_loss/perplexity = 5.39289570/219.8390503 secs/batch = 0.6121s, grad.norm=0.61357772\n",
      " 10314: 7 [ 1025/ 1327], train_loss/perplexity = 5.20638275/182.4329529 secs/batch = 0.6095s, grad.norm=0.58732033\n",
      " 10319: 7 [ 1030/ 1327], train_loss/perplexity = 5.04233837/154.8316498 secs/batch = 0.6101s, grad.norm=0.54953521\n",
      " 10324: 7 [ 1035/ 1327], train_loss/perplexity = 4.94141769/139.9685364 secs/batch = 0.6116s, grad.norm=0.58570522\n",
      " 10329: 7 [ 1040/ 1327], train_loss/perplexity = 5.21276951/183.6018372 secs/batch = 0.6136s, grad.norm=0.54049110\n",
      " 10334: 7 [ 1045/ 1327], train_loss/perplexity = 4.86790228/130.0478210 secs/batch = 0.6153s, grad.norm=0.58900779\n",
      " 10339: 7 [ 1050/ 1327], train_loss/perplexity = 4.86343765/129.4685059 secs/batch = 0.6041s, grad.norm=0.57884717\n",
      " 10344: 7 [ 1055/ 1327], train_loss/perplexity = 5.12336302/167.8990631 secs/batch = 0.6060s, grad.norm=0.62434971\n",
      " 10349: 7 [ 1060/ 1327], train_loss/perplexity = 4.69086933/108.9478531 secs/batch = 0.6113s, grad.norm=0.64427757\n",
      " 10354: 7 [ 1065/ 1327], train_loss/perplexity = 4.81217766/122.9991760 secs/batch = 0.6116s, grad.norm=0.55183309\n",
      " 10359: 7 [ 1070/ 1327], train_loss/perplexity = 5.16182375/174.4823761 secs/batch = 0.6097s, grad.norm=0.59151208\n",
      " 10364: 7 [ 1075/ 1327], train_loss/perplexity = 4.89775133/133.9881439 secs/batch = 0.6083s, grad.norm=0.59026283\n",
      " 10369: 7 [ 1080/ 1327], train_loss/perplexity = 4.79313803/120.6794739 secs/batch = 0.6118s, grad.norm=0.54495138\n",
      " 10374: 7 [ 1085/ 1327], train_loss/perplexity = 4.70632839/110.6451645 secs/batch = 0.6053s, grad.norm=0.57792789\n",
      " 10379: 7 [ 1090/ 1327], train_loss/perplexity = 4.91928244/136.9043427 secs/batch = 0.6136s, grad.norm=0.59269702\n",
      " 10384: 7 [ 1095/ 1327], train_loss/perplexity = 5.07409620/159.8276825 secs/batch = 0.6140s, grad.norm=0.62601072\n",
      " 10389: 7 [ 1100/ 1327], train_loss/perplexity = 4.99119711/147.1124268 secs/batch = 0.6138s, grad.norm=0.63361716\n",
      " 10394: 7 [ 1105/ 1327], train_loss/perplexity = 4.84828949/127.5220718 secs/batch = 0.6120s, grad.norm=0.61136067\n",
      " 10399: 7 [ 1110/ 1327], train_loss/perplexity = 5.40344715/222.1709595 secs/batch = 0.6110s, grad.norm=0.63733733\n",
      " 10404: 7 [ 1115/ 1327], train_loss/perplexity = 4.86405087/129.5479279 secs/batch = 0.6080s, grad.norm=0.56344467\n",
      " 10409: 7 [ 1120/ 1327], train_loss/perplexity = 5.08364439/161.3610535 secs/batch = 0.6066s, grad.norm=0.62655920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10414: 7 [ 1125/ 1327], train_loss/perplexity = 5.33459187/207.3880920 secs/batch = 0.6117s, grad.norm=0.62399614\n",
      " 10419: 7 [ 1130/ 1327], train_loss/perplexity = 5.00400019/149.0080261 secs/batch = 0.6109s, grad.norm=0.57455671\n",
      " 10424: 7 [ 1135/ 1327], train_loss/perplexity = 5.00539541/149.2160797 secs/batch = 0.6058s, grad.norm=0.55016595\n",
      " 10429: 7 [ 1140/ 1327], train_loss/perplexity = 5.26866913/194.1573944 secs/batch = 0.6148s, grad.norm=0.57848465\n",
      " 10434: 7 [ 1145/ 1327], train_loss/perplexity = 5.01488113/150.6382294 secs/batch = 0.6081s, grad.norm=0.61764604\n",
      " 10439: 7 [ 1150/ 1327], train_loss/perplexity = 5.00478935/149.1256714 secs/batch = 0.6093s, grad.norm=0.57699555\n",
      " 10444: 7 [ 1155/ 1327], train_loss/perplexity = 5.13370371/169.6442719 secs/batch = 0.6083s, grad.norm=0.58778876\n",
      " 10449: 7 [ 1160/ 1327], train_loss/perplexity = 5.05417776/156.6756592 secs/batch = 0.6150s, grad.norm=0.58410144\n",
      " 10454: 7 [ 1165/ 1327], train_loss/perplexity = 5.13340855/169.5942078 secs/batch = 0.6105s, grad.norm=0.64779830\n",
      " 10459: 7 [ 1170/ 1327], train_loss/perplexity = 5.00652742/149.3850861 secs/batch = 0.6102s, grad.norm=0.63089931\n",
      " 10464: 7 [ 1175/ 1327], train_loss/perplexity = 4.75368357/116.0108337 secs/batch = 0.6156s, grad.norm=0.62835854\n",
      " 10469: 7 [ 1180/ 1327], train_loss/perplexity = 4.71244335/111.3238297 secs/batch = 0.6016s, grad.norm=0.62700433\n",
      " 10474: 7 [ 1185/ 1327], train_loss/perplexity = 5.01471233/150.6128082 secs/batch = 0.6106s, grad.norm=0.57421875\n",
      " 10479: 7 [ 1190/ 1327], train_loss/perplexity = 5.03174591/153.2002563 secs/batch = 0.6124s, grad.norm=0.57122922\n",
      " 10484: 7 [ 1195/ 1327], train_loss/perplexity = 4.84904432/127.6183701 secs/batch = 0.6090s, grad.norm=0.60702741\n",
      " 10489: 7 [ 1200/ 1327], train_loss/perplexity = 4.76241827/117.0285873 secs/batch = 0.6157s, grad.norm=0.63191170\n",
      " 10494: 7 [ 1205/ 1327], train_loss/perplexity = 4.83380365/125.6881256 secs/batch = 0.6043s, grad.norm=0.61216712\n",
      " 10499: 7 [ 1210/ 1327], train_loss/perplexity = 4.59384060/98.8734360 secs/batch = 0.6187s, grad.norm=0.57111639\n",
      " 10504: 7 [ 1215/ 1327], train_loss/perplexity = 4.72889137/113.1700287 secs/batch = 0.6008s, grad.norm=0.57671088\n",
      " 10509: 7 [ 1220/ 1327], train_loss/perplexity = 4.83879805/126.3174362 secs/batch = 0.6137s, grad.norm=0.59344769\n",
      " 10514: 7 [ 1225/ 1327], train_loss/perplexity = 4.74481726/114.9867935 secs/batch = 0.6059s, grad.norm=0.58144265\n",
      " 10519: 7 [ 1230/ 1327], train_loss/perplexity = 4.86894703/130.1837616 secs/batch = 0.6154s, grad.norm=0.58030337\n",
      " 10524: 7 [ 1235/ 1327], train_loss/perplexity = 4.89201355/133.2215576 secs/batch = 0.6174s, grad.norm=0.58264673\n",
      " 10529: 7 [ 1240/ 1327], train_loss/perplexity = 5.04394484/155.0805817 secs/batch = 0.6087s, grad.norm=0.57958043\n",
      " 10534: 7 [ 1245/ 1327], train_loss/perplexity = 4.87166739/130.5383911 secs/batch = 0.6107s, grad.norm=0.55241895\n",
      " 10539: 7 [ 1250/ 1327], train_loss/perplexity = 4.99736261/148.0222473 secs/batch = 0.6137s, grad.norm=0.53955865\n",
      " 10544: 7 [ 1255/ 1327], train_loss/perplexity = 5.00641012/149.3675537 secs/batch = 0.6085s, grad.norm=0.58423489\n",
      " 10549: 7 [ 1260/ 1327], train_loss/perplexity = 4.99667835/147.9210052 secs/batch = 0.6242s, grad.norm=0.62672585\n",
      " 10554: 7 [ 1265/ 1327], train_loss/perplexity = 5.04046631/154.5420685 secs/batch = 0.6124s, grad.norm=0.60092902\n",
      " 10559: 7 [ 1270/ 1327], train_loss/perplexity = 4.88765717/132.6424561 secs/batch = 0.6043s, grad.norm=0.59807092\n",
      " 10564: 7 [ 1275/ 1327], train_loss/perplexity = 5.10077572/164.1491852 secs/batch = 0.6139s, grad.norm=0.63279992\n",
      " 10569: 7 [ 1280/ 1327], train_loss/perplexity = 4.90593481/135.0891266 secs/batch = 0.6767s, grad.norm=0.62303782\n",
      " 10574: 7 [ 1285/ 1327], train_loss/perplexity = 4.92304516/137.4204407 secs/batch = 0.6180s, grad.norm=0.54854035\n",
      " 10579: 7 [ 1290/ 1327], train_loss/perplexity = 5.02805948/152.6365356 secs/batch = 0.6137s, grad.norm=0.56999683\n",
      " 10584: 7 [ 1295/ 1327], train_loss/perplexity = 5.05852652/157.3584747 secs/batch = 0.6141s, grad.norm=0.55246115\n",
      " 10589: 7 [ 1300/ 1327], train_loss/perplexity = 5.16539526/175.1066589 secs/batch = 0.6038s, grad.norm=0.65872335\n",
      " 10594: 7 [ 1305/ 1327], train_loss/perplexity = 5.26611710/193.6625214 secs/batch = 0.6039s, grad.norm=0.60866511\n",
      " 10599: 7 [ 1310/ 1327], train_loss/perplexity = 5.52053261/249.7680359 secs/batch = 0.6543s, grad.norm=0.57615066\n",
      " 10604: 7 [ 1315/ 1327], train_loss/perplexity = 5.32225609/204.8455048 secs/batch = 0.6155s, grad.norm=0.65914124\n",
      " 10609: 7 [ 1320/ 1327], train_loss/perplexity = 5.29141474/198.6242218 secs/batch = 0.6166s, grad.norm=0.56336254\n",
      " 10614: 7 [ 1325/ 1327], train_loss/perplexity = 5.20818377/182.7618256 secs/batch = 0.6063s, grad.norm=0.56120336\n",
      "Epoch training time: 812.1896588802338\n",
      "Saved char model cv/epoch007_5.1376.model\n",
      " 10621: 8 [    5/ 1327], train_loss/perplexity = 5.33274317/207.0050507 secs/batch = 0.6131s, grad.norm=0.56265467\n",
      " 10626: 8 [   10/ 1327], train_loss/perplexity = 4.87610769/131.1193085 secs/batch = 0.6048s, grad.norm=0.62698054\n",
      " 10631: 8 [   15/ 1327], train_loss/perplexity = 5.01067114/150.0053711 secs/batch = 0.6048s, grad.norm=0.58444035\n",
      " 10636: 8 [   20/ 1327], train_loss/perplexity = 5.29989529/200.3158264 secs/batch = 0.6099s, grad.norm=0.61062354\n",
      " 10641: 8 [   25/ 1327], train_loss/perplexity = 5.14486647/171.5485687 secs/batch = 0.6110s, grad.norm=0.59136289\n",
      " 10646: 8 [   30/ 1327], train_loss/perplexity = 5.08839130/162.1288300 secs/batch = 0.6107s, grad.norm=0.56883806\n",
      " 10651: 8 [   35/ 1327], train_loss/perplexity = 4.92353010/137.4871063 secs/batch = 0.6081s, grad.norm=0.60251969\n",
      " 10656: 8 [   40/ 1327], train_loss/perplexity = 5.01430893/150.5520630 secs/batch = 0.6146s, grad.norm=0.60918468\n",
      " 10661: 8 [   45/ 1327], train_loss/perplexity = 4.71486473/111.5937119 secs/batch = 0.6097s, grad.norm=0.58206928\n",
      " 10666: 8 [   50/ 1327], train_loss/perplexity = 5.05356693/156.5799866 secs/batch = 0.6068s, grad.norm=0.57267731\n",
      " 10671: 8 [   55/ 1327], train_loss/perplexity = 4.95930338/142.4944916 secs/batch = 0.6120s, grad.norm=0.61590755\n",
      " 10676: 8 [   60/ 1327], train_loss/perplexity = 5.26335430/193.1282196 secs/batch = 0.6102s, grad.norm=0.62220126\n",
      " 10681: 8 [   65/ 1327], train_loss/perplexity = 4.78988123/120.2870789 secs/batch = 0.6037s, grad.norm=0.57349563\n",
      " 10686: 8 [   70/ 1327], train_loss/perplexity = 4.64672089/104.2425995 secs/batch = 0.6058s, grad.norm=0.56243151\n",
      " 10691: 8 [   75/ 1327], train_loss/perplexity = 4.66374350/106.0322723 secs/batch = 0.6113s, grad.norm=0.57142150\n",
      " 10696: 8 [   80/ 1327], train_loss/perplexity = 4.97359562/144.5456848 secs/batch = 0.6255s, grad.norm=0.59489578\n",
      " 10701: 8 [   85/ 1327], train_loss/perplexity = 5.00848436/149.6777039 secs/batch = 0.6095s, grad.norm=0.59959841\n",
      " 10706: 8 [   90/ 1327], train_loss/perplexity = 5.04326153/154.9746399 secs/batch = 0.6095s, grad.norm=0.63543338\n",
      " 10711: 8 [   95/ 1327], train_loss/perplexity = 4.87602758/131.1088104 secs/batch = 0.6129s, grad.norm=0.56825507\n",
      " 10716: 8 [  100/ 1327], train_loss/perplexity = 5.09875679/163.8181152 secs/batch = 0.6116s, grad.norm=0.57637864\n",
      " 10721: 8 [  105/ 1327], train_loss/perplexity = 5.14915180/172.2852936 secs/batch = 0.6118s, grad.norm=0.62328714\n",
      " 10726: 8 [  110/ 1327], train_loss/perplexity = 4.88322401/132.0557251 secs/batch = 0.6174s, grad.norm=0.59890443\n",
      " 10731: 8 [  115/ 1327], train_loss/perplexity = 4.77729750/118.7829056 secs/batch = 0.6155s, grad.norm=0.58811712\n",
      " 10736: 8 [  120/ 1327], train_loss/perplexity = 4.93598366/139.2100067 secs/batch = 0.6124s, grad.norm=0.66493720\n",
      " 10741: 8 [  125/ 1327], train_loss/perplexity = 5.07146502/159.4076843 secs/batch = 0.6103s, grad.norm=0.64135754\n",
      " 10746: 8 [  130/ 1327], train_loss/perplexity = 4.94561815/140.5577087 secs/batch = 0.6516s, grad.norm=0.64377028\n",
      " 10751: 8 [  135/ 1327], train_loss/perplexity = 4.96021271/142.6241302 secs/batch = 0.6115s, grad.norm=0.61973000\n",
      " 10756: 8 [  140/ 1327], train_loss/perplexity = 5.22330236/185.5459137 secs/batch = 0.6058s, grad.norm=0.58582091\n",
      " 10761: 8 [  145/ 1327], train_loss/perplexity = 5.21215963/183.4898987 secs/batch = 0.6131s, grad.norm=0.59944010\n",
      " 10766: 8 [  150/ 1327], train_loss/perplexity = 5.10386181/164.6565552 secs/batch = 0.6106s, grad.norm=0.59571409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10771: 8 [  155/ 1327], train_loss/perplexity = 5.36156750/213.0586548 secs/batch = 0.6216s, grad.norm=0.59836304\n",
      " 10776: 8 [  160/ 1327], train_loss/perplexity = 4.98250580/145.8393707 secs/batch = 0.6131s, grad.norm=0.58259207\n",
      " 10781: 8 [  165/ 1327], train_loss/perplexity = 5.24694824/189.9855957 secs/batch = 0.6079s, grad.norm=0.56204426\n",
      " 10786: 8 [  170/ 1327], train_loss/perplexity = 4.99524641/147.7093353 secs/batch = 0.6078s, grad.norm=0.63118058\n",
      " 10791: 8 [  175/ 1327], train_loss/perplexity = 5.22589159/186.0269623 secs/batch = 0.6088s, grad.norm=0.56578004\n",
      " 10796: 8 [  180/ 1327], train_loss/perplexity = 5.12873268/168.8030548 secs/batch = 0.6152s, grad.norm=0.56614137\n",
      " 10801: 8 [  185/ 1327], train_loss/perplexity = 5.36349869/213.4705048 secs/batch = 0.6167s, grad.norm=0.57931477\n",
      " 10806: 8 [  190/ 1327], train_loss/perplexity = 4.87679529/131.2095032 secs/batch = 0.6132s, grad.norm=0.54133523\n",
      " 10811: 8 [  195/ 1327], train_loss/perplexity = 5.13870621/170.4950409 secs/batch = 0.6159s, grad.norm=0.54885197\n",
      " 10816: 8 [  200/ 1327], train_loss/perplexity = 5.06876945/158.9785767 secs/batch = 0.6126s, grad.norm=0.62134528\n",
      " 10821: 8 [  205/ 1327], train_loss/perplexity = 5.11611795/166.6870270 secs/batch = 0.6073s, grad.norm=0.55723578\n",
      " 10826: 8 [  210/ 1327], train_loss/perplexity = 5.05303383/156.4965363 secs/batch = 0.6126s, grad.norm=0.57383919\n",
      " 10831: 8 [  215/ 1327], train_loss/perplexity = 5.16994476/175.9051208 secs/batch = 0.6129s, grad.norm=0.55784202\n",
      " 10836: 8 [  220/ 1327], train_loss/perplexity = 5.20580864/182.3282471 secs/batch = 0.6152s, grad.norm=0.60367084\n",
      " 10841: 8 [  225/ 1327], train_loss/perplexity = 5.30678368/201.7004547 secs/batch = 0.6156s, grad.norm=0.55792063\n",
      " 10846: 8 [  230/ 1327], train_loss/perplexity = 5.16105318/174.3479767 secs/batch = 0.6125s, grad.norm=0.60934645\n",
      " 10851: 8 [  235/ 1327], train_loss/perplexity = 5.07283640/159.6264496 secs/batch = 0.6056s, grad.norm=0.63493747\n",
      " 10856: 8 [  240/ 1327], train_loss/perplexity = 4.88744974/132.6149445 secs/batch = 0.6082s, grad.norm=0.62421560\n",
      " 10861: 8 [  245/ 1327], train_loss/perplexity = 5.19394922/180.1787109 secs/batch = 0.6081s, grad.norm=0.56056869\n",
      " 10866: 8 [  250/ 1327], train_loss/perplexity = 4.88266850/131.9823914 secs/batch = 0.6161s, grad.norm=0.57421118\n",
      " 10871: 8 [  255/ 1327], train_loss/perplexity = 4.95045948/141.2398529 secs/batch = 0.6083s, grad.norm=0.54012626\n",
      " 10876: 8 [  260/ 1327], train_loss/perplexity = 5.31573486/203.5140076 secs/batch = 0.6144s, grad.norm=0.62368697\n",
      " 10881: 8 [  265/ 1327], train_loss/perplexity = 5.28200006/196.7630157 secs/batch = 0.6100s, grad.norm=0.55475277\n",
      " 10886: 8 [  270/ 1327], train_loss/perplexity = 5.31808567/203.9929962 secs/batch = 0.6085s, grad.norm=0.57518142\n",
      " 10891: 8 [  275/ 1327], train_loss/perplexity = 5.42324543/226.6133881 secs/batch = 0.6095s, grad.norm=0.59132892\n",
      " 10896: 8 [  280/ 1327], train_loss/perplexity = 5.14956331/172.3562012 secs/batch = 0.6086s, grad.norm=0.62264460\n",
      " 10901: 8 [  285/ 1327], train_loss/perplexity = 5.35812140/212.3256989 secs/batch = 0.6114s, grad.norm=0.65377325\n",
      " 10906: 8 [  290/ 1327], train_loss/perplexity = 5.24977064/190.5225677 secs/batch = 0.6124s, grad.norm=0.59828597\n",
      " 10911: 8 [  295/ 1327], train_loss/perplexity = 4.94634247/140.6595612 secs/batch = 0.6155s, grad.norm=0.56703275\n",
      " 10916: 8 [  300/ 1327], train_loss/perplexity = 4.51288795/91.1847763 secs/batch = 0.6077s, grad.norm=0.55197221\n",
      " 10921: 8 [  305/ 1327], train_loss/perplexity = 4.99400759/147.5264587 secs/batch = 0.6098s, grad.norm=0.57375795\n",
      " 10926: 8 [  310/ 1327], train_loss/perplexity = 5.02533913/152.2218628 secs/batch = 0.6075s, grad.norm=0.57970059\n",
      " 10931: 8 [  315/ 1327], train_loss/perplexity = 4.72336864/112.5467453 secs/batch = 0.6064s, grad.norm=0.56362242\n",
      " 10936: 8 [  320/ 1327], train_loss/perplexity = 4.79026127/120.3328018 secs/batch = 0.6110s, grad.norm=0.65037805\n",
      " 10941: 8 [  325/ 1327], train_loss/perplexity = 4.63369894/102.8939590 secs/batch = 0.6127s, grad.norm=0.62927681\n",
      " 10946: 8 [  330/ 1327], train_loss/perplexity = 5.10451651/164.7643890 secs/batch = 0.6076s, grad.norm=0.65580922\n",
      " 10951: 8 [  335/ 1327], train_loss/perplexity = 4.40496111/81.8559570 secs/batch = 0.6167s, grad.norm=0.57028842\n",
      " 10956: 8 [  340/ 1327], train_loss/perplexity = 5.21625233/184.2424011 secs/batch = 0.6104s, grad.norm=0.60898805\n",
      " 10961: 8 [  345/ 1327], train_loss/perplexity = 5.04688215/155.5367737 secs/batch = 0.6210s, grad.norm=0.57513434\n",
      " 10966: 8 [  350/ 1327], train_loss/perplexity = 5.15815544/173.8434906 secs/batch = 0.6154s, grad.norm=0.59705651\n",
      " 10971: 8 [  355/ 1327], train_loss/perplexity = 5.20735264/182.6099854 secs/batch = 0.6055s, grad.norm=0.59215438\n",
      " 10976: 8 [  360/ 1327], train_loss/perplexity = 5.26075220/192.6263275 secs/batch = 0.6078s, grad.norm=0.56552523\n",
      " 10981: 8 [  365/ 1327], train_loss/perplexity = 5.19870234/181.0371704 secs/batch = 0.6083s, grad.norm=0.56216264\n",
      " 10986: 8 [  370/ 1327], train_loss/perplexity = 5.23679304/188.0660095 secs/batch = 0.6130s, grad.norm=0.62622809\n",
      " 10991: 8 [  375/ 1327], train_loss/perplexity = 4.60751724/100.2349777 secs/batch = 0.6259s, grad.norm=0.56275451\n",
      " 10996: 8 [  380/ 1327], train_loss/perplexity = 4.83381987/125.6901627 secs/batch = 0.6163s, grad.norm=0.60700637\n",
      " 11001: 8 [  385/ 1327], train_loss/perplexity = 5.01920938/151.2916412 secs/batch = 0.6086s, grad.norm=0.60806358\n",
      " 11006: 8 [  390/ 1327], train_loss/perplexity = 5.03667593/153.9573975 secs/batch = 0.6080s, grad.norm=0.58107108\n",
      " 11011: 8 [  395/ 1327], train_loss/perplexity = 5.29639530/199.6159515 secs/batch = 0.6133s, grad.norm=0.61569518\n",
      " 11016: 8 [  400/ 1327], train_loss/perplexity = 4.98639631/146.4078674 secs/batch = 0.6088s, grad.norm=0.57843179\n",
      " 11021: 8 [  405/ 1327], train_loss/perplexity = 5.28288412/196.9370422 secs/batch = 0.6098s, grad.norm=0.59759259\n",
      " 11026: 8 [  410/ 1327], train_loss/perplexity = 5.02319288/151.8955078 secs/batch = 0.6093s, grad.norm=0.57346427\n",
      " 11031: 8 [  415/ 1327], train_loss/perplexity = 4.87998533/131.6287384 secs/batch = 0.6155s, grad.norm=0.58311063\n",
      " 11036: 8 [  420/ 1327], train_loss/perplexity = 4.75408125/116.0569763 secs/batch = 0.6117s, grad.norm=0.59692067\n",
      " 11041: 8 [  425/ 1327], train_loss/perplexity = 5.06944466/159.0859528 secs/batch = 0.6425s, grad.norm=0.73326737\n",
      " 11046: 8 [  430/ 1327], train_loss/perplexity = 5.16217756/174.5441284 secs/batch = 0.6130s, grad.norm=0.57460046\n",
      " 11051: 8 [  435/ 1327], train_loss/perplexity = 5.21512175/184.0342255 secs/batch = 0.6101s, grad.norm=0.58448499\n",
      " 11056: 8 [  440/ 1327], train_loss/perplexity = 4.90325785/134.7279816 secs/batch = 0.6113s, grad.norm=0.62815887\n",
      " 11061: 8 [  445/ 1327], train_loss/perplexity = 5.09886169/163.8353119 secs/batch = 0.6137s, grad.norm=0.61932272\n",
      " 11066: 8 [  450/ 1327], train_loss/perplexity = 4.95643950/142.0869904 secs/batch = 0.6107s, grad.norm=0.63201386\n",
      " 11071: 8 [  455/ 1327], train_loss/perplexity = 4.80567074/122.2014313 secs/batch = 0.6093s, grad.norm=0.58427346\n",
      " 11076: 8 [  460/ 1327], train_loss/perplexity = 5.00811195/149.6219788 secs/batch = 0.6103s, grad.norm=0.60865992\n",
      " 11081: 8 [  465/ 1327], train_loss/perplexity = 4.86978817/130.2933197 secs/batch = 0.6131s, grad.norm=0.63853049\n",
      " 11086: 8 [  470/ 1327], train_loss/perplexity = 5.31608868/203.5860291 secs/batch = 0.6071s, grad.norm=0.62379289\n",
      " 11091: 8 [  475/ 1327], train_loss/perplexity = 4.90765953/135.3223267 secs/batch = 0.6125s, grad.norm=0.54321295\n",
      " 11096: 8 [  480/ 1327], train_loss/perplexity = 5.07819891/160.4847412 secs/batch = 0.6159s, grad.norm=0.61514878\n",
      " 11101: 8 [  485/ 1327], train_loss/perplexity = 4.98118925/145.6474915 secs/batch = 0.6179s, grad.norm=0.60340297\n",
      " 11106: 8 [  490/ 1327], train_loss/perplexity = 4.90755701/135.3084564 secs/batch = 0.6131s, grad.norm=0.64543343\n",
      " 11111: 8 [  495/ 1327], train_loss/perplexity = 4.86358309/129.4873352 secs/batch = 0.6113s, grad.norm=0.59605306\n",
      " 11116: 8 [  500/ 1327], train_loss/perplexity = 5.16579533/175.1767273 secs/batch = 0.6108s, grad.norm=0.58436215\n",
      " 11121: 8 [  505/ 1327], train_loss/perplexity = 5.04000711/154.4711151 secs/batch = 0.6143s, grad.norm=0.59066153\n",
      " 11126: 8 [  510/ 1327], train_loss/perplexity = 5.39557219/220.4282379 secs/batch = 0.6111s, grad.norm=0.55694604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11131: 8 [  515/ 1327], train_loss/perplexity = 5.10896397/165.4988098 secs/batch = 0.6103s, grad.norm=0.59816426\n",
      " 11136: 8 [  520/ 1327], train_loss/perplexity = 5.29446316/199.2306366 secs/batch = 0.5995s, grad.norm=0.55437362\n",
      " 11141: 8 [  525/ 1327], train_loss/perplexity = 4.85987997/129.0087128 secs/batch = 0.6161s, grad.norm=0.59400403\n",
      " 11146: 8 [  530/ 1327], train_loss/perplexity = 4.98212194/145.7834015 secs/batch = 0.6121s, grad.norm=0.65916747\n",
      " 11151: 8 [  535/ 1327], train_loss/perplexity = 5.03017235/152.9593658 secs/batch = 0.6122s, grad.norm=0.62355155\n",
      " 11156: 8 [  540/ 1327], train_loss/perplexity = 5.08823156/162.1029358 secs/batch = 0.6372s, grad.norm=0.54311848\n",
      " 11161: 8 [  545/ 1327], train_loss/perplexity = 5.25885057/192.2603760 secs/batch = 0.6085s, grad.norm=0.55133110\n",
      " 11166: 8 [  550/ 1327], train_loss/perplexity = 5.09072447/162.5075531 secs/batch = 0.6150s, grad.norm=0.57904625\n",
      " 11171: 8 [  555/ 1327], train_loss/perplexity = 4.95719576/142.1944885 secs/batch = 0.6189s, grad.norm=0.57433826\n",
      " 11176: 8 [  560/ 1327], train_loss/perplexity = 5.01761913/151.0512390 secs/batch = 0.6157s, grad.norm=0.65266067\n",
      " 11181: 8 [  565/ 1327], train_loss/perplexity = 5.02925205/152.8186646 secs/batch = 0.6148s, grad.norm=0.61515707\n",
      " 11186: 8 [  570/ 1327], train_loss/perplexity = 4.91312838/136.0644073 secs/batch = 0.6096s, grad.norm=0.59240985\n",
      " 11191: 8 [  575/ 1327], train_loss/perplexity = 4.83508635/125.8494492 secs/batch = 0.6131s, grad.norm=0.63264692\n",
      " 11196: 8 [  580/ 1327], train_loss/perplexity = 5.16981745/175.8827209 secs/batch = 0.6107s, grad.norm=0.63346231\n",
      " 11201: 8 [  585/ 1327], train_loss/perplexity = 4.81786919/123.7012253 secs/batch = 0.6090s, grad.norm=0.56225878\n",
      " 11206: 8 [  590/ 1327], train_loss/perplexity = 5.07840967/160.5185699 secs/batch = 0.6164s, grad.norm=0.57277948\n",
      " 11211: 8 [  595/ 1327], train_loss/perplexity = 4.97338152/144.5147400 secs/batch = 0.6185s, grad.norm=0.64259124\n",
      " 11216: 8 [  600/ 1327], train_loss/perplexity = 5.28161001/196.6862946 secs/batch = 0.6114s, grad.norm=0.58042687\n",
      " 11221: 8 [  605/ 1327], train_loss/perplexity = 5.23041964/186.8712006 secs/batch = 0.6112s, grad.norm=0.59916514\n",
      " 11226: 8 [  610/ 1327], train_loss/perplexity = 5.30905819/202.1597443 secs/batch = 0.6152s, grad.norm=0.60831088\n",
      " 11231: 8 [  615/ 1327], train_loss/perplexity = 4.78662872/119.8964844 secs/batch = 0.6164s, grad.norm=0.57690549\n",
      " 11236: 8 [  620/ 1327], train_loss/perplexity = 5.14510870/171.5901337 secs/batch = 0.6123s, grad.norm=0.60338181\n",
      " 11241: 8 [  625/ 1327], train_loss/perplexity = 5.26465702/193.3799744 secs/batch = 0.6054s, grad.norm=0.58239627\n",
      " 11246: 8 [  630/ 1327], train_loss/perplexity = 5.26785278/193.9989624 secs/batch = 0.6087s, grad.norm=0.60670704\n",
      " 11251: 8 [  635/ 1327], train_loss/perplexity = 5.01063442/149.9998627 secs/batch = 0.6124s, grad.norm=0.61638659\n",
      " 11256: 8 [  640/ 1327], train_loss/perplexity = 5.07830334/160.5015106 secs/batch = 0.6134s, grad.norm=0.55204570\n",
      " 11261: 8 [  645/ 1327], train_loss/perplexity = 5.22587347/186.0235901 secs/batch = 0.6052s, grad.norm=0.58899003\n",
      " 11266: 8 [  650/ 1327], train_loss/perplexity = 4.90831423/135.4109497 secs/batch = 0.6116s, grad.norm=0.67362767\n",
      " 11271: 8 [  655/ 1327], train_loss/perplexity = 4.98201704/145.7681122 secs/batch = 0.6111s, grad.norm=0.54354227\n",
      " 11276: 8 [  660/ 1327], train_loss/perplexity = 4.89165354/133.1735992 secs/batch = 0.6110s, grad.norm=0.54646587\n",
      " 11281: 8 [  665/ 1327], train_loss/perplexity = 5.09412861/163.0616913 secs/batch = 0.6068s, grad.norm=0.55835599\n",
      " 11286: 8 [  670/ 1327], train_loss/perplexity = 4.97233009/144.3628693 secs/batch = 0.6153s, grad.norm=0.56771898\n",
      " 11291: 8 [  675/ 1327], train_loss/perplexity = 4.76048660/116.8027496 secs/batch = 0.6064s, grad.norm=0.65840948\n",
      " 11296: 8 [  680/ 1327], train_loss/perplexity = 5.06376076/158.1842957 secs/batch = 0.6117s, grad.norm=0.59651452\n",
      " 11301: 8 [  685/ 1327], train_loss/perplexity = 5.04075527/154.5867310 secs/batch = 0.6020s, grad.norm=0.60255480\n",
      " 11306: 8 [  690/ 1327], train_loss/perplexity = 5.19210243/179.8462677 secs/batch = 0.6102s, grad.norm=0.56501764\n",
      " 11311: 8 [  695/ 1327], train_loss/perplexity = 4.96976566/143.9931335 secs/batch = 0.6081s, grad.norm=0.57068586\n",
      " 11316: 8 [  700/ 1327], train_loss/perplexity = 5.25935936/192.3582153 secs/batch = 0.6126s, grad.norm=0.65429246\n",
      " 11321: 8 [  705/ 1327], train_loss/perplexity = 4.92897081/138.2371674 secs/batch = 0.6102s, grad.norm=0.57655108\n",
      " 11326: 8 [  710/ 1327], train_loss/perplexity = 4.90333557/134.7384644 secs/batch = 0.6088s, grad.norm=0.60234541\n",
      " 11331: 8 [  715/ 1327], train_loss/perplexity = 4.91689634/136.5780640 secs/batch = 0.6169s, grad.norm=0.58567375\n",
      " 11336: 8 [  720/ 1327], train_loss/perplexity = 4.99406338/147.5346985 secs/batch = 0.6467s, grad.norm=0.59150904\n",
      " 11341: 8 [  725/ 1327], train_loss/perplexity = 4.76437521/117.2578354 secs/batch = 0.6068s, grad.norm=0.57335907\n",
      " 11346: 8 [  730/ 1327], train_loss/perplexity = 4.92748022/138.0312653 secs/batch = 0.6092s, grad.norm=0.59261334\n",
      " 11351: 8 [  735/ 1327], train_loss/perplexity = 5.12095690/167.4955750 secs/batch = 0.6106s, grad.norm=0.60554588\n",
      " 11356: 8 [  740/ 1327], train_loss/perplexity = 4.50168467/90.1689072 secs/batch = 0.6045s, grad.norm=0.55709165\n",
      " 11361: 8 [  745/ 1327], train_loss/perplexity = 5.00639772/149.3657074 secs/batch = 0.6084s, grad.norm=0.59001350\n",
      " 11366: 8 [  750/ 1327], train_loss/perplexity = 4.85665751/128.5936584 secs/batch = 0.6170s, grad.norm=0.63317734\n",
      " 11371: 8 [  755/ 1327], train_loss/perplexity = 4.85353518/128.1927795 secs/batch = 0.6074s, grad.norm=0.57389158\n",
      " 11376: 8 [  760/ 1327], train_loss/perplexity = 4.75474358/116.1338730 secs/batch = 0.6122s, grad.norm=0.62423611\n",
      " 11381: 8 [  765/ 1327], train_loss/perplexity = 4.84844494/127.5419006 secs/batch = 0.6073s, grad.norm=0.61807817\n",
      " 11386: 8 [  770/ 1327], train_loss/perplexity = 4.77255535/118.2209549 secs/batch = 0.6145s, grad.norm=0.68171006\n",
      " 11391: 8 [  775/ 1327], train_loss/perplexity = 4.89673853/133.8525085 secs/batch = 0.6083s, grad.norm=0.60602576\n",
      " 11396: 8 [  780/ 1327], train_loss/perplexity = 5.15972948/174.1173401 secs/batch = 0.6122s, grad.norm=0.58749074\n",
      " 11401: 8 [  785/ 1327], train_loss/perplexity = 4.98010588/145.4897919 secs/batch = 0.6041s, grad.norm=0.59775102\n",
      " 11406: 8 [  790/ 1327], train_loss/perplexity = 4.81515551/123.3659973 secs/batch = 0.6116s, grad.norm=0.60707110\n",
      " 11411: 8 [  795/ 1327], train_loss/perplexity = 5.11493683/166.4902649 secs/batch = 0.6155s, grad.norm=0.58518320\n",
      " 11416: 8 [  800/ 1327], train_loss/perplexity = 5.07739735/160.3561554 secs/batch = 0.6100s, grad.norm=0.57861924\n",
      " 11421: 8 [  805/ 1327], train_loss/perplexity = 5.36524010/213.8425751 secs/batch = 0.6093s, grad.norm=0.56378603\n",
      " 11426: 8 [  810/ 1327], train_loss/perplexity = 4.98639965/146.4083557 secs/batch = 0.6072s, grad.norm=0.58463377\n",
      " 11431: 8 [  815/ 1327], train_loss/perplexity = 4.94342136/140.2492676 secs/batch = 0.6092s, grad.norm=0.58891261\n",
      " 11436: 8 [  820/ 1327], train_loss/perplexity = 4.59754896/99.2407761 secs/batch = 0.6108s, grad.norm=0.53851432\n",
      " 11441: 8 [  825/ 1327], train_loss/perplexity = 4.81564379/123.4262466 secs/batch = 0.6111s, grad.norm=0.56764764\n",
      " 11446: 8 [  830/ 1327], train_loss/perplexity = 4.70073986/110.0285492 secs/batch = 0.6092s, grad.norm=0.60280526\n",
      " 11451: 8 [  835/ 1327], train_loss/perplexity = 4.93502140/139.0761261 secs/batch = 0.6097s, grad.norm=0.59041154\n",
      " 11456: 8 [  840/ 1327], train_loss/perplexity = 5.05580616/156.9309845 secs/batch = 0.6099s, grad.norm=0.58110291\n",
      " 11461: 8 [  845/ 1327], train_loss/perplexity = 4.87183046/130.5596771 secs/batch = 0.6140s, grad.norm=0.58846009\n",
      " 11466: 8 [  850/ 1327], train_loss/perplexity = 4.94424677/140.3650818 secs/batch = 0.6113s, grad.norm=0.58171821\n",
      " 11471: 8 [  855/ 1327], train_loss/perplexity = 4.95960999/142.5381927 secs/batch = 0.6137s, grad.norm=0.64570421\n",
      " 11476: 8 [  860/ 1327], train_loss/perplexity = 4.67448997/107.1778870 secs/batch = 0.6061s, grad.norm=0.57601148\n",
      " 11481: 8 [  865/ 1327], train_loss/perplexity = 5.15961695/174.0977478 secs/batch = 0.6128s, grad.norm=0.61566275\n",
      " 11486: 8 [  870/ 1327], train_loss/perplexity = 5.09845304/163.7683716 secs/batch = 0.6062s, grad.norm=0.60801256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11491: 8 [  875/ 1327], train_loss/perplexity = 4.66754198/106.4357986 secs/batch = 0.6131s, grad.norm=0.64366889\n",
      " 11496: 8 [  880/ 1327], train_loss/perplexity = 4.84748936/127.4200821 secs/batch = 0.6159s, grad.norm=0.56797403\n",
      " 11501: 8 [  885/ 1327], train_loss/perplexity = 4.88389015/132.1437225 secs/batch = 0.6101s, grad.norm=0.58474892\n",
      " 11506: 8 [  890/ 1327], train_loss/perplexity = 5.09829712/163.7428284 secs/batch = 0.6115s, grad.norm=0.57099283\n",
      " 11511: 8 [  895/ 1327], train_loss/perplexity = 5.10581207/164.9779968 secs/batch = 0.6125s, grad.norm=0.57659733\n",
      " 11516: 8 [  900/ 1327], train_loss/perplexity = 5.00058270/148.4996643 secs/batch = 0.6178s, grad.norm=0.61286741\n",
      " 11521: 8 [  905/ 1327], train_loss/perplexity = 4.82696629/124.8316803 secs/batch = 0.6206s, grad.norm=0.56966084\n",
      " 11526: 8 [  910/ 1327], train_loss/perplexity = 4.91494179/136.3113708 secs/batch = 0.6171s, grad.norm=0.61191571\n",
      " 11531: 8 [  915/ 1327], train_loss/perplexity = 5.18986177/179.4437408 secs/batch = 0.6239s, grad.norm=0.59158152\n",
      " 11536: 8 [  920/ 1327], train_loss/perplexity = 5.29519796/199.3770905 secs/batch = 0.6175s, grad.norm=0.63115031\n",
      " 11541: 8 [  925/ 1327], train_loss/perplexity = 5.01153898/150.1356201 secs/batch = 0.6091s, grad.norm=0.62344640\n",
      " 11546: 8 [  930/ 1327], train_loss/perplexity = 4.97450590/144.6773224 secs/batch = 0.6207s, grad.norm=0.60052568\n",
      " 11551: 8 [  935/ 1327], train_loss/perplexity = 5.05570650/156.9153595 secs/batch = 0.6088s, grad.norm=0.56815988\n",
      " 11556: 8 [  940/ 1327], train_loss/perplexity = 4.99881649/148.2376099 secs/batch = 0.6103s, grad.norm=0.55681950\n",
      " 11561: 8 [  945/ 1327], train_loss/perplexity = 5.15601206/173.4712830 secs/batch = 0.6113s, grad.norm=0.56208211\n",
      " 11566: 8 [  950/ 1327], train_loss/perplexity = 4.96713638/143.6150360 secs/batch = 0.6216s, grad.norm=0.61863458\n",
      " 11571: 8 [  955/ 1327], train_loss/perplexity = 5.07764006/160.3950806 secs/batch = 0.6157s, grad.norm=0.58764631\n",
      " 11576: 8 [  960/ 1327], train_loss/perplexity = 5.32277679/204.9522095 secs/batch = 0.6111s, grad.norm=0.60331184\n",
      " 11581: 8 [  965/ 1327], train_loss/perplexity = 5.08535957/161.6380463 secs/batch = 0.6496s, grad.norm=0.58520001\n",
      " 11586: 8 [  970/ 1327], train_loss/perplexity = 5.25956297/192.3973846 secs/batch = 0.6047s, grad.norm=0.55956054\n",
      " 11591: 8 [  975/ 1327], train_loss/perplexity = 4.98911762/146.8068237 secs/batch = 0.6069s, grad.norm=0.58065897\n",
      " 11596: 8 [  980/ 1327], train_loss/perplexity = 4.78662348/119.8958511 secs/batch = 0.6024s, grad.norm=0.55651343\n",
      " 11601: 8 [  985/ 1327], train_loss/perplexity = 5.01956415/151.3453217 secs/batch = 0.6059s, grad.norm=0.60632646\n",
      " 11606: 8 [  990/ 1327], train_loss/perplexity = 5.15578890/173.4325714 secs/batch = 0.6173s, grad.norm=0.61875015\n",
      " 11611: 8 [  995/ 1327], train_loss/perplexity = 5.18537951/178.6412354 secs/batch = 0.6132s, grad.norm=0.62051386\n",
      " 11616: 8 [ 1000/ 1327], train_loss/perplexity = 4.66587734/106.2587662 secs/batch = 0.6129s, grad.norm=0.56972468\n",
      " 11621: 8 [ 1005/ 1327], train_loss/perplexity = 5.11135340/165.8947296 secs/batch = 0.6091s, grad.norm=0.59196061\n",
      " 11626: 8 [ 1010/ 1327], train_loss/perplexity = 4.70416451/110.4060059 secs/batch = 0.6085s, grad.norm=0.58534980\n",
      " 11631: 8 [ 1015/ 1327], train_loss/perplexity = 5.15392303/173.1092682 secs/batch = 0.6129s, grad.norm=0.60558957\n",
      " 11636: 8 [ 1020/ 1327], train_loss/perplexity = 5.31317854/202.9944305 secs/batch = 0.6153s, grad.norm=0.58455890\n",
      " 11641: 8 [ 1025/ 1327], train_loss/perplexity = 5.15692997/173.6305847 secs/batch = 0.6109s, grad.norm=0.64667457\n",
      " 11646: 8 [ 1030/ 1327], train_loss/perplexity = 4.97075033/144.1349945 secs/batch = 0.6123s, grad.norm=0.58409780\n",
      " 11651: 8 [ 1035/ 1327], train_loss/perplexity = 4.85631800/128.5500031 secs/batch = 0.6184s, grad.norm=0.60011691\n",
      " 11656: 8 [ 1040/ 1327], train_loss/perplexity = 5.13717794/170.2346802 secs/batch = 0.6081s, grad.norm=0.56213725\n",
      " 11661: 8 [ 1045/ 1327], train_loss/perplexity = 4.74947548/115.5236740 secs/batch = 0.6078s, grad.norm=0.58451384\n",
      " 11666: 8 [ 1050/ 1327], train_loss/perplexity = 4.74990892/115.5737610 secs/batch = 0.6100s, grad.norm=0.56693059\n",
      " 11671: 8 [ 1055/ 1327], train_loss/perplexity = 5.00757504/149.5416565 secs/batch = 0.6078s, grad.norm=0.59297806\n",
      " 11676: 8 [ 1060/ 1327], train_loss/perplexity = 4.63536453/103.0654831 secs/batch = 0.6047s, grad.norm=0.60515755\n",
      " 11681: 8 [ 1065/ 1327], train_loss/perplexity = 4.71823788/111.9707718 secs/batch = 0.6117s, grad.norm=0.57908940\n",
      " 11686: 8 [ 1070/ 1327], train_loss/perplexity = 5.06794071/158.8468781 secs/batch = 0.6068s, grad.norm=0.60186070\n",
      " 11691: 8 [ 1075/ 1327], train_loss/perplexity = 4.81417894/123.2455826 secs/batch = 0.6062s, grad.norm=0.59336513\n",
      " 11696: 8 [ 1080/ 1327], train_loss/perplexity = 4.73428154/113.7816849 secs/batch = 0.6068s, grad.norm=0.59301645\n",
      " 11701: 8 [ 1085/ 1327], train_loss/perplexity = 4.63041162/102.5562668 secs/batch = 0.6067s, grad.norm=0.60730094\n",
      " 11706: 8 [ 1090/ 1327], train_loss/perplexity = 4.84673357/127.3238144 secs/batch = 0.6051s, grad.norm=0.62459797\n",
      " 11711: 8 [ 1095/ 1327], train_loss/perplexity = 4.93719244/139.3783875 secs/batch = 0.6117s, grad.norm=0.60964835\n",
      " 11716: 8 [ 1100/ 1327], train_loss/perplexity = 4.89772320/133.9843750 secs/batch = 0.6081s, grad.norm=0.62781751\n",
      " 11721: 8 [ 1105/ 1327], train_loss/perplexity = 4.74509192/115.0183792 secs/batch = 0.6185s, grad.norm=0.59416652\n",
      " 11726: 8 [ 1110/ 1327], train_loss/perplexity = 5.29548502/199.4343262 secs/batch = 0.6054s, grad.norm=0.71211100\n",
      " 11731: 8 [ 1115/ 1327], train_loss/perplexity = 4.79809523/121.2791901 secs/batch = 0.6141s, grad.norm=0.56326759\n",
      " 11736: 8 [ 1120/ 1327], train_loss/perplexity = 4.98328924/145.9536743 secs/batch = 0.6061s, grad.norm=0.60190976\n",
      " 11741: 8 [ 1125/ 1327], train_loss/perplexity = 5.21067572/183.2178192 secs/batch = 0.6103s, grad.norm=0.58785021\n",
      " 11746: 8 [ 1130/ 1327], train_loss/perplexity = 4.88299751/132.0258179 secs/batch = 0.6145s, grad.norm=0.58598554\n",
      " 11751: 8 [ 1135/ 1327], train_loss/perplexity = 4.93103313/138.5225525 secs/batch = 0.6144s, grad.norm=0.54576129\n",
      " 11756: 8 [ 1140/ 1327], train_loss/perplexity = 5.20071936/181.4026947 secs/batch = 0.6152s, grad.norm=0.59355605\n",
      " 11761: 8 [ 1145/ 1327], train_loss/perplexity = 4.95568514/141.9798431 secs/batch = 0.6098s, grad.norm=0.66639787\n",
      " 11766: 8 [ 1150/ 1327], train_loss/perplexity = 4.93975925/139.7366028 secs/batch = 0.6153s, grad.norm=0.58818245\n",
      " 11771: 8 [ 1155/ 1327], train_loss/perplexity = 5.01366806/150.4556122 secs/batch = 0.6129s, grad.norm=0.60967344\n",
      " 11776: 8 [ 1160/ 1327], train_loss/perplexity = 4.92903614/138.2462006 secs/batch = 0.6108s, grad.norm=0.60073406\n",
      " 11781: 8 [ 1165/ 1327], train_loss/perplexity = 5.06136084/157.8051147 secs/batch = 0.6149s, grad.norm=0.62471348\n",
      " 11786: 8 [ 1170/ 1327], train_loss/perplexity = 4.90514851/134.9829559 secs/batch = 0.6079s, grad.norm=0.59158993\n",
      " 11791: 8 [ 1175/ 1327], train_loss/perplexity = 4.70394754/110.3820496 secs/batch = 0.6144s, grad.norm=0.71173531\n",
      " 11796: 8 [ 1180/ 1327], train_loss/perplexity = 4.67790222/107.5442352 secs/batch = 0.6141s, grad.norm=0.60803246\n",
      " 11801: 8 [ 1185/ 1327], train_loss/perplexity = 4.87916279/131.5205078 secs/batch = 0.6131s, grad.norm=0.58040518\n",
      " 11806: 8 [ 1190/ 1327], train_loss/perplexity = 4.93596601/139.2075500 secs/batch = 0.6049s, grad.norm=0.57485920\n",
      " 11811: 8 [ 1195/ 1327], train_loss/perplexity = 4.74098301/114.5467453 secs/batch = 0.6117s, grad.norm=0.60662681\n",
      " 11816: 8 [ 1200/ 1327], train_loss/perplexity = 4.67001295/106.6991272 secs/batch = 0.6131s, grad.norm=0.63166308\n",
      " 11821: 8 [ 1205/ 1327], train_loss/perplexity = 4.74457741/114.9592133 secs/batch = 0.6149s, grad.norm=0.59355134\n",
      " 11826: 8 [ 1210/ 1327], train_loss/perplexity = 4.54897404/94.5353699 secs/batch = 0.6519s, grad.norm=0.61789829\n",
      " 11831: 8 [ 1215/ 1327], train_loss/perplexity = 4.65457010/105.0640411 secs/batch = 0.6058s, grad.norm=0.56662720\n",
      " 11836: 8 [ 1220/ 1327], train_loss/perplexity = 4.80441332/122.0478668 secs/batch = 0.6121s, grad.norm=0.62470436\n",
      " 11841: 8 [ 1225/ 1327], train_loss/perplexity = 4.64399147/103.9584656 secs/batch = 0.6066s, grad.norm=0.65551472\n",
      " 11846: 8 [ 1230/ 1327], train_loss/perplexity = 4.78729200/119.9760361 secs/batch = 0.6093s, grad.norm=0.59929651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11851: 8 [ 1235/ 1327], train_loss/perplexity = 4.81900167/123.8413925 secs/batch = 0.6066s, grad.norm=0.58141810\n",
      " 11856: 8 [ 1240/ 1327], train_loss/perplexity = 4.94122696/139.9418488 secs/batch = 0.6125s, grad.norm=0.55523401\n",
      " 11861: 8 [ 1245/ 1327], train_loss/perplexity = 4.80074024/121.6003952 secs/batch = 0.6110s, grad.norm=0.57135558\n",
      " 11866: 8 [ 1250/ 1327], train_loss/perplexity = 4.93378639/138.9044647 secs/batch = 0.6191s, grad.norm=0.55863655\n",
      " 11871: 8 [ 1255/ 1327], train_loss/perplexity = 4.93486595/139.0545044 secs/batch = 0.6189s, grad.norm=0.58643848\n",
      " 11876: 8 [ 1260/ 1327], train_loss/perplexity = 4.90635777/135.1462860 secs/batch = 0.6126s, grad.norm=0.66448188\n",
      " 11881: 8 [ 1265/ 1327], train_loss/perplexity = 4.98382759/146.0322723 secs/batch = 0.6097s, grad.norm=0.63961399\n",
      " 11886: 8 [ 1270/ 1327], train_loss/perplexity = 4.80594778/122.2352905 secs/batch = 0.6107s, grad.norm=0.58647126\n",
      " 11891: 8 [ 1275/ 1327], train_loss/perplexity = 4.98922396/146.8224335 secs/batch = 0.6114s, grad.norm=0.61478698\n",
      " 11896: 8 [ 1280/ 1327], train_loss/perplexity = 4.79445839/120.8389130 secs/batch = 0.6150s, grad.norm=0.63864744\n",
      " 11901: 8 [ 1285/ 1327], train_loss/perplexity = 4.80428791/122.0325623 secs/batch = 0.6110s, grad.norm=0.58500624\n",
      " 11906: 8 [ 1290/ 1327], train_loss/perplexity = 4.92819881/138.1304932 secs/batch = 0.6132s, grad.norm=0.58342844\n",
      " 11911: 8 [ 1295/ 1327], train_loss/perplexity = 4.97384739/144.5820770 secs/batch = 0.6192s, grad.norm=0.56669617\n",
      " 11916: 8 [ 1300/ 1327], train_loss/perplexity = 5.06588221/158.5202332 secs/batch = 0.6108s, grad.norm=0.60953236\n",
      " 11921: 8 [ 1305/ 1327], train_loss/perplexity = 5.21192789/183.4473877 secs/batch = 0.6058s, grad.norm=0.61284083\n",
      " 11926: 8 [ 1310/ 1327], train_loss/perplexity = 5.42732191/227.5390625 secs/batch = 0.6174s, grad.norm=0.57420921\n",
      " 11931: 8 [ 1315/ 1327], train_loss/perplexity = 5.25008821/190.5830841 secs/batch = 0.6118s, grad.norm=0.65347826\n",
      " 11936: 8 [ 1320/ 1327], train_loss/perplexity = 5.21731663/184.4385986 secs/batch = 0.6097s, grad.norm=0.60248959\n",
      " 11941: 8 [ 1325/ 1327], train_loss/perplexity = 5.09111643/162.5712585 secs/batch = 0.6242s, grad.norm=0.56625026\n",
      "Epoch training time: 812.5742917060852\n",
      "Saved char model cv/epoch008_5.0572.model\n",
      " 11948: 9 [    5/ 1327], train_loss/perplexity = 5.25589085/191.6921844 secs/batch = 0.6098s, grad.norm=0.59900075\n",
      " 11953: 9 [   10/ 1327], train_loss/perplexity = 4.78993320/120.2933350 secs/batch = 0.6045s, grad.norm=0.58865696\n",
      " 11958: 9 [   15/ 1327], train_loss/perplexity = 4.94532633/140.5166931 secs/batch = 0.6126s, grad.norm=0.59593046\n",
      " 11963: 9 [   20/ 1327], train_loss/perplexity = 5.21556664/184.1161194 secs/batch = 0.6110s, grad.norm=0.67303252\n",
      " 11968: 9 [   25/ 1327], train_loss/perplexity = 5.05085802/156.1563873 secs/batch = 0.6071s, grad.norm=0.58861399\n",
      " 11973: 9 [   30/ 1327], train_loss/perplexity = 4.99281168/147.3501434 secs/batch = 0.6515s, grad.norm=0.58520699\n",
      " 11978: 9 [   35/ 1327], train_loss/perplexity = 4.85293722/128.1161499 secs/batch = 0.6080s, grad.norm=0.57866055\n",
      " 11983: 9 [   40/ 1327], train_loss/perplexity = 4.93823242/139.5234070 secs/batch = 0.6089s, grad.norm=0.63130921\n",
      " 11988: 9 [   45/ 1327], train_loss/perplexity = 4.63442087/102.9682693 secs/batch = 0.6111s, grad.norm=0.59077442\n",
      " 11993: 9 [   50/ 1327], train_loss/perplexity = 4.95633698/142.0724335 secs/batch = 0.6152s, grad.norm=0.60961992\n",
      " 11998: 9 [   55/ 1327], train_loss/perplexity = 4.89621353/133.7822571 secs/batch = 0.6162s, grad.norm=0.59926796\n",
      " 12003: 9 [   60/ 1327], train_loss/perplexity = 5.18717766/178.9627380 secs/batch = 0.6088s, grad.norm=0.64453733\n",
      " 12008: 9 [   65/ 1327], train_loss/perplexity = 4.68388510/108.1895828 secs/batch = 0.6127s, grad.norm=0.58799630\n",
      " 12013: 9 [   70/ 1327], train_loss/perplexity = 4.60639906/100.1229630 secs/batch = 0.6052s, grad.norm=0.59908998\n",
      " 12018: 9 [   75/ 1327], train_loss/perplexity = 4.57644320/97.1681671 secs/batch = 0.6107s, grad.norm=0.59045744\n",
      " 12023: 9 [   80/ 1327], train_loss/perplexity = 4.89221907/133.2489319 secs/batch = 0.6171s, grad.norm=0.58882594\n",
      " 12028: 9 [   85/ 1327], train_loss/perplexity = 4.94511414/140.4868774 secs/batch = 0.6069s, grad.norm=0.61383861\n",
      " 12033: 9 [   90/ 1327], train_loss/perplexity = 4.95582914/142.0002899 secs/batch = 0.6116s, grad.norm=0.61058021\n",
      " 12038: 9 [   95/ 1327], train_loss/perplexity = 4.79628944/121.0603790 secs/batch = 0.6179s, grad.norm=0.61459899\n",
      " 12043: 9 [  100/ 1327], train_loss/perplexity = 5.03109217/153.1001282 secs/batch = 0.6128s, grad.norm=0.63745731\n",
      " 12048: 9 [  105/ 1327], train_loss/perplexity = 5.03527069/153.7412109 secs/batch = 0.6080s, grad.norm=0.62457752\n",
      " 12053: 9 [  110/ 1327], train_loss/perplexity = 4.83716297/126.1110611 secs/batch = 0.6211s, grad.norm=0.57604301\n",
      " 12058: 9 [  115/ 1327], train_loss/perplexity = 4.71158743/111.2285843 secs/batch = 0.6104s, grad.norm=0.61019045\n",
      " 12063: 9 [  120/ 1327], train_loss/perplexity = 4.89560938/133.7014618 secs/batch = 0.6056s, grad.norm=0.67038858\n",
      " 12068: 9 [  125/ 1327], train_loss/perplexity = 4.98051310/145.5490417 secs/batch = 0.6109s, grad.norm=0.64248437\n",
      " 12073: 9 [  130/ 1327], train_loss/perplexity = 4.85451841/128.3188782 secs/batch = 0.5982s, grad.norm=0.61187732\n",
      " 12078: 9 [  135/ 1327], train_loss/perplexity = 4.85803270/128.7706299 secs/batch = 0.6079s, grad.norm=0.62296265\n",
      " 12083: 9 [  140/ 1327], train_loss/perplexity = 5.12574911/168.3001709 secs/batch = 0.6130s, grad.norm=0.59456855\n",
      " 12088: 9 [  145/ 1327], train_loss/perplexity = 5.08506775/161.5908813 secs/batch = 0.6185s, grad.norm=0.61683816\n",
      " 12093: 9 [  150/ 1327], train_loss/perplexity = 5.01440001/150.5657654 secs/batch = 0.6137s, grad.norm=0.58854413\n",
      " 12098: 9 [  155/ 1327], train_loss/perplexity = 5.29278660/198.8968964 secs/batch = 0.6113s, grad.norm=0.57921988\n",
      " 12103: 9 [  160/ 1327], train_loss/perplexity = 4.91111946/135.7913361 secs/batch = 0.6160s, grad.norm=0.58613473\n",
      " 12108: 9 [  165/ 1327], train_loss/perplexity = 5.15473652/173.2501526 secs/batch = 0.6136s, grad.norm=0.57973439\n",
      " 12113: 9 [  170/ 1327], train_loss/perplexity = 4.90637159/135.1481476 secs/batch = 0.6107s, grad.norm=0.60593098\n",
      " 12118: 9 [  175/ 1327], train_loss/perplexity = 5.13008213/169.0310059 secs/batch = 0.5994s, grad.norm=0.59404218\n",
      " 12123: 9 [  180/ 1327], train_loss/perplexity = 5.04732895/155.6062775 secs/batch = 0.6119s, grad.norm=0.58405411\n",
      " 12128: 9 [  185/ 1327], train_loss/perplexity = 5.25277710/191.0962219 secs/batch = 0.6099s, grad.norm=0.57447684\n",
      " 12133: 9 [  190/ 1327], train_loss/perplexity = 4.79063272/120.3775101 secs/batch = 0.6121s, grad.norm=0.54592901\n",
      " 12138: 9 [  195/ 1327], train_loss/perplexity = 5.06413746/158.2438965 secs/batch = 0.6096s, grad.norm=0.58819544\n",
      " 12143: 9 [  200/ 1327], train_loss/perplexity = 4.92374706/137.5169373 secs/batch = 0.6114s, grad.norm=0.58030593\n",
      " 12148: 9 [  205/ 1327], train_loss/perplexity = 5.05821800/157.3099365 secs/batch = 0.6122s, grad.norm=0.61688608\n",
      " 12153: 9 [  210/ 1327], train_loss/perplexity = 5.01516867/150.6815491 secs/batch = 0.6131s, grad.norm=0.58012170\n",
      " 12158: 9 [  215/ 1327], train_loss/perplexity = 5.07329893/159.7003021 secs/batch = 0.6125s, grad.norm=0.55779338\n",
      " 12163: 9 [  220/ 1327], train_loss/perplexity = 5.15529490/173.3469238 secs/batch = 0.6138s, grad.norm=0.59821159\n",
      " 12168: 9 [  225/ 1327], train_loss/perplexity = 5.24269199/189.1786804 secs/batch = 0.6246s, grad.norm=0.58344263\n",
      " 12173: 9 [  230/ 1327], train_loss/perplexity = 5.10300398/164.5153656 secs/batch = 0.6092s, grad.norm=0.63022918\n",
      " 12178: 9 [  235/ 1327], train_loss/perplexity = 4.96983099/144.0025482 secs/batch = 0.6098s, grad.norm=0.60503638\n",
      " 12183: 9 [  240/ 1327], train_loss/perplexity = 4.78438520/119.6277924 secs/batch = 0.6037s, grad.norm=0.61095864\n",
      " 12188: 9 [  245/ 1327], train_loss/perplexity = 5.09194040/162.7052765 secs/batch = 0.6091s, grad.norm=0.59027469\n",
      " 12193: 9 [  250/ 1327], train_loss/perplexity = 4.80750990/122.4263840 secs/batch = 0.6067s, grad.norm=0.56650490\n",
      " 12198: 9 [  255/ 1327], train_loss/perplexity = 4.85605240/128.5158691 secs/batch = 0.6101s, grad.norm=0.58766288\n",
      " 12203: 9 [  260/ 1327], train_loss/perplexity = 5.20961523/183.0236206 secs/batch = 0.6074s, grad.norm=0.69014645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12208: 9 [  265/ 1327], train_loss/perplexity = 5.20558357/182.2872162 secs/batch = 0.6094s, grad.norm=0.56349796\n",
      " 12213: 9 [  270/ 1327], train_loss/perplexity = 5.26538563/193.5209198 secs/batch = 0.6094s, grad.norm=0.56868672\n",
      " 12218: 9 [  275/ 1327], train_loss/perplexity = 5.31807566/203.9909515 secs/batch = 0.6499s, grad.norm=0.59586734\n",
      " 12223: 9 [  280/ 1327], train_loss/perplexity = 5.05853319/157.3595276 secs/batch = 0.6166s, grad.norm=0.60055935\n",
      " 12228: 9 [  285/ 1327], train_loss/perplexity = 5.28768587/197.8849640 secs/batch = 0.6074s, grad.norm=0.64660507\n",
      " 12233: 9 [  290/ 1327], train_loss/perplexity = 5.12763166/168.6173096 secs/batch = 0.6078s, grad.norm=0.61846858\n",
      " 12238: 9 [  295/ 1327], train_loss/perplexity = 4.85741043/128.6905212 secs/batch = 0.6143s, grad.norm=0.61280048\n",
      " 12243: 9 [  300/ 1327], train_loss/perplexity = 4.45827055/86.3380661 secs/batch = 0.6076s, grad.norm=0.56760353\n",
      " 12248: 9 [  305/ 1327], train_loss/perplexity = 4.90128851/134.4629211 secs/batch = 0.6085s, grad.norm=0.59792978\n",
      " 12253: 9 [  310/ 1327], train_loss/perplexity = 4.90457869/134.9060669 secs/batch = 0.6110s, grad.norm=0.58521116\n",
      " 12258: 9 [  315/ 1327], train_loss/perplexity = 4.61907911/101.4006119 secs/batch = 0.6052s, grad.norm=0.59139758\n",
      " 12263: 9 [  320/ 1327], train_loss/perplexity = 4.67847013/107.6053238 secs/batch = 0.6145s, grad.norm=0.63047040\n",
      " 12268: 9 [  325/ 1327], train_loss/perplexity = 4.55061054/94.6902008 secs/batch = 0.6108s, grad.norm=0.61230439\n",
      " 12273: 9 [  330/ 1327], train_loss/perplexity = 4.97998428/145.4720917 secs/batch = 0.6079s, grad.norm=0.61975425\n",
      " 12278: 9 [  335/ 1327], train_loss/perplexity = 4.36662054/78.7769547 secs/batch = 0.6151s, grad.norm=0.56182879\n",
      " 12283: 9 [  340/ 1327], train_loss/perplexity = 5.09480000/163.1712036 secs/batch = 0.6065s, grad.norm=0.58661693\n",
      " 12288: 9 [  345/ 1327], train_loss/perplexity = 4.96857929/143.8224182 secs/batch = 0.6075s, grad.norm=0.55659723\n",
      " 12293: 9 [  350/ 1327], train_loss/perplexity = 5.06701469/158.6998444 secs/batch = 0.6088s, grad.norm=0.61950350\n",
      " 12298: 9 [  355/ 1327], train_loss/perplexity = 5.10263968/164.4554443 secs/batch = 0.6090s, grad.norm=0.62525791\n",
      " 12303: 9 [  360/ 1327], train_loss/perplexity = 5.20397902/181.9949646 secs/batch = 0.6098s, grad.norm=0.59203261\n",
      " 12308: 9 [  365/ 1327], train_loss/perplexity = 5.11148310/165.9162445 secs/batch = 0.6129s, grad.norm=0.58699453\n",
      " 12313: 9 [  370/ 1327], train_loss/perplexity = 5.14594078/171.7329712 secs/batch = 0.6081s, grad.norm=0.63287801\n",
      " 12318: 9 [  375/ 1327], train_loss/perplexity = 4.50210333/90.2066650 secs/batch = 0.6115s, grad.norm=0.56346488\n",
      " 12323: 9 [  380/ 1327], train_loss/perplexity = 4.72091532/112.2709656 secs/batch = 0.6123s, grad.norm=0.61682910\n",
      " 12328: 9 [  385/ 1327], train_loss/perplexity = 4.93656397/139.2908173 secs/batch = 0.6126s, grad.norm=0.63812268\n",
      " 12333: 9 [  390/ 1327], train_loss/perplexity = 4.95481443/141.8562775 secs/batch = 0.6168s, grad.norm=0.59137428\n",
      " 12338: 9 [  395/ 1327], train_loss/perplexity = 5.16978264/175.8766022 secs/batch = 0.6068s, grad.norm=0.59955698\n",
      " 12343: 9 [  400/ 1327], train_loss/perplexity = 4.90075588/134.3913269 secs/batch = 0.6117s, grad.norm=0.57424366\n",
      " 12348: 9 [  405/ 1327], train_loss/perplexity = 5.22331429/185.5481262 secs/batch = 0.6105s, grad.norm=0.63285452\n",
      " 12353: 9 [  410/ 1327], train_loss/perplexity = 4.91786003/136.7097473 secs/batch = 0.6168s, grad.norm=0.57716292\n",
      " 12358: 9 [  415/ 1327], train_loss/perplexity = 4.79256058/120.6098022 secs/batch = 0.6135s, grad.norm=0.64522922\n",
      " 12363: 9 [  420/ 1327], train_loss/perplexity = 4.67257547/106.9728928 secs/batch = 0.6073s, grad.norm=0.60770726\n",
      " 12368: 9 [  425/ 1327], train_loss/perplexity = 4.95164108/141.4068298 secs/batch = 0.6116s, grad.norm=0.61396098\n",
      " 12373: 9 [  430/ 1327], train_loss/perplexity = 5.08561325/161.6790619 secs/batch = 0.6141s, grad.norm=0.60491985\n",
      " 12378: 9 [  435/ 1327], train_loss/perplexity = 5.10741615/165.2428436 secs/batch = 0.6096s, grad.norm=0.61012542\n",
      " 12383: 9 [  440/ 1327], train_loss/perplexity = 4.82171679/124.1780930 secs/batch = 0.6128s, grad.norm=0.64397687\n",
      " 12388: 9 [  445/ 1327], train_loss/perplexity = 4.99505043/147.6803894 secs/batch = 0.6083s, grad.norm=0.60226762\n",
      " 12393: 9 [  450/ 1327], train_loss/perplexity = 4.84568262/127.1900787 secs/batch = 0.6106s, grad.norm=0.58500236\n",
      " 12398: 9 [  455/ 1327], train_loss/perplexity = 4.75139380/115.7454987 secs/batch = 0.6098s, grad.norm=0.60648602\n",
      " 12403: 9 [  460/ 1327], train_loss/perplexity = 4.89949894/134.2225037 secs/batch = 0.6114s, grad.norm=0.61676532\n",
      " 12408: 9 [  465/ 1327], train_loss/perplexity = 4.76670122/117.5308914 secs/batch = 0.6177s, grad.norm=0.61663902\n",
      " 12413: 9 [  470/ 1327], train_loss/perplexity = 5.26653671/193.7438049 secs/batch = 0.6268s, grad.norm=0.62675506\n",
      " 12418: 9 [  475/ 1327], train_loss/perplexity = 4.81062269/122.8080673 secs/batch = 0.6096s, grad.norm=0.55296326\n",
      " 12423: 9 [  480/ 1327], train_loss/perplexity = 4.95079565/141.2873383 secs/batch = 0.6132s, grad.norm=0.61121792\n",
      " 12428: 9 [  485/ 1327], train_loss/perplexity = 4.86191082/129.2709808 secs/batch = 0.6171s, grad.norm=0.57036376\n",
      " 12433: 9 [  490/ 1327], train_loss/perplexity = 4.80730772/122.4016342 secs/batch = 0.6182s, grad.norm=0.67423695\n",
      " 12438: 9 [  495/ 1327], train_loss/perplexity = 4.77159786/118.1078110 secs/batch = 0.6091s, grad.norm=0.63155633\n",
      " 12443: 9 [  500/ 1327], train_loss/perplexity = 5.06909657/159.0305939 secs/batch = 0.6053s, grad.norm=0.59653300\n",
      " 12448: 9 [  505/ 1327], train_loss/perplexity = 4.95420170/141.7693939 secs/batch = 0.6088s, grad.norm=0.58777541\n",
      " 12453: 9 [  510/ 1327], train_loss/perplexity = 5.34309721/209.1595154 secs/batch = 0.6057s, grad.norm=0.56247860\n",
      " 12458: 9 [  515/ 1327], train_loss/perplexity = 5.06472301/158.3365784 secs/batch = 0.6094s, grad.norm=0.60714793\n",
      " 12463: 9 [  520/ 1327], train_loss/perplexity = 5.18717861/178.9629211 secs/batch = 0.6501s, grad.norm=0.57289916\n",
      " 12468: 9 [  525/ 1327], train_loss/perplexity = 4.76975965/117.8909073 secs/batch = 0.6138s, grad.norm=0.57214838\n",
      " 12473: 9 [  530/ 1327], train_loss/perplexity = 4.90463114/134.9131317 secs/batch = 0.6078s, grad.norm=0.61353201\n",
      " 12478: 9 [  535/ 1327], train_loss/perplexity = 4.97247124/144.3832550 secs/batch = 0.6128s, grad.norm=0.64108604\n",
      " 12483: 9 [  540/ 1327], train_loss/perplexity = 5.04345274/155.0042877 secs/batch = 0.6075s, grad.norm=0.55518651\n",
      " 12488: 9 [  545/ 1327], train_loss/perplexity = 5.17341805/176.5171509 secs/batch = 0.6094s, grad.norm=0.57386589\n",
      " 12493: 9 [  550/ 1327], train_loss/perplexity = 5.03621006/153.8856964 secs/batch = 0.6139s, grad.norm=0.58570570\n",
      " 12498: 9 [  555/ 1327], train_loss/perplexity = 4.87412596/130.8597260 secs/batch = 0.6101s, grad.norm=0.58905578\n",
      " 12503: 9 [  560/ 1327], train_loss/perplexity = 4.91325855/136.0821228 secs/batch = 0.6063s, grad.norm=0.62878311\n",
      " 12508: 9 [  565/ 1327], train_loss/perplexity = 4.96208572/142.8915100 secs/batch = 0.6055s, grad.norm=0.60770047\n",
      " 12513: 9 [  570/ 1327], train_loss/perplexity = 4.81576586/123.4413147 secs/batch = 0.6150s, grad.norm=0.62102246\n",
      " 12518: 9 [  575/ 1327], train_loss/perplexity = 4.73112154/113.4226990 secs/batch = 0.6069s, grad.norm=0.61781269\n",
      " 12523: 9 [  580/ 1327], train_loss/perplexity = 5.10137606/164.2477722 secs/batch = 0.6071s, grad.norm=0.62473935\n",
      " 12528: 9 [  585/ 1327], train_loss/perplexity = 4.69462013/109.3572617 secs/batch = 0.6362s, grad.norm=0.56859028\n",
      " 12533: 9 [  590/ 1327], train_loss/perplexity = 5.00317717/148.8854370 secs/batch = 0.6101s, grad.norm=0.57997912\n",
      " 12538: 9 [  595/ 1327], train_loss/perplexity = 4.96170044/142.8364716 secs/batch = 0.6122s, grad.norm=0.73699015\n",
      " 12543: 9 [  600/ 1327], train_loss/perplexity = 5.21499252/184.0104523 secs/batch = 0.6134s, grad.norm=0.58056402\n",
      " 12548: 9 [  605/ 1327], train_loss/perplexity = 5.13817453/170.4044189 secs/batch = 0.6125s, grad.norm=0.60401756\n",
      " 12553: 9 [  610/ 1327], train_loss/perplexity = 5.23241901/187.2452087 secs/batch = 0.6095s, grad.norm=0.61135662\n",
      " 12558: 9 [  615/ 1327], train_loss/perplexity = 4.69144726/109.0108337 secs/batch = 0.6156s, grad.norm=0.59578210\n",
      " 12563: 9 [  620/ 1327], train_loss/perplexity = 5.07727623/160.3367462 secs/batch = 0.6173s, grad.norm=0.58374816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12568: 9 [  625/ 1327], train_loss/perplexity = 5.17407990/176.6340179 secs/batch = 0.6078s, grad.norm=0.61212039\n",
      " 12573: 9 [  630/ 1327], train_loss/perplexity = 5.20980358/183.0581055 secs/batch = 0.6106s, grad.norm=0.60676330\n",
      " 12578: 9 [  635/ 1327], train_loss/perplexity = 4.93886709/139.6119995 secs/batch = 0.6093s, grad.norm=0.63277960\n",
      " 12583: 9 [  640/ 1327], train_loss/perplexity = 5.00153399/148.6409912 secs/batch = 0.6107s, grad.norm=0.57052946\n",
      " 12588: 9 [  645/ 1327], train_loss/perplexity = 5.15964031/174.1018219 secs/batch = 0.6138s, grad.norm=0.62138814\n",
      " 12593: 9 [  650/ 1327], train_loss/perplexity = 4.81001091/122.7329559 secs/batch = 0.6110s, grad.norm=0.62115753\n",
      " 12598: 9 [  655/ 1327], train_loss/perplexity = 4.89324665/133.3859253 secs/batch = 0.6091s, grad.norm=0.63931829\n",
      " 12603: 9 [  660/ 1327], train_loss/perplexity = 4.79786730/121.2515488 secs/batch = 0.6050s, grad.norm=0.56999159\n",
      " 12608: 9 [  665/ 1327], train_loss/perplexity = 4.98321199/145.9423981 secs/batch = 0.6131s, grad.norm=0.56717110\n",
      " 12613: 9 [  670/ 1327], train_loss/perplexity = 4.89241743/133.2753754 secs/batch = 0.6098s, grad.norm=0.58770889\n",
      " 12618: 9 [  675/ 1327], train_loss/perplexity = 4.68452597/108.2589417 secs/batch = 0.6129s, grad.norm=0.65440780\n",
      " 12623: 9 [  680/ 1327], train_loss/perplexity = 4.97616100/144.9169769 secs/batch = 0.6066s, grad.norm=0.60763252\n",
      " 12628: 9 [  685/ 1327], train_loss/perplexity = 4.97431803/144.6501465 secs/batch = 0.6082s, grad.norm=0.61982292\n",
      " 12633: 9 [  690/ 1327], train_loss/perplexity = 5.15441847/173.1950531 secs/batch = 0.6064s, grad.norm=0.56543618\n",
      " 12638: 9 [  695/ 1327], train_loss/perplexity = 4.92970419/138.3385773 secs/batch = 0.6091s, grad.norm=0.57083434\n",
      " 12643: 9 [  700/ 1327], train_loss/perplexity = 5.17124939/176.1347656 secs/batch = 0.6070s, grad.norm=0.60278195\n",
      " 12648: 9 [  705/ 1327], train_loss/perplexity = 4.87034369/130.3657074 secs/batch = 0.6170s, grad.norm=0.61490804\n",
      " 12653: 9 [  710/ 1327], train_loss/perplexity = 4.83015490/125.2303543 secs/batch = 0.6133s, grad.norm=0.62625319\n",
      " 12658: 9 [  715/ 1327], train_loss/perplexity = 4.82069731/124.0515594 secs/batch = 0.6185s, grad.norm=0.57518095\n",
      " 12663: 9 [  720/ 1327], train_loss/perplexity = 4.89388275/133.4708099 secs/batch = 0.6119s, grad.norm=0.59132540\n",
      " 12668: 9 [  725/ 1327], train_loss/perplexity = 4.70668364/110.6844788 secs/batch = 0.6079s, grad.norm=0.58767289\n",
      " 12673: 9 [  730/ 1327], train_loss/perplexity = 4.85651255/128.5750275 secs/batch = 0.6045s, grad.norm=0.61166364\n",
      " 12678: 9 [  735/ 1327], train_loss/perplexity = 5.06194544/157.8973999 secs/batch = 0.6099s, grad.norm=0.59383738\n",
      " 12683: 9 [  740/ 1327], train_loss/perplexity = 4.45368481/85.9430466 secs/batch = 0.6123s, grad.norm=0.56680608\n",
      " 12688: 9 [  745/ 1327], train_loss/perplexity = 4.91644716/136.5167236 secs/batch = 0.6106s, grad.norm=0.57053971\n",
      " 12693: 9 [  750/ 1327], train_loss/perplexity = 4.74221373/114.6878052 secs/batch = 0.6034s, grad.norm=0.58331162\n",
      " 12698: 9 [  755/ 1327], train_loss/perplexity = 4.79271269/120.6281509 secs/batch = 0.6141s, grad.norm=0.59562117\n",
      " 12703: 9 [  760/ 1327], train_loss/perplexity = 4.69940758/109.8820572 secs/batch = 0.6073s, grad.norm=0.60108376\n",
      " 12708: 9 [  765/ 1327], train_loss/perplexity = 4.76683092/117.5461349 secs/batch = 0.6474s, grad.norm=0.59540147\n",
      " 12713: 9 [  770/ 1327], train_loss/perplexity = 4.70136595/110.0974579 secs/batch = 0.6161s, grad.norm=0.68443936\n",
      " 12718: 9 [  775/ 1327], train_loss/perplexity = 4.82268143/124.2979431 secs/batch = 0.6129s, grad.norm=0.59009272\n",
      " 12723: 9 [  780/ 1327], train_loss/perplexity = 5.07979918/160.7417755 secs/batch = 0.6448s, grad.norm=0.61381352\n",
      " 12728: 9 [  785/ 1327], train_loss/perplexity = 4.91975355/136.9688568 secs/batch = 0.6143s, grad.norm=0.63695323\n",
      " 12733: 9 [  790/ 1327], train_loss/perplexity = 4.76534557/117.3716736 secs/batch = 0.6156s, grad.norm=0.61925536\n",
      " 12738: 9 [  795/ 1327], train_loss/perplexity = 5.05070877/156.1330872 secs/batch = 0.6206s, grad.norm=0.58134764\n",
      " 12743: 9 [  800/ 1327], train_loss/perplexity = 5.00526667/149.1968689 secs/batch = 0.6146s, grad.norm=0.59521937\n",
      " 12748: 9 [  805/ 1327], train_loss/perplexity = 5.28234577/196.8310547 secs/batch = 0.6081s, grad.norm=0.59833133\n",
      " 12753: 9 [  810/ 1327], train_loss/perplexity = 4.90922976/135.5349731 secs/batch = 0.6103s, grad.norm=0.60272491\n",
      " 12758: 9 [  815/ 1327], train_loss/perplexity = 4.85688591/128.6230316 secs/batch = 0.6144s, grad.norm=0.61673051\n",
      " 12763: 9 [  820/ 1327], train_loss/perplexity = 4.55069351/94.6980591 secs/batch = 0.5986s, grad.norm=0.55873919\n",
      " 12768: 9 [  825/ 1327], train_loss/perplexity = 4.74472427/114.9760971 secs/batch = 0.6172s, grad.norm=0.58091390\n",
      " 12773: 9 [  830/ 1327], train_loss/perplexity = 4.62402773/101.9036484 secs/batch = 0.6134s, grad.norm=0.60309154\n",
      " 12778: 9 [  835/ 1327], train_loss/perplexity = 4.84865284/127.5684204 secs/batch = 0.6082s, grad.norm=0.63216519\n",
      " 12783: 9 [  840/ 1327], train_loss/perplexity = 4.98939848/146.8480682 secs/batch = 0.6134s, grad.norm=0.57828283\n",
      " 12788: 9 [  845/ 1327], train_loss/perplexity = 4.79477787/120.8775253 secs/batch = 0.6104s, grad.norm=0.59152758\n",
      " 12793: 9 [  850/ 1327], train_loss/perplexity = 4.86667442/129.8882446 secs/batch = 0.6116s, grad.norm=0.57247514\n",
      " 12798: 9 [  855/ 1327], train_loss/perplexity = 4.88804293/132.6936340 secs/batch = 0.6156s, grad.norm=0.66090983\n",
      " 12803: 9 [  860/ 1327], train_loss/perplexity = 4.59822989/99.3083725 secs/batch = 0.6165s, grad.norm=0.58956110\n",
      " 12808: 9 [  865/ 1327], train_loss/perplexity = 5.07069349/159.2847595 secs/batch = 0.6091s, grad.norm=0.61166841\n",
      " 12813: 9 [  870/ 1327], train_loss/perplexity = 4.96848440/143.8087616 secs/batch = 0.6111s, grad.norm=0.61453795\n",
      " 12818: 9 [  875/ 1327], train_loss/perplexity = 4.61183643/100.6688538 secs/batch = 0.6122s, grad.norm=0.61588907\n",
      " 12823: 9 [  880/ 1327], train_loss/perplexity = 4.72320747/112.5286102 secs/batch = 0.6114s, grad.norm=0.57098019\n",
      " 12828: 9 [  885/ 1327], train_loss/perplexity = 4.81057596/122.8023300 secs/batch = 0.6120s, grad.norm=0.61274159\n",
      " 12833: 9 [  890/ 1327], train_loss/perplexity = 5.01779032/151.0771027 secs/batch = 0.6077s, grad.norm=0.59665960\n",
      " 12838: 9 [  895/ 1327], train_loss/perplexity = 5.08088493/160.9163971 secs/batch = 0.6137s, grad.norm=0.58703864\n",
      " 12843: 9 [  900/ 1327], train_loss/perplexity = 4.93563890/139.1620178 secs/batch = 0.6072s, grad.norm=0.58908784\n",
      " 12848: 9 [  905/ 1327], train_loss/perplexity = 4.76130724/116.8986435 secs/batch = 0.6078s, grad.norm=0.60352093\n",
      " 12853: 9 [  910/ 1327], train_loss/perplexity = 4.87646437/131.1660919 secs/batch = 0.6067s, grad.norm=0.60749245\n",
      " 12858: 9 [  915/ 1327], train_loss/perplexity = 5.10325003/164.5558472 secs/batch = 0.6189s, grad.norm=0.61483413\n",
      " 12863: 9 [  920/ 1327], train_loss/perplexity = 5.22672176/186.1814575 secs/batch = 0.6122s, grad.norm=0.65473551\n",
      " 12868: 9 [  925/ 1327], train_loss/perplexity = 4.94488335/140.4544678 secs/batch = 0.6145s, grad.norm=0.64523709\n",
      " 12873: 9 [  930/ 1327], train_loss/perplexity = 4.91628075/136.4940186 secs/batch = 0.6097s, grad.norm=0.61818105\n",
      " 12878: 9 [  935/ 1327], train_loss/perplexity = 4.97730112/145.0822906 secs/batch = 0.6089s, grad.norm=0.58564985\n",
      " 12883: 9 [  940/ 1327], train_loss/perplexity = 4.94426250/140.3672943 secs/batch = 0.6104s, grad.norm=0.60538352\n",
      " 12888: 9 [  945/ 1327], train_loss/perplexity = 5.11510277/166.5178986 secs/batch = 0.5990s, grad.norm=0.59736753\n",
      " 12893: 9 [  950/ 1327], train_loss/perplexity = 4.86683083/129.9085541 secs/batch = 0.6087s, grad.norm=0.58948326\n",
      " 12898: 9 [  955/ 1327], train_loss/perplexity = 5.01371479/150.4626312 secs/batch = 0.6132s, grad.norm=0.59828407\n",
      " 12903: 9 [  960/ 1327], train_loss/perplexity = 5.23096466/186.9730835 secs/batch = 0.6123s, grad.norm=0.58277565\n",
      " 12908: 9 [  965/ 1327], train_loss/perplexity = 5.00941992/149.8178101 secs/batch = 0.6142s, grad.norm=0.60904908\n",
      " 12913: 9 [  970/ 1327], train_loss/perplexity = 5.23125410/187.0272064 secs/batch = 0.6096s, grad.norm=0.57791531\n",
      " 12918: 9 [  975/ 1327], train_loss/perplexity = 4.91860151/136.8111572 secs/batch = 0.6108s, grad.norm=0.61324739\n",
      " 12923: 9 [  980/ 1327], train_loss/perplexity = 4.74217606/114.6834869 secs/batch = 0.6088s, grad.norm=0.58328772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12928: 9 [  985/ 1327], train_loss/perplexity = 4.94331026/140.2336884 secs/batch = 0.6045s, grad.norm=0.60772550\n",
      " 12933: 9 [  990/ 1327], train_loss/perplexity = 5.07529831/160.0199127 secs/batch = 0.6115s, grad.norm=0.62829846\n",
      " 12938: 9 [  995/ 1327], train_loss/perplexity = 5.09346342/162.9532623 secs/batch = 0.6117s, grad.norm=0.60947883\n",
      " 12943: 9 [ 1000/ 1327], train_loss/perplexity = 4.58725405/98.2243423 secs/batch = 0.6077s, grad.norm=0.57348883\n",
      " 12948: 9 [ 1005/ 1327], train_loss/perplexity = 5.05790472/157.2606659 secs/batch = 0.6097s, grad.norm=0.60750461\n",
      " 12953: 9 [ 1010/ 1327], train_loss/perplexity = 4.61378002/100.8647003 secs/batch = 0.6543s, grad.norm=0.57124275\n",
      " 12958: 9 [ 1015/ 1327], train_loss/perplexity = 5.07102633/159.3377686 secs/batch = 0.6068s, grad.norm=0.62148649\n",
      " 12963: 9 [ 1020/ 1327], train_loss/perplexity = 5.23361015/187.4683685 secs/batch = 0.6117s, grad.norm=0.61580390\n",
      " 12968: 9 [ 1025/ 1327], train_loss/perplexity = 5.02771902/152.5845795 secs/batch = 0.6120s, grad.norm=0.59273088\n",
      " 12973: 9 [ 1030/ 1327], train_loss/perplexity = 4.89767075/133.9773560 secs/batch = 0.6103s, grad.norm=0.55858231\n",
      " 12978: 9 [ 1035/ 1327], train_loss/perplexity = 4.75234747/115.8559341 secs/batch = 0.6081s, grad.norm=0.58292711\n",
      " 12983: 9 [ 1040/ 1327], train_loss/perplexity = 5.03538609/153.7589417 secs/batch = 0.6156s, grad.norm=0.59200990\n",
      " 12988: 9 [ 1045/ 1327], train_loss/perplexity = 4.68785095/108.6194992 secs/batch = 0.6094s, grad.norm=0.61559081\n",
      " 12993: 9 [ 1050/ 1327], train_loss/perplexity = 4.68128395/107.9085312 secs/batch = 0.6130s, grad.norm=0.59381825\n",
      " 12998: 9 [ 1055/ 1327], train_loss/perplexity = 4.94396925/140.3261414 secs/batch = 0.6046s, grad.norm=0.60698807\n",
      " 13003: 9 [ 1060/ 1327], train_loss/perplexity = 4.53085852/92.8382263 secs/batch = 0.6204s, grad.norm=0.66092426\n",
      " 13008: 9 [ 1065/ 1327], train_loss/perplexity = 4.64142323/103.6918182 secs/batch = 0.6144s, grad.norm=0.57984942\n",
      " 13013: 9 [ 1070/ 1327], train_loss/perplexity = 4.97654295/144.9723358 secs/batch = 0.6180s, grad.norm=0.59613144\n",
      " 13018: 9 [ 1075/ 1327], train_loss/perplexity = 4.71991014/112.1581726 secs/batch = 0.6070s, grad.norm=0.59379774\n",
      " 13023: 9 [ 1080/ 1327], train_loss/perplexity = 4.65191984/104.7859650 secs/batch = 0.6150s, grad.norm=0.58007228\n",
      " 13028: 9 [ 1085/ 1327], train_loss/perplexity = 4.57055473/96.5976791 secs/batch = 0.6065s, grad.norm=0.60433066\n",
      " 13033: 9 [ 1090/ 1327], train_loss/perplexity = 4.77392912/118.3834686 secs/batch = 0.6086s, grad.norm=0.59223992\n",
      " 13038: 9 [ 1095/ 1327], train_loss/perplexity = 4.86885786/130.1721497 secs/batch = 0.6126s, grad.norm=0.63264209\n",
      " 13043: 9 [ 1100/ 1327], train_loss/perplexity = 4.75799513/116.5121002 secs/batch = 0.6157s, grad.norm=0.62647295\n",
      " 13048: 9 [ 1105/ 1327], train_loss/perplexity = 4.68495703/108.3056183 secs/batch = 0.6116s, grad.norm=0.60906911\n",
      " 13053: 9 [ 1110/ 1327], train_loss/perplexity = 5.19305897/180.0183868 secs/batch = 0.6119s, grad.norm=0.67346758\n",
      " 13058: 9 [ 1115/ 1327], train_loss/perplexity = 4.72552443/112.7896347 secs/batch = 0.6165s, grad.norm=0.58229005\n",
      " 13063: 9 [ 1120/ 1327], train_loss/perplexity = 4.90237284/134.6088104 secs/batch = 0.6110s, grad.norm=0.62922996\n",
      " 13068: 9 [ 1125/ 1327], train_loss/perplexity = 5.15018177/172.4628296 secs/batch = 0.6094s, grad.norm=0.59901625\n",
      " 13073: 9 [ 1130/ 1327], train_loss/perplexity = 4.82747936/124.8957443 secs/batch = 0.6145s, grad.norm=0.59481406\n",
      " 13078: 9 [ 1135/ 1327], train_loss/perplexity = 4.85334110/128.1678925 secs/batch = 0.6149s, grad.norm=0.59771538\n",
      " 13083: 9 [ 1140/ 1327], train_loss/perplexity = 5.07164812/159.4368896 secs/batch = 0.6111s, grad.norm=0.60456747\n",
      " 13088: 9 [ 1145/ 1327], train_loss/perplexity = 4.86479568/129.6444550 secs/batch = 0.6113s, grad.norm=0.62232345\n",
      " 13093: 9 [ 1150/ 1327], train_loss/perplexity = 4.86560297/129.7491455 secs/batch = 0.6183s, grad.norm=0.61237520\n",
      " 13098: 9 [ 1155/ 1327], train_loss/perplexity = 4.95204258/141.4636230 secs/batch = 0.6131s, grad.norm=0.61897093\n",
      " 13103: 9 [ 1160/ 1327], train_loss/perplexity = 4.88491774/132.2795868 secs/batch = 0.6150s, grad.norm=0.61596334\n",
      " 13108: 9 [ 1165/ 1327], train_loss/perplexity = 4.98289490/145.8961182 secs/batch = 0.6110s, grad.norm=0.64209437\n",
      " 13113: 9 [ 1170/ 1327], train_loss/perplexity = 4.82415962/124.4818115 secs/batch = 0.6192s, grad.norm=0.62527424\n",
      " 13118: 9 [ 1175/ 1327], train_loss/perplexity = 4.59070158/98.5635529 secs/batch = 0.6164s, grad.norm=0.65002501\n",
      " 13123: 9 [ 1180/ 1327], train_loss/perplexity = 4.58078527/97.5910034 secs/batch = 0.6069s, grad.norm=0.62386221\n",
      " 13128: 9 [ 1185/ 1327], train_loss/perplexity = 4.80227804/121.7875366 secs/batch = 0.6112s, grad.norm=0.60100979\n",
      " 13133: 9 [ 1190/ 1327], train_loss/perplexity = 4.85197401/127.9927979 secs/batch = 0.6096s, grad.norm=0.59104055\n",
      " 13138: 9 [ 1195/ 1327], train_loss/perplexity = 4.69146395/109.0126495 secs/batch = 0.6113s, grad.norm=0.60071975\n",
      " 13143: 9 [ 1200/ 1327], train_loss/perplexity = 4.62171459/101.6682053 secs/batch = 0.6178s, grad.norm=0.62896585\n",
      " 13148: 9 [ 1205/ 1327], train_loss/perplexity = 4.65598392/105.2126923 secs/batch = 0.6164s, grad.norm=0.63140434\n",
      " 13153: 9 [ 1210/ 1327], train_loss/perplexity = 4.40954781/82.2322693 secs/batch = 0.6156s, grad.norm=0.60641366\n",
      " 13158: 9 [ 1215/ 1327], train_loss/perplexity = 4.58688641/98.1882324 secs/batch = 0.6081s, grad.norm=0.58018804\n",
      " 13163: 9 [ 1220/ 1327], train_loss/perplexity = 4.67006397/106.7045670 secs/batch = 0.6124s, grad.norm=0.60463750\n",
      " 13168: 9 [ 1225/ 1327], train_loss/perplexity = 4.57431173/96.9612808 secs/batch = 0.6158s, grad.norm=0.63788116\n",
      " 13173: 9 [ 1230/ 1327], train_loss/perplexity = 4.69548607/109.4519958 secs/batch = 0.6108s, grad.norm=0.59678531\n",
      " 13178: 9 [ 1235/ 1327], train_loss/perplexity = 4.73449898/113.8064270 secs/batch = 0.6050s, grad.norm=0.60488784\n",
      " 13183: 9 [ 1240/ 1327], train_loss/perplexity = 4.86272860/129.3767395 secs/batch = 0.6049s, grad.norm=0.58431119\n",
      " 13188: 9 [ 1245/ 1327], train_loss/perplexity = 4.77402782/118.3951569 secs/batch = 0.6096s, grad.norm=0.60023701\n",
      " 13193: 9 [ 1250/ 1327], train_loss/perplexity = 4.86619234/129.8256378 secs/batch = 0.6155s, grad.norm=0.56284541\n",
      " 13198: 9 [ 1255/ 1327], train_loss/perplexity = 4.83326483/125.6204224 secs/batch = 0.6460s, grad.norm=0.60448521\n",
      " 13203: 9 [ 1260/ 1327], train_loss/perplexity = 4.79228878/120.5770264 secs/batch = 0.6034s, grad.norm=0.63368112\n",
      " 13208: 9 [ 1265/ 1327], train_loss/perplexity = 4.85798883/128.7649689 secs/batch = 0.6126s, grad.norm=0.64636743\n",
      " 13213: 9 [ 1270/ 1327], train_loss/perplexity = 4.71554375/111.6695175 secs/batch = 0.6061s, grad.norm=0.59307170\n",
      " 13218: 9 [ 1275/ 1327], train_loss/perplexity = 4.84358835/126.9239807 secs/batch = 0.6085s, grad.norm=0.62243617\n",
      " 13223: 9 [ 1280/ 1327], train_loss/perplexity = 4.75171614/115.7828140 secs/batch = 0.6084s, grad.norm=0.64638996\n",
      " 13228: 9 [ 1285/ 1327], train_loss/perplexity = 4.73084879/113.3917694 secs/batch = 0.6081s, grad.norm=0.60010654\n",
      " 13233: 9 [ 1290/ 1327], train_loss/perplexity = 4.88809872/132.7010345 secs/batch = 0.6076s, grad.norm=0.61202854\n",
      " 13238: 9 [ 1295/ 1327], train_loss/perplexity = 4.89157248/133.1628113 secs/batch = 0.6113s, grad.norm=0.57131511\n",
      " 13243: 9 [ 1300/ 1327], train_loss/perplexity = 5.00068092/148.5142517 secs/batch = 0.6060s, grad.norm=0.57232785\n",
      " 13248: 9 [ 1305/ 1327], train_loss/perplexity = 5.11926842/167.2129974 secs/batch = 0.6110s, grad.norm=0.64767236\n",
      " 13253: 9 [ 1310/ 1327], train_loss/perplexity = 5.33593416/207.6666565 secs/batch = 0.6101s, grad.norm=0.60637796\n",
      " 13258: 9 [ 1315/ 1327], train_loss/perplexity = 5.15212631/172.7985229 secs/batch = 0.6140s, grad.norm=0.68252683\n",
      " 13263: 9 [ 1320/ 1327], train_loss/perplexity = 5.15544271/173.3725433 secs/batch = 0.6116s, grad.norm=0.64848840\n",
      " 13268: 9 [ 1325/ 1327], train_loss/perplexity = 5.01414156/150.5268555 secs/batch = 0.6089s, grad.norm=0.60141081\n",
      "Epoch training time: 813.0613415241241\n",
      "Saved char model cv/epoch009_4.9794.model\n",
      " 13275: 10 [    5/ 1327], train_loss/perplexity = 5.19198084/179.8244019 secs/batch = 0.6129s, grad.norm=0.64335960\n",
      " 13280: 10 [   10/ 1327], train_loss/perplexity = 4.70477772/110.4737244 secs/batch = 0.6119s, grad.norm=0.59773088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13285: 10 [   15/ 1327], train_loss/perplexity = 4.90245533/134.6199036 secs/batch = 0.6105s, grad.norm=0.59818399\n",
      " 13290: 10 [   20/ 1327], train_loss/perplexity = 5.10761070/165.2749939 secs/batch = 0.6148s, grad.norm=0.62102312\n",
      " 13295: 10 [   25/ 1327], train_loss/perplexity = 5.02361679/151.9599152 secs/batch = 0.6213s, grad.norm=0.60678667\n",
      " 13300: 10 [   30/ 1327], train_loss/perplexity = 4.95378399/141.7101746 secs/batch = 0.6125s, grad.norm=0.62264276\n",
      " 13305: 10 [   35/ 1327], train_loss/perplexity = 4.78515720/119.7201843 secs/batch = 0.6150s, grad.norm=0.56949705\n",
      " 13310: 10 [   40/ 1327], train_loss/perplexity = 4.86904144/130.1960602 secs/batch = 0.6291s, grad.norm=0.66223502\n",
      " 13315: 10 [   45/ 1327], train_loss/perplexity = 4.55294371/94.9113922 secs/batch = 0.6144s, grad.norm=0.56736618\n",
      " 13320: 10 [   50/ 1327], train_loss/perplexity = 4.86707020/129.9396667 secs/batch = 0.6120s, grad.norm=0.58712733\n",
      " 13325: 10 [   55/ 1327], train_loss/perplexity = 4.81102037/122.8569107 secs/batch = 0.6076s, grad.norm=0.63204008\n",
      " 13330: 10 [   60/ 1327], train_loss/perplexity = 5.07627249/160.1758881 secs/batch = 0.6155s, grad.norm=0.62491632\n",
      " 13335: 10 [   65/ 1327], train_loss/perplexity = 4.60709381/100.1925507 secs/batch = 0.6072s, grad.norm=0.57896572\n",
      " 13340: 10 [   70/ 1327], train_loss/perplexity = 4.51227665/91.1290512 secs/batch = 0.6126s, grad.norm=0.60476559\n",
      " 13345: 10 [   75/ 1327], train_loss/perplexity = 4.47236538/87.5635986 secs/batch = 0.6631s, grad.norm=0.58809298\n",
      " 13350: 10 [   80/ 1327], train_loss/perplexity = 4.83293343/125.5787964 secs/batch = 0.6100s, grad.norm=0.61916679\n",
      " 13355: 10 [   85/ 1327], train_loss/perplexity = 4.83930731/126.3817749 secs/batch = 0.6087s, grad.norm=0.62301308\n",
      " 13360: 10 [   90/ 1327], train_loss/perplexity = 4.85845184/128.8246002 secs/batch = 0.6119s, grad.norm=0.66248810\n",
      " 13365: 10 [   95/ 1327], train_loss/perplexity = 4.69121981/108.9860382 secs/batch = 0.6120s, grad.norm=0.59321010\n",
      " 13370: 10 [  100/ 1327], train_loss/perplexity = 5.04205990/154.7885284 secs/batch = 0.6125s, grad.norm=0.76651227\n",
      " 13375: 10 [  105/ 1327], train_loss/perplexity = 4.94816351/140.9159393 secs/batch = 0.6110s, grad.norm=0.66877681\n",
      " 13380: 10 [  110/ 1327], train_loss/perplexity = 4.75366640/116.0088425 secs/batch = 0.6131s, grad.norm=0.60580617\n",
      " 13385: 10 [  115/ 1327], train_loss/perplexity = 4.62616587/102.1217651 secs/batch = 0.6081s, grad.norm=0.62614816\n",
      " 13390: 10 [  120/ 1327], train_loss/perplexity = 4.80387783/121.9825287 secs/batch = 0.6073s, grad.norm=0.63946873\n",
      " 13395: 10 [  125/ 1327], train_loss/perplexity = 4.89751196/133.9560699 secs/batch = 0.6071s, grad.norm=0.64321202\n",
      " 13400: 10 [  130/ 1327], train_loss/perplexity = 4.78146362/119.2788010 secs/batch = 0.6087s, grad.norm=0.64413780\n",
      " 13405: 10 [  135/ 1327], train_loss/perplexity = 4.76117897/116.8836441 secs/batch = 0.6120s, grad.norm=0.59916085\n",
      " 13410: 10 [  140/ 1327], train_loss/perplexity = 5.08183575/161.0694733 secs/batch = 0.6039s, grad.norm=0.60976762\n",
      " 13415: 10 [  145/ 1327], train_loss/perplexity = 5.02914667/152.8025665 secs/batch = 0.6068s, grad.norm=0.60034168\n",
      " 13420: 10 [  150/ 1327], train_loss/perplexity = 4.93751812/139.4237823 secs/batch = 0.6103s, grad.norm=0.62315923\n",
      " 13425: 10 [  155/ 1327], train_loss/perplexity = 5.19253874/179.9247589 secs/batch = 0.6063s, grad.norm=0.57549369\n",
      " 13430: 10 [  160/ 1327], train_loss/perplexity = 4.84029293/126.5064011 secs/batch = 0.6091s, grad.norm=0.59841079\n",
      " 13435: 10 [  165/ 1327], train_loss/perplexity = 5.09326649/162.9211731 secs/batch = 0.6052s, grad.norm=0.58908272\n",
      " 13440: 10 [  170/ 1327], train_loss/perplexity = 4.85232306/128.0374756 secs/batch = 0.6088s, grad.norm=0.62542498\n",
      " 13445: 10 [  175/ 1327], train_loss/perplexity = 5.07663965/160.2347107 secs/batch = 0.6094s, grad.norm=0.59038198\n",
      " 13450: 10 [  180/ 1327], train_loss/perplexity = 4.99622154/147.8534393 secs/batch = 0.6090s, grad.norm=0.59318763\n",
      " 13455: 10 [  185/ 1327], train_loss/perplexity = 5.18458462/178.4992828 secs/batch = 0.6088s, grad.norm=0.59845728\n",
      " 13460: 10 [  190/ 1327], train_loss/perplexity = 4.71399975/111.4972305 secs/batch = 0.6071s, grad.norm=0.56962776\n",
      " 13465: 10 [  195/ 1327], train_loss/perplexity = 4.97453308/144.6812592 secs/batch = 0.6086s, grad.norm=0.57795835\n",
      " 13470: 10 [  200/ 1327], train_loss/perplexity = 4.88107204/131.7718506 secs/batch = 0.6151s, grad.norm=0.58810389\n",
      " 13475: 10 [  205/ 1327], train_loss/perplexity = 5.00833368/149.6551514 secs/batch = 0.6158s, grad.norm=0.60098344\n",
      " 13480: 10 [  210/ 1327], train_loss/perplexity = 4.92462587/137.6378326 secs/batch = 0.6059s, grad.norm=0.59217328\n",
      " 13485: 10 [  215/ 1327], train_loss/perplexity = 5.06696987/158.6927338 secs/batch = 0.6132s, grad.norm=0.55575198\n",
      " 13490: 10 [  220/ 1327], train_loss/perplexity = 5.01724052/150.9940643 secs/batch = 0.6197s, grad.norm=0.59506375\n",
      " 13495: 10 [  225/ 1327], train_loss/perplexity = 5.16254807/174.6088104 secs/batch = 0.6110s, grad.norm=0.59115630\n",
      " 13500: 10 [  230/ 1327], train_loss/perplexity = 5.02160263/151.6541595 secs/batch = 0.6155s, grad.norm=0.64987677\n",
      " 13505: 10 [  235/ 1327], train_loss/perplexity = 4.89479303/133.5923615 secs/batch = 0.6103s, grad.norm=0.58239645\n",
      " 13510: 10 [  240/ 1327], train_loss/perplexity = 4.71742821/111.8801498 secs/batch = 0.6057s, grad.norm=0.59514731\n",
      " 13515: 10 [  245/ 1327], train_loss/perplexity = 5.02910805/152.7966614 secs/batch = 0.6176s, grad.norm=0.60334814\n",
      " 13520: 10 [  250/ 1327], train_loss/perplexity = 4.74126577/114.5791397 secs/batch = 0.6090s, grad.norm=0.56348032\n",
      " 13525: 10 [  255/ 1327], train_loss/perplexity = 4.77422667/118.4187012 secs/batch = 0.6114s, grad.norm=0.58412212\n",
      " 13530: 10 [  260/ 1327], train_loss/perplexity = 5.11962271/167.2722473 secs/batch = 0.6119s, grad.norm=0.64213705\n",
      " 13535: 10 [  265/ 1327], train_loss/perplexity = 5.13377142/169.6557617 secs/batch = 0.6073s, grad.norm=0.56711960\n",
      " 13540: 10 [  270/ 1327], train_loss/perplexity = 5.19981623/181.2389374 secs/batch = 0.6202s, grad.norm=0.58250457\n",
      " 13545: 10 [  275/ 1327], train_loss/perplexity = 5.27735901/195.8519440 secs/batch = 0.6100s, grad.norm=0.61480618\n",
      " 13550: 10 [  280/ 1327], train_loss/perplexity = 5.00283575/148.8346252 secs/batch = 0.6178s, grad.norm=0.63117391\n",
      " 13555: 10 [  285/ 1327], train_loss/perplexity = 5.22443151/185.7555389 secs/batch = 0.6107s, grad.norm=0.64170200\n",
      " 13560: 10 [  290/ 1327], train_loss/perplexity = 5.05004930/156.0301514 secs/batch = 0.6134s, grad.norm=0.62575930\n",
      " 13565: 10 [  295/ 1327], train_loss/perplexity = 4.79054451/120.3668900 secs/batch = 0.6178s, grad.norm=0.60003990\n",
      " 13570: 10 [  300/ 1327], train_loss/perplexity = 4.35439301/77.8195724 secs/batch = 0.6128s, grad.norm=0.55329716\n",
      " 13575: 10 [  305/ 1327], train_loss/perplexity = 4.83216524/125.4823685 secs/batch = 0.6176s, grad.norm=0.58633941\n",
      " 13580: 10 [  310/ 1327], train_loss/perplexity = 4.87545681/131.0339966 secs/batch = 0.6195s, grad.norm=0.63434017\n",
      " 13585: 10 [  315/ 1327], train_loss/perplexity = 4.55640936/95.2408905 secs/batch = 0.6139s, grad.norm=0.62785119\n",
      " 13590: 10 [  320/ 1327], train_loss/perplexity = 4.52660990/92.4446335 secs/batch = 0.6596s, grad.norm=0.62518907\n",
      " 13595: 10 [  325/ 1327], train_loss/perplexity = 4.45573282/86.1192398 secs/batch = 0.6079s, grad.norm=0.66012514\n",
      " 13600: 10 [  330/ 1327], train_loss/perplexity = 4.91608381/136.4671326 secs/batch = 0.6129s, grad.norm=0.60524851\n",
      " 13605: 10 [  335/ 1327], train_loss/perplexity = 4.28069448/72.2906265 secs/batch = 0.6112s, grad.norm=0.59455985\n",
      " 13610: 10 [  340/ 1327], train_loss/perplexity = 5.03545284/153.7692108 secs/batch = 0.6083s, grad.norm=0.58815008\n",
      " 13615: 10 [  345/ 1327], train_loss/perplexity = 4.91979265/136.9742126 secs/batch = 0.6165s, grad.norm=0.56961149\n",
      " 13620: 10 [  350/ 1327], train_loss/perplexity = 4.96220779/142.9089661 secs/batch = 0.6127s, grad.norm=0.61558789\n",
      " 13625: 10 [  355/ 1327], train_loss/perplexity = 4.99209213/147.2441559 secs/batch = 0.6063s, grad.norm=0.57718122\n",
      " 13630: 10 [  360/ 1327], train_loss/perplexity = 5.13945436/170.6226501 secs/batch = 0.6130s, grad.norm=0.61153644\n",
      " 13635: 10 [  365/ 1327], train_loss/perplexity = 5.02039528/151.4711609 secs/batch = 0.6042s, grad.norm=0.58363593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13640: 10 [  370/ 1327], train_loss/perplexity = 5.07180309/159.4615936 secs/batch = 0.6096s, grad.norm=0.64474791\n",
      " 13645: 10 [  375/ 1327], train_loss/perplexity = 4.44233322/84.9729691 secs/batch = 0.6151s, grad.norm=0.59172398\n",
      " 13650: 10 [  380/ 1327], train_loss/perplexity = 4.70058489/110.0114975 secs/batch = 0.6113s, grad.norm=0.62508076\n",
      " 13655: 10 [  385/ 1327], train_loss/perplexity = 4.85269165/128.0846863 secs/batch = 0.6138s, grad.norm=0.62439382\n",
      " 13660: 10 [  390/ 1327], train_loss/perplexity = 4.88406086/132.1662903 secs/batch = 0.6170s, grad.norm=0.60045153\n",
      " 13665: 10 [  395/ 1327], train_loss/perplexity = 5.11076832/165.7976837 secs/batch = 0.6082s, grad.norm=0.60814369\n",
      " 13670: 10 [  400/ 1327], train_loss/perplexity = 4.82397461/124.4587860 secs/batch = 0.6013s, grad.norm=0.60051227\n",
      " 13675: 10 [  405/ 1327], train_loss/perplexity = 5.17040873/175.9867554 secs/batch = 0.6127s, grad.norm=0.64616412\n",
      " 13680: 10 [  410/ 1327], train_loss/perplexity = 4.83919907/126.3680954 secs/batch = 0.6078s, grad.norm=0.59512657\n",
      " 13685: 10 [  415/ 1327], train_loss/perplexity = 4.70694780/110.7137222 secs/batch = 0.6073s, grad.norm=0.66891152\n",
      " 13690: 10 [  420/ 1327], train_loss/perplexity = 4.60577822/100.0608215 secs/batch = 0.6087s, grad.norm=0.65645289\n",
      " 13695: 10 [  425/ 1327], train_loss/perplexity = 4.85143614/127.9239731 secs/batch = 0.6177s, grad.norm=0.68046886\n",
      " 13700: 10 [  430/ 1327], train_loss/perplexity = 5.03283834/153.3677063 secs/batch = 0.6229s, grad.norm=0.59012932\n",
      " 13705: 10 [  435/ 1327], train_loss/perplexity = 5.05679798/157.0867157 secs/batch = 0.6084s, grad.norm=0.62663543\n",
      " 13710: 10 [  440/ 1327], train_loss/perplexity = 4.74022007/114.4593887 secs/batch = 0.6059s, grad.norm=0.67299229\n",
      " 13715: 10 [  445/ 1327], train_loss/perplexity = 4.91729116/136.6320038 secs/batch = 0.6087s, grad.norm=0.66561174\n",
      " 13720: 10 [  450/ 1327], train_loss/perplexity = 4.79086924/120.4059830 secs/batch = 0.6080s, grad.norm=0.59669846\n",
      " 13725: 10 [  455/ 1327], train_loss/perplexity = 4.66145706/105.7901154 secs/batch = 0.6133s, grad.norm=0.61658728\n",
      " 13730: 10 [  460/ 1327], train_loss/perplexity = 4.83929443/126.3801498 secs/batch = 0.5993s, grad.norm=0.59926814\n",
      " 13735: 10 [  465/ 1327], train_loss/perplexity = 4.66793823/106.4779816 secs/batch = 0.6111s, grad.norm=0.63543057\n",
      " 13740: 10 [  470/ 1327], train_loss/perplexity = 5.15237045/172.8407135 secs/batch = 0.6128s, grad.norm=0.59376550\n",
      " 13745: 10 [  475/ 1327], train_loss/perplexity = 4.76966000/117.8791580 secs/batch = 0.6174s, grad.norm=0.58410245\n",
      " 13750: 10 [  480/ 1327], train_loss/perplexity = 4.84714937/127.3767700 secs/batch = 0.6104s, grad.norm=0.61289847\n",
      " 13755: 10 [  485/ 1327], train_loss/perplexity = 4.81167841/122.9377823 secs/batch = 0.6099s, grad.norm=0.58905226\n",
      " 13760: 10 [  490/ 1327], train_loss/perplexity = 4.73233223/113.5601044 secs/batch = 0.6120s, grad.norm=0.69445926\n",
      " 13765: 10 [  495/ 1327], train_loss/perplexity = 4.70758963/110.7848053 secs/batch = 0.6103s, grad.norm=0.62667376\n",
      " 13770: 10 [  500/ 1327], train_loss/perplexity = 5.03632879/153.9039612 secs/batch = 0.6097s, grad.norm=0.59659386\n",
      " 13775: 10 [  505/ 1327], train_loss/perplexity = 4.88613510/132.4407196 secs/batch = 0.6062s, grad.norm=0.59861618\n",
      " 13780: 10 [  510/ 1327], train_loss/perplexity = 5.27142382/194.6929779 secs/batch = 0.6080s, grad.norm=0.58239532\n",
      " 13785: 10 [  515/ 1327], train_loss/perplexity = 4.95789433/142.2938538 secs/batch = 0.6187s, grad.norm=0.60647464\n",
      " 13790: 10 [  520/ 1327], train_loss/perplexity = 5.12204075/167.6772003 secs/batch = 0.6071s, grad.norm=0.58387733\n",
      " 13795: 10 [  525/ 1327], train_loss/perplexity = 4.70471144/110.4664001 secs/batch = 0.6168s, grad.norm=0.59895581\n",
      " 13800: 10 [  530/ 1327], train_loss/perplexity = 4.82237864/124.2603073 secs/batch = 0.6082s, grad.norm=0.66292155\n",
      " 13805: 10 [  535/ 1327], train_loss/perplexity = 4.90201616/134.5608063 secs/batch = 0.6114s, grad.norm=0.61442751\n",
      " 13810: 10 [  540/ 1327], train_loss/perplexity = 4.94890976/141.0211334 secs/batch = 0.6136s, grad.norm=0.57525182\n",
      " 13815: 10 [  545/ 1327], train_loss/perplexity = 5.05687666/157.0990753 secs/batch = 0.6124s, grad.norm=0.61957705\n",
      " 13820: 10 [  550/ 1327], train_loss/perplexity = 4.96361828/143.1106720 secs/batch = 0.6100s, grad.norm=0.63225365\n",
      " 13825: 10 [  555/ 1327], train_loss/perplexity = 4.79960585/121.4625320 secs/batch = 0.6116s, grad.norm=0.61406690\n",
      " 13830: 10 [  560/ 1327], train_loss/perplexity = 4.90370178/134.7878113 secs/batch = 0.6115s, grad.norm=0.65151137\n",
      " 13835: 10 [  565/ 1327], train_loss/perplexity = 4.85707569/128.6474457 secs/batch = 0.6467s, grad.norm=0.61241555\n",
      " 13840: 10 [  570/ 1327], train_loss/perplexity = 4.77220154/118.1791306 secs/batch = 0.6060s, grad.norm=0.63274264\n",
      " 13845: 10 [  575/ 1327], train_loss/perplexity = 4.60524225/100.0072098 secs/batch = 0.6101s, grad.norm=0.62184095\n",
      " 13850: 10 [  580/ 1327], train_loss/perplexity = 4.99840117/148.1760559 secs/batch = 0.6101s, grad.norm=0.61871386\n",
      " 13855: 10 [  585/ 1327], train_loss/perplexity = 4.62007904/101.5020523 secs/batch = 0.6142s, grad.norm=0.60177124\n",
      " 13860: 10 [  590/ 1327], train_loss/perplexity = 4.95257092/141.5383759 secs/batch = 0.6055s, grad.norm=0.62753761\n",
      " 13865: 10 [  595/ 1327], train_loss/perplexity = 4.84029102/126.5061646 secs/batch = 0.6072s, grad.norm=0.65580517\n",
      " 13870: 10 [  600/ 1327], train_loss/perplexity = 5.11534023/166.5574341 secs/batch = 0.6102s, grad.norm=0.59506589\n",
      " 13875: 10 [  605/ 1327], train_loss/perplexity = 5.03036022/152.9881134 secs/batch = 0.6083s, grad.norm=0.60153979\n",
      " 13880: 10 [  610/ 1327], train_loss/perplexity = 5.13459635/169.7957611 secs/batch = 0.6100s, grad.norm=0.62018651\n",
      " 13885: 10 [  615/ 1327], train_loss/perplexity = 4.61633015/101.1222458 secs/batch = 0.6111s, grad.norm=0.59206861\n",
      " 13890: 10 [  620/ 1327], train_loss/perplexity = 5.00422430/149.0414276 secs/batch = 0.6066s, grad.norm=0.62501013\n",
      " 13895: 10 [  625/ 1327], train_loss/perplexity = 5.14793730/172.0761871 secs/batch = 0.6039s, grad.norm=0.64410472\n",
      " 13900: 10 [  630/ 1327], train_loss/perplexity = 5.14140654/170.9560547 secs/batch = 0.6170s, grad.norm=0.59866035\n",
      " 13905: 10 [  635/ 1327], train_loss/perplexity = 4.85932684/128.9373779 secs/batch = 0.6079s, grad.norm=0.63399315\n",
      " 13910: 10 [  640/ 1327], train_loss/perplexity = 4.95658493/142.1076660 secs/batch = 0.6180s, grad.norm=0.58563620\n",
      " 13915: 10 [  645/ 1327], train_loss/perplexity = 5.03599596/153.8527527 secs/batch = 0.6177s, grad.norm=0.60664588\n",
      " 13920: 10 [  650/ 1327], train_loss/perplexity = 4.72830439/113.1036224 secs/batch = 0.6200s, grad.norm=0.65764344\n",
      " 13925: 10 [  655/ 1327], train_loss/perplexity = 4.87475157/130.9416199 secs/batch = 0.6141s, grad.norm=0.60093898\n",
      " 13930: 10 [  660/ 1327], train_loss/perplexity = 4.74482775/114.9879990 secs/batch = 0.6097s, grad.norm=0.66018343\n",
      " 13935: 10 [  665/ 1327], train_loss/perplexity = 4.93066454/138.4714966 secs/batch = 0.6079s, grad.norm=0.59464568\n",
      " 13940: 10 [  670/ 1327], train_loss/perplexity = 4.80541182/122.1697922 secs/batch = 0.6075s, grad.norm=0.58234876\n",
      " 13945: 10 [  675/ 1327], train_loss/perplexity = 4.65644026/105.2607117 secs/batch = 0.6071s, grad.norm=0.67113173\n",
      " 13950: 10 [  680/ 1327], train_loss/perplexity = 4.87179852/130.5555115 secs/batch = 0.6184s, grad.norm=0.62710667\n",
      " 13955: 10 [  685/ 1327], train_loss/perplexity = 4.82418776/124.4853134 secs/batch = 0.6093s, grad.norm=0.60104841\n",
      " 13960: 10 [  690/ 1327], train_loss/perplexity = 5.02749062/152.5497284 secs/batch = 0.6123s, grad.norm=0.58372891\n",
      " 13965: 10 [  695/ 1327], train_loss/perplexity = 4.85740900/128.6903381 secs/batch = 0.6100s, grad.norm=0.59551752\n",
      " 13970: 10 [  700/ 1327], train_loss/perplexity = 5.11117601/165.8652954 secs/batch = 0.6137s, grad.norm=0.65827864\n",
      " 13975: 10 [  705/ 1327], train_loss/perplexity = 4.84122181/126.6239700 secs/batch = 0.6115s, grad.norm=0.62288904\n",
      " 13980: 10 [  710/ 1327], train_loss/perplexity = 4.74812746/115.3680496 secs/batch = 0.6091s, grad.norm=0.59921163\n",
      " 13985: 10 [  715/ 1327], train_loss/perplexity = 4.73478365/113.8388290 secs/batch = 0.6160s, grad.norm=0.60401219\n",
      " 13990: 10 [  720/ 1327], train_loss/perplexity = 4.81087923/122.8395767 secs/batch = 0.6107s, grad.norm=0.60634983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13995: 10 [  725/ 1327], train_loss/perplexity = 4.64179850/103.7307358 secs/batch = 0.6065s, grad.norm=0.60612029\n",
      " 14000: 10 [  730/ 1327], train_loss/perplexity = 4.81971073/123.9292374 secs/batch = 0.6068s, grad.norm=0.61638653\n",
      " 14005: 10 [  735/ 1327], train_loss/perplexity = 4.97900438/145.3296204 secs/batch = 0.6067s, grad.norm=0.61127937\n",
      " 14010: 10 [  740/ 1327], train_loss/perplexity = 4.34744501/77.2807617 secs/batch = 0.6040s, grad.norm=0.55796576\n",
      " 14015: 10 [  745/ 1327], train_loss/perplexity = 4.83678389/126.0632629 secs/batch = 0.6069s, grad.norm=0.58782178\n",
      " 14020: 10 [  750/ 1327], train_loss/perplexity = 4.65088320/104.6773987 secs/batch = 0.6119s, grad.norm=0.60887545\n",
      " 14025: 10 [  755/ 1327], train_loss/perplexity = 4.72193861/112.3859177 secs/batch = 0.6074s, grad.norm=0.58477628\n",
      " 14030: 10 [  760/ 1327], train_loss/perplexity = 4.59681559/99.1680222 secs/batch = 0.6162s, grad.norm=0.62005121\n",
      " 14035: 10 [  765/ 1327], train_loss/perplexity = 4.71986485/112.1530914 secs/batch = 0.6064s, grad.norm=0.65169555\n",
      " 14040: 10 [  770/ 1327], train_loss/perplexity = 4.60552406/100.0353928 secs/batch = 0.6137s, grad.norm=0.67914265\n",
      " 14045: 10 [  775/ 1327], train_loss/perplexity = 4.71633148/111.7575150 secs/batch = 0.6075s, grad.norm=0.63069791\n",
      " 14050: 10 [  780/ 1327], train_loss/perplexity = 5.01366997/150.4558868 secs/batch = 0.6100s, grad.norm=0.63434565\n",
      " 14055: 10 [  785/ 1327], train_loss/perplexity = 4.83564091/125.9192581 secs/batch = 0.6067s, grad.norm=0.62652218\n",
      " 14060: 10 [  790/ 1327], train_loss/perplexity = 4.69054556/108.9125824 secs/batch = 0.6077s, grad.norm=0.64619207\n",
      " 14065: 10 [  795/ 1327], train_loss/perplexity = 4.99816704/148.1413727 secs/batch = 0.6099s, grad.norm=0.59100014\n",
      " 14070: 10 [  800/ 1327], train_loss/perplexity = 4.93776178/139.4577637 secs/batch = 0.6123s, grad.norm=0.63670504\n",
      " 14075: 10 [  805/ 1327], train_loss/perplexity = 5.21203756/183.4674988 secs/batch = 0.6112s, grad.norm=0.60324299\n",
      " 14080: 10 [  810/ 1327], train_loss/perplexity = 4.83250332/125.5247955 secs/batch = 0.6208s, grad.norm=0.59919363\n",
      " 14085: 10 [  815/ 1327], train_loss/perplexity = 4.79398727/120.7819977 secs/batch = 0.6157s, grad.norm=0.62634414\n",
      " 14090: 10 [  820/ 1327], train_loss/perplexity = 4.48031807/88.2627411 secs/batch = 0.6155s, grad.norm=0.57634699\n",
      " 14095: 10 [  825/ 1327], train_loss/perplexity = 4.69453287/109.3477173 secs/batch = 0.6047s, grad.norm=0.59911507\n",
      " 14100: 10 [  830/ 1327], train_loss/perplexity = 4.55964899/95.5499344 secs/batch = 0.6075s, grad.norm=0.61346251\n",
      " 14105: 10 [  835/ 1327], train_loss/perplexity = 4.78821278/120.0865555 secs/batch = 0.6097s, grad.norm=0.61153436\n",
      " 14110: 10 [  840/ 1327], train_loss/perplexity = 4.92521048/137.7183228 secs/batch = 0.6062s, grad.norm=0.60151798\n",
      " 14115: 10 [  845/ 1327], train_loss/perplexity = 4.72698021/112.9539490 secs/batch = 0.6138s, grad.norm=0.62180817\n",
      " 14120: 10 [  850/ 1327], train_loss/perplexity = 4.81320333/123.1253967 secs/batch = 0.6071s, grad.norm=0.60062605\n",
      " 14125: 10 [  855/ 1327], train_loss/perplexity = 4.80480862/122.0961227 secs/batch = 0.6113s, grad.norm=0.66076332\n",
      " 14130: 10 [  860/ 1327], train_loss/perplexity = 4.51177502/91.0833511 secs/batch = 0.6440s, grad.norm=0.59910417\n",
      " 14135: 10 [  865/ 1327], train_loss/perplexity = 4.98779488/146.6127625 secs/batch = 0.6066s, grad.norm=0.64347374\n",
      " 14140: 10 [  870/ 1327], train_loss/perplexity = 4.90325594/134.7277374 secs/batch = 0.6107s, grad.norm=0.61367935\n",
      " 14145: 10 [  875/ 1327], train_loss/perplexity = 4.52333069/92.1419830 secs/batch = 0.6095s, grad.norm=0.64494264\n",
      " 14150: 10 [  880/ 1327], train_loss/perplexity = 4.67600203/107.3400726 secs/batch = 0.6137s, grad.norm=0.58281475\n",
      " 14155: 10 [  885/ 1327], train_loss/perplexity = 4.76926899/117.8330765 secs/batch = 0.6073s, grad.norm=0.62823457\n",
      " 14160: 10 [  890/ 1327], train_loss/perplexity = 4.93362427/138.8819427 secs/batch = 0.6085s, grad.norm=0.60093117\n",
      " 14165: 10 [  895/ 1327], train_loss/perplexity = 4.97061205/144.1150665 secs/batch = 0.6127s, grad.norm=0.58528411\n",
      " 14170: 10 [  900/ 1327], train_loss/perplexity = 4.88292027/132.0156250 secs/batch = 0.6137s, grad.norm=0.61733443\n",
      " 14175: 10 [  905/ 1327], train_loss/perplexity = 4.68980312/108.8317490 secs/batch = 0.6095s, grad.norm=0.60456145\n",
      " 14180: 10 [  910/ 1327], train_loss/perplexity = 4.81562328/123.4237137 secs/batch = 0.6108s, grad.norm=0.61975908\n",
      " 14185: 10 [  915/ 1327], train_loss/perplexity = 5.03541231/153.7629852 secs/batch = 0.6110s, grad.norm=0.62218881\n",
      " 14190: 10 [  920/ 1327], train_loss/perplexity = 5.14633036/171.7998810 secs/batch = 0.6129s, grad.norm=0.64495420\n",
      " 14195: 10 [  925/ 1327], train_loss/perplexity = 4.89302540/133.3564148 secs/batch = 0.6124s, grad.norm=0.65783310\n",
      " 14200: 10 [  930/ 1327], train_loss/perplexity = 4.86098337/129.1511383 secs/batch = 0.6075s, grad.norm=0.60428637\n",
      " 14205: 10 [  935/ 1327], train_loss/perplexity = 4.97045040/144.0917664 secs/batch = 0.6113s, grad.norm=0.60331845\n",
      " 14210: 10 [  940/ 1327], train_loss/perplexity = 4.89143658/133.1447144 secs/batch = 0.6078s, grad.norm=0.58740258\n",
      " 14215: 10 [  945/ 1327], train_loss/perplexity = 5.05175209/156.2960663 secs/batch = 0.6145s, grad.norm=0.60181332\n",
      " 14220: 10 [  950/ 1327], train_loss/perplexity = 4.81904554/123.8468246 secs/batch = 0.6061s, grad.norm=0.61528689\n",
      " 14225: 10 [  955/ 1327], train_loss/perplexity = 4.92579699/137.7991180 secs/batch = 0.6124s, grad.norm=0.60579139\n",
      " 14230: 10 [  960/ 1327], train_loss/perplexity = 5.16241932/174.5863190 secs/batch = 0.6079s, grad.norm=0.57578081\n",
      " 14235: 10 [  965/ 1327], train_loss/perplexity = 4.91576433/136.4235382 secs/batch = 0.6099s, grad.norm=0.59175694\n",
      " 14240: 10 [  970/ 1327], train_loss/perplexity = 5.14440870/171.4700623 secs/batch = 0.6181s, grad.norm=0.60299224\n",
      " 14245: 10 [  975/ 1327], train_loss/perplexity = 4.90290117/134.6799469 secs/batch = 0.6148s, grad.norm=0.62009948\n",
      " 14250: 10 [  980/ 1327], train_loss/perplexity = 4.65717793/105.3383865 secs/batch = 0.6166s, grad.norm=0.58099121\n",
      " 14255: 10 [  985/ 1327], train_loss/perplexity = 4.86903286/130.1949463 secs/batch = 0.6087s, grad.norm=0.62235534\n",
      " 14260: 10 [  990/ 1327], train_loss/perplexity = 4.97086620/144.1517029 secs/batch = 0.6108s, grad.norm=0.61003649\n",
      " 14265: 10 [  995/ 1327], train_loss/perplexity = 5.03710175/154.0229645 secs/batch = 0.6137s, grad.norm=0.60388690\n",
      " 14270: 10 [ 1000/ 1327], train_loss/perplexity = 4.51831198/91.6807098 secs/batch = 0.6078s, grad.norm=0.58342659\n",
      " 14275: 10 [ 1005/ 1327], train_loss/perplexity = 5.00288010/148.8412170 secs/batch = 0.6113s, grad.norm=0.61650473\n",
      " 14280: 10 [ 1010/ 1327], train_loss/perplexity = 4.56074715/95.6549225 secs/batch = 0.6145s, grad.norm=0.58646601\n",
      " 14285: 10 [ 1015/ 1327], train_loss/perplexity = 5.03928518/154.3596344 secs/batch = 0.6158s, grad.norm=0.62434977\n",
      " 14290: 10 [ 1020/ 1327], train_loss/perplexity = 5.21398687/183.8254852 secs/batch = 0.6712s, grad.norm=0.63172448\n",
      " 14295: 10 [ 1025/ 1327], train_loss/perplexity = 4.98631716/146.3962708 secs/batch = 0.6097s, grad.norm=0.57364106\n",
      " 14300: 10 [ 1030/ 1327], train_loss/perplexity = 4.78612041/119.8355560 secs/batch = 0.6086s, grad.norm=0.57852781\n",
      " 14305: 10 [ 1035/ 1327], train_loss/perplexity = 4.70362949/110.3469467 secs/batch = 0.6180s, grad.norm=0.62189448\n",
      " 14310: 10 [ 1040/ 1327], train_loss/perplexity = 4.97541666/144.8091431 secs/batch = 0.6170s, grad.norm=0.57509178\n",
      " 14315: 10 [ 1045/ 1327], train_loss/perplexity = 4.62430573/101.9319763 secs/batch = 0.6117s, grad.norm=0.63432103\n",
      " 14320: 10 [ 1050/ 1327], train_loss/perplexity = 4.63063049/102.5787201 secs/batch = 0.6131s, grad.norm=0.61598051\n",
      " 14325: 10 [ 1055/ 1327], train_loss/perplexity = 4.88462448/132.2407990 secs/batch = 0.6168s, grad.norm=0.64018440\n",
      " 14330: 10 [ 1060/ 1327], train_loss/perplexity = 4.44697380/85.3682098 secs/batch = 0.6064s, grad.norm=0.64318204\n",
      " 14335: 10 [ 1065/ 1327], train_loss/perplexity = 4.58665371/98.1653900 secs/batch = 0.6189s, grad.norm=0.60125577\n",
      " 14340: 10 [ 1070/ 1327], train_loss/perplexity = 4.88554955/132.3631897 secs/batch = 0.6110s, grad.norm=0.60640943\n",
      " 14345: 10 [ 1075/ 1327], train_loss/perplexity = 4.66497755/106.1632004 secs/batch = 0.6078s, grad.norm=0.62617946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14350: 10 [ 1080/ 1327], train_loss/perplexity = 4.58736801/98.2355347 secs/batch = 0.6254s, grad.norm=0.59333616\n",
      " 14355: 10 [ 1085/ 1327], train_loss/perplexity = 4.48811436/88.9535522 secs/batch = 0.6112s, grad.norm=0.59601444\n",
      " 14360: 10 [ 1090/ 1327], train_loss/perplexity = 4.69106293/108.9689407 secs/batch = 0.6116s, grad.norm=0.59837854\n",
      " 14365: 10 [ 1095/ 1327], train_loss/perplexity = 4.78299713/119.4618607 secs/batch = 0.6142s, grad.norm=0.61775488\n",
      " 14370: 10 [ 1100/ 1327], train_loss/perplexity = 4.68459368/108.2662735 secs/batch = 0.6096s, grad.norm=0.66907078\n",
      " 14375: 10 [ 1105/ 1327], train_loss/perplexity = 4.59236145/98.7272949 secs/batch = 0.6506s, grad.norm=0.61430413\n",
      " 14380: 10 [ 1110/ 1327], train_loss/perplexity = 5.06309938/158.0797119 secs/batch = 0.6150s, grad.norm=0.69983685\n",
      " 14385: 10 [ 1115/ 1327], train_loss/perplexity = 4.65697145/105.3166428 secs/batch = 0.6198s, grad.norm=0.60442930\n",
      " 14390: 10 [ 1120/ 1327], train_loss/perplexity = 4.83643675/126.0195084 secs/batch = 0.6092s, grad.norm=0.62941742\n",
      " 14395: 10 [ 1125/ 1327], train_loss/perplexity = 5.09684372/163.5050201 secs/batch = 0.6122s, grad.norm=0.61748403\n",
      " 14400: 10 [ 1130/ 1327], train_loss/perplexity = 4.75182152/115.7950134 secs/batch = 0.6028s, grad.norm=0.60334188\n",
      " 14405: 10 [ 1135/ 1327], train_loss/perplexity = 4.77610922/118.6418381 secs/batch = 0.6080s, grad.norm=0.56446832\n",
      " 14410: 10 [ 1140/ 1327], train_loss/perplexity = 4.97886324/145.3091125 secs/batch = 0.6131s, grad.norm=0.59977764\n",
      " 14415: 10 [ 1145/ 1327], train_loss/perplexity = 4.79097700/120.4189606 secs/batch = 0.6156s, grad.norm=0.63344377\n",
      " 14420: 10 [ 1150/ 1327], train_loss/perplexity = 4.77356625/118.3405228 secs/batch = 0.6128s, grad.norm=0.61190093\n",
      " 14425: 10 [ 1155/ 1327], train_loss/perplexity = 4.86905050/130.1972351 secs/batch = 0.6126s, grad.norm=0.62586021\n",
      " 14430: 10 [ 1160/ 1327], train_loss/perplexity = 4.84266996/126.8074722 secs/batch = 0.6108s, grad.norm=0.62411606\n",
      " 14435: 10 [ 1165/ 1327], train_loss/perplexity = 4.89594412/133.7462158 secs/batch = 0.6072s, grad.norm=0.61245459\n",
      " 14440: 10 [ 1170/ 1327], train_loss/perplexity = 4.74447155/114.9470444 secs/batch = 0.6052s, grad.norm=0.59284818\n",
      " 14445: 10 [ 1175/ 1327], train_loss/perplexity = 4.54942560/94.5780640 secs/batch = 0.6076s, grad.norm=0.65200973\n",
      " 14450: 10 [ 1180/ 1327], train_loss/perplexity = 4.49311018/89.3990631 secs/batch = 0.6049s, grad.norm=0.64488542\n",
      " 14455: 10 [ 1185/ 1327], train_loss/perplexity = 4.71831322/111.9792099 secs/batch = 0.6160s, grad.norm=0.59651220\n",
      " 14460: 10 [ 1190/ 1327], train_loss/perplexity = 4.79877138/121.3612213 secs/batch = 0.6163s, grad.norm=0.60919845\n",
      " 14465: 10 [ 1195/ 1327], train_loss/perplexity = 4.65062141/104.6499939 secs/batch = 0.6128s, grad.norm=0.65705943\n",
      " 14470: 10 [ 1200/ 1327], train_loss/perplexity = 4.57146263/96.6854248 secs/batch = 0.6121s, grad.norm=0.65310109\n",
      " 14475: 10 [ 1205/ 1327], train_loss/perplexity = 4.57322979/96.8564301 secs/batch = 0.6148s, grad.norm=0.63096482\n",
      " 14480: 10 [ 1210/ 1327], train_loss/perplexity = 4.32887077/75.8585739 secs/batch = 0.6106s, grad.norm=0.61371738\n",
      " 14485: 10 [ 1215/ 1327], train_loss/perplexity = 4.48861647/88.9982300 secs/batch = 0.6132s, grad.norm=0.58029926\n",
      " 14490: 10 [ 1220/ 1327], train_loss/perplexity = 4.59917402/99.4021759 secs/batch = 0.6053s, grad.norm=0.62279123\n",
      " 14495: 10 [ 1225/ 1327], train_loss/perplexity = 4.48234367/88.4417114 secs/batch = 0.6109s, grad.norm=0.64612639\n",
      " 14500: 10 [ 1230/ 1327], train_loss/perplexity = 4.63955498/103.4982758 secs/batch = 0.6066s, grad.norm=0.60264385\n",
      " 14505: 10 [ 1235/ 1327], train_loss/perplexity = 4.65526390/105.1369629 secs/batch = 0.6076s, grad.norm=0.59546065\n",
      " 14510: 10 [ 1240/ 1327], train_loss/perplexity = 4.79965067/121.4679794 secs/batch = 0.6099s, grad.norm=0.60292101\n",
      " 14515: 10 [ 1245/ 1327], train_loss/perplexity = 4.68174505/107.9582977 secs/batch = 0.6135s, grad.norm=0.58581430\n",
      " 14520: 10 [ 1250/ 1327], train_loss/perplexity = 4.82791185/124.9497757 secs/batch = 0.6132s, grad.norm=0.58811188\n",
      " 14525: 10 [ 1255/ 1327], train_loss/perplexity = 4.81057501/122.8022079 secs/batch = 0.6100s, grad.norm=0.58734858\n",
      " 14530: 10 [ 1260/ 1327], train_loss/perplexity = 4.70297050/110.2742538 secs/batch = 0.6097s, grad.norm=0.61033827\n",
      " 14535: 10 [ 1265/ 1327], train_loss/perplexity = 4.79856920/121.3366852 secs/batch = 0.6096s, grad.norm=0.66674644\n",
      " 14540: 10 [ 1270/ 1327], train_loss/perplexity = 4.62695646/102.2025299 secs/batch = 0.6158s, grad.norm=0.61196196\n",
      " 14545: 10 [ 1275/ 1327], train_loss/perplexity = 4.82016182/123.9851532 secs/batch = 0.6122s, grad.norm=0.62624449\n",
      " 14550: 10 [ 1280/ 1327], train_loss/perplexity = 4.65669107/105.2871170 secs/batch = 0.6109s, grad.norm=0.63112432\n",
      " 14555: 10 [ 1285/ 1327], train_loss/perplexity = 4.63807297/103.3450089 secs/batch = 0.6090s, grad.norm=0.62680650\n",
      " 14560: 10 [ 1290/ 1327], train_loss/perplexity = 4.80265856/121.8338928 secs/batch = 0.6092s, grad.norm=0.59619355\n",
      " 14565: 10 [ 1295/ 1327], train_loss/perplexity = 4.77029324/117.9538269 secs/batch = 0.6055s, grad.norm=0.57453322\n",
      " 14570: 10 [ 1300/ 1327], train_loss/perplexity = 4.95812798/142.3271027 secs/batch = 0.6194s, grad.norm=0.63443482\n",
      " 14575: 10 [ 1305/ 1327], train_loss/perplexity = 5.07686424/160.2706909 secs/batch = 0.6108s, grad.norm=0.65789831\n",
      " 14580: 10 [ 1310/ 1327], train_loss/perplexity = 5.28960705/198.2655029 secs/batch = 0.6048s, grad.norm=0.58577436\n",
      " 14585: 10 [ 1315/ 1327], train_loss/perplexity = 5.13009119/169.0325317 secs/batch = 0.6099s, grad.norm=0.63955450\n",
      " 14590: 10 [ 1320/ 1327], train_loss/perplexity = 5.03725195/154.0461121 secs/batch = 0.6140s, grad.norm=0.59066135\n",
      " 14595: 10 [ 1325/ 1327], train_loss/perplexity = 4.96181726/142.8531647 secs/batch = 0.6076s, grad.norm=0.58798093\n",
      "Epoch training time: 812.761048078537\n",
      "Saved char model cv/epoch010_4.9116.model\n",
      " 14602: 11 [    5/ 1327], train_loss/perplexity = 5.08772659/162.0211029 secs/batch = 0.6096s, grad.norm=0.61401117\n",
      " 14607: 11 [   10/ 1327], train_loss/perplexity = 4.64497328/104.0605850 secs/batch = 0.6095s, grad.norm=0.59025687\n",
      " 14612: 11 [   15/ 1327], train_loss/perplexity = 4.82314730/124.3558578 secs/batch = 0.6154s, grad.norm=0.63185531\n",
      " 14617: 11 [   20/ 1327], train_loss/perplexity = 5.05940914/157.4974365 secs/batch = 0.6132s, grad.norm=0.62912774\n",
      " 14622: 11 [   25/ 1327], train_loss/perplexity = 4.94158220/139.9915619 secs/batch = 0.6167s, grad.norm=0.61238098\n",
      " 14627: 11 [   30/ 1327], train_loss/perplexity = 4.89055824/133.0278168 secs/batch = 0.6086s, grad.norm=0.61104381\n",
      " 14632: 11 [   35/ 1327], train_loss/perplexity = 4.68916273/108.7620773 secs/batch = 0.6115s, grad.norm=0.56430924\n",
      " 14637: 11 [   40/ 1327], train_loss/perplexity = 4.82521009/124.6126480 secs/batch = 0.6154s, grad.norm=0.64616024\n",
      " 14642: 11 [   45/ 1327], train_loss/perplexity = 4.50501204/90.4694366 secs/batch = 0.6159s, grad.norm=0.59418046\n",
      " 14647: 11 [   50/ 1327], train_loss/perplexity = 4.77168369/118.1179504 secs/batch = 0.6016s, grad.norm=0.61541641\n",
      " 14652: 11 [   55/ 1327], train_loss/perplexity = 4.71808243/111.9533691 secs/batch = 0.6117s, grad.norm=0.62742448\n",
      " 14657: 11 [   60/ 1327], train_loss/perplexity = 4.98633671/146.3991394 secs/batch = 0.6043s, grad.norm=0.64099991\n",
      " 14662: 11 [   65/ 1327], train_loss/perplexity = 4.54415989/94.0813522 secs/batch = 0.6082s, grad.norm=0.59535325\n",
      " 14667: 11 [   70/ 1327], train_loss/perplexity = 4.42907000/83.8533936 secs/batch = 0.6149s, grad.norm=0.60373795\n",
      " 14672: 11 [   75/ 1327], train_loss/perplexity = 4.38052273/79.8797760 secs/batch = 0.6133s, grad.norm=0.60012329\n",
      " 14677: 11 [   80/ 1327], train_loss/perplexity = 4.71474695/111.5805740 secs/batch = 0.6123s, grad.norm=0.61742592\n",
      " 14682: 11 [   85/ 1327], train_loss/perplexity = 4.78733635/119.9813538 secs/batch = 0.6316s, grad.norm=0.61630380\n",
      " 14687: 11 [   90/ 1327], train_loss/perplexity = 4.78296995/119.4586105 secs/batch = 0.6146s, grad.norm=0.63282830\n",
      " 14692: 11 [   95/ 1327], train_loss/perplexity = 4.64651537/104.2211761 secs/batch = 0.6106s, grad.norm=0.60683829\n",
      " 14697: 11 [  100/ 1327], train_loss/perplexity = 4.90570736/135.0584106 secs/batch = 0.6159s, grad.norm=0.62379497\n",
      " 14702: 11 [  105/ 1327], train_loss/perplexity = 4.88835716/132.7353363 secs/batch = 0.6057s, grad.norm=0.68183994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14707: 11 [  110/ 1327], train_loss/perplexity = 4.65098143/104.6876755 secs/batch = 0.6176s, grad.norm=0.62435597\n",
      " 14712: 11 [  115/ 1327], train_loss/perplexity = 4.54239559/93.9155121 secs/batch = 0.6099s, grad.norm=0.61616939\n",
      " 14717: 11 [  120/ 1327], train_loss/perplexity = 4.72952843/113.2421494 secs/batch = 0.6147s, grad.norm=0.65804690\n",
      " 14722: 11 [  125/ 1327], train_loss/perplexity = 4.82128572/124.1245804 secs/batch = 0.6079s, grad.norm=0.66444463\n",
      " 14727: 11 [  130/ 1327], train_loss/perplexity = 4.69085217/108.9459839 secs/batch = 0.6144s, grad.norm=0.67137039\n",
      " 14732: 11 [  135/ 1327], train_loss/perplexity = 4.69557953/109.4622269 secs/batch = 0.6148s, grad.norm=0.62757367\n",
      " 14737: 11 [  140/ 1327], train_loss/perplexity = 5.00304794/148.8662109 secs/batch = 0.6150s, grad.norm=0.60306519\n",
      " 14742: 11 [  145/ 1327], train_loss/perplexity = 4.98298073/145.9086456 secs/batch = 0.6147s, grad.norm=0.61034495\n",
      " 14747: 11 [  150/ 1327], train_loss/perplexity = 4.88879061/132.7928772 secs/batch = 0.6132s, grad.norm=0.66860378\n",
      " 14752: 11 [  155/ 1327], train_loss/perplexity = 5.14956379/172.3562927 secs/batch = 0.6108s, grad.norm=0.58685893\n",
      " 14757: 11 [  160/ 1327], train_loss/perplexity = 4.78836966/120.1053925 secs/batch = 0.6048s, grad.norm=0.59462869\n",
      " 14762: 11 [  165/ 1327], train_loss/perplexity = 5.02415943/152.0424042 secs/batch = 0.6053s, grad.norm=0.60948151\n",
      " 14767: 11 [  170/ 1327], train_loss/perplexity = 4.75393248/116.0397110 secs/batch = 0.6525s, grad.norm=0.62387085\n",
      " 14772: 11 [  175/ 1327], train_loss/perplexity = 5.03526163/153.7398071 secs/batch = 0.6093s, grad.norm=0.60444474\n",
      " 14777: 11 [  180/ 1327], train_loss/perplexity = 4.88712454/132.5718231 secs/batch = 0.6077s, grad.norm=0.59879810\n",
      " 14782: 11 [  185/ 1327], train_loss/perplexity = 5.14020348/170.7505035 secs/batch = 0.6072s, grad.norm=0.59840143\n",
      " 14787: 11 [  190/ 1327], train_loss/perplexity = 4.63288736/102.8104858 secs/batch = 0.6106s, grad.norm=0.56573802\n",
      " 14792: 11 [  195/ 1327], train_loss/perplexity = 4.90685320/135.2132568 secs/batch = 0.6152s, grad.norm=0.59183985\n",
      " 14797: 11 [  200/ 1327], train_loss/perplexity = 4.85682392/128.6150665 secs/batch = 0.6134s, grad.norm=0.62315112\n",
      " 14802: 11 [  205/ 1327], train_loss/perplexity = 4.93025732/138.4151306 secs/batch = 0.6087s, grad.norm=0.60703105\n",
      " 14807: 11 [  210/ 1327], train_loss/perplexity = 4.90641165/135.1535645 secs/batch = 0.6118s, grad.norm=0.59799325\n",
      " 14812: 11 [  215/ 1327], train_loss/perplexity = 4.94125795/139.9461823 secs/batch = 0.6150s, grad.norm=0.58749378\n",
      " 14817: 11 [  220/ 1327], train_loss/perplexity = 4.98365355/146.0068512 secs/batch = 0.6074s, grad.norm=0.62379670\n",
      " 14822: 11 [  225/ 1327], train_loss/perplexity = 5.08914709/162.2514191 secs/batch = 0.6069s, grad.norm=0.58767855\n",
      " 14827: 11 [  230/ 1327], train_loss/perplexity = 4.92201996/137.2796326 secs/batch = 0.6001s, grad.norm=0.65649164\n",
      " 14832: 11 [  235/ 1327], train_loss/perplexity = 4.81969166/123.9268723 secs/batch = 0.6091s, grad.norm=0.60535228\n",
      " 14837: 11 [  240/ 1327], train_loss/perplexity = 4.66815567/106.5011368 secs/batch = 0.6082s, grad.norm=0.62695688\n",
      " 14842: 11 [  245/ 1327], train_loss/perplexity = 4.99038267/146.9926605 secs/batch = 0.6151s, grad.norm=0.60778952\n",
      " 14847: 11 [  250/ 1327], train_loss/perplexity = 4.64750195/104.3240509 secs/batch = 0.6123s, grad.norm=0.58973253\n",
      " 14852: 11 [  255/ 1327], train_loss/perplexity = 4.74456120/114.9573517 secs/batch = 0.6122s, grad.norm=0.59635133\n",
      " 14857: 11 [  260/ 1327], train_loss/perplexity = 5.02799129/152.6261292 secs/batch = 0.6188s, grad.norm=0.66315639\n",
      " 14862: 11 [  265/ 1327], train_loss/perplexity = 5.11400700/166.3355255 secs/batch = 0.6071s, grad.norm=0.59445447\n",
      " 14867: 11 [  270/ 1327], train_loss/perplexity = 5.12323618/167.8777771 secs/batch = 0.6100s, grad.norm=0.59366918\n",
      " 14872: 11 [  275/ 1327], train_loss/perplexity = 5.18461752/178.5051575 secs/batch = 0.6076s, grad.norm=0.63342971\n",
      " 14877: 11 [  280/ 1327], train_loss/perplexity = 4.90054703/134.3632660 secs/batch = 0.6339s, grad.norm=0.59822911\n",
      " 14882: 11 [  285/ 1327], train_loss/perplexity = 5.11963987/167.2751160 secs/batch = 0.6154s, grad.norm=0.63822979\n",
      " 14887: 11 [  290/ 1327], train_loss/perplexity = 5.02426386/152.0582733 secs/batch = 0.6137s, grad.norm=0.67352980\n",
      " 14892: 11 [  295/ 1327], train_loss/perplexity = 4.74173880/114.6333542 secs/batch = 0.6104s, grad.norm=0.60595250\n",
      " 14897: 11 [  300/ 1327], train_loss/perplexity = 4.32669592/75.6937714 secs/batch = 0.6108s, grad.norm=0.57575464\n",
      " 14902: 11 [  305/ 1327], train_loss/perplexity = 4.75826502/116.5435486 secs/batch = 0.6169s, grad.norm=0.61468065\n",
      " 14907: 11 [  310/ 1327], train_loss/perplexity = 4.82435322/124.5059128 secs/batch = 0.6114s, grad.norm=0.63841462\n",
      " 14912: 11 [  315/ 1327], train_loss/perplexity = 4.49175692/89.2781601 secs/batch = 0.6108s, grad.norm=0.66727811\n",
      " 14917: 11 [  320/ 1327], train_loss/perplexity = 4.46107578/86.5806046 secs/batch = 0.6123s, grad.norm=0.64454061\n",
      " 14922: 11 [  325/ 1327], train_loss/perplexity = 4.38771152/80.4560852 secs/batch = 0.6101s, grad.norm=0.65418142\n",
      " 14927: 11 [  330/ 1327], train_loss/perplexity = 4.81051397/122.7947159 secs/batch = 0.6095s, grad.norm=0.61309445\n",
      " 14932: 11 [  335/ 1327], train_loss/perplexity = 4.18963671/65.9988098 secs/batch = 0.6057s, grad.norm=0.58937764\n",
      " 14937: 11 [  340/ 1327], train_loss/perplexity = 4.99529076/147.7158813 secs/batch = 0.6184s, grad.norm=0.60977477\n",
      " 14942: 11 [  345/ 1327], train_loss/perplexity = 4.86397934/129.5386505 secs/batch = 0.6073s, grad.norm=0.57504791\n",
      " 14947: 11 [  350/ 1327], train_loss/perplexity = 4.92692947/137.9552612 secs/batch = 0.6122s, grad.norm=0.61766821\n",
      " 14952: 11 [  355/ 1327], train_loss/perplexity = 4.92333508/137.4602966 secs/batch = 0.6126s, grad.norm=0.59178752\n",
      " 14957: 11 [  360/ 1327], train_loss/perplexity = 5.03963852/154.4141846 secs/batch = 0.6089s, grad.norm=0.60299623\n",
      " 14962: 11 [  365/ 1327], train_loss/perplexity = 4.95348501/141.6678162 secs/batch = 0.6141s, grad.norm=0.57170588\n",
      " 14967: 11 [  370/ 1327], train_loss/perplexity = 4.97208500/144.3274994 secs/batch = 0.6121s, grad.norm=0.64253676\n",
      " 14972: 11 [  375/ 1327], train_loss/perplexity = 4.33781242/76.5399170 secs/batch = 0.6074s, grad.norm=0.59033453\n",
      " 14977: 11 [  380/ 1327], train_loss/perplexity = 4.57879448/97.3969116 secs/batch = 0.6055s, grad.norm=0.63710153\n",
      " 14982: 11 [  385/ 1327], train_loss/perplexity = 4.78461933/119.6558075 secs/batch = 0.6121s, grad.norm=0.61879897\n",
      " 14987: 11 [  390/ 1327], train_loss/perplexity = 4.81258774/123.0496292 secs/batch = 0.6174s, grad.norm=0.60551345\n",
      " 14992: 11 [  395/ 1327], train_loss/perplexity = 5.03553200/153.7813873 secs/batch = 0.6093s, grad.norm=0.61785358\n",
      " 14997: 11 [  400/ 1327], train_loss/perplexity = 4.77280331/118.2502670 secs/batch = 0.6013s, grad.norm=0.61116737\n",
      " 15002: 11 [  405/ 1327], train_loss/perplexity = 5.13934231/170.6035309 secs/batch = 0.6077s, grad.norm=0.65711427\n",
      " 15007: 11 [  410/ 1327], train_loss/perplexity = 4.80974674/122.7005386 secs/batch = 0.6059s, grad.norm=0.61541450\n",
      " 15012: 11 [  415/ 1327], train_loss/perplexity = 4.64327240/103.8837433 secs/batch = 0.6212s, grad.norm=0.68668628\n",
      " 15017: 11 [  420/ 1327], train_loss/perplexity = 4.51676512/91.5390015 secs/batch = 0.6115s, grad.norm=0.60443413\n",
      " 15022: 11 [  425/ 1327], train_loss/perplexity = 4.73889494/114.3078156 secs/batch = 0.6157s, grad.norm=0.60933065\n",
      " 15027: 11 [  430/ 1327], train_loss/perplexity = 4.94394732/140.3230591 secs/batch = 0.6100s, grad.norm=0.62849355\n",
      " 15032: 11 [  435/ 1327], train_loss/perplexity = 4.98355722/145.9927826 secs/batch = 0.6109s, grad.norm=0.63325638\n",
      " 15037: 11 [  440/ 1327], train_loss/perplexity = 4.63914394/103.4557495 secs/batch = 0.6109s, grad.norm=0.66075665\n",
      " 15042: 11 [  445/ 1327], train_loss/perplexity = 4.87196207/130.5768738 secs/batch = 0.6076s, grad.norm=0.65259558\n",
      " 15047: 11 [  450/ 1327], train_loss/perplexity = 4.74760342/115.3076096 secs/batch = 0.6158s, grad.norm=0.62075907\n",
      " 15052: 11 [  455/ 1327], train_loss/perplexity = 4.64554214/104.1197968 secs/batch = 0.6012s, grad.norm=0.65351093\n",
      " 15057: 11 [  460/ 1327], train_loss/perplexity = 4.78472614/119.6685867 secs/batch = 0.6145s, grad.norm=0.61006570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15062: 11 [  465/ 1327], train_loss/perplexity = 4.57004452/96.5484085 secs/batch = 0.6270s, grad.norm=0.62874639\n",
      " 15067: 11 [  470/ 1327], train_loss/perplexity = 5.11507750/166.5136871 secs/batch = 0.6037s, grad.norm=0.62236303\n",
      " 15072: 11 [  475/ 1327], train_loss/perplexity = 4.65207481/104.8022079 secs/batch = 0.6104s, grad.norm=0.58909553\n",
      " 15077: 11 [  480/ 1327], train_loss/perplexity = 4.80692911/122.3553009 secs/batch = 0.6111s, grad.norm=0.66130716\n",
      " 15082: 11 [  485/ 1327], train_loss/perplexity = 4.71648073/111.7741928 secs/batch = 0.6045s, grad.norm=0.60696685\n",
      " 15087: 11 [  490/ 1327], train_loss/perplexity = 4.63240242/102.7606430 secs/batch = 0.6088s, grad.norm=0.66570240\n",
      " 15092: 11 [  495/ 1327], train_loss/perplexity = 4.62195683/101.6928329 secs/batch = 0.6138s, grad.norm=0.61230272\n",
      " 15097: 11 [  500/ 1327], train_loss/perplexity = 4.95771360/142.2681427 secs/batch = 0.6091s, grad.norm=0.59875822\n",
      " 15102: 11 [  505/ 1327], train_loss/perplexity = 4.85224628/128.0276489 secs/batch = 0.6080s, grad.norm=0.60542649\n",
      " 15107: 11 [  510/ 1327], train_loss/perplexity = 5.22478294/185.8208313 secs/batch = 0.6186s, grad.norm=0.61292773\n",
      " 15112: 11 [  515/ 1327], train_loss/perplexity = 4.92071915/137.1011810 secs/batch = 0.6537s, grad.norm=0.60296309\n",
      " 15117: 11 [  520/ 1327], train_loss/perplexity = 5.05355835/156.5786285 secs/batch = 0.6054s, grad.norm=0.60597187\n",
      " 15122: 11 [  525/ 1327], train_loss/perplexity = 4.64031124/103.5765762 secs/batch = 0.6136s, grad.norm=0.62756085\n",
      " 15127: 11 [  530/ 1327], train_loss/perplexity = 4.76316452/117.1159592 secs/batch = 0.6138s, grad.norm=0.66049290\n",
      " 15132: 11 [  535/ 1327], train_loss/perplexity = 4.80441999/122.0486832 secs/batch = 0.6132s, grad.norm=0.64098489\n",
      " 15137: 11 [  540/ 1327], train_loss/perplexity = 4.91067886/135.7315216 secs/batch = 0.6094s, grad.norm=0.58406264\n",
      " 15142: 11 [  545/ 1327], train_loss/perplexity = 5.00410032/149.0229492 secs/batch = 0.6080s, grad.norm=0.60315216\n",
      " 15147: 11 [  550/ 1327], train_loss/perplexity = 4.90575743/135.0651703 secs/batch = 0.6106s, grad.norm=0.62613338\n",
      " 15152: 11 [  555/ 1327], train_loss/perplexity = 4.73716736/114.1105118 secs/batch = 0.6117s, grad.norm=0.61511260\n",
      " 15157: 11 [  560/ 1327], train_loss/perplexity = 4.80536175/122.1636734 secs/batch = 0.6082s, grad.norm=0.64997995\n",
      " 15162: 11 [  565/ 1327], train_loss/perplexity = 4.76911259/117.8146439 secs/batch = 0.6048s, grad.norm=0.62838769\n",
      " 15167: 11 [  570/ 1327], train_loss/perplexity = 4.72860575/113.1377106 secs/batch = 0.6125s, grad.norm=0.63497818\n",
      " 15172: 11 [  575/ 1327], train_loss/perplexity = 4.54196882/93.8754425 secs/batch = 0.6030s, grad.norm=0.63576567\n",
      " 15177: 11 [  580/ 1327], train_loss/perplexity = 4.91515779/136.3408203 secs/batch = 0.6145s, grad.norm=0.63863385\n",
      " 15182: 11 [  585/ 1327], train_loss/perplexity = 4.54274654/93.9484787 secs/batch = 0.6089s, grad.norm=0.60401767\n",
      " 15187: 11 [  590/ 1327], train_loss/perplexity = 4.82469225/124.5481339 secs/batch = 0.6033s, grad.norm=0.60837340\n",
      " 15192: 11 [  595/ 1327], train_loss/perplexity = 4.81468105/123.3074799 secs/batch = 0.6065s, grad.norm=0.65388840\n",
      " 15197: 11 [  600/ 1327], train_loss/perplexity = 5.07051373/159.2561188 secs/batch = 0.6118s, grad.norm=0.62711293\n",
      " 15202: 11 [  605/ 1327], train_loss/perplexity = 5.00928640/149.7978058 secs/batch = 0.6114s, grad.norm=0.61050916\n",
      " 15207: 11 [  610/ 1327], train_loss/perplexity = 5.05028343/156.0666962 secs/batch = 0.6104s, grad.norm=0.61811554\n",
      " 15212: 11 [  615/ 1327], train_loss/perplexity = 4.56316185/95.8861771 secs/batch = 0.6120s, grad.norm=0.59888226\n",
      " 15217: 11 [  620/ 1327], train_loss/perplexity = 4.93763304/139.4398041 secs/batch = 0.6120s, grad.norm=0.63398337\n",
      " 15222: 11 [  625/ 1327], train_loss/perplexity = 5.03421736/153.5793457 secs/batch = 0.6103s, grad.norm=0.60386503\n",
      " 15227: 11 [  630/ 1327], train_loss/perplexity = 5.08539915/161.6444397 secs/batch = 0.6107s, grad.norm=0.61447173\n",
      " 15232: 11 [  635/ 1327], train_loss/perplexity = 4.80367565/121.9578705 secs/batch = 0.6098s, grad.norm=0.65686512\n",
      " 15237: 11 [  640/ 1327], train_loss/perplexity = 4.86313820/129.4297485 secs/batch = 0.6098s, grad.norm=0.61622453\n",
      " 15242: 11 [  645/ 1327], train_loss/perplexity = 5.02170467/151.6696320 secs/batch = 0.6147s, grad.norm=0.63896424\n",
      " 15247: 11 [  650/ 1327], train_loss/perplexity = 4.66389418/106.0482483 secs/batch = 0.6051s, grad.norm=0.62745953\n",
      " 15252: 11 [  655/ 1327], train_loss/perplexity = 4.82022572/123.9930725 secs/batch = 0.6028s, grad.norm=0.62133163\n",
      " 15257: 11 [  660/ 1327], train_loss/perplexity = 4.65246296/104.8428879 secs/batch = 0.6191s, grad.norm=0.59889257\n",
      " 15262: 11 [  665/ 1327], train_loss/perplexity = 4.84982157/127.7175980 secs/batch = 0.6163s, grad.norm=0.63710696\n",
      " 15267: 11 [  670/ 1327], train_loss/perplexity = 4.76567364/117.4101791 secs/batch = 0.6279s, grad.norm=0.59947991\n",
      " 15272: 11 [  675/ 1327], train_loss/perplexity = 4.55357027/94.9708786 secs/batch = 0.6076s, grad.norm=0.66074300\n",
      " 15277: 11 [  680/ 1327], train_loss/perplexity = 4.85339117/128.1743164 secs/batch = 0.6066s, grad.norm=0.62658358\n",
      " 15282: 11 [  685/ 1327], train_loss/perplexity = 4.78658438/119.8911667 secs/batch = 0.6186s, grad.norm=0.60768914\n",
      " 15287: 11 [  690/ 1327], train_loss/perplexity = 4.97332764/144.5069580 secs/batch = 0.6052s, grad.norm=0.60333782\n",
      " 15292: 11 [  695/ 1327], train_loss/perplexity = 4.77022457/117.9457245 secs/batch = 0.6072s, grad.norm=0.60049242\n",
      " 15297: 11 [  700/ 1327], train_loss/perplexity = 5.02337074/151.9225311 secs/batch = 0.6093s, grad.norm=0.61835754\n",
      " 15302: 11 [  705/ 1327], train_loss/perplexity = 4.79399014/120.7823486 secs/batch = 0.6018s, grad.norm=0.65448689\n",
      " 15307: 11 [  710/ 1327], train_loss/perplexity = 4.73539782/113.9087677 secs/batch = 0.6199s, grad.norm=0.62186676\n",
      " 15312: 11 [  715/ 1327], train_loss/perplexity = 4.68591785/108.4097290 secs/batch = 0.6052s, grad.norm=0.61935878\n",
      " 15317: 11 [  720/ 1327], train_loss/perplexity = 4.72242689/112.4408035 secs/batch = 0.6104s, grad.norm=0.62167722\n",
      " 15322: 11 [  725/ 1327], train_loss/perplexity = 4.55229282/94.8496323 secs/batch = 0.6151s, grad.norm=0.59746063\n",
      " 15327: 11 [  730/ 1327], train_loss/perplexity = 4.73650026/114.0344086 secs/batch = 0.6068s, grad.norm=0.63326097\n",
      " 15332: 11 [  735/ 1327], train_loss/perplexity = 4.88337421/132.0755615 secs/batch = 0.6081s, grad.norm=0.61835617\n",
      " 15337: 11 [  740/ 1327], train_loss/perplexity = 4.23800516/69.2695312 secs/batch = 0.6113s, grad.norm=0.59345514\n",
      " 15342: 11 [  745/ 1327], train_loss/perplexity = 4.77493286/118.5023575 secs/batch = 0.6073s, grad.norm=0.58602643\n",
      " 15347: 11 [  750/ 1327], train_loss/perplexity = 4.60661745/100.1448288 secs/batch = 0.6072s, grad.norm=0.60715836\n",
      " 15352: 11 [  755/ 1327], train_loss/perplexity = 4.64436293/103.9970932 secs/batch = 0.6040s, grad.norm=0.60983342\n",
      " 15357: 11 [  760/ 1327], train_loss/perplexity = 4.51050758/90.9679794 secs/batch = 0.6529s, grad.norm=0.61406112\n",
      " 15362: 11 [  765/ 1327], train_loss/perplexity = 4.65234375/104.8303909 secs/batch = 0.6057s, grad.norm=0.63759452\n",
      " 15367: 11 [  770/ 1327], train_loss/perplexity = 4.57639599/97.1635818 secs/batch = 0.6156s, grad.norm=0.69179440\n",
      " 15372: 11 [  775/ 1327], train_loss/perplexity = 4.66622734/106.2959671 secs/batch = 0.6142s, grad.norm=0.63722479\n",
      " 15377: 11 [  780/ 1327], train_loss/perplexity = 4.98251581/145.8408356 secs/batch = 0.6080s, grad.norm=0.65084577\n",
      " 15382: 11 [  785/ 1327], train_loss/perplexity = 4.83723307/126.1199036 secs/batch = 0.6122s, grad.norm=0.64122385\n",
      " 15387: 11 [  790/ 1327], train_loss/perplexity = 4.62198162/101.6953506 secs/batch = 0.6074s, grad.norm=0.69682193\n",
      " 15392: 11 [  795/ 1327], train_loss/perplexity = 4.91105413/135.7824707 secs/batch = 0.6091s, grad.norm=0.59238505\n",
      " 15397: 11 [  800/ 1327], train_loss/perplexity = 4.86977911/130.2921295 secs/batch = 0.6103s, grad.norm=0.63485372\n",
      " 15402: 11 [  805/ 1327], train_loss/perplexity = 5.17525482/176.8416748 secs/batch = 0.6061s, grad.norm=0.62428862\n",
      " 15407: 11 [  810/ 1327], train_loss/perplexity = 4.76711369/117.5793839 secs/batch = 0.6147s, grad.norm=0.60024714\n",
      " 15412: 11 [  815/ 1327], train_loss/perplexity = 4.78130627/119.2600327 secs/batch = 0.6114s, grad.norm=0.62660062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15417: 11 [  820/ 1327], train_loss/perplexity = 4.44400501/85.1151505 secs/batch = 0.6110s, grad.norm=0.59717798\n",
      " 15422: 11 [  825/ 1327], train_loss/perplexity = 4.62629461/102.1349106 secs/batch = 0.6262s, grad.norm=0.60141462\n",
      " 15427: 11 [  830/ 1327], train_loss/perplexity = 4.49856138/89.8877258 secs/batch = 0.6106s, grad.norm=0.60137200\n",
      " 15432: 11 [  835/ 1327], train_loss/perplexity = 4.72848511/113.1240616 secs/batch = 0.6073s, grad.norm=0.62404948\n",
      " 15437: 11 [  840/ 1327], train_loss/perplexity = 4.84048414/126.5305939 secs/batch = 0.6117s, grad.norm=0.61640567\n",
      " 15442: 11 [  845/ 1327], train_loss/perplexity = 4.64052153/103.5983658 secs/batch = 0.6089s, grad.norm=0.64668691\n",
      " 15447: 11 [  850/ 1327], train_loss/perplexity = 4.73766327/114.1671143 secs/batch = 0.6118s, grad.norm=0.60589087\n",
      " 15452: 11 [  855/ 1327], train_loss/perplexity = 4.75080585/115.6774673 secs/batch = 0.6042s, grad.norm=0.69890499\n",
      " 15457: 11 [  860/ 1327], train_loss/perplexity = 4.43956518/84.7380905 secs/batch = 0.6043s, grad.norm=0.58988982\n",
      " 15462: 11 [  865/ 1327], train_loss/perplexity = 4.91310740/136.0615540 secs/batch = 0.6144s, grad.norm=0.61110556\n",
      " 15467: 11 [  870/ 1327], train_loss/perplexity = 4.83626652/125.9980621 secs/batch = 0.6121s, grad.norm=0.63401759\n",
      " 15472: 11 [  875/ 1327], train_loss/perplexity = 4.42633343/83.6242371 secs/batch = 0.6088s, grad.norm=0.61604595\n",
      " 15477: 11 [  880/ 1327], train_loss/perplexity = 4.64371204/103.9294205 secs/batch = 0.6041s, grad.norm=0.59506607\n",
      " 15482: 11 [  885/ 1327], train_loss/perplexity = 4.65339613/104.9407730 secs/batch = 0.6056s, grad.norm=0.60786003\n",
      " 15487: 11 [  890/ 1327], train_loss/perplexity = 4.89372778/133.4501190 secs/batch = 0.6117s, grad.norm=0.60797417\n",
      " 15492: 11 [  895/ 1327], train_loss/perplexity = 4.88605928/132.4306793 secs/batch = 0.6082s, grad.norm=0.59593773\n",
      " 15497: 11 [  900/ 1327], train_loss/perplexity = 4.80450344/122.0588684 secs/batch = 0.6121s, grad.norm=0.62652987\n",
      " 15502: 11 [  905/ 1327], train_loss/perplexity = 4.63424921/102.9505920 secs/batch = 0.6162s, grad.norm=0.59981048\n",
      " 15507: 11 [  910/ 1327], train_loss/perplexity = 4.74989176/115.5717773 secs/batch = 0.6147s, grad.norm=0.64417350\n",
      " 15512: 11 [  915/ 1327], train_loss/perplexity = 4.95679474/142.1374817 secs/batch = 0.6153s, grad.norm=0.63859379\n",
      " 15517: 11 [  920/ 1327], train_loss/perplexity = 5.09575987/163.3279114 secs/batch = 0.6067s, grad.norm=0.62783331\n",
      " 15522: 11 [  925/ 1327], train_loss/perplexity = 4.78059864/119.1756744 secs/batch = 0.6112s, grad.norm=0.64098662\n",
      " 15527: 11 [  930/ 1327], train_loss/perplexity = 4.80826855/122.5192947 secs/batch = 0.6146s, grad.norm=0.62505925\n",
      " 15532: 11 [  935/ 1327], train_loss/perplexity = 4.89831209/134.0633087 secs/batch = 0.6081s, grad.norm=0.60388982\n",
      " 15537: 11 [  940/ 1327], train_loss/perplexity = 4.81370211/123.1868286 secs/batch = 0.6150s, grad.norm=0.58357567\n",
      " 15542: 11 [  945/ 1327], train_loss/perplexity = 4.99342299/147.4402466 secs/batch = 0.6095s, grad.norm=0.58576626\n",
      " 15547: 11 [  950/ 1327], train_loss/perplexity = 4.73743391/114.1409302 secs/batch = 0.6042s, grad.norm=0.60160649\n",
      " 15552: 11 [  955/ 1327], train_loss/perplexity = 4.85400057/128.2524414 secs/batch = 0.6219s, grad.norm=0.60641217\n",
      " 15557: 11 [  960/ 1327], train_loss/perplexity = 5.11629152/166.7159576 secs/batch = 0.6104s, grad.norm=0.61034173\n",
      " 15562: 11 [  965/ 1327], train_loss/perplexity = 4.86950350/130.2562256 secs/batch = 0.6133s, grad.norm=0.61112809\n",
      " 15567: 11 [  970/ 1327], train_loss/perplexity = 5.12553358/168.2639008 secs/batch = 0.6044s, grad.norm=0.61638159\n",
      " 15572: 11 [  975/ 1327], train_loss/perplexity = 4.82158470/124.1616974 secs/batch = 0.6176s, grad.norm=0.63301259\n",
      " 15577: 11 [  980/ 1327], train_loss/perplexity = 4.60340500/99.8236389 secs/batch = 0.6170s, grad.norm=0.60574424\n",
      " 15582: 11 [  985/ 1327], train_loss/perplexity = 4.78456402/119.6491852 secs/batch = 0.6138s, grad.norm=0.61879975\n",
      " 15587: 11 [  990/ 1327], train_loss/perplexity = 4.93704033/139.3571930 secs/batch = 0.6037s, grad.norm=0.62411290\n",
      " 15592: 11 [  995/ 1327], train_loss/perplexity = 4.96697426/143.5917511 secs/batch = 0.6146s, grad.norm=0.61160666\n",
      " 15597: 11 [ 1000/ 1327], train_loss/perplexity = 4.46928120/87.2939529 secs/batch = 0.6107s, grad.norm=0.61055249\n",
      " 15602: 11 [ 1005/ 1327], train_loss/perplexity = 4.86919308/130.2158051 secs/batch = 0.6518s, grad.norm=0.61148959\n",
      " 15607: 11 [ 1010/ 1327], train_loss/perplexity = 4.48897696/89.0303192 secs/batch = 0.6115s, grad.norm=0.57499576\n",
      " 15612: 11 [ 1015/ 1327], train_loss/perplexity = 4.97832489/145.2308960 secs/batch = 0.6148s, grad.norm=0.62362957\n",
      " 15617: 11 [ 1020/ 1327], train_loss/perplexity = 5.14016628/170.7441559 secs/batch = 0.6081s, grad.norm=0.63393039\n",
      " 15622: 11 [ 1025/ 1327], train_loss/perplexity = 4.92729616/138.0058594 secs/batch = 0.6114s, grad.norm=0.62023407\n",
      " 15627: 11 [ 1030/ 1327], train_loss/perplexity = 4.75030994/115.6201172 secs/batch = 0.6077s, grad.norm=0.57657719\n",
      " 15632: 11 [ 1035/ 1327], train_loss/perplexity = 4.66131210/105.7747803 secs/batch = 0.6098s, grad.norm=0.61892444\n",
      " 15637: 11 [ 1040/ 1327], train_loss/perplexity = 4.90547991/135.0276947 secs/batch = 0.6107s, grad.norm=0.59261006\n",
      " 15642: 11 [ 1045/ 1327], train_loss/perplexity = 4.55212450/94.8336716 secs/batch = 0.6104s, grad.norm=0.62039077\n",
      " 15647: 11 [ 1050/ 1327], train_loss/perplexity = 4.59136152/98.6286240 secs/batch = 0.6114s, grad.norm=0.64971375\n",
      " 15652: 11 [ 1055/ 1327], train_loss/perplexity = 4.79144096/120.4748459 secs/batch = 0.6150s, grad.norm=0.64580840\n",
      " 15657: 11 [ 1060/ 1327], train_loss/perplexity = 4.37793398/79.6732559 secs/batch = 0.6119s, grad.norm=0.64621389\n",
      " 15662: 11 [ 1065/ 1327], train_loss/perplexity = 4.50773001/90.7156601 secs/batch = 0.6344s, grad.norm=0.61701137\n",
      " 15667: 11 [ 1070/ 1327], train_loss/perplexity = 4.84823227/127.5147781 secs/batch = 0.6136s, grad.norm=0.62995869\n",
      " 15672: 11 [ 1075/ 1327], train_loss/perplexity = 4.61056900/100.5413437 secs/batch = 0.6092s, grad.norm=0.64098942\n",
      " 15677: 11 [ 1080/ 1327], train_loss/perplexity = 4.53900385/93.5975189 secs/batch = 0.6101s, grad.norm=0.59503233\n",
      " 15682: 11 [ 1085/ 1327], train_loss/perplexity = 4.42683935/83.6665573 secs/batch = 0.6092s, grad.norm=0.59307659\n",
      " 15687: 11 [ 1090/ 1327], train_loss/perplexity = 4.62409067/101.9100647 secs/batch = 0.6086s, grad.norm=0.61136609\n",
      " 15692: 11 [ 1095/ 1327], train_loss/perplexity = 4.75769138/116.4767151 secs/batch = 0.6118s, grad.norm=0.64113927\n",
      " 15697: 11 [ 1100/ 1327], train_loss/perplexity = 4.57506084/97.0339432 secs/batch = 0.6022s, grad.norm=0.64931899\n",
      " 15702: 11 [ 1105/ 1327], train_loss/perplexity = 4.54430723/94.0952148 secs/batch = 0.6082s, grad.norm=0.64108092\n",
      " 15707: 11 [ 1110/ 1327], train_loss/perplexity = 5.00611448/149.3234100 secs/batch = 0.6073s, grad.norm=0.66825020\n",
      " 15712: 11 [ 1115/ 1327], train_loss/perplexity = 4.57456541/96.9858780 secs/batch = 0.6076s, grad.norm=0.62430710\n",
      " 15717: 11 [ 1120/ 1327], train_loss/perplexity = 4.74078131/114.5236435 secs/batch = 0.6109s, grad.norm=0.62659508\n",
      " 15722: 11 [ 1125/ 1327], train_loss/perplexity = 5.03016376/152.9580536 secs/batch = 0.6111s, grad.norm=0.62936193\n",
      " 15727: 11 [ 1130/ 1327], train_loss/perplexity = 4.68986368/108.8383408 secs/batch = 0.6099s, grad.norm=0.65983021\n",
      " 15732: 11 [ 1135/ 1327], train_loss/perplexity = 4.69759083/109.6826096 secs/batch = 0.6099s, grad.norm=0.59333938\n",
      " 15737: 11 [ 1140/ 1327], train_loss/perplexity = 4.92736101/138.0148163 secs/batch = 0.6062s, grad.norm=0.61229968\n",
      " 15742: 11 [ 1145/ 1327], train_loss/perplexity = 4.71633148/111.7575150 secs/batch = 0.6159s, grad.norm=0.63674098\n",
      " 15747: 11 [ 1150/ 1327], train_loss/perplexity = 4.68026543/107.7986832 secs/batch = 0.6148s, grad.norm=0.61853880\n",
      " 15752: 11 [ 1155/ 1327], train_loss/perplexity = 4.79635382/121.0681763 secs/batch = 0.6133s, grad.norm=0.62833297\n",
      " 15757: 11 [ 1160/ 1327], train_loss/perplexity = 4.76191711/116.9699554 secs/batch = 0.6109s, grad.norm=0.66811979\n",
      " 15762: 11 [ 1165/ 1327], train_loss/perplexity = 4.85979891/128.9982605 secs/batch = 0.6102s, grad.norm=0.63081872\n",
      " 15767: 11 [ 1170/ 1327], train_loss/perplexity = 4.69283628/109.1623535 secs/batch = 0.6086s, grad.norm=0.64791834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15772: 11 [ 1175/ 1327], train_loss/perplexity = 4.45106792/85.7184372 secs/batch = 0.6151s, grad.norm=0.64977163\n",
      " 15777: 11 [ 1180/ 1327], train_loss/perplexity = 4.43346930/84.2231064 secs/batch = 0.6018s, grad.norm=0.62429374\n",
      " 15782: 11 [ 1185/ 1327], train_loss/perplexity = 4.64007092/103.5516891 secs/batch = 0.6080s, grad.norm=0.61689496\n",
      " 15787: 11 [ 1190/ 1327], train_loss/perplexity = 4.72661114/112.9122696 secs/batch = 0.6095s, grad.norm=0.62138456\n",
      " 15792: 11 [ 1195/ 1327], train_loss/perplexity = 4.58006334/97.5205688 secs/batch = 0.6126s, grad.norm=0.63121694\n",
      " 15797: 11 [ 1200/ 1327], train_loss/perplexity = 4.51452971/91.3346024 secs/batch = 0.6231s, grad.norm=0.69915646\n",
      " 15802: 11 [ 1205/ 1327], train_loss/perplexity = 4.55209303/94.8306808 secs/batch = 0.6104s, grad.norm=0.64363670\n",
      " 15807: 11 [ 1210/ 1327], train_loss/perplexity = 4.24546623/69.7882919 secs/batch = 0.6028s, grad.norm=0.61905277\n",
      " 15812: 11 [ 1215/ 1327], train_loss/perplexity = 4.44906092/85.5465698 secs/batch = 0.6117s, grad.norm=0.59559220\n",
      " 15817: 11 [ 1220/ 1327], train_loss/perplexity = 4.53240204/92.9816360 secs/batch = 0.6104s, grad.norm=0.61906248\n",
      " 15822: 11 [ 1225/ 1327], train_loss/perplexity = 4.41286707/82.5056763 secs/batch = 0.6090s, grad.norm=0.67829156\n",
      " 15827: 11 [ 1230/ 1327], train_loss/perplexity = 4.56418228/95.9840775 secs/batch = 0.6089s, grad.norm=0.60965365\n",
      " 15832: 11 [ 1235/ 1327], train_loss/perplexity = 4.59467363/98.9558334 secs/batch = 0.6021s, grad.norm=0.61773151\n",
      " 15837: 11 [ 1240/ 1327], train_loss/perplexity = 4.73109436/113.4196167 secs/batch = 0.6055s, grad.norm=0.63021487\n",
      " 15842: 11 [ 1245/ 1327], train_loss/perplexity = 4.64968348/104.5518875 secs/batch = 0.6107s, grad.norm=0.63432425\n",
      " 15847: 11 [ 1250/ 1327], train_loss/perplexity = 4.77719498/118.7707291 secs/batch = 0.6267s, grad.norm=0.58497262\n",
      " 15852: 11 [ 1255/ 1327], train_loss/perplexity = 4.72922659/113.2079697 secs/batch = 0.6088s, grad.norm=0.60713017\n",
      " 15857: 11 [ 1260/ 1327], train_loss/perplexity = 4.62442875/101.9445190 secs/batch = 0.6189s, grad.norm=0.62200856\n",
      " 15862: 11 [ 1265/ 1327], train_loss/perplexity = 4.75742149/116.4452820 secs/batch = 0.6119s, grad.norm=0.64954549\n",
      " 15867: 11 [ 1270/ 1327], train_loss/perplexity = 4.60082960/99.5668793 secs/batch = 0.6101s, grad.norm=0.62720847\n",
      " 15872: 11 [ 1275/ 1327], train_loss/perplexity = 4.74778080/115.3280640 secs/batch = 0.6105s, grad.norm=0.61666578\n",
      " 15877: 11 [ 1280/ 1327], train_loss/perplexity = 4.60617208/100.1002426 secs/batch = 0.6060s, grad.norm=0.65267843\n",
      " 15882: 11 [ 1285/ 1327], train_loss/perplexity = 4.51510334/91.3870087 secs/batch = 0.6105s, grad.norm=0.64622468\n",
      " 15887: 11 [ 1290/ 1327], train_loss/perplexity = 4.75370884/116.0137634 secs/batch = 0.6105s, grad.norm=0.63350499\n",
      " 15892: 11 [ 1295/ 1327], train_loss/perplexity = 4.75930786/116.6651459 secs/batch = 0.6082s, grad.norm=0.60364270\n",
      " 15897: 11 [ 1300/ 1327], train_loss/perplexity = 4.87682009/131.2127533 secs/batch = 0.6568s, grad.norm=0.63555968\n",
      " 15902: 11 [ 1305/ 1327], train_loss/perplexity = 5.01956844/151.3459778 secs/batch = 0.6120s, grad.norm=0.69349980\n",
      " 15907: 11 [ 1310/ 1327], train_loss/perplexity = 5.20985031/183.0666504 secs/batch = 0.6050s, grad.norm=0.60391778\n",
      " 15912: 11 [ 1315/ 1327], train_loss/perplexity = 5.04812527/155.7302399 secs/batch = 0.6125s, grad.norm=0.66205895\n",
      " 15917: 11 [ 1320/ 1327], train_loss/perplexity = 4.97451591/144.6787720 secs/batch = 0.6057s, grad.norm=0.60313892\n",
      " 15922: 11 [ 1325/ 1327], train_loss/perplexity = 4.91943693/136.9254913 secs/batch = 0.6067s, grad.norm=0.61202168\n",
      "Epoch training time: 812.2800464630127\n",
      "Saved char model cv/epoch011_4.8494.model\n",
      " 15929: 12 [    5/ 1327], train_loss/perplexity = 5.03939962/154.3773041 secs/batch = 0.6091s, grad.norm=0.62092811\n",
      " 15934: 12 [   10/ 1327], train_loss/perplexity = 4.58391523/97.8969345 secs/batch = 0.6142s, grad.norm=0.62275565\n",
      " 15939: 12 [   15/ 1327], train_loss/perplexity = 4.78050089/119.1640244 secs/batch = 0.6122s, grad.norm=0.62296808\n",
      " 15944: 12 [   20/ 1327], train_loss/perplexity = 4.97841883/145.2445374 secs/batch = 0.6147s, grad.norm=0.63347340\n",
      " 15949: 12 [   25/ 1327], train_loss/perplexity = 4.89510870/133.6345367 secs/batch = 0.6072s, grad.norm=0.61819470\n",
      " 15954: 12 [   30/ 1327], train_loss/perplexity = 4.85099030/127.8669510 secs/batch = 0.6092s, grad.norm=0.62779254\n",
      " 15959: 12 [   35/ 1327], train_loss/perplexity = 4.66472578/106.1364746 secs/batch = 0.6081s, grad.norm=0.57996827\n",
      " 15964: 12 [   40/ 1327], train_loss/perplexity = 4.73186779/113.5073700 secs/batch = 0.6115s, grad.norm=0.61352241\n",
      " 15969: 12 [   45/ 1327], train_loss/perplexity = 4.44683647/85.3564911 secs/batch = 0.6096s, grad.norm=0.60099608\n",
      " 15974: 12 [   50/ 1327], train_loss/perplexity = 4.75959158/116.6982574 secs/batch = 0.6127s, grad.norm=0.63269734\n",
      " 15979: 12 [   55/ 1327], train_loss/perplexity = 4.68268538/108.0598679 secs/batch = 0.6102s, grad.norm=0.65783566\n",
      " 15984: 12 [   60/ 1327], train_loss/perplexity = 4.92641401/137.8841705 secs/batch = 0.6143s, grad.norm=0.67581683\n",
      " 15989: 12 [   65/ 1327], train_loss/perplexity = 4.45248652/85.8401184 secs/batch = 0.6090s, grad.norm=0.61042470\n",
      " 15994: 12 [   70/ 1327], train_loss/perplexity = 4.39834166/81.3159103 secs/batch = 0.6312s, grad.norm=0.63755637\n",
      " 15999: 12 [   75/ 1327], train_loss/perplexity = 4.33638906/76.4310532 secs/batch = 0.6071s, grad.norm=0.60563779\n",
      " 16004: 12 [   80/ 1327], train_loss/perplexity = 4.67747927/107.4987564 secs/batch = 0.6088s, grad.norm=0.63819838\n",
      " 16009: 12 [   85/ 1327], train_loss/perplexity = 4.72704363/112.9611130 secs/batch = 0.6124s, grad.norm=0.61132526\n",
      " 16014: 12 [   90/ 1327], train_loss/perplexity = 4.68803596/108.6395950 secs/batch = 0.6093s, grad.norm=0.61779898\n",
      " 16019: 12 [   95/ 1327], train_loss/perplexity = 4.56899881/96.4475021 secs/batch = 0.6108s, grad.norm=0.61173815\n",
      " 16024: 12 [  100/ 1327], train_loss/perplexity = 4.83610058/125.9771576 secs/batch = 0.6179s, grad.norm=0.66449279\n",
      " 16029: 12 [  105/ 1327], train_loss/perplexity = 4.76733971/117.6059570 secs/batch = 0.6118s, grad.norm=0.68990326\n",
      " 16034: 12 [  110/ 1327], train_loss/perplexity = 4.58414268/97.9192047 secs/batch = 0.6120s, grad.norm=0.63711512\n",
      " 16039: 12 [  115/ 1327], train_loss/perplexity = 4.49004984/89.1258850 secs/batch = 0.6132s, grad.norm=0.63492781\n",
      " 16044: 12 [  120/ 1327], train_loss/perplexity = 4.66092062/105.7333755 secs/batch = 0.6498s, grad.norm=0.67277884\n",
      " 16049: 12 [  125/ 1327], train_loss/perplexity = 4.74177790/114.6378326 secs/batch = 0.6128s, grad.norm=0.66092700\n",
      " 16054: 12 [  130/ 1327], train_loss/perplexity = 4.69003439/108.8569260 secs/batch = 0.6332s, grad.norm=0.67041415\n",
      " 16059: 12 [  135/ 1327], train_loss/perplexity = 4.67885399/107.6466370 secs/batch = 0.6044s, grad.norm=0.62680453\n",
      " 16064: 12 [  140/ 1327], train_loss/perplexity = 4.90765858/135.3221893 secs/batch = 0.6063s, grad.norm=0.63114625\n",
      " 16069: 12 [  145/ 1327], train_loss/perplexity = 4.91311455/136.0625305 secs/batch = 0.6088s, grad.norm=0.63680702\n",
      " 16074: 12 [  150/ 1327], train_loss/perplexity = 4.84560776/127.1805573 secs/batch = 0.6172s, grad.norm=0.67832756\n",
      " 16079: 12 [  155/ 1327], train_loss/perplexity = 5.09653997/163.4553680 secs/batch = 0.6046s, grad.norm=0.62443006\n",
      " 16084: 12 [  160/ 1327], train_loss/perplexity = 4.72069693/112.2464523 secs/batch = 0.6110s, grad.norm=0.62824553\n",
      " 16089: 12 [  165/ 1327], train_loss/perplexity = 4.94889212/141.0186462 secs/batch = 0.6116s, grad.norm=0.61967444\n",
      " 16094: 12 [  170/ 1327], train_loss/perplexity = 4.68763590/108.5961456 secs/batch = 0.6092s, grad.norm=0.64788580\n",
      " 16099: 12 [  175/ 1327], train_loss/perplexity = 4.96743822/143.6584015 secs/batch = 0.6066s, grad.norm=0.61027634\n",
      " 16104: 12 [  180/ 1327], train_loss/perplexity = 4.84165382/126.6786804 secs/batch = 0.6117s, grad.norm=0.61505288\n",
      " 16109: 12 [  185/ 1327], train_loss/perplexity = 5.09724855/163.5712280 secs/batch = 0.6098s, grad.norm=0.61499608\n",
      " 16114: 12 [  190/ 1327], train_loss/perplexity = 4.60925674/100.4094925 secs/batch = 0.6161s, grad.norm=0.59738058\n",
      " 16119: 12 [  195/ 1327], train_loss/perplexity = 4.85251474/128.0620270 secs/batch = 0.6126s, grad.norm=0.60679036\n",
      " 16124: 12 [  200/ 1327], train_loss/perplexity = 4.79187298/120.5269012 secs/batch = 0.6115s, grad.norm=0.62465394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16129: 12 [  205/ 1327], train_loss/perplexity = 4.90690136/135.2197723 secs/batch = 0.6161s, grad.norm=0.61631894\n",
      " 16134: 12 [  210/ 1327], train_loss/perplexity = 4.82147217/124.1477203 secs/batch = 0.6075s, grad.norm=0.63521081\n",
      " 16139: 12 [  215/ 1327], train_loss/perplexity = 4.92087460/137.1224823 secs/batch = 0.6056s, grad.norm=0.59760159\n",
      " 16144: 12 [  220/ 1327], train_loss/perplexity = 4.90227365/134.5954590 secs/batch = 0.6117s, grad.norm=0.61437690\n",
      " 16149: 12 [  225/ 1327], train_loss/perplexity = 5.05709934/157.1340637 secs/batch = 0.6114s, grad.norm=0.62475514\n",
      " 16154: 12 [  230/ 1327], train_loss/perplexity = 4.88957596/132.8972015 secs/batch = 0.6141s, grad.norm=0.64051497\n",
      " 16159: 12 [  235/ 1327], train_loss/perplexity = 4.76520777/117.3554993 secs/batch = 0.6110s, grad.norm=0.60713756\n",
      " 16164: 12 [  240/ 1327], train_loss/perplexity = 4.58860779/98.3573990 secs/batch = 0.6126s, grad.norm=0.65526146\n",
      " 16169: 12 [  245/ 1327], train_loss/perplexity = 4.91347885/136.1121063 secs/batch = 0.6080s, grad.norm=0.60742515\n",
      " 16174: 12 [  250/ 1327], train_loss/perplexity = 4.64210606/103.7626495 secs/batch = 0.6108s, grad.norm=0.60791445\n",
      " 16179: 12 [  255/ 1327], train_loss/perplexity = 4.67112207/106.8175354 secs/batch = 0.6135s, grad.norm=0.59146583\n",
      " 16184: 12 [  260/ 1327], train_loss/perplexity = 4.92930937/138.2839813 secs/batch = 0.6187s, grad.norm=0.64615643\n",
      " 16189: 12 [  265/ 1327], train_loss/perplexity = 5.01972771/151.3700867 secs/batch = 0.6112s, grad.norm=0.60781902\n",
      " 16194: 12 [  270/ 1327], train_loss/perplexity = 5.07163620/159.4349823 secs/batch = 0.6081s, grad.norm=0.61805475\n",
      " 16199: 12 [  275/ 1327], train_loss/perplexity = 5.11977911/167.2984161 secs/batch = 0.6101s, grad.norm=0.61926526\n",
      " 16204: 12 [  280/ 1327], train_loss/perplexity = 4.86109495/129.1655579 secs/batch = 0.6169s, grad.norm=0.61167109\n",
      " 16209: 12 [  285/ 1327], train_loss/perplexity = 5.09476089/163.1648254 secs/batch = 0.6131s, grad.norm=0.62867802\n",
      " 16214: 12 [  290/ 1327], train_loss/perplexity = 4.89880848/134.1298676 secs/batch = 0.6076s, grad.norm=0.65467936\n",
      " 16219: 12 [  295/ 1327], train_loss/perplexity = 4.64459324/104.0210419 secs/batch = 0.6114s, grad.norm=0.59933496\n",
      " 16224: 12 [  300/ 1327], train_loss/perplexity = 4.28293896/72.4530640 secs/batch = 0.6062s, grad.norm=0.58919877\n",
      " 16229: 12 [  305/ 1327], train_loss/perplexity = 4.73931742/114.3561172 secs/batch = 0.6111s, grad.norm=0.63247025\n",
      " 16234: 12 [  310/ 1327], train_loss/perplexity = 4.76229715/117.0144196 secs/batch = 0.6139s, grad.norm=0.65224975\n",
      " 16239: 12 [  315/ 1327], train_loss/perplexity = 4.36916208/78.9774246 secs/batch = 0.6042s, grad.norm=0.62412483\n",
      " 16244: 12 [  320/ 1327], train_loss/perplexity = 4.35844564/78.1355896 secs/batch = 0.6068s, grad.norm=0.62309474\n",
      " 16249: 12 [  325/ 1327], train_loss/perplexity = 4.31580544/74.8739090 secs/batch = 0.6436s, grad.norm=0.60888571\n",
      " 16254: 12 [  330/ 1327], train_loss/perplexity = 4.75378799/116.0229492 secs/batch = 0.6084s, grad.norm=0.64245605\n",
      " 16259: 12 [  335/ 1327], train_loss/perplexity = 4.18645382/65.7890778 secs/batch = 0.6088s, grad.norm=0.61060184\n",
      " 16264: 12 [  340/ 1327], train_loss/perplexity = 4.92664957/137.9166565 secs/batch = 0.6015s, grad.norm=0.61983401\n",
      " 16269: 12 [  345/ 1327], train_loss/perplexity = 4.78458929/119.6522141 secs/batch = 0.6177s, grad.norm=0.60502154\n",
      " 16274: 12 [  350/ 1327], train_loss/perplexity = 4.85216713/128.0175171 secs/batch = 0.6118s, grad.norm=0.66523242\n",
      " 16279: 12 [  355/ 1327], train_loss/perplexity = 4.83324909/125.6184464 secs/batch = 0.6080s, grad.norm=0.59199166\n",
      " 16284: 12 [  360/ 1327], train_loss/perplexity = 4.99186230/147.2103119 secs/batch = 0.6051s, grad.norm=0.64043468\n",
      " 16289: 12 [  365/ 1327], train_loss/perplexity = 4.89384079/133.4652100 secs/batch = 0.6442s, grad.norm=0.58172530\n",
      " 16294: 12 [  370/ 1327], train_loss/perplexity = 4.93804359/139.4970703 secs/batch = 0.6159s, grad.norm=0.67069638\n",
      " 16299: 12 [  375/ 1327], train_loss/perplexity = 4.30150461/73.8107681 secs/batch = 0.6091s, grad.norm=0.59952790\n",
      " 16304: 12 [  380/ 1327], train_loss/perplexity = 4.53128862/92.8781662 secs/batch = 0.6069s, grad.norm=0.63990653\n",
      " 16309: 12 [  385/ 1327], train_loss/perplexity = 4.72236300/112.4336166 secs/batch = 0.6004s, grad.norm=0.67367661\n",
      " 16314: 12 [  390/ 1327], train_loss/perplexity = 4.77812481/118.8812180 secs/batch = 0.6162s, grad.norm=0.59683907\n",
      " 16319: 12 [  395/ 1327], train_loss/perplexity = 4.95378733/141.7106476 secs/batch = 0.6140s, grad.norm=0.61306340\n",
      " 16324: 12 [  400/ 1327], train_loss/perplexity = 4.72966719/113.2578659 secs/batch = 0.6148s, grad.norm=0.61859924\n",
      " 16329: 12 [  405/ 1327], train_loss/perplexity = 5.04923630/155.9033508 secs/batch = 0.6119s, grad.norm=0.63919222\n",
      " 16334: 12 [  410/ 1327], train_loss/perplexity = 4.73411083/113.7622604 secs/batch = 0.6122s, grad.norm=0.61098546\n",
      " 16339: 12 [  415/ 1327], train_loss/perplexity = 4.60296965/99.7801895 secs/batch = 0.6083s, grad.norm=0.66124088\n",
      " 16344: 12 [  420/ 1327], train_loss/perplexity = 4.38787222/80.4690170 secs/batch = 0.6088s, grad.norm=0.61952895\n",
      " 16349: 12 [  425/ 1327], train_loss/perplexity = 4.69022942/108.8781586 secs/batch = 0.6149s, grad.norm=0.62248588\n",
      " 16354: 12 [  430/ 1327], train_loss/perplexity = 4.90166283/134.5132599 secs/batch = 0.6079s, grad.norm=0.62517107\n",
      " 16359: 12 [  435/ 1327], train_loss/perplexity = 4.89106750/133.0955811 secs/batch = 0.6198s, grad.norm=0.66988844\n",
      " 16364: 12 [  440/ 1327], train_loss/perplexity = 4.56492138/96.0550385 secs/batch = 0.6145s, grad.norm=0.63164258\n",
      " 16369: 12 [  445/ 1327], train_loss/perplexity = 4.84948444/127.6745529 secs/batch = 0.6158s, grad.norm=0.65357345\n",
      " 16374: 12 [  450/ 1327], train_loss/perplexity = 4.69071484/108.9310226 secs/batch = 0.6109s, grad.norm=0.65227383\n",
      " 16379: 12 [  455/ 1327], train_loss/perplexity = 4.57983637/97.4984360 secs/batch = 0.6166s, grad.norm=0.61353022\n",
      " 16384: 12 [  460/ 1327], train_loss/perplexity = 4.69663000/109.5772705 secs/batch = 0.6145s, grad.norm=0.61943650\n",
      " 16389: 12 [  465/ 1327], train_loss/perplexity = 4.48788929/88.9335327 secs/batch = 0.6068s, grad.norm=0.64376122\n",
      " 16394: 12 [  470/ 1327], train_loss/perplexity = 5.03726006/154.0473480 secs/batch = 0.6106s, grad.norm=0.63577241\n",
      " 16399: 12 [  475/ 1327], train_loss/perplexity = 4.57695103/97.2175293 secs/batch = 0.6073s, grad.norm=0.57684284\n",
      " 16404: 12 [  480/ 1327], train_loss/perplexity = 4.71905041/112.0617905 secs/batch = 0.6149s, grad.norm=0.65615124\n",
      " 16409: 12 [  485/ 1327], train_loss/perplexity = 4.69046259/108.9035492 secs/batch = 0.6092s, grad.norm=0.62795216\n",
      " 16414: 12 [  490/ 1327], train_loss/perplexity = 4.58933640/98.4290924 secs/batch = 0.6145s, grad.norm=0.67567974\n",
      " 16419: 12 [  495/ 1327], train_loss/perplexity = 4.54998016/94.6305313 secs/batch = 0.6126s, grad.norm=0.61762732\n",
      " 16424: 12 [  500/ 1327], train_loss/perplexity = 4.90125608/134.4585724 secs/batch = 0.6101s, grad.norm=0.61514103\n",
      " 16429: 12 [  505/ 1327], train_loss/perplexity = 4.76609039/117.4591217 secs/batch = 0.6099s, grad.norm=0.59748620\n",
      " 16434: 12 [  510/ 1327], train_loss/perplexity = 5.15911055/174.0096130 secs/batch = 0.6219s, grad.norm=0.63562047\n",
      " 16439: 12 [  515/ 1327], train_loss/perplexity = 4.84454870/127.0459366 secs/batch = 0.6099s, grad.norm=0.60852265\n",
      " 16444: 12 [  520/ 1327], train_loss/perplexity = 4.97649813/144.9658356 secs/batch = 0.6140s, grad.norm=0.62404472\n",
      " 16449: 12 [  525/ 1327], train_loss/perplexity = 4.58591080/98.0924911 secs/batch = 0.6177s, grad.norm=0.62016749\n",
      " 16454: 12 [  530/ 1327], train_loss/perplexity = 4.67025661/106.7251282 secs/batch = 0.6127s, grad.norm=0.64460844\n",
      " 16459: 12 [  535/ 1327], train_loss/perplexity = 4.73532248/113.9001846 secs/batch = 0.6135s, grad.norm=0.62960833\n",
      " 16464: 12 [  540/ 1327], train_loss/perplexity = 4.83873320/126.3092422 secs/batch = 0.6209s, grad.norm=0.60522199\n",
      " 16469: 12 [  545/ 1327], train_loss/perplexity = 4.92894793/138.2340088 secs/batch = 0.6145s, grad.norm=0.62761658\n",
      " 16474: 12 [  550/ 1327], train_loss/perplexity = 4.86172771/129.2473145 secs/batch = 0.6210s, grad.norm=0.63182741\n",
      " 16479: 12 [  555/ 1327], train_loss/perplexity = 4.70133066/110.0935745 secs/batch = 0.6071s, grad.norm=0.64792186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16484: 12 [  560/ 1327], train_loss/perplexity = 4.70849228/110.8848495 secs/batch = 0.6172s, grad.norm=0.62569952\n",
      " 16489: 12 [  565/ 1327], train_loss/perplexity = 4.70240402/110.2118073 secs/batch = 0.6041s, grad.norm=0.64657927\n",
      " 16494: 12 [  570/ 1327], train_loss/perplexity = 4.66454506/106.1172943 secs/batch = 0.6107s, grad.norm=0.64513922\n",
      " 16499: 12 [  575/ 1327], train_loss/perplexity = 4.47634506/87.9127655 secs/batch = 0.6093s, grad.norm=0.62573260\n",
      " 16504: 12 [  580/ 1327], train_loss/perplexity = 4.83349895/125.6498337 secs/batch = 0.6117s, grad.norm=0.64225924\n",
      " 16509: 12 [  585/ 1327], train_loss/perplexity = 4.45837879/86.3474121 secs/batch = 0.6134s, grad.norm=0.61173093\n",
      " 16514: 12 [  590/ 1327], train_loss/perplexity = 4.76043510/116.7967300 secs/batch = 0.6144s, grad.norm=0.61396921\n",
      " 16519: 12 [  595/ 1327], train_loss/perplexity = 4.72249794/112.4487915 secs/batch = 0.6079s, grad.norm=0.67387217\n",
      " 16524: 12 [  600/ 1327], train_loss/perplexity = 5.01993227/151.4010468 secs/batch = 0.6143s, grad.norm=0.61550742\n",
      " 16529: 12 [  605/ 1327], train_loss/perplexity = 4.91745949/136.6549988 secs/batch = 0.6102s, grad.norm=0.62574416\n",
      " 16534: 12 [  610/ 1327], train_loss/perplexity = 4.99421167/147.5565796 secs/batch = 0.6507s, grad.norm=0.64473850\n",
      " 16539: 12 [  615/ 1327], train_loss/perplexity = 4.54428816/94.0934219 secs/batch = 0.6077s, grad.norm=0.65259159\n",
      " 16544: 12 [  620/ 1327], train_loss/perplexity = 4.89774227/133.9869385 secs/batch = 0.6158s, grad.norm=0.63004035\n",
      " 16549: 12 [  625/ 1327], train_loss/perplexity = 4.99448299/147.5966187 secs/batch = 0.6103s, grad.norm=0.63381296\n",
      " 16554: 12 [  630/ 1327], train_loss/perplexity = 4.99235773/147.2832642 secs/batch = 0.6105s, grad.norm=0.62675154\n",
      " 16559: 12 [  635/ 1327], train_loss/perplexity = 4.72281456/112.4843979 secs/batch = 0.6100s, grad.norm=0.64338291\n",
      " 16564: 12 [  640/ 1327], train_loss/perplexity = 4.79636574/121.0696182 secs/batch = 0.6137s, grad.norm=0.60896087\n",
      " 16569: 12 [  645/ 1327], train_loss/perplexity = 4.95333099/141.6459961 secs/batch = 0.6113s, grad.norm=0.65789980\n",
      " 16574: 12 [  650/ 1327], train_loss/perplexity = 4.61633778/101.1230164 secs/batch = 0.6115s, grad.norm=0.68685424\n",
      " 16579: 12 [  655/ 1327], train_loss/perplexity = 4.73274469/113.6069489 secs/batch = 0.6070s, grad.norm=0.63421863\n",
      " 16584: 12 [  660/ 1327], train_loss/perplexity = 4.61385107/100.8718643 secs/batch = 0.6142s, grad.norm=0.62095833\n",
      " 16589: 12 [  665/ 1327], train_loss/perplexity = 4.80860472/122.5604935 secs/batch = 0.6078s, grad.norm=0.63414168\n",
      " 16594: 12 [  670/ 1327], train_loss/perplexity = 4.67027950/106.7275696 secs/batch = 0.6065s, grad.norm=0.63268751\n",
      " 16599: 12 [  675/ 1327], train_loss/perplexity = 4.51542187/91.4161224 secs/batch = 0.6015s, grad.norm=0.66856110\n",
      " 16604: 12 [  680/ 1327], train_loss/perplexity = 4.77527142/118.5424881 secs/batch = 0.6066s, grad.norm=0.65655750\n",
      " 16609: 12 [  685/ 1327], train_loss/perplexity = 4.71113157/111.1778946 secs/batch = 0.6080s, grad.norm=0.65064669\n",
      " 16614: 12 [  690/ 1327], train_loss/perplexity = 4.93465519/139.0251923 secs/batch = 0.6071s, grad.norm=0.60355747\n",
      " 16619: 12 [  695/ 1327], train_loss/perplexity = 4.74094820/114.5427628 secs/batch = 0.6129s, grad.norm=0.61549056\n",
      " 16624: 12 [  700/ 1327], train_loss/perplexity = 5.00764227/149.5517120 secs/batch = 0.6067s, grad.norm=0.64744705\n",
      " 16629: 12 [  705/ 1327], train_loss/perplexity = 4.73565483/113.9380417 secs/batch = 0.6204s, grad.norm=0.64277631\n",
      " 16634: 12 [  710/ 1327], train_loss/perplexity = 4.65795183/105.4199448 secs/batch = 0.6123s, grad.norm=0.60520184\n",
      " 16639: 12 [  715/ 1327], train_loss/perplexity = 4.63870478/103.4103241 secs/batch = 0.6259s, grad.norm=0.62051946\n",
      " 16644: 12 [  720/ 1327], train_loss/perplexity = 4.72175598/112.3653946 secs/batch = 0.6107s, grad.norm=0.65081310\n",
      " 16649: 12 [  725/ 1327], train_loss/perplexity = 4.51777077/91.6311035 secs/batch = 0.6152s, grad.norm=0.62271988\n",
      " 16654: 12 [  730/ 1327], train_loss/perplexity = 4.71088791/111.1508102 secs/batch = 0.6142s, grad.norm=0.63405043\n",
      " 16659: 12 [  735/ 1327], train_loss/perplexity = 4.83759165/126.1651382 secs/batch = 0.6066s, grad.norm=0.62452501\n",
      " 16664: 12 [  740/ 1327], train_loss/perplexity = 4.20413828/66.9628677 secs/batch = 0.6124s, grad.norm=0.58609319\n",
      " 16669: 12 [  745/ 1327], train_loss/perplexity = 4.69771671/109.6964188 secs/batch = 0.6101s, grad.norm=0.60224748\n",
      " 16674: 12 [  750/ 1327], train_loss/perplexity = 4.57226849/96.7633667 secs/batch = 0.6036s, grad.norm=0.62115127\n",
      " 16679: 12 [  755/ 1327], train_loss/perplexity = 4.59631824/99.1187134 secs/batch = 0.6084s, grad.norm=0.61973494\n",
      " 16684: 12 [  760/ 1327], train_loss/perplexity = 4.40929079/82.2111359 secs/batch = 0.6167s, grad.norm=0.62982309\n",
      " 16689: 12 [  765/ 1327], train_loss/perplexity = 4.57430649/96.9607697 secs/batch = 0.6050s, grad.norm=0.67969799\n",
      " 16694: 12 [  770/ 1327], train_loss/perplexity = 4.49775791/89.8155289 secs/batch = 0.6063s, grad.norm=0.67725790\n",
      " 16699: 12 [  775/ 1327], train_loss/perplexity = 4.62763548/102.2719574 secs/batch = 0.6072s, grad.norm=0.65281612\n",
      " 16704: 12 [  780/ 1327], train_loss/perplexity = 4.89299250/133.3520355 secs/batch = 0.6092s, grad.norm=0.62957048\n",
      " 16709: 12 [  785/ 1327], train_loss/perplexity = 4.70061684/110.0150146 secs/batch = 0.6070s, grad.norm=0.62941456\n",
      " 16714: 12 [  790/ 1327], train_loss/perplexity = 4.55427504/95.0378342 secs/batch = 0.6058s, grad.norm=0.66792983\n",
      " 16719: 12 [  795/ 1327], train_loss/perplexity = 4.86011600/129.0391693 secs/batch = 0.6113s, grad.norm=0.63462549\n",
      " 16724: 12 [  800/ 1327], train_loss/perplexity = 4.81966209/123.9232101 secs/batch = 0.6124s, grad.norm=0.63453484\n",
      " 16729: 12 [  805/ 1327], train_loss/perplexity = 5.12922812/168.8867035 secs/batch = 0.6289s, grad.norm=0.63729566\n",
      " 16734: 12 [  810/ 1327], train_loss/perplexity = 4.69789076/109.7155151 secs/batch = 0.6070s, grad.norm=0.60294598\n",
      " 16739: 12 [  815/ 1327], train_loss/perplexity = 4.68621826/108.4423065 secs/batch = 0.6124s, grad.norm=0.65837681\n",
      " 16744: 12 [  820/ 1327], train_loss/perplexity = 4.38356543/80.1231995 secs/batch = 0.6094s, grad.norm=0.59207171\n",
      " 16749: 12 [  825/ 1327], train_loss/perplexity = 4.56758785/96.3115082 secs/batch = 0.6038s, grad.norm=0.61705786\n",
      " 16754: 12 [  830/ 1327], train_loss/perplexity = 4.43827581/84.6288986 secs/batch = 0.6061s, grad.norm=0.66749507\n",
      " 16759: 12 [  835/ 1327], train_loss/perplexity = 4.65092516/104.6817856 secs/batch = 0.6111s, grad.norm=0.64459383\n",
      " 16764: 12 [  840/ 1327], train_loss/perplexity = 4.81903124/123.8450546 secs/batch = 0.6090s, grad.norm=0.62848324\n",
      " 16769: 12 [  845/ 1327], train_loss/perplexity = 4.59414959/98.9039917 secs/batch = 0.6135s, grad.norm=0.62881804\n",
      " 16774: 12 [  850/ 1327], train_loss/perplexity = 4.67471933/107.2024765 secs/batch = 0.6094s, grad.norm=0.61907697\n",
      " 16779: 12 [  855/ 1327], train_loss/perplexity = 4.68257284/108.0477066 secs/batch = 0.6048s, grad.norm=0.68330163\n",
      " 16784: 12 [  860/ 1327], train_loss/perplexity = 4.38023996/79.8571930 secs/batch = 0.6109s, grad.norm=0.63936049\n",
      " 16789: 12 [  865/ 1327], train_loss/perplexity = 4.89289808/133.3394470 secs/batch = 0.6070s, grad.norm=0.64883810\n",
      " 16794: 12 [  870/ 1327], train_loss/perplexity = 4.77779484/118.8419952 secs/batch = 0.6149s, grad.norm=0.66266966\n",
      " 16799: 12 [  875/ 1327], train_loss/perplexity = 4.35821629/78.1176682 secs/batch = 0.6054s, grad.norm=0.62286305\n",
      " 16804: 12 [  880/ 1327], train_loss/perplexity = 4.55697298/95.2945862 secs/batch = 0.6121s, grad.norm=0.58986461\n",
      " 16809: 12 [  885/ 1327], train_loss/perplexity = 4.64114141/103.6626053 secs/batch = 0.6151s, grad.norm=0.63020730\n",
      " 16814: 12 [  890/ 1327], train_loss/perplexity = 4.81312895/123.1162415 secs/batch = 0.6070s, grad.norm=0.63133800\n",
      " 16819: 12 [  895/ 1327], train_loss/perplexity = 4.86586618/129.7833099 secs/batch = 0.6069s, grad.norm=0.62592870\n",
      " 16824: 12 [  900/ 1327], train_loss/perplexity = 4.72451258/112.6755676 secs/batch = 0.6117s, grad.norm=0.60498160\n",
      " 16829: 12 [  905/ 1327], train_loss/perplexity = 4.56706905/96.2615585 secs/batch = 0.6272s, grad.norm=0.59753913\n",
      " 16834: 12 [  910/ 1327], train_loss/perplexity = 4.69472742/109.3689957 secs/batch = 0.6183s, grad.norm=0.64904380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16839: 12 [  915/ 1327], train_loss/perplexity = 4.86840963/130.1138306 secs/batch = 0.6141s, grad.norm=0.63290119\n",
      " 16844: 12 [  920/ 1327], train_loss/perplexity = 5.01615524/150.8302765 secs/batch = 0.6117s, grad.norm=0.70374554\n",
      " 16849: 12 [  925/ 1327], train_loss/perplexity = 4.76971388/117.8855057 secs/batch = 0.6094s, grad.norm=0.66782606\n",
      " 16854: 12 [  930/ 1327], train_loss/perplexity = 4.73249102/113.5781326 secs/batch = 0.6079s, grad.norm=0.60115319\n",
      " 16859: 12 [  935/ 1327], train_loss/perplexity = 4.86116171/129.1741791 secs/batch = 0.6079s, grad.norm=0.62715596\n",
      " 16864: 12 [  940/ 1327], train_loss/perplexity = 4.74792194/115.3443451 secs/batch = 0.6163s, grad.norm=0.59480220\n",
      " 16869: 12 [  945/ 1327], train_loss/perplexity = 4.93871689/139.5910187 secs/batch = 0.6121s, grad.norm=0.60789108\n",
      " 16874: 12 [  950/ 1327], train_loss/perplexity = 4.69456863/109.3516312 secs/batch = 0.6103s, grad.norm=0.61405122\n",
      " 16879: 12 [  955/ 1327], train_loss/perplexity = 4.79526758/120.9367371 secs/batch = 0.6480s, grad.norm=0.60967559\n",
      " 16884: 12 [  960/ 1327], train_loss/perplexity = 5.02738523/152.5336456 secs/batch = 0.6167s, grad.norm=0.62451100\n",
      " 16889: 12 [  965/ 1327], train_loss/perplexity = 4.81442118/123.2754364 secs/batch = 0.6156s, grad.norm=0.65655363\n",
      " 16894: 12 [  970/ 1327], train_loss/perplexity = 5.04961967/155.9631348 secs/batch = 0.6107s, grad.norm=0.60983807\n",
      " 16899: 12 [  975/ 1327], train_loss/perplexity = 4.71643114/111.7686539 secs/batch = 0.6157s, grad.norm=0.61920244\n",
      " 16904: 12 [  980/ 1327], train_loss/perplexity = 4.56343079/95.9119720 secs/batch = 0.6069s, grad.norm=0.61948806\n",
      " 16909: 12 [  985/ 1327], train_loss/perplexity = 4.75057173/115.6503830 secs/batch = 0.6103s, grad.norm=0.63970047\n",
      " 16914: 12 [  990/ 1327], train_loss/perplexity = 4.88316536/132.0479889 secs/batch = 0.6145s, grad.norm=0.63642448\n",
      " 16919: 12 [  995/ 1327], train_loss/perplexity = 4.90978527/135.6102905 secs/batch = 0.6113s, grad.norm=0.60349786\n",
      " 16924: 12 [ 1000/ 1327], train_loss/perplexity = 4.41095304/82.3479080 secs/batch = 0.6126s, grad.norm=0.58087516\n",
      " 16929: 12 [ 1005/ 1327], train_loss/perplexity = 4.85615206/128.5286713 secs/batch = 0.6051s, grad.norm=0.62248451\n",
      " 16934: 12 [ 1010/ 1327], train_loss/perplexity = 4.45845270/86.3537903 secs/batch = 0.6106s, grad.norm=0.61331421\n",
      " 16939: 12 [ 1015/ 1327], train_loss/perplexity = 4.90908241/135.5150146 secs/batch = 0.6104s, grad.norm=0.62492061\n",
      " 16944: 12 [ 1020/ 1327], train_loss/perplexity = 5.08851480/162.1488647 secs/batch = 0.6159s, grad.norm=0.64019680\n",
      " 16949: 12 [ 1025/ 1327], train_loss/perplexity = 4.85584593/128.4893341 secs/batch = 0.6110s, grad.norm=0.60802341\n",
      " 16954: 12 [ 1030/ 1327], train_loss/perplexity = 4.69147825/109.0142136 secs/batch = 0.6143s, grad.norm=0.61438894\n",
      " 16959: 12 [ 1035/ 1327], train_loss/perplexity = 4.58541155/98.0435257 secs/batch = 0.6093s, grad.norm=0.61382896\n",
      " 16964: 12 [ 1040/ 1327], train_loss/perplexity = 4.89322090/133.3824921 secs/batch = 0.6088s, grad.norm=0.60360402\n",
      " 16969: 12 [ 1045/ 1327], train_loss/perplexity = 4.45315313/85.8973618 secs/batch = 0.6090s, grad.norm=0.59753275\n",
      " 16974: 12 [ 1050/ 1327], train_loss/perplexity = 4.53445578/93.1727982 secs/batch = 0.6177s, grad.norm=0.66023767\n",
      " 16979: 12 [ 1055/ 1327], train_loss/perplexity = 4.68162203/107.9450226 secs/batch = 0.6105s, grad.norm=0.65493065\n",
      " 16984: 12 [ 1060/ 1327], train_loss/perplexity = 4.31141376/74.5458069 secs/batch = 0.6133s, grad.norm=0.67125171\n",
      " 16989: 12 [ 1065/ 1327], train_loss/perplexity = 4.45133638/85.7414474 secs/batch = 0.6135s, grad.norm=0.61566299\n",
      " 16994: 12 [ 1070/ 1327], train_loss/perplexity = 4.78010225/119.1165314 secs/batch = 0.6080s, grad.norm=0.63121319\n",
      " 16999: 12 [ 1075/ 1327], train_loss/perplexity = 4.52763605/92.5395432 secs/batch = 0.6090s, grad.norm=0.62958759\n",
      " 17004: 12 [ 1080/ 1327], train_loss/perplexity = 4.46235180/86.6911469 secs/batch = 0.6101s, grad.norm=0.60734862\n",
      " 17009: 12 [ 1085/ 1327], train_loss/perplexity = 4.34622574/77.1865921 secs/batch = 0.6130s, grad.norm=0.58487231\n",
      " 17014: 12 [ 1090/ 1327], train_loss/perplexity = 4.55166483/94.7900848 secs/batch = 0.6131s, grad.norm=0.61672658\n",
      " 17019: 12 [ 1095/ 1327], train_loss/perplexity = 4.67951536/107.7178574 secs/batch = 0.6116s, grad.norm=0.63951999\n",
      " 17024: 12 [ 1100/ 1327], train_loss/perplexity = 4.48260736/88.4650345 secs/batch = 0.6073s, grad.norm=0.66967571\n",
      " 17029: 12 [ 1105/ 1327], train_loss/perplexity = 4.48041821/88.2715836 secs/batch = 0.6025s, grad.norm=0.69914538\n",
      " 17034: 12 [ 1110/ 1327], train_loss/perplexity = 4.94952726/141.1082458 secs/batch = 0.6148s, grad.norm=0.67500246\n",
      " 17039: 12 [ 1115/ 1327], train_loss/perplexity = 4.47172737/87.5077515 secs/batch = 0.6125s, grad.norm=0.60162306\n",
      " 17044: 12 [ 1120/ 1327], train_loss/perplexity = 4.71430206/111.5309448 secs/batch = 0.6106s, grad.norm=0.64564228\n",
      " 17049: 12 [ 1125/ 1327], train_loss/perplexity = 4.96921444/143.9137878 secs/batch = 0.6113s, grad.norm=0.63759798\n",
      " 17054: 12 [ 1130/ 1327], train_loss/perplexity = 4.62039328/101.5339584 secs/batch = 0.6099s, grad.norm=0.63656616\n",
      " 17059: 12 [ 1135/ 1327], train_loss/perplexity = 4.61143637/100.6285858 secs/batch = 0.6141s, grad.norm=0.60261053\n",
      " 17064: 12 [ 1140/ 1327], train_loss/perplexity = 4.86242199/129.3370819 secs/batch = 0.6192s, grad.norm=0.64360738\n",
      " 17069: 12 [ 1145/ 1327], train_loss/perplexity = 4.65966988/105.6012115 secs/batch = 0.6083s, grad.norm=0.65534085\n",
      " 17074: 12 [ 1150/ 1327], train_loss/perplexity = 4.62208176/101.7055359 secs/batch = 0.6223s, grad.norm=0.60352457\n",
      " 17079: 12 [ 1155/ 1327], train_loss/perplexity = 4.73611450/113.9904327 secs/batch = 0.6123s, grad.norm=0.69094378\n",
      " 17084: 12 [ 1160/ 1327], train_loss/perplexity = 4.67022276/106.7215118 secs/batch = 0.6133s, grad.norm=0.62002254\n",
      " 17089: 12 [ 1165/ 1327], train_loss/perplexity = 4.80134726/121.6742325 secs/batch = 0.6116s, grad.norm=0.63441020\n",
      " 17094: 12 [ 1170/ 1327], train_loss/perplexity = 4.61531496/101.0196381 secs/batch = 0.6111s, grad.norm=0.62174374\n",
      " 17099: 12 [ 1175/ 1327], train_loss/perplexity = 4.39844894/81.3246307 secs/batch = 0.6042s, grad.norm=0.66581434\n",
      " 17104: 12 [ 1180/ 1327], train_loss/perplexity = 4.35381460/77.7745743 secs/batch = 0.6155s, grad.norm=0.65389758\n",
      " 17109: 12 [ 1185/ 1327], train_loss/perplexity = 4.60690641/100.1737747 secs/batch = 0.6052s, grad.norm=0.61250818\n",
      " 17114: 12 [ 1190/ 1327], train_loss/perplexity = 4.69387579/109.2758942 secs/batch = 0.6089s, grad.norm=0.62981778\n",
      " 17119: 12 [ 1195/ 1327], train_loss/perplexity = 4.52346897/92.1547241 secs/batch = 0.6073s, grad.norm=0.66610932\n",
      " 17124: 12 [ 1200/ 1327], train_loss/perplexity = 4.47243118/87.5693588 secs/batch = 0.6118s, grad.norm=0.63423526\n",
      " 17129: 12 [ 1205/ 1327], train_loss/perplexity = 4.44979334/85.6092529 secs/batch = 0.6123s, grad.norm=0.65290993\n",
      " 17134: 12 [ 1210/ 1327], train_loss/perplexity = 4.18709469/65.8312531 secs/batch = 0.6106s, grad.norm=0.63439983\n",
      " 17139: 12 [ 1215/ 1327], train_loss/perplexity = 4.42224216/83.2828064 secs/batch = 0.6067s, grad.norm=0.62049115\n",
      " 17144: 12 [ 1220/ 1327], train_loss/perplexity = 4.51247263/91.1469116 secs/batch = 0.6170s, grad.norm=0.62626725\n",
      " 17149: 12 [ 1225/ 1327], train_loss/perplexity = 4.31417465/74.7518997 secs/batch = 0.6189s, grad.norm=0.67008066\n",
      " 17154: 12 [ 1230/ 1327], train_loss/perplexity = 4.49545431/89.6088715 secs/batch = 0.6072s, grad.norm=0.59700561\n",
      " 17159: 12 [ 1235/ 1327], train_loss/perplexity = 4.48222923/88.4315872 secs/batch = 0.6081s, grad.norm=0.61991990\n",
      " 17164: 12 [ 1240/ 1327], train_loss/perplexity = 4.68682957/108.5086136 secs/batch = 0.6113s, grad.norm=0.61315864\n",
      " 17169: 12 [ 1245/ 1327], train_loss/perplexity = 4.61343384/100.8297882 secs/batch = 0.6072s, grad.norm=0.62006754\n",
      " 17174: 12 [ 1250/ 1327], train_loss/perplexity = 4.71686363/111.8170013 secs/batch = 0.6103s, grad.norm=0.59518909\n",
      " 17179: 12 [ 1255/ 1327], train_loss/perplexity = 4.72115946/112.2983856 secs/batch = 0.6089s, grad.norm=0.62288320\n",
      " 17184: 12 [ 1260/ 1327], train_loss/perplexity = 4.56154537/95.7313080 secs/batch = 0.6124s, grad.norm=0.61333239\n",
      " 17189: 12 [ 1265/ 1327], train_loss/perplexity = 4.68974590/108.8255234 secs/batch = 0.6091s, grad.norm=0.65826756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17194: 12 [ 1270/ 1327], train_loss/perplexity = 4.49292278/89.3823090 secs/batch = 0.6164s, grad.norm=0.62231368\n",
      " 17199: 12 [ 1275/ 1327], train_loss/perplexity = 4.69086456/108.9473343 secs/batch = 0.6179s, grad.norm=0.65119618\n",
      " 17204: 12 [ 1280/ 1327], train_loss/perplexity = 4.53789425/93.4937210 secs/batch = 0.6139s, grad.norm=0.64049071\n",
      " 17209: 12 [ 1285/ 1327], train_loss/perplexity = 4.52135134/91.9597855 secs/batch = 0.6198s, grad.norm=0.64765304\n",
      " 17214: 12 [ 1290/ 1327], train_loss/perplexity = 4.69435215/109.3279572 secs/batch = 0.6122s, grad.norm=0.62896878\n",
      " 17219: 12 [ 1295/ 1327], train_loss/perplexity = 4.68505764/108.3165131 secs/batch = 0.6520s, grad.norm=0.61208898\n",
      " 17224: 12 [ 1300/ 1327], train_loss/perplexity = 4.80502892/122.1230240 secs/batch = 0.6167s, grad.norm=0.63644379\n",
      " 17229: 12 [ 1305/ 1327], train_loss/perplexity = 4.95642138/142.0844116 secs/batch = 0.6631s, grad.norm=0.67699838\n",
      " 17234: 12 [ 1310/ 1327], train_loss/perplexity = 5.17030239/175.9680481 secs/batch = 0.6118s, grad.norm=0.62016273\n",
      " 17239: 12 [ 1315/ 1327], train_loss/perplexity = 5.02671432/152.4313507 secs/batch = 0.6123s, grad.norm=0.66637230\n",
      " 17244: 12 [ 1320/ 1327], train_loss/perplexity = 4.95071077/141.2753448 secs/batch = 0.6074s, grad.norm=0.62371796\n",
      " 17249: 12 [ 1325/ 1327], train_loss/perplexity = 4.84738541/127.4068375 secs/batch = 0.6097s, grad.norm=0.61811900\n",
      "Epoch training time: 812.7367429733276\n",
      "Saved char model cv/epoch012_4.7960.model\n",
      " 17256: 13 [    5/ 1327], train_loss/perplexity = 4.97263765/144.4072876 secs/batch = 0.6160s, grad.norm=0.65575993\n",
      " 17261: 13 [   10/ 1327], train_loss/perplexity = 4.50498724/90.4671860 secs/batch = 0.6157s, grad.norm=0.63773543\n",
      " 17266: 13 [   15/ 1327], train_loss/perplexity = 4.78124189/119.2523575 secs/batch = 0.6068s, grad.norm=0.62104183\n",
      " 17271: 13 [   20/ 1327], train_loss/perplexity = 4.95111609/141.3326111 secs/batch = 0.6496s, grad.norm=0.64958501\n",
      " 17276: 13 [   25/ 1327], train_loss/perplexity = 4.83035088/125.2548981 secs/batch = 0.6043s, grad.norm=0.63222021\n",
      " 17281: 13 [   30/ 1327], train_loss/perplexity = 4.78492737/119.6926727 secs/batch = 0.6086s, grad.norm=0.62562621\n",
      " 17286: 13 [   35/ 1327], train_loss/perplexity = 4.59353495/98.8432159 secs/batch = 0.6139s, grad.norm=0.59319389\n",
      " 17291: 13 [   40/ 1327], train_loss/perplexity = 4.66915560/106.6076813 secs/batch = 0.6019s, grad.norm=0.64388299\n",
      " 17296: 13 [   45/ 1327], train_loss/perplexity = 4.40382051/81.7626495 secs/batch = 0.6061s, grad.norm=0.60658711\n",
      " 17301: 13 [   50/ 1327], train_loss/perplexity = 4.66106129/105.7482529 secs/batch = 0.6195s, grad.norm=0.62490737\n",
      " 17306: 13 [   55/ 1327], train_loss/perplexity = 4.59575939/99.0633316 secs/batch = 0.6028s, grad.norm=0.64250094\n",
      " 17311: 13 [   60/ 1327], train_loss/perplexity = 4.90260029/134.6394196 secs/batch = 0.6041s, grad.norm=0.65705580\n",
      " 17316: 13 [   65/ 1327], train_loss/perplexity = 4.43228912/84.1237640 secs/batch = 0.6157s, grad.norm=0.60343528\n",
      " 17321: 13 [   70/ 1327], train_loss/perplexity = 4.29621983/73.4217224 secs/batch = 0.6019s, grad.norm=0.63629138\n",
      " 17326: 13 [   75/ 1327], train_loss/perplexity = 4.27479315/71.8652725 secs/batch = 0.6101s, grad.norm=0.62924552\n",
      " 17331: 13 [   80/ 1327], train_loss/perplexity = 4.61006975/100.4911575 secs/batch = 0.6090s, grad.norm=0.65351111\n",
      " 17336: 13 [   85/ 1327], train_loss/perplexity = 4.63515759/103.0441513 secs/batch = 0.6171s, grad.norm=0.62508380\n",
      " 17341: 13 [   90/ 1327], train_loss/perplexity = 4.64567232/104.1333542 secs/batch = 0.6018s, grad.norm=0.62380648\n",
      " 17346: 13 [   95/ 1327], train_loss/perplexity = 4.50869036/90.8028183 secs/batch = 0.6090s, grad.norm=0.62325680\n",
      " 17351: 13 [  100/ 1327], train_loss/perplexity = 4.80554152/122.1856384 secs/batch = 0.6073s, grad.norm=0.64529985\n",
      " 17356: 13 [  105/ 1327], train_loss/perplexity = 4.76681519/117.5442886 secs/batch = 0.6054s, grad.norm=0.69029325\n",
      " 17361: 13 [  110/ 1327], train_loss/perplexity = 4.53902626/93.5996170 secs/batch = 0.6071s, grad.norm=0.65709066\n",
      " 17366: 13 [  115/ 1327], train_loss/perplexity = 4.45009947/85.6354599 secs/batch = 0.6223s, grad.norm=0.63115162\n",
      " 17371: 13 [  120/ 1327], train_loss/perplexity = 4.55343628/94.9581528 secs/batch = 0.6124s, grad.norm=0.64488208\n",
      " 17376: 13 [  125/ 1327], train_loss/perplexity = 4.64787722/104.3632126 secs/batch = 0.6047s, grad.norm=0.66396445\n",
      " 17381: 13 [  130/ 1327], train_loss/perplexity = 4.59252214/98.7431641 secs/batch = 0.6138s, grad.norm=0.69787288\n",
      " 17386: 13 [  135/ 1327], train_loss/perplexity = 4.59647322/99.1340714 secs/batch = 0.6088s, grad.norm=0.64164585\n",
      " 17391: 13 [  140/ 1327], train_loss/perplexity = 4.86708021/129.9409637 secs/batch = 0.6066s, grad.norm=0.62230337\n",
      " 17396: 13 [  145/ 1327], train_loss/perplexity = 4.86744213/129.9880066 secs/batch = 0.6077s, grad.norm=0.65599716\n",
      " 17401: 13 [  150/ 1327], train_loss/perplexity = 4.70874453/110.9128265 secs/batch = 0.6133s, grad.norm=0.64229321\n",
      " 17406: 13 [  155/ 1327], train_loss/perplexity = 5.05209684/156.3499603 secs/batch = 0.6109s, grad.norm=0.62569469\n",
      " 17411: 13 [  160/ 1327], train_loss/perplexity = 4.64122581/103.6713486 secs/batch = 0.6107s, grad.norm=0.63156587\n",
      " 17416: 13 [  165/ 1327], train_loss/perplexity = 4.93469715/139.0310364 secs/batch = 0.6175s, grad.norm=0.64299524\n",
      " 17421: 13 [  170/ 1327], train_loss/perplexity = 4.62618113/102.1233215 secs/batch = 0.6147s, grad.norm=0.63765770\n",
      " 17426: 13 [  175/ 1327], train_loss/perplexity = 4.91869164/136.8234863 secs/batch = 0.6100s, grad.norm=0.61292207\n",
      " 17431: 13 [  180/ 1327], train_loss/perplexity = 4.82461691/124.5387497 secs/batch = 0.6124s, grad.norm=0.63459975\n",
      " 17436: 13 [  185/ 1327], train_loss/perplexity = 5.03697252/154.0030670 secs/batch = 0.6062s, grad.norm=0.64635706\n",
      " 17441: 13 [  190/ 1327], train_loss/perplexity = 4.57718086/97.2398758 secs/batch = 0.6025s, grad.norm=0.58537525\n",
      " 17446: 13 [  195/ 1327], train_loss/perplexity = 4.83296156/125.5823288 secs/batch = 0.6094s, grad.norm=0.59348220\n",
      " 17451: 13 [  200/ 1327], train_loss/perplexity = 4.74964285/115.5430145 secs/batch = 0.6070s, grad.norm=0.65747952\n",
      " 17456: 13 [  205/ 1327], train_loss/perplexity = 4.84739447/127.4079895 secs/batch = 0.6081s, grad.norm=0.63667065\n",
      " 17461: 13 [  210/ 1327], train_loss/perplexity = 4.74419117/114.9148178 secs/batch = 0.6119s, grad.norm=0.62610412\n",
      " 17466: 13 [  215/ 1327], train_loss/perplexity = 4.85960770/128.9736023 secs/batch = 0.6152s, grad.norm=0.61778647\n",
      " 17471: 13 [  220/ 1327], train_loss/perplexity = 4.84070349/126.5583496 secs/batch = 0.6078s, grad.norm=0.61581075\n",
      " 17476: 13 [  225/ 1327], train_loss/perplexity = 5.00356388/148.9430237 secs/batch = 0.6076s, grad.norm=0.63159662\n",
      " 17481: 13 [  230/ 1327], train_loss/perplexity = 4.87363958/130.7960968 secs/batch = 0.6090s, grad.norm=0.65745366\n",
      " 17486: 13 [  235/ 1327], train_loss/perplexity = 4.70948362/110.9948273 secs/batch = 0.6087s, grad.norm=0.62004024\n",
      " 17491: 13 [  240/ 1327], train_loss/perplexity = 4.53732347/93.4403687 secs/batch = 0.6045s, grad.norm=0.64651537\n",
      " 17496: 13 [  245/ 1327], train_loss/perplexity = 4.84287214/126.8331146 secs/batch = 0.6180s, grad.norm=0.62671167\n",
      " 17501: 13 [  250/ 1327], train_loss/perplexity = 4.61458683/100.9461136 secs/batch = 0.6074s, grad.norm=0.63194907\n",
      " 17506: 13 [  255/ 1327], train_loss/perplexity = 4.65611506/105.2264862 secs/batch = 0.6027s, grad.norm=0.63510352\n",
      " 17511: 13 [  260/ 1327], train_loss/perplexity = 4.89315939/133.3742828 secs/batch = 0.6099s, grad.norm=0.68714786\n",
      " 17516: 13 [  265/ 1327], train_loss/perplexity = 5.00388956/148.9915466 secs/batch = 0.6100s, grad.norm=0.62645227\n",
      " 17521: 13 [  270/ 1327], train_loss/perplexity = 5.00610065/149.3213501 secs/batch = 0.6019s, grad.norm=0.62352663\n",
      " 17526: 13 [  275/ 1327], train_loss/perplexity = 5.10829353/165.3878784 secs/batch = 0.6120s, grad.norm=0.65378278\n",
      " 17531: 13 [  280/ 1327], train_loss/perplexity = 4.81316566/123.1207581 secs/batch = 0.6067s, grad.norm=0.62284577\n",
      " 17536: 13 [  285/ 1327], train_loss/perplexity = 5.04103708/154.6302948 secs/batch = 0.6046s, grad.norm=0.64966726\n",
      " 17541: 13 [  290/ 1327], train_loss/perplexity = 4.85069704/127.8294601 secs/batch = 0.6108s, grad.norm=0.66848695\n",
      " 17546: 13 [  295/ 1327], train_loss/perplexity = 4.60457993/99.9409943 secs/batch = 0.6100s, grad.norm=0.61947721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17551: 13 [  300/ 1327], train_loss/perplexity = 4.20091248/66.7472076 secs/batch = 0.6081s, grad.norm=0.58804131\n",
      " 17556: 13 [  305/ 1327], train_loss/perplexity = 4.67652369/107.3960800 secs/batch = 0.6002s, grad.norm=0.65689087\n",
      " 17561: 13 [  310/ 1327], train_loss/perplexity = 4.68884850/108.7279053 secs/batch = 0.6094s, grad.norm=0.65006679\n",
      " 17566: 13 [  315/ 1327], train_loss/perplexity = 4.31065559/74.4893112 secs/batch = 0.6079s, grad.norm=0.67115313\n",
      " 17571: 13 [  320/ 1327], train_loss/perplexity = 4.27255011/71.7042542 secs/batch = 0.6161s, grad.norm=0.64040303\n",
      " 17576: 13 [  325/ 1327], train_loss/perplexity = 4.25619268/70.5409012 secs/batch = 0.6055s, grad.norm=0.64803427\n",
      " 17581: 13 [  330/ 1327], train_loss/perplexity = 4.76902580/117.8044205 secs/batch = 0.6161s, grad.norm=0.65974844\n",
      " 17586: 13 [  335/ 1327], train_loss/perplexity = 4.10770178/60.8068085 secs/batch = 0.6115s, grad.norm=0.61114359\n",
      " 17591: 13 [  340/ 1327], train_loss/perplexity = 4.87762928/131.3189697 secs/batch = 0.6136s, grad.norm=0.63694078\n",
      " 17596: 13 [  345/ 1327], train_loss/perplexity = 4.69935274/109.8760300 secs/batch = 0.6139s, grad.norm=0.61009395\n",
      " 17601: 13 [  350/ 1327], train_loss/perplexity = 4.81017494/122.7530899 secs/batch = 0.6111s, grad.norm=0.68199652\n",
      " 17606: 13 [  355/ 1327], train_loss/perplexity = 4.80362892/121.9521713 secs/batch = 0.6068s, grad.norm=0.61652392\n",
      " 17611: 13 [  360/ 1327], train_loss/perplexity = 4.93347216/138.8608246 secs/batch = 0.6528s, grad.norm=0.63102239\n",
      " 17616: 13 [  365/ 1327], train_loss/perplexity = 4.85197592/127.9930420 secs/batch = 0.6154s, grad.norm=0.59921080\n",
      " 17621: 13 [  370/ 1327], train_loss/perplexity = 4.90813494/135.3866730 secs/batch = 0.6368s, grad.norm=0.67261243\n",
      " 17626: 13 [  375/ 1327], train_loss/perplexity = 4.25255823/70.2849884 secs/batch = 0.6159s, grad.norm=0.62573749\n",
      " 17631: 13 [  380/ 1327], train_loss/perplexity = 4.44911909/85.5515442 secs/batch = 0.6141s, grad.norm=0.66816342\n",
      " 17636: 13 [  385/ 1327], train_loss/perplexity = 4.62325525/101.8249588 secs/batch = 0.6162s, grad.norm=0.65509933\n",
      " 17641: 13 [  390/ 1327], train_loss/perplexity = 4.68149424/107.9312286 secs/batch = 0.6084s, grad.norm=0.62471700\n",
      " 17646: 13 [  395/ 1327], train_loss/perplexity = 4.92833996/138.1499939 secs/batch = 0.6097s, grad.norm=0.64852899\n",
      " 17651: 13 [  400/ 1327], train_loss/perplexity = 4.68443823/108.2494431 secs/batch = 0.6050s, grad.norm=0.63061488\n",
      " 17656: 13 [  405/ 1327], train_loss/perplexity = 4.99682045/147.9420166 secs/batch = 0.6135s, grad.norm=0.68132442\n",
      " 17661: 13 [  410/ 1327], train_loss/perplexity = 4.67700958/107.4482727 secs/batch = 0.6050s, grad.norm=0.61590332\n",
      " 17666: 13 [  415/ 1327], train_loss/perplexity = 4.55253410/94.8725204 secs/batch = 0.6098s, grad.norm=0.65894163\n",
      " 17671: 13 [  420/ 1327], train_loss/perplexity = 4.37782812/79.6648254 secs/batch = 0.6128s, grad.norm=0.66660345\n",
      " 17676: 13 [  425/ 1327], train_loss/perplexity = 4.63030624/102.5454636 secs/batch = 0.6171s, grad.norm=0.66745394\n",
      " 17681: 13 [  430/ 1327], train_loss/perplexity = 4.85142994/127.9231796 secs/batch = 0.6060s, grad.norm=0.65989387\n",
      " 17686: 13 [  435/ 1327], train_loss/perplexity = 4.83228874/125.4978638 secs/batch = 0.6146s, grad.norm=0.65888029\n",
      " 17691: 13 [  440/ 1327], train_loss/perplexity = 4.46935940/87.3007812 secs/batch = 0.6188s, grad.norm=0.64646119\n",
      " 17696: 13 [  445/ 1327], train_loss/perplexity = 4.77242947/118.2060699 secs/batch = 0.6096s, grad.norm=0.65408486\n",
      " 17701: 13 [  450/ 1327], train_loss/perplexity = 4.63687325/103.2210922 secs/batch = 0.6131s, grad.norm=0.64307314\n",
      " 17706: 13 [  455/ 1327], train_loss/perplexity = 4.56458282/96.0225296 secs/batch = 0.6206s, grad.norm=0.63521636\n",
      " 17711: 13 [  460/ 1327], train_loss/perplexity = 4.59101963/98.5949097 secs/batch = 0.6186s, grad.norm=0.64676702\n",
      " 17716: 13 [  465/ 1327], train_loss/perplexity = 4.44209146/84.9524307 secs/batch = 0.6141s, grad.norm=0.66465837\n",
      " 17721: 13 [  470/ 1327], train_loss/perplexity = 4.97524214/144.7838745 secs/batch = 0.6080s, grad.norm=0.63853866\n",
      " 17726: 13 [  475/ 1327], train_loss/perplexity = 4.51755190/91.6110535 secs/batch = 0.6208s, grad.norm=0.59175956\n",
      " 17731: 13 [  480/ 1327], train_loss/perplexity = 4.69998121/109.9451065 secs/batch = 0.6125s, grad.norm=0.64858633\n",
      " 17736: 13 [  485/ 1327], train_loss/perplexity = 4.66314697/105.9690399 secs/batch = 0.6142s, grad.norm=0.63850212\n",
      " 17741: 13 [  490/ 1327], train_loss/perplexity = 4.50675392/90.6271591 secs/batch = 0.6200s, grad.norm=0.69629645\n",
      " 17746: 13 [  495/ 1327], train_loss/perplexity = 4.52444649/92.2448502 secs/batch = 0.6104s, grad.norm=0.63491291\n",
      " 17751: 13 [  500/ 1327], train_loss/perplexity = 4.78596592/119.8170395 secs/batch = 0.6083s, grad.norm=0.62476593\n",
      " 17756: 13 [  505/ 1327], train_loss/perplexity = 4.72683001/112.9369812 secs/batch = 0.6100s, grad.norm=0.61882907\n",
      " 17761: 13 [  510/ 1327], train_loss/perplexity = 5.12793446/168.6683655 secs/batch = 0.6083s, grad.norm=0.62134624\n",
      " 17766: 13 [  515/ 1327], train_loss/perplexity = 4.81031179/122.7698898 secs/batch = 0.6198s, grad.norm=0.61652988\n",
      " 17771: 13 [  520/ 1327], train_loss/perplexity = 4.95432806/141.7873077 secs/batch = 0.6183s, grad.norm=0.63321632\n",
      " 17776: 13 [  525/ 1327], train_loss/perplexity = 4.53642416/93.3563766 secs/batch = 0.6120s, grad.norm=0.62902665\n",
      " 17781: 13 [  530/ 1327], train_loss/perplexity = 4.64487696/104.0505600 secs/batch = 0.6074s, grad.norm=0.65977603\n",
      " 17786: 13 [  535/ 1327], train_loss/perplexity = 4.70527363/110.5285263 secs/batch = 0.6115s, grad.norm=0.62850958\n",
      " 17791: 13 [  540/ 1327], train_loss/perplexity = 4.83084249/125.3164978 secs/batch = 0.6127s, grad.norm=0.61840278\n",
      " 17796: 13 [  545/ 1327], train_loss/perplexity = 4.84841156/127.5376434 secs/batch = 0.6127s, grad.norm=0.63466656\n",
      " 17801: 13 [  550/ 1327], train_loss/perplexity = 4.78810072/120.0730972 secs/batch = 0.6105s, grad.norm=0.64835113\n",
      " 17806: 13 [  555/ 1327], train_loss/perplexity = 4.64824772/104.4018860 secs/batch = 0.6127s, grad.norm=0.66674733\n",
      " 17811: 13 [  560/ 1327], train_loss/perplexity = 4.66876936/106.5665131 secs/batch = 0.6487s, grad.norm=0.63485622\n",
      " 17816: 13 [  565/ 1327], train_loss/perplexity = 4.66500998/106.1666412 secs/batch = 0.6357s, grad.norm=0.66073942\n",
      " 17821: 13 [  570/ 1327], train_loss/perplexity = 4.62007952/101.5021057 secs/batch = 0.6057s, grad.norm=0.65697110\n",
      " 17826: 13 [  575/ 1327], train_loss/perplexity = 4.44577312/85.2657700 secs/batch = 0.6110s, grad.norm=0.63761348\n",
      " 17831: 13 [  580/ 1327], train_loss/perplexity = 4.79095173/120.4159164 secs/batch = 0.6144s, grad.norm=0.65705484\n",
      " 17836: 13 [  585/ 1327], train_loss/perplexity = 4.45006323/85.6323547 secs/batch = 0.6114s, grad.norm=0.65017897\n",
      " 17841: 13 [  590/ 1327], train_loss/perplexity = 4.73574305/113.9480972 secs/batch = 0.6152s, grad.norm=0.65335900\n",
      " 17846: 13 [  595/ 1327], train_loss/perplexity = 4.69630957/109.5421677 secs/batch = 0.6129s, grad.norm=0.68159491\n",
      " 17851: 13 [  600/ 1327], train_loss/perplexity = 4.98836088/146.6957703 secs/batch = 0.6039s, grad.norm=0.65752602\n",
      " 17856: 13 [  605/ 1327], train_loss/perplexity = 4.87262726/130.6637573 secs/batch = 0.6076s, grad.norm=0.62259817\n",
      " 17861: 13 [  610/ 1327], train_loss/perplexity = 4.95525312/141.9185181 secs/batch = 0.6159s, grad.norm=0.64502496\n",
      " 17866: 13 [  615/ 1327], train_loss/perplexity = 4.45601082/86.1431808 secs/batch = 0.6088s, grad.norm=0.62179315\n",
      " 17871: 13 [  620/ 1327], train_loss/perplexity = 4.89370537/133.4471283 secs/batch = 0.6083s, grad.norm=0.63789058\n",
      " 17876: 13 [  625/ 1327], train_loss/perplexity = 4.94126272/139.9468536 secs/batch = 0.6106s, grad.norm=0.62179792\n",
      " 17881: 13 [  630/ 1327], train_loss/perplexity = 4.96458626/143.2492676 secs/batch = 0.6063s, grad.norm=0.63659567\n",
      " 17886: 13 [  635/ 1327], train_loss/perplexity = 4.68341684/108.1389389 secs/batch = 0.6109s, grad.norm=0.62999892\n",
      " 17891: 13 [  640/ 1327], train_loss/perplexity = 4.76835680/117.7256393 secs/batch = 0.6117s, grad.norm=0.64285254\n",
      " 17896: 13 [  645/ 1327], train_loss/perplexity = 4.90659761/135.1786957 secs/batch = 0.6098s, grad.norm=0.65318972\n",
      " 17901: 13 [  650/ 1327], train_loss/perplexity = 4.49511003/89.5780258 secs/batch = 0.6110s, grad.norm=0.64328766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17906: 13 [  655/ 1327], train_loss/perplexity = 4.66860819/106.5493393 secs/batch = 0.6114s, grad.norm=0.62974632\n",
      " 17911: 13 [  660/ 1327], train_loss/perplexity = 4.57144117/96.6833496 secs/batch = 0.6138s, grad.norm=0.62416315\n",
      " 17916: 13 [  665/ 1327], train_loss/perplexity = 4.74358654/114.8453598 secs/batch = 0.6057s, grad.norm=0.64894897\n",
      " 17921: 13 [  670/ 1327], train_loss/perplexity = 4.60670853/100.1539536 secs/batch = 0.6146s, grad.norm=0.61407357\n",
      " 17926: 13 [  675/ 1327], train_loss/perplexity = 4.47436190/87.7385941 secs/batch = 0.6104s, grad.norm=0.68365949\n",
      " 17931: 13 [  680/ 1327], train_loss/perplexity = 4.73108912/113.4190216 secs/batch = 0.6111s, grad.norm=0.63318235\n",
      " 17936: 13 [  685/ 1327], train_loss/perplexity = 4.65585995/105.1996460 secs/batch = 0.6031s, grad.norm=0.66126466\n",
      " 17941: 13 [  690/ 1327], train_loss/perplexity = 4.88561106/132.3713226 secs/batch = 0.6075s, grad.norm=0.61205733\n",
      " 17946: 13 [  695/ 1327], train_loss/perplexity = 4.67973900/107.7419510 secs/batch = 0.6144s, grad.norm=0.64765066\n",
      " 17951: 13 [  700/ 1327], train_loss/perplexity = 4.92729855/138.0061951 secs/batch = 0.6062s, grad.norm=0.66528660\n",
      " 17956: 13 [  705/ 1327], train_loss/perplexity = 4.67562628/107.2997437 secs/batch = 0.6529s, grad.norm=0.66412318\n",
      " 17961: 13 [  710/ 1327], train_loss/perplexity = 4.57150412/96.6894302 secs/batch = 0.6104s, grad.norm=0.61563307\n",
      " 17966: 13 [  715/ 1327], train_loss/perplexity = 4.58334303/97.8409348 secs/batch = 0.6085s, grad.norm=0.64250410\n",
      " 17971: 13 [  720/ 1327], train_loss/perplexity = 4.63463640/102.9904633 secs/batch = 0.6111s, grad.norm=0.65370548\n",
      " 17976: 13 [  725/ 1327], train_loss/perplexity = 4.46838331/87.2156067 secs/batch = 0.6134s, grad.norm=0.64014339\n",
      " 17981: 13 [  730/ 1327], train_loss/perplexity = 4.70457029/110.4508133 secs/batch = 0.6094s, grad.norm=0.65162176\n",
      " 17986: 13 [  735/ 1327], train_loss/perplexity = 4.80233383/121.7943344 secs/batch = 0.6134s, grad.norm=0.66310912\n",
      " 17991: 13 [  740/ 1327], train_loss/perplexity = 4.21753407/67.8659286 secs/batch = 0.6056s, grad.norm=0.61636472\n",
      " 17996: 13 [  745/ 1327], train_loss/perplexity = 4.67551422/107.2877197 secs/batch = 0.6036s, grad.norm=0.62237608\n",
      " 18001: 13 [  750/ 1327], train_loss/perplexity = 4.51750612/91.6068573 secs/batch = 0.6089s, grad.norm=0.66095120\n",
      " 18006: 13 [  755/ 1327], train_loss/perplexity = 4.48782635/88.9279404 secs/batch = 0.6061s, grad.norm=0.60136819\n",
      " 18011: 13 [  760/ 1327], train_loss/perplexity = 4.33618498/76.4154587 secs/batch = 0.6087s, grad.norm=0.61802220\n",
      " 18016: 13 [  765/ 1327], train_loss/perplexity = 4.49116135/89.2250061 secs/batch = 0.6095s, grad.norm=0.67234874\n",
      " 18021: 13 [  770/ 1327], train_loss/perplexity = 4.44187260/84.9338379 secs/batch = 0.6041s, grad.norm=0.72259575\n",
      " 18026: 13 [  775/ 1327], train_loss/perplexity = 4.57173300/96.7115631 secs/batch = 0.6082s, grad.norm=0.66947711\n",
      " 18031: 13 [  780/ 1327], train_loss/perplexity = 4.84718084/127.3807755 secs/batch = 0.6085s, grad.norm=0.64953977\n",
      " 18036: 13 [  785/ 1327], train_loss/perplexity = 4.68000221/107.7703094 secs/batch = 0.6130s, grad.norm=0.63633829\n",
      " 18041: 13 [  790/ 1327], train_loss/perplexity = 4.50135803/90.1394577 secs/batch = 0.6100s, grad.norm=0.65752143\n",
      " 18046: 13 [  795/ 1327], train_loss/perplexity = 4.80990696/122.7201996 secs/batch = 0.6127s, grad.norm=0.62199122\n",
      " 18051: 13 [  800/ 1327], train_loss/perplexity = 4.75032330/115.6216583 secs/batch = 0.6092s, grad.norm=0.63807887\n",
      " 18056: 13 [  805/ 1327], train_loss/perplexity = 5.04302645/154.9382172 secs/batch = 0.6252s, grad.norm=0.62794721\n",
      " 18061: 13 [  810/ 1327], train_loss/perplexity = 4.67451715/107.1808014 secs/batch = 0.6139s, grad.norm=0.60845923\n",
      " 18066: 13 [  815/ 1327], train_loss/perplexity = 4.61042452/100.5268173 secs/batch = 0.6123s, grad.norm=0.66364521\n",
      " 18071: 13 [  820/ 1327], train_loss/perplexity = 4.36055136/78.3002930 secs/batch = 0.6108s, grad.norm=0.59434307\n",
      " 18076: 13 [  825/ 1327], train_loss/perplexity = 4.52132273/91.9571533 secs/batch = 0.6113s, grad.norm=0.62091595\n",
      " 18081: 13 [  830/ 1327], train_loss/perplexity = 4.39533854/81.0720749 secs/batch = 0.6156s, grad.norm=0.63927692\n",
      " 18086: 13 [  835/ 1327], train_loss/perplexity = 4.60583973/100.0669785 secs/batch = 0.6132s, grad.norm=0.65582317\n",
      " 18091: 13 [  840/ 1327], train_loss/perplexity = 4.72687817/112.9424210 secs/batch = 0.6110s, grad.norm=0.61682314\n",
      " 18096: 13 [  845/ 1327], train_loss/perplexity = 4.53917408/93.6134491 secs/batch = 0.6101s, grad.norm=0.65681052\n",
      " 18101: 13 [  850/ 1327], train_loss/perplexity = 4.63087416/102.6037140 secs/batch = 0.6149s, grad.norm=0.61746979\n",
      " 18106: 13 [  855/ 1327], train_loss/perplexity = 4.63737392/103.2727890 secs/batch = 0.6202s, grad.norm=0.70631790\n",
      " 18111: 13 [  860/ 1327], train_loss/perplexity = 4.31270647/74.6422348 secs/batch = 0.6141s, grad.norm=0.60355705\n",
      " 18116: 13 [  865/ 1327], train_loss/perplexity = 4.82182741/124.1918335 secs/batch = 0.6074s, grad.norm=0.63748056\n",
      " 18121: 13 [  870/ 1327], train_loss/perplexity = 4.72429562/112.6511230 secs/batch = 0.6148s, grad.norm=0.64055133\n",
      " 18126: 13 [  875/ 1327], train_loss/perplexity = 4.31777096/75.0212173 secs/batch = 0.6130s, grad.norm=0.63409954\n",
      " 18131: 13 [  880/ 1327], train_loss/perplexity = 4.52875853/92.6434784 secs/batch = 0.6137s, grad.norm=0.62207532\n",
      " 18136: 13 [  885/ 1327], train_loss/perplexity = 4.57317495/96.8511200 secs/batch = 0.6114s, grad.norm=0.63374406\n",
      " 18141: 13 [  890/ 1327], train_loss/perplexity = 4.80109644/121.6437225 secs/batch = 0.6156s, grad.norm=0.66378146\n",
      " 18146: 13 [  895/ 1327], train_loss/perplexity = 4.78567410/119.7820816 secs/batch = 0.6058s, grad.norm=0.62657237\n",
      " 18151: 13 [  900/ 1327], train_loss/perplexity = 4.64410448/103.9702148 secs/batch = 0.6119s, grad.norm=0.61200792\n",
      " 18156: 13 [  905/ 1327], train_loss/perplexity = 4.53124523/92.8741379 secs/batch = 0.6102s, grad.norm=0.62246400\n",
      " 18161: 13 [  910/ 1327], train_loss/perplexity = 4.59136248/98.6287155 secs/batch = 0.6104s, grad.norm=0.65111715\n",
      " 18166: 13 [  915/ 1327], train_loss/perplexity = 4.81233454/123.0184708 secs/batch = 0.6138s, grad.norm=0.64660269\n",
      " 18171: 13 [  920/ 1327], train_loss/perplexity = 4.94524813/140.5057068 secs/batch = 0.6068s, grad.norm=0.65391725\n",
      " 18176: 13 [  925/ 1327], train_loss/perplexity = 4.74085045/114.5315628 secs/batch = 0.6132s, grad.norm=0.68090653\n",
      " 18181: 13 [  930/ 1327], train_loss/perplexity = 4.70238686/110.2099152 secs/batch = 0.6060s, grad.norm=0.61968166\n",
      " 18186: 13 [  935/ 1327], train_loss/perplexity = 4.75393009/116.0394363 secs/batch = 0.6120s, grad.norm=0.60881543\n",
      " 18191: 13 [  940/ 1327], train_loss/perplexity = 4.70984459/111.0349045 secs/batch = 0.6130s, grad.norm=0.61488223\n",
      " 18196: 13 [  945/ 1327], train_loss/perplexity = 4.89837694/134.0719910 secs/batch = 0.6155s, grad.norm=0.60204959\n",
      " 18201: 13 [  950/ 1327], train_loss/perplexity = 4.64237642/103.7907028 secs/batch = 0.6459s, grad.norm=0.61282986\n",
      " 18206: 13 [  955/ 1327], train_loss/perplexity = 4.75974703/116.7163925 secs/batch = 0.6232s, grad.norm=0.63695014\n",
      " 18211: 13 [  960/ 1327], train_loss/perplexity = 4.97954321/145.4079437 secs/batch = 0.6061s, grad.norm=0.64496678\n",
      " 18216: 13 [  965/ 1327], train_loss/perplexity = 4.77141809/118.0865784 secs/batch = 0.6101s, grad.norm=0.64095622\n",
      " 18221: 13 [  970/ 1327], train_loss/perplexity = 4.97280788/144.4318695 secs/batch = 0.6111s, grad.norm=0.61390293\n",
      " 18226: 13 [  975/ 1327], train_loss/perplexity = 4.65631056/105.2470627 secs/batch = 0.6095s, grad.norm=0.64507324\n",
      " 18231: 13 [  980/ 1327], train_loss/perplexity = 4.49865484/89.8961258 secs/batch = 0.6100s, grad.norm=0.62278152\n",
      " 18236: 13 [  985/ 1327], train_loss/perplexity = 4.66477013/106.1411819 secs/batch = 0.6138s, grad.norm=0.63014501\n",
      " 18241: 13 [  990/ 1327], train_loss/perplexity = 4.84028101/126.5048981 secs/batch = 0.6070s, grad.norm=0.64655524\n",
      " 18246: 13 [  995/ 1327], train_loss/perplexity = 4.86140299/129.2053528 secs/batch = 0.6088s, grad.norm=0.61335951\n",
      " 18251: 13 [ 1000/ 1327], train_loss/perplexity = 4.36577797/78.7106094 secs/batch = 0.6127s, grad.norm=0.60890472\n",
      " 18256: 13 [ 1005/ 1327], train_loss/perplexity = 4.80419064/122.0206909 secs/batch = 0.6117s, grad.norm=0.64002860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18261: 13 [ 1010/ 1327], train_loss/perplexity = 4.40752983/82.0664978 secs/batch = 0.6146s, grad.norm=0.61809206\n",
      " 18266: 13 [ 1015/ 1327], train_loss/perplexity = 4.88326883/132.0616455 secs/batch = 0.6132s, grad.norm=0.64913821\n",
      " 18271: 13 [ 1020/ 1327], train_loss/perplexity = 5.02769613/152.5810852 secs/batch = 0.6147s, grad.norm=0.65701979\n",
      " 18276: 13 [ 1025/ 1327], train_loss/perplexity = 4.83482409/125.8164520 secs/batch = 0.6113s, grad.norm=0.64482963\n",
      " 18281: 13 [ 1030/ 1327], train_loss/perplexity = 4.64187670/103.7388535 secs/batch = 0.6032s, grad.norm=0.62066752\n",
      " 18286: 13 [ 1035/ 1327], train_loss/perplexity = 4.57222795/96.7594452 secs/batch = 0.6092s, grad.norm=0.62257916\n",
      " 18291: 13 [ 1040/ 1327], train_loss/perplexity = 4.83245850/125.5191727 secs/batch = 0.6043s, grad.norm=0.62086368\n",
      " 18296: 13 [ 1045/ 1327], train_loss/perplexity = 4.42960358/83.8981476 secs/batch = 0.6081s, grad.norm=0.60499650\n",
      " 18301: 13 [ 1050/ 1327], train_loss/perplexity = 4.47228241/87.5563354 secs/batch = 0.6459s, grad.norm=0.63242203\n",
      " 18306: 13 [ 1055/ 1327], train_loss/perplexity = 4.65731287/105.3526077 secs/batch = 0.6090s, grad.norm=0.66449362\n",
      " 18311: 13 [ 1060/ 1327], train_loss/perplexity = 4.27865601/72.1434174 secs/batch = 0.6139s, grad.norm=0.66284811\n",
      " 18316: 13 [ 1065/ 1327], train_loss/perplexity = 4.37628889/79.5422974 secs/batch = 0.6101s, grad.norm=0.61101031\n",
      " 18321: 13 [ 1070/ 1327], train_loss/perplexity = 4.72950983/113.2400436 secs/batch = 0.6163s, grad.norm=0.64247066\n",
      " 18326: 13 [ 1075/ 1327], train_loss/perplexity = 4.47408533/87.7143326 secs/batch = 0.6077s, grad.norm=0.61527228\n",
      " 18331: 13 [ 1080/ 1327], train_loss/perplexity = 4.42771769/83.7400742 secs/batch = 0.6108s, grad.norm=0.61818075\n",
      " 18336: 13 [ 1085/ 1327], train_loss/perplexity = 4.34730768/77.2701492 secs/batch = 0.6126s, grad.norm=0.61209828\n",
      " 18341: 13 [ 1090/ 1327], train_loss/perplexity = 4.51011896/90.9326324 secs/batch = 0.6110s, grad.norm=0.65374118\n",
      " 18346: 13 [ 1095/ 1327], train_loss/perplexity = 4.64428425/103.9889069 secs/batch = 0.6116s, grad.norm=0.64844406\n",
      " 18351: 13 [ 1100/ 1327], train_loss/perplexity = 4.43115711/84.0285950 secs/batch = 0.6048s, grad.norm=0.66711259\n",
      " 18356: 13 [ 1105/ 1327], train_loss/perplexity = 4.44580412/85.2684174 secs/batch = 0.6150s, grad.norm=0.64252859\n",
      " 18361: 13 [ 1110/ 1327], train_loss/perplexity = 4.83889437/126.3295975 secs/batch = 0.6051s, grad.norm=0.70028949\n",
      " 18366: 13 [ 1115/ 1327], train_loss/perplexity = 4.43934870/84.7197418 secs/batch = 0.6059s, grad.norm=0.58793461\n",
      " 18371: 13 [ 1120/ 1327], train_loss/perplexity = 4.67214489/106.9268417 secs/batch = 0.6085s, grad.norm=0.64575225\n",
      " 18376: 13 [ 1125/ 1327], train_loss/perplexity = 4.89878368/134.1265411 secs/batch = 0.6117s, grad.norm=0.65118587\n",
      " 18381: 13 [ 1130/ 1327], train_loss/perplexity = 4.59170914/98.6629181 secs/batch = 0.6153s, grad.norm=0.63521302\n",
      " 18386: 13 [ 1135/ 1327], train_loss/perplexity = 4.61608839/101.0978012 secs/batch = 0.6111s, grad.norm=0.60586631\n",
      " 18391: 13 [ 1140/ 1327], train_loss/perplexity = 4.82997704/125.2080841 secs/batch = 0.6076s, grad.norm=0.64546162\n",
      " 18396: 13 [ 1145/ 1327], train_loss/perplexity = 4.60837317/100.3208084 secs/batch = 0.6229s, grad.norm=0.63930303\n",
      " 18401: 13 [ 1150/ 1327], train_loss/perplexity = 4.63066292/102.5820465 secs/batch = 0.6116s, grad.norm=0.63116246\n",
      " 18406: 13 [ 1155/ 1327], train_loss/perplexity = 4.69901466/109.8388901 secs/batch = 0.6098s, grad.norm=0.65501142\n",
      " 18411: 13 [ 1160/ 1327], train_loss/perplexity = 4.62394524/101.8952408 secs/batch = 0.6095s, grad.norm=0.65094042\n",
      " 18416: 13 [ 1165/ 1327], train_loss/perplexity = 4.72720718/112.9795914 secs/batch = 0.6155s, grad.norm=0.68187422\n",
      " 18421: 13 [ 1170/ 1327], train_loss/perplexity = 4.53989744/93.6811905 secs/batch = 0.6173s, grad.norm=0.63679916\n",
      " 18426: 13 [ 1175/ 1327], train_loss/perplexity = 4.34627295/77.1902313 secs/batch = 0.6096s, grad.norm=0.68928617\n",
      " 18431: 13 [ 1180/ 1327], train_loss/perplexity = 4.31228638/74.6108856 secs/batch = 0.6140s, grad.norm=0.64251298\n",
      " 18436: 13 [ 1185/ 1327], train_loss/perplexity = 4.54157162/93.8381653 secs/batch = 0.6149s, grad.norm=0.63265204\n",
      " 18441: 13 [ 1190/ 1327], train_loss/perplexity = 4.61601973/101.0908585 secs/batch = 0.6104s, grad.norm=0.62270129\n",
      " 18446: 13 [ 1195/ 1327], train_loss/perplexity = 4.42080736/83.1633987 secs/batch = 0.6198s, grad.norm=0.63992548\n",
      " 18451: 13 [ 1200/ 1327], train_loss/perplexity = 4.39544725/81.0808868 secs/batch = 0.6133s, grad.norm=0.65857613\n",
      " 18456: 13 [ 1205/ 1327], train_loss/perplexity = 4.44654179/85.3313370 secs/batch = 0.6151s, grad.norm=0.65740722\n",
      " 18461: 13 [ 1210/ 1327], train_loss/perplexity = 4.14385271/63.0452499 secs/batch = 0.6162s, grad.norm=0.66240352\n",
      " 18466: 13 [ 1215/ 1327], train_loss/perplexity = 4.33108473/76.0267105 secs/batch = 0.6124s, grad.norm=0.61550033\n",
      " 18471: 13 [ 1220/ 1327], train_loss/perplexity = 4.44712973/85.3815231 secs/batch = 0.6124s, grad.norm=0.65999448\n",
      " 18476: 13 [ 1225/ 1327], train_loss/perplexity = 4.29157639/73.0815811 secs/batch = 0.6136s, grad.norm=0.67429000\n",
      " 18481: 13 [ 1230/ 1327], train_loss/perplexity = 4.46300507/86.7478027 secs/batch = 0.6088s, grad.norm=0.61540800\n",
      " 18486: 13 [ 1235/ 1327], train_loss/perplexity = 4.44586563/85.2736588 secs/batch = 0.6057s, grad.norm=0.64226174\n",
      " 18491: 13 [ 1240/ 1327], train_loss/perplexity = 4.65138340/104.7297668 secs/batch = 0.6042s, grad.norm=0.62466520\n",
      " 18496: 13 [ 1245/ 1327], train_loss/perplexity = 4.53214169/92.9574356 secs/batch = 0.6216s, grad.norm=0.62322283\n",
      " 18501: 13 [ 1250/ 1327], train_loss/perplexity = 4.62976837/102.4903183 secs/batch = 0.6170s, grad.norm=0.59454060\n",
      " 18506: 13 [ 1255/ 1327], train_loss/perplexity = 4.63484764/103.0122223 secs/batch = 0.6148s, grad.norm=0.62060094\n",
      " 18511: 13 [ 1260/ 1327], train_loss/perplexity = 4.52304983/92.1161118 secs/batch = 0.6083s, grad.norm=0.66877967\n",
      " 18516: 13 [ 1265/ 1327], train_loss/perplexity = 4.66687918/106.3652802 secs/batch = 0.6074s, grad.norm=0.68391335\n",
      " 18521: 13 [ 1270/ 1327], train_loss/perplexity = 4.50417566/90.3937988 secs/batch = 0.6073s, grad.norm=0.64878130\n",
      " 18526: 13 [ 1275/ 1327], train_loss/perplexity = 4.61656475/101.1459732 secs/batch = 0.6079s, grad.norm=0.64651972\n",
      " 18531: 13 [ 1280/ 1327], train_loss/perplexity = 4.45842171/86.3511124 secs/batch = 0.6091s, grad.norm=0.65855855\n",
      " 18536: 13 [ 1285/ 1327], train_loss/perplexity = 4.42402649/83.4315491 secs/batch = 0.6076s, grad.norm=0.64144701\n",
      " 18541: 13 [ 1290/ 1327], train_loss/perplexity = 4.60367298/99.8503952 secs/batch = 0.6147s, grad.norm=0.63901395\n",
      " 18546: 13 [ 1295/ 1327], train_loss/perplexity = 4.65889883/105.5198212 secs/batch = 0.6082s, grad.norm=0.61364126\n",
      " 18551: 13 [ 1300/ 1327], train_loss/perplexity = 4.75028181/115.6168594 secs/batch = 0.6070s, grad.norm=0.61735910\n",
      " 18556: 13 [ 1305/ 1327], train_loss/perplexity = 4.90192270/134.5482330 secs/batch = 0.6086s, grad.norm=0.68221313\n",
      " 18561: 13 [ 1310/ 1327], train_loss/perplexity = 5.12688732/168.4918365 secs/batch = 0.6070s, grad.norm=0.62368512\n",
      " 18566: 13 [ 1315/ 1327], train_loss/perplexity = 4.97767448/145.1364746 secs/batch = 0.6111s, grad.norm=0.68749493\n",
      " 18571: 13 [ 1320/ 1327], train_loss/perplexity = 4.90724993/135.2669067 secs/batch = 0.6047s, grad.norm=0.64415413\n",
      " 18576: 13 [ 1325/ 1327], train_loss/perplexity = 4.84143305/126.6507187 secs/batch = 0.6122s, grad.norm=0.62046504\n",
      "Epoch training time: 812.561331987381\n",
      "Saved char model cv/epoch013_4.7427.model\n",
      " 18583: 14 [    5/ 1327], train_loss/perplexity = 4.89514637/133.6395721 secs/batch = 0.6133s, grad.norm=0.66292518\n",
      " 18588: 14 [   10/ 1327], train_loss/perplexity = 4.46201658/86.6620941 secs/batch = 0.6212s, grad.norm=0.66697747\n",
      " 18593: 14 [   15/ 1327], train_loss/perplexity = 4.68792677/108.6277390 secs/batch = 0.6593s, grad.norm=0.62754369\n",
      " 18598: 14 [   20/ 1327], train_loss/perplexity = 4.87551594/131.0417480 secs/batch = 0.6145s, grad.norm=0.64132947\n",
      " 18603: 14 [   25/ 1327], train_loss/perplexity = 4.81653023/123.5357056 secs/batch = 0.6037s, grad.norm=0.66679311\n",
      " 18608: 14 [   30/ 1327], train_loss/perplexity = 4.75104713/115.7053833 secs/batch = 0.6093s, grad.norm=0.65012443\n",
      " 18613: 14 [   35/ 1327], train_loss/perplexity = 4.55310965/94.9271393 secs/batch = 0.6180s, grad.norm=0.62062401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18618: 14 [   40/ 1327], train_loss/perplexity = 4.63101578/102.6182480 secs/batch = 0.5970s, grad.norm=0.63538647\n",
      " 18623: 14 [   45/ 1327], train_loss/perplexity = 4.36407137/78.5764008 secs/batch = 0.6160s, grad.norm=0.60332566\n",
      " 18628: 14 [   50/ 1327], train_loss/perplexity = 4.60941410/100.4252930 secs/batch = 0.6105s, grad.norm=0.62056398\n",
      " 18633: 14 [   55/ 1327], train_loss/perplexity = 4.51814508/91.6654053 secs/batch = 0.6111s, grad.norm=0.64741695\n",
      " 18638: 14 [   60/ 1327], train_loss/perplexity = 4.81689024/123.5801849 secs/batch = 0.6099s, grad.norm=0.68121278\n",
      " 18643: 14 [   65/ 1327], train_loss/perplexity = 4.36403275/78.5733643 secs/batch = 0.6063s, grad.norm=0.63516873\n",
      " 18648: 14 [   70/ 1327], train_loss/perplexity = 4.24533844/69.7793732 secs/batch = 0.6098s, grad.norm=0.64621180\n",
      " 18653: 14 [   75/ 1327], train_loss/perplexity = 4.21094465/67.4201965 secs/batch = 0.6160s, grad.norm=0.64020497\n",
      " 18658: 14 [   80/ 1327], train_loss/perplexity = 4.54414988/94.0804138 secs/batch = 0.6123s, grad.norm=0.66552293\n",
      " 18663: 14 [   85/ 1327], train_loss/perplexity = 4.57905006/97.4218063 secs/batch = 0.6117s, grad.norm=0.63161832\n",
      " 18668: 14 [   90/ 1327], train_loss/perplexity = 4.59455967/98.9445572 secs/batch = 0.6162s, grad.norm=0.65192646\n",
      " 18673: 14 [   95/ 1327], train_loss/perplexity = 4.45180893/85.7819748 secs/batch = 0.6136s, grad.norm=0.62069219\n",
      " 18678: 14 [  100/ 1327], train_loss/perplexity = 4.75067806/115.6626816 secs/batch = 0.6108s, grad.norm=0.66869342\n",
      " 18683: 14 [  105/ 1327], train_loss/perplexity = 4.68690729/108.5170517 secs/batch = 0.6090s, grad.norm=0.68006200\n",
      " 18688: 14 [  110/ 1327], train_loss/perplexity = 4.47431469/87.7344513 secs/batch = 0.6048s, grad.norm=0.65982205\n",
      " 18693: 14 [  115/ 1327], train_loss/perplexity = 4.40342188/81.7300644 secs/batch = 0.6520s, grad.norm=0.65262800\n",
      " 18698: 14 [  120/ 1327], train_loss/perplexity = 4.52437496/92.2382584 secs/batch = 0.6134s, grad.norm=0.64222157\n",
      " 18703: 14 [  125/ 1327], train_loss/perplexity = 4.62114620/101.6104279 secs/batch = 0.6157s, grad.norm=0.67966825\n",
      " 18708: 14 [  130/ 1327], train_loss/perplexity = 4.53785801/93.4903336 secs/batch = 0.6081s, grad.norm=0.66466200\n",
      " 18713: 14 [  135/ 1327], train_loss/perplexity = 4.49481249/89.5513763 secs/batch = 0.6151s, grad.norm=0.61997968\n",
      " 18718: 14 [  140/ 1327], train_loss/perplexity = 4.86054850/129.0949860 secs/batch = 0.6074s, grad.norm=0.63706243\n",
      " 18723: 14 [  145/ 1327], train_loss/perplexity = 4.79673672/121.1145401 secs/batch = 0.6108s, grad.norm=0.65554714\n",
      " 18728: 14 [  150/ 1327], train_loss/perplexity = 4.71831179/111.9790497 secs/batch = 0.6191s, grad.norm=0.67425287\n",
      " 18733: 14 [  155/ 1327], train_loss/perplexity = 4.99271441/147.3358154 secs/batch = 0.6121s, grad.norm=0.64044464\n",
      " 18738: 14 [  160/ 1327], train_loss/perplexity = 4.62686396/102.1930771 secs/batch = 0.6080s, grad.norm=0.64101523\n",
      " 18743: 14 [  165/ 1327], train_loss/perplexity = 4.87621546/131.1334381 secs/batch = 0.6159s, grad.norm=0.63284141\n",
      " 18748: 14 [  170/ 1327], train_loss/perplexity = 4.55997753/95.5813293 secs/batch = 0.6151s, grad.norm=0.64272964\n",
      " 18753: 14 [  175/ 1327], train_loss/perplexity = 4.87603998/131.1104279 secs/batch = 0.6118s, grad.norm=0.63102925\n",
      " 18758: 14 [  180/ 1327], train_loss/perplexity = 4.78846216/120.1165085 secs/batch = 0.6080s, grad.norm=0.63974863\n",
      " 18763: 14 [  185/ 1327], train_loss/perplexity = 4.97831869/145.2299957 secs/batch = 0.6105s, grad.norm=0.64298135\n",
      " 18768: 14 [  190/ 1327], train_loss/perplexity = 4.51704264/91.5644073 secs/batch = 0.6086s, grad.norm=0.61091381\n",
      " 18773: 14 [  195/ 1327], train_loss/perplexity = 4.78298473/119.4603729 secs/batch = 0.6097s, grad.norm=0.61843729\n",
      " 18778: 14 [  200/ 1327], train_loss/perplexity = 4.66872263/106.5615387 secs/batch = 0.6104s, grad.norm=0.65536296\n",
      " 18783: 14 [  205/ 1327], train_loss/perplexity = 4.83000135/125.2111282 secs/batch = 0.6082s, grad.norm=0.66354018\n",
      " 18788: 14 [  210/ 1327], train_loss/perplexity = 4.72063589/112.2396011 secs/batch = 0.6180s, grad.norm=0.62779039\n",
      " 18793: 14 [  215/ 1327], train_loss/perplexity = 4.83809423/126.2285614 secs/batch = 0.6247s, grad.norm=0.61477512\n",
      " 18798: 14 [  220/ 1327], train_loss/perplexity = 4.78038359/119.1500473 secs/batch = 0.6088s, grad.norm=0.64464700\n",
      " 18803: 14 [  225/ 1327], train_loss/perplexity = 4.98042011/145.5355072 secs/batch = 0.6045s, grad.norm=0.65067679\n",
      " 18808: 14 [  230/ 1327], train_loss/perplexity = 4.80945826/122.6651459 secs/batch = 0.6122s, grad.norm=0.68334860\n",
      " 18813: 14 [  235/ 1327], train_loss/perplexity = 4.65330410/104.9311142 secs/batch = 0.6168s, grad.norm=0.62560791\n",
      " 18818: 14 [  240/ 1327], train_loss/perplexity = 4.47676849/87.9500046 secs/batch = 0.6080s, grad.norm=0.68555832\n",
      " 18823: 14 [  245/ 1327], train_loss/perplexity = 4.83064365/125.2915802 secs/batch = 0.6126s, grad.norm=0.65507156\n",
      " 18828: 14 [  250/ 1327], train_loss/perplexity = 4.52198458/92.0180359 secs/batch = 0.6018s, grad.norm=0.61028004\n",
      " 18833: 14 [  255/ 1327], train_loss/perplexity = 4.55484247/95.0917740 secs/batch = 0.6045s, grad.norm=0.60624671\n",
      " 18838: 14 [  260/ 1327], train_loss/perplexity = 4.82107449/124.0983582 secs/batch = 0.6116s, grad.norm=0.66152620\n",
      " 18843: 14 [  265/ 1327], train_loss/perplexity = 4.97316837/144.4839478 secs/batch = 0.6065s, grad.norm=0.62447232\n",
      " 18848: 14 [  270/ 1327], train_loss/perplexity = 4.98482656/146.1782227 secs/batch = 0.6092s, grad.norm=0.65999103\n",
      " 18853: 14 [  275/ 1327], train_loss/perplexity = 4.99824190/148.1524658 secs/batch = 0.6100s, grad.norm=0.65133756\n",
      " 18858: 14 [  280/ 1327], train_loss/perplexity = 4.83077669/125.3082504 secs/batch = 0.6070s, grad.norm=0.63699043\n",
      " 18863: 14 [  285/ 1327], train_loss/perplexity = 4.98390055/146.0429230 secs/batch = 0.6158s, grad.norm=0.65397620\n",
      " 18868: 14 [  290/ 1327], train_loss/perplexity = 4.79690027/121.1343536 secs/batch = 0.6077s, grad.norm=0.68653202\n",
      " 18873: 14 [  295/ 1327], train_loss/perplexity = 4.56650972/96.2077332 secs/batch = 0.6082s, grad.norm=0.64774168\n",
      " 18878: 14 [  300/ 1327], train_loss/perplexity = 4.16931009/64.6708221 secs/batch = 0.6133s, grad.norm=0.62200040\n",
      " 18883: 14 [  305/ 1327], train_loss/perplexity = 4.62053490/101.5483322 secs/batch = 0.6116s, grad.norm=0.66391689\n",
      " 18888: 14 [  310/ 1327], train_loss/perplexity = 4.58061743/97.5746231 secs/batch = 0.6245s, grad.norm=0.64986384\n",
      " 18893: 14 [  315/ 1327], train_loss/perplexity = 4.27040052/71.5502853 secs/batch = 0.6137s, grad.norm=0.62462610\n",
      " 18898: 14 [  320/ 1327], train_loss/perplexity = 4.22642279/68.4718552 secs/batch = 0.6086s, grad.norm=0.68268156\n",
      " 18903: 14 [  325/ 1327], train_loss/perplexity = 4.17995691/65.3630371 secs/batch = 0.6134s, grad.norm=0.63340807\n",
      " 18908: 14 [  330/ 1327], train_loss/perplexity = 4.68983030/108.8347092 secs/batch = 0.6085s, grad.norm=0.65340668\n",
      " 18913: 14 [  335/ 1327], train_loss/perplexity = 4.09084845/59.7905998 secs/batch = 0.6114s, grad.norm=0.60986435\n",
      " 18918: 14 [  340/ 1327], train_loss/perplexity = 4.82192516/124.2039719 secs/batch = 0.6115s, grad.norm=0.65331650\n",
      " 18923: 14 [  345/ 1327], train_loss/perplexity = 4.70023155/109.9726334 secs/batch = 0.6125s, grad.norm=0.62436062\n",
      " 18928: 14 [  350/ 1327], train_loss/perplexity = 4.72368670/112.5825424 secs/batch = 0.6120s, grad.norm=0.66128671\n",
      " 18933: 14 [  355/ 1327], train_loss/perplexity = 4.71632624/111.7569275 secs/batch = 0.6069s, grad.norm=0.62483591\n",
      " 18938: 14 [  360/ 1327], train_loss/perplexity = 4.87065172/130.4058838 secs/batch = 0.6104s, grad.norm=0.65387279\n",
      " 18943: 14 [  365/ 1327], train_loss/perplexity = 4.80088091/121.6175079 secs/batch = 0.6103s, grad.norm=0.62589508\n",
      " 18948: 14 [  370/ 1327], train_loss/perplexity = 4.84954262/127.6819763 secs/batch = 0.6017s, grad.norm=0.66159213\n",
      " 18953: 14 [  375/ 1327], train_loss/perplexity = 4.21100235/67.4240875 secs/batch = 0.6196s, grad.norm=0.64472318\n",
      " 18958: 14 [  380/ 1327], train_loss/perplexity = 4.39269066/80.8576889 secs/batch = 0.6123s, grad.norm=0.68142056\n",
      " 18963: 14 [  385/ 1327], train_loss/perplexity = 4.61402416/100.8893280 secs/batch = 0.6220s, grad.norm=0.63663012\n",
      " 18968: 14 [  390/ 1327], train_loss/perplexity = 4.64359140/103.9168854 secs/batch = 0.6093s, grad.norm=0.61736113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18973: 14 [  395/ 1327], train_loss/perplexity = 4.79165030/120.5000687 secs/batch = 0.6171s, grad.norm=0.63503844\n",
      " 18978: 14 [  400/ 1327], train_loss/perplexity = 4.63926458/103.4682236 secs/batch = 0.6182s, grad.norm=0.65267181\n",
      " 18983: 14 [  405/ 1327], train_loss/perplexity = 4.94768810/140.8489532 secs/batch = 0.6103s, grad.norm=0.67171550\n",
      " 18988: 14 [  410/ 1327], train_loss/perplexity = 4.62628126/102.1335526 secs/batch = 0.6427s, grad.norm=0.61224735\n",
      " 18993: 14 [  415/ 1327], train_loss/perplexity = 4.48462009/88.6432648 secs/batch = 0.6122s, grad.norm=0.66306365\n",
      " 18998: 14 [  420/ 1327], train_loss/perplexity = 4.28220129/72.3996353 secs/batch = 0.6100s, grad.norm=0.65047181\n",
      " 19003: 14 [  425/ 1327], train_loss/perplexity = 4.54265976/93.9403305 secs/batch = 0.6180s, grad.norm=0.63412482\n",
      " 19008: 14 [  430/ 1327], train_loss/perplexity = 4.76627636/117.4809723 secs/batch = 0.6124s, grad.norm=0.64772218\n",
      " 19013: 14 [  435/ 1327], train_loss/perplexity = 4.78130770/119.2602081 secs/batch = 0.6068s, grad.norm=0.65364218\n",
      " 19018: 14 [  440/ 1327], train_loss/perplexity = 4.44357300/85.0783844 secs/batch = 0.6125s, grad.norm=0.67535639\n",
      " 19023: 14 [  445/ 1327], train_loss/perplexity = 4.67577076/107.3152466 secs/batch = 0.6066s, grad.norm=0.67319024\n",
      " 19028: 14 [  450/ 1327], train_loss/perplexity = 4.60431385/99.9144058 secs/batch = 0.6100s, grad.norm=0.68309200\n",
      " 19033: 14 [  455/ 1327], train_loss/perplexity = 4.46411037/86.8437347 secs/batch = 0.6152s, grad.norm=0.66782981\n",
      " 19038: 14 [  460/ 1327], train_loss/perplexity = 4.61185598/100.6708221 secs/batch = 0.6126s, grad.norm=0.66369867\n",
      " 19043: 14 [  465/ 1327], train_loss/perplexity = 4.42159748/83.2291336 secs/batch = 0.6085s, grad.norm=0.66208535\n",
      " 19048: 14 [  470/ 1327], train_loss/perplexity = 4.96426773/143.2036438 secs/batch = 0.6092s, grad.norm=0.65580493\n",
      " 19053: 14 [  475/ 1327], train_loss/perplexity = 4.47896481/88.1433792 secs/batch = 0.5982s, grad.norm=0.59944499\n",
      " 19058: 14 [  480/ 1327], train_loss/perplexity = 4.61534929/101.0231094 secs/batch = 0.6205s, grad.norm=0.66057938\n",
      " 19063: 14 [  485/ 1327], train_loss/perplexity = 4.55457687/95.0665207 secs/batch = 0.6165s, grad.norm=0.63878214\n",
      " 19068: 14 [  490/ 1327], train_loss/perplexity = 4.45001793/85.6284790 secs/batch = 0.6193s, grad.norm=0.67645037\n",
      " 19073: 14 [  495/ 1327], train_loss/perplexity = 4.46800900/87.1829681 secs/batch = 0.6086s, grad.norm=0.64575231\n",
      " 19078: 14 [  500/ 1327], train_loss/perplexity = 4.75220442/115.8393631 secs/batch = 0.6091s, grad.norm=0.62879831\n",
      " 19083: 14 [  505/ 1327], train_loss/perplexity = 4.66265011/105.9163971 secs/batch = 0.6086s, grad.norm=0.60767615\n",
      " 19088: 14 [  510/ 1327], train_loss/perplexity = 5.09653187/163.4540405 secs/batch = 0.6058s, grad.norm=0.62746334\n",
      " 19093: 14 [  515/ 1327], train_loss/perplexity = 4.74234772/114.7031784 secs/batch = 0.6085s, grad.norm=0.62707061\n",
      " 19098: 14 [  520/ 1327], train_loss/perplexity = 4.88878632/132.7923126 secs/batch = 0.6085s, grad.norm=0.64851308\n",
      " 19103: 14 [  525/ 1327], train_loss/perplexity = 4.46237421/86.6930923 secs/batch = 0.6057s, grad.norm=0.62831765\n",
      " 19108: 14 [  530/ 1327], train_loss/perplexity = 4.57928514/97.4447098 secs/batch = 0.6143s, grad.norm=0.66481709\n",
      " 19113: 14 [  535/ 1327], train_loss/perplexity = 4.66642904/106.3174057 secs/batch = 0.6119s, grad.norm=0.65609419\n",
      " 19118: 14 [  540/ 1327], train_loss/perplexity = 4.73750401/114.1489334 secs/batch = 0.6153s, grad.norm=0.63069475\n",
      " 19123: 14 [  545/ 1327], train_loss/perplexity = 4.83214092/125.4793167 secs/batch = 0.6050s, grad.norm=0.64679229\n",
      " 19128: 14 [  550/ 1327], train_loss/perplexity = 4.71148109/111.2167587 secs/batch = 0.6162s, grad.norm=0.63987547\n",
      " 19133: 14 [  555/ 1327], train_loss/perplexity = 4.57765436/97.2859268 secs/batch = 0.6474s, grad.norm=0.63144237\n",
      " 19138: 14 [  560/ 1327], train_loss/perplexity = 4.61923742/101.4166641 secs/batch = 0.6074s, grad.norm=0.67288542\n",
      " 19143: 14 [  565/ 1327], train_loss/perplexity = 4.59875441/99.3604736 secs/batch = 0.6083s, grad.norm=0.66152948\n",
      " 19148: 14 [  570/ 1327], train_loss/perplexity = 4.57906866/97.4236145 secs/batch = 0.6078s, grad.norm=0.68428326\n",
      " 19153: 14 [  575/ 1327], train_loss/perplexity = 4.34078312/76.7676315 secs/batch = 0.6075s, grad.norm=0.64315784\n",
      " 19158: 14 [  580/ 1327], train_loss/perplexity = 4.73632812/114.0147858 secs/batch = 0.6145s, grad.norm=0.65548581\n",
      " 19163: 14 [  585/ 1327], train_loss/perplexity = 4.41136408/82.3817596 secs/batch = 0.6129s, grad.norm=0.68444842\n",
      " 19168: 14 [  590/ 1327], train_loss/perplexity = 4.66124773/105.7679672 secs/batch = 0.6097s, grad.norm=0.63990468\n",
      " 19173: 14 [  595/ 1327], train_loss/perplexity = 4.63250446/102.7711258 secs/batch = 0.6122s, grad.norm=0.67958820\n",
      " 19178: 14 [  600/ 1327], train_loss/perplexity = 4.89020920/132.9813843 secs/batch = 0.6038s, grad.norm=0.64359158\n",
      " 19183: 14 [  605/ 1327], train_loss/perplexity = 4.79431343/120.8214035 secs/batch = 0.6197s, grad.norm=0.64437461\n",
      " 19188: 14 [  610/ 1327], train_loss/perplexity = 4.85188293/127.9811401 secs/batch = 0.6220s, grad.norm=0.66835868\n",
      " 19193: 14 [  615/ 1327], train_loss/perplexity = 4.42133665/83.2074280 secs/batch = 0.6059s, grad.norm=0.63889802\n",
      " 19198: 14 [  620/ 1327], train_loss/perplexity = 4.80056620/121.5792389 secs/batch = 0.6155s, grad.norm=0.65024197\n",
      " 19203: 14 [  625/ 1327], train_loss/perplexity = 4.87887764/131.4830170 secs/batch = 0.6057s, grad.norm=0.63407040\n",
      " 19208: 14 [  630/ 1327], train_loss/perplexity = 4.92859459/138.1851654 secs/batch = 0.6111s, grad.norm=0.65275079\n",
      " 19213: 14 [  635/ 1327], train_loss/perplexity = 4.64904308/104.4849548 secs/batch = 0.6075s, grad.norm=0.64893407\n",
      " 19218: 14 [  640/ 1327], train_loss/perplexity = 4.70807076/110.8381195 secs/batch = 0.6096s, grad.norm=0.62759227\n",
      " 19223: 14 [  645/ 1327], train_loss/perplexity = 4.88355541/132.0995026 secs/batch = 0.6116s, grad.norm=0.69988286\n",
      " 19228: 14 [  650/ 1327], train_loss/perplexity = 4.49886036/89.9146042 secs/batch = 0.6088s, grad.norm=0.66533643\n",
      " 19233: 14 [  655/ 1327], train_loss/perplexity = 4.62146854/101.6431885 secs/batch = 0.6536s, grad.norm=0.64093399\n",
      " 19238: 14 [  660/ 1327], train_loss/perplexity = 4.49361897/89.4445572 secs/batch = 0.6125s, grad.norm=0.62401402\n",
      " 19243: 14 [  665/ 1327], train_loss/perplexity = 4.72231007/112.4276657 secs/batch = 0.6088s, grad.norm=0.68088841\n",
      " 19248: 14 [  670/ 1327], train_loss/perplexity = 4.62009001/101.5031662 secs/batch = 0.6073s, grad.norm=0.62888414\n",
      " 19253: 14 [  675/ 1327], train_loss/perplexity = 4.42221928/83.2809067 secs/batch = 0.6106s, grad.norm=0.68531525\n",
      " 19258: 14 [  680/ 1327], train_loss/perplexity = 4.65038872/104.6256485 secs/batch = 0.6096s, grad.norm=0.66424876\n",
      " 19263: 14 [  685/ 1327], train_loss/perplexity = 4.55576754/95.1797791 secs/batch = 0.6126s, grad.norm=0.66633236\n",
      " 19268: 14 [  690/ 1327], train_loss/perplexity = 4.84175301/126.6912460 secs/batch = 0.6083s, grad.norm=0.65679902\n",
      " 19273: 14 [  695/ 1327], train_loss/perplexity = 4.63589239/103.1199036 secs/batch = 0.6102s, grad.norm=0.65788412\n",
      " 19278: 14 [  700/ 1327], train_loss/perplexity = 4.92329645/137.4549866 secs/batch = 0.6117s, grad.norm=0.70621938\n",
      " 19283: 14 [  705/ 1327], train_loss/perplexity = 4.58033991/97.5475464 secs/batch = 0.6093s, grad.norm=0.65526563\n",
      " 19288: 14 [  710/ 1327], train_loss/perplexity = 4.55676746/95.2750015 secs/batch = 0.6175s, grad.norm=0.66428924\n",
      " 19293: 14 [  715/ 1327], train_loss/perplexity = 4.54014921/93.7047806 secs/batch = 0.6140s, grad.norm=0.65632218\n",
      " 19298: 14 [  720/ 1327], train_loss/perplexity = 4.52168369/91.9903488 secs/batch = 0.6168s, grad.norm=0.65245241\n",
      " 19303: 14 [  725/ 1327], train_loss/perplexity = 4.44182777/84.9300308 secs/batch = 0.6117s, grad.norm=0.64667350\n",
      " 19308: 14 [  730/ 1327], train_loss/perplexity = 4.61923647/101.4165649 secs/batch = 0.6120s, grad.norm=0.64862078\n",
      " 19313: 14 [  735/ 1327], train_loss/perplexity = 4.70921469/110.9649811 secs/batch = 0.6130s, grad.norm=0.65513188\n",
      " 19318: 14 [  740/ 1327], train_loss/perplexity = 4.16292048/64.2589188 secs/batch = 0.6063s, grad.norm=0.60365921\n",
      " 19323: 14 [  745/ 1327], train_loss/perplexity = 4.59274960/98.7656250 secs/batch = 0.6109s, grad.norm=0.63049853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19328: 14 [  750/ 1327], train_loss/perplexity = 4.46900702/87.2700195 secs/batch = 0.6252s, grad.norm=0.68099213\n",
      " 19333: 14 [  755/ 1327], train_loss/perplexity = 4.45487738/86.0456009 secs/batch = 0.6123s, grad.norm=0.63724774\n",
      " 19338: 14 [  760/ 1327], train_loss/perplexity = 4.33179331/76.0805969 secs/batch = 0.6211s, grad.norm=0.62579179\n",
      " 19343: 14 [  765/ 1327], train_loss/perplexity = 4.44314194/85.0417175 secs/batch = 0.6137s, grad.norm=0.68449247\n",
      " 19348: 14 [  770/ 1327], train_loss/perplexity = 4.37480068/79.4240036 secs/batch = 0.6083s, grad.norm=0.70044267\n",
      " 19353: 14 [  775/ 1327], train_loss/perplexity = 4.49311686/89.3996582 secs/batch = 0.6058s, grad.norm=0.67697048\n",
      " 19358: 14 [  780/ 1327], train_loss/perplexity = 4.83567715/125.9238205 secs/batch = 0.6114s, grad.norm=0.65502524\n",
      " 19363: 14 [  785/ 1327], train_loss/perplexity = 4.61075163/100.5597076 secs/batch = 0.6158s, grad.norm=0.63378626\n",
      " 19368: 14 [  790/ 1327], train_loss/perplexity = 4.46340418/86.7824326 secs/batch = 0.6101s, grad.norm=0.66641903\n",
      " 19373: 14 [  795/ 1327], train_loss/perplexity = 4.76909113/117.8121185 secs/batch = 0.6171s, grad.norm=0.63353819\n",
      " 19378: 14 [  800/ 1327], train_loss/perplexity = 4.66846895/106.5345078 secs/batch = 0.6106s, grad.norm=0.65713358\n",
      " 19383: 14 [  805/ 1327], train_loss/perplexity = 5.02879381/152.7486572 secs/batch = 0.6546s, grad.norm=0.66089231\n",
      " 19388: 14 [  810/ 1327], train_loss/perplexity = 4.59807158/99.2926559 secs/batch = 0.6108s, grad.norm=0.64025545\n",
      " 19393: 14 [  815/ 1327], train_loss/perplexity = 4.57310247/96.8441010 secs/batch = 0.6170s, grad.norm=0.66440320\n",
      " 19398: 14 [  820/ 1327], train_loss/perplexity = 4.29597712/73.4039001 secs/batch = 0.6122s, grad.norm=0.60483682\n",
      " 19403: 14 [  825/ 1327], train_loss/perplexity = 4.51446915/91.3290710 secs/batch = 0.6134s, grad.norm=0.63077348\n",
      " 19408: 14 [  830/ 1327], train_loss/perplexity = 4.30428934/74.0165939 secs/batch = 0.6039s, grad.norm=0.65599746\n",
      " 19413: 14 [  835/ 1327], train_loss/perplexity = 4.57317066/96.8507080 secs/batch = 0.6135s, grad.norm=0.67324722\n",
      " 19418: 14 [  840/ 1327], train_loss/perplexity = 4.69921970/109.8614120 secs/batch = 0.6102s, grad.norm=0.63437110\n",
      " 19423: 14 [  845/ 1327], train_loss/perplexity = 4.47611523/87.8925629 secs/batch = 0.6103s, grad.norm=0.63936269\n",
      " 19428: 14 [  850/ 1327], train_loss/perplexity = 4.61334801/100.8211365 secs/batch = 0.6131s, grad.norm=0.63175488\n",
      " 19433: 14 [  855/ 1327], train_loss/perplexity = 4.56964970/96.5102997 secs/batch = 0.6064s, grad.norm=0.67824191\n",
      " 19438: 14 [  860/ 1327], train_loss/perplexity = 4.28876066/72.8760910 secs/batch = 0.6053s, grad.norm=0.62184542\n",
      " 19443: 14 [  865/ 1327], train_loss/perplexity = 4.81360197/123.1744919 secs/batch = 0.6055s, grad.norm=0.66139317\n",
      " 19448: 14 [  870/ 1327], train_loss/perplexity = 4.66430616/106.0919495 secs/batch = 0.6122s, grad.norm=0.66474873\n",
      " 19453: 14 [  875/ 1327], train_loss/perplexity = 4.28351259/72.4946365 secs/batch = 0.6159s, grad.norm=0.65167820\n",
      " 19458: 14 [  880/ 1327], train_loss/perplexity = 4.43805170/84.6099319 secs/batch = 0.6086s, grad.norm=0.62639219\n",
      " 19463: 14 [  885/ 1327], train_loss/perplexity = 4.51073408/90.9885864 secs/batch = 0.6091s, grad.norm=0.63630682\n",
      " 19468: 14 [  890/ 1327], train_loss/perplexity = 4.75022984/115.6108551 secs/batch = 0.6096s, grad.norm=0.62599641\n",
      " 19473: 14 [  895/ 1327], train_loss/perplexity = 4.77812672/118.8814392 secs/batch = 0.6055s, grad.norm=0.64071095\n",
      " 19478: 14 [  900/ 1327], train_loss/perplexity = 4.62209940/101.7073288 secs/batch = 0.6493s, grad.norm=0.64542389\n",
      " 19483: 14 [  905/ 1327], train_loss/perplexity = 4.45752907/86.2740707 secs/batch = 0.6030s, grad.norm=0.61387777\n",
      " 19488: 14 [  910/ 1327], train_loss/perplexity = 4.58601141/98.1023560 secs/batch = 0.6087s, grad.norm=0.67575198\n",
      " 19493: 14 [  915/ 1327], train_loss/perplexity = 4.76109505/116.8738403 secs/batch = 0.6096s, grad.norm=0.66440099\n",
      " 19498: 14 [  920/ 1327], train_loss/perplexity = 4.89879036/134.1274414 secs/batch = 0.6030s, grad.norm=0.65477312\n",
      " 19503: 14 [  925/ 1327], train_loss/perplexity = 4.68578911/108.3957748 secs/batch = 0.6133s, grad.norm=0.68192559\n",
      " 19508: 14 [  930/ 1327], train_loss/perplexity = 4.65404606/105.0090027 secs/batch = 0.6120s, grad.norm=0.64656544\n",
      " 19513: 14 [  935/ 1327], train_loss/perplexity = 4.72848368/113.1239014 secs/batch = 0.6110s, grad.norm=0.63410556\n",
      " 19518: 14 [  940/ 1327], train_loss/perplexity = 4.67519379/107.2533493 secs/batch = 0.6118s, grad.norm=0.60788131\n",
      " 19523: 14 [  945/ 1327], train_loss/perplexity = 4.86216879/129.3043365 secs/batch = 0.6100s, grad.norm=0.63320595\n",
      " 19528: 14 [  950/ 1327], train_loss/perplexity = 4.64592075/104.1592255 secs/batch = 0.6060s, grad.norm=0.60393053\n",
      " 19533: 14 [  955/ 1327], train_loss/perplexity = 4.71544313/111.6582794 secs/batch = 0.6081s, grad.norm=0.63562894\n",
      " 19538: 14 [  960/ 1327], train_loss/perplexity = 4.96744442/143.6592865 secs/batch = 0.6028s, grad.norm=0.66880643\n",
      " 19543: 14 [  965/ 1327], train_loss/perplexity = 4.70587921/110.5954819 secs/batch = 0.6139s, grad.norm=0.63660777\n",
      " 19548: 14 [  970/ 1327], train_loss/perplexity = 4.91919899/136.8929138 secs/batch = 0.6044s, grad.norm=0.62842613\n",
      " 19553: 14 [  975/ 1327], train_loss/perplexity = 4.65621901/105.2374268 secs/batch = 0.6095s, grad.norm=0.64990938\n",
      " 19558: 14 [  980/ 1327], train_loss/perplexity = 4.45109463/85.7207260 secs/batch = 0.6127s, grad.norm=0.62567919\n",
      " 19563: 14 [  985/ 1327], train_loss/perplexity = 4.59983492/99.4678955 secs/batch = 0.6003s, grad.norm=0.64179111\n",
      " 19568: 14 [  990/ 1327], train_loss/perplexity = 4.82124949/124.1200790 secs/batch = 0.6042s, grad.norm=0.64562708\n",
      " 19573: 14 [  995/ 1327], train_loss/perplexity = 4.82090235/124.0770035 secs/batch = 0.6195s, grad.norm=0.62111223\n",
      " 19578: 14 [ 1000/ 1327], train_loss/perplexity = 4.29132700/73.0633621 secs/batch = 0.6060s, grad.norm=0.61154890\n",
      " 19583: 14 [ 1005/ 1327], train_loss/perplexity = 4.75752354/116.4571686 secs/batch = 0.6149s, grad.norm=0.64491725\n",
      " 19588: 14 [ 1010/ 1327], train_loss/perplexity = 4.34154463/76.8261185 secs/batch = 0.6104s, grad.norm=0.64985240\n",
      " 19593: 14 [ 1015/ 1327], train_loss/perplexity = 4.83017683/125.2331009 secs/batch = 0.6117s, grad.norm=0.65821022\n",
      " 19598: 14 [ 1020/ 1327], train_loss/perplexity = 4.98903751/146.7950745 secs/batch = 0.6044s, grad.norm=0.65810782\n",
      " 19603: 14 [ 1025/ 1327], train_loss/perplexity = 4.79625893/121.0566864 secs/batch = 0.6093s, grad.norm=0.64300466\n",
      " 19608: 14 [ 1030/ 1327], train_loss/perplexity = 4.57172108/96.7104111 secs/batch = 0.6111s, grad.norm=0.61633819\n",
      " 19613: 14 [ 1035/ 1327], train_loss/perplexity = 4.50849247/90.7848511 secs/batch = 0.6064s, grad.norm=0.65017015\n",
      " 19618: 14 [ 1040/ 1327], train_loss/perplexity = 4.77741861/118.7972946 secs/batch = 0.6087s, grad.norm=0.63591790\n",
      " 19623: 14 [ 1045/ 1327], train_loss/perplexity = 4.36417103/78.5842285 secs/batch = 0.6099s, grad.norm=0.64361334\n",
      " 19628: 14 [ 1050/ 1327], train_loss/perplexity = 4.41693878/82.8423004 secs/batch = 0.6058s, grad.norm=0.65814668\n",
      " 19633: 14 [ 1055/ 1327], train_loss/perplexity = 4.58765984/98.2642059 secs/batch = 0.6169s, grad.norm=0.66455805\n",
      " 19638: 14 [ 1060/ 1327], train_loss/perplexity = 4.19806242/66.5572433 secs/batch = 0.6109s, grad.norm=0.65825868\n",
      " 19643: 14 [ 1065/ 1327], train_loss/perplexity = 4.34850931/77.3630524 secs/batch = 0.6128s, grad.norm=0.65744162\n",
      " 19648: 14 [ 1070/ 1327], train_loss/perplexity = 4.69213533/109.0858688 secs/batch = 0.6136s, grad.norm=0.64540011\n",
      " 19653: 14 [ 1075/ 1327], train_loss/perplexity = 4.42932606/83.8748703 secs/batch = 0.6078s, grad.norm=0.65227222\n",
      " 19658: 14 [ 1080/ 1327], train_loss/perplexity = 4.37521362/79.4568100 secs/batch = 0.6146s, grad.norm=0.61545515\n",
      " 19663: 14 [ 1085/ 1327], train_loss/perplexity = 4.26277971/71.0070877 secs/batch = 0.6101s, grad.norm=0.63712680\n",
      " 19668: 14 [ 1090/ 1327], train_loss/perplexity = 4.46594620/87.0033112 secs/batch = 0.6121s, grad.norm=0.64477646\n",
      " 19673: 14 [ 1095/ 1327], train_loss/perplexity = 4.58186007/97.6959457 secs/batch = 0.6141s, grad.norm=0.67653096\n",
      " 19678: 14 [ 1100/ 1327], train_loss/perplexity = 4.35330677/77.7350922 secs/batch = 0.6144s, grad.norm=0.68218434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19683: 14 [ 1105/ 1327], train_loss/perplexity = 4.36978102/79.0263214 secs/batch = 0.6130s, grad.norm=0.65556067\n",
      " 19688: 14 [ 1110/ 1327], train_loss/perplexity = 4.84255171/126.7924805 secs/batch = 0.6112s, grad.norm=0.71026427\n",
      " 19693: 14 [ 1115/ 1327], train_loss/perplexity = 4.39237738/80.8323593 secs/batch = 0.6142s, grad.norm=0.63570535\n",
      " 19698: 14 [ 1120/ 1327], train_loss/perplexity = 4.59520245/99.0081787 secs/batch = 0.6129s, grad.norm=0.66511780\n",
      " 19703: 14 [ 1125/ 1327], train_loss/perplexity = 4.84672308/127.3224792 secs/batch = 0.6063s, grad.norm=0.68057013\n",
      " 19708: 14 [ 1130/ 1327], train_loss/perplexity = 4.53388309/93.1194534 secs/batch = 0.6135s, grad.norm=0.64178848\n",
      " 19713: 14 [ 1135/ 1327], train_loss/perplexity = 4.54417324/94.0826111 secs/batch = 0.6084s, grad.norm=0.60626775\n",
      " 19718: 14 [ 1140/ 1327], train_loss/perplexity = 4.77805567/118.8729935 secs/batch = 0.6024s, grad.norm=0.66856885\n",
      " 19723: 14 [ 1145/ 1327], train_loss/perplexity = 4.59519863/99.0077972 secs/batch = 0.6452s, grad.norm=0.73249632\n",
      " 19728: 14 [ 1150/ 1327], train_loss/perplexity = 4.57164383/96.7029419 secs/batch = 0.6066s, grad.norm=0.63036406\n",
      " 19733: 14 [ 1155/ 1327], train_loss/perplexity = 4.62155676/101.6521606 secs/batch = 0.6101s, grad.norm=0.65443581\n",
      " 19738: 14 [ 1160/ 1327], train_loss/perplexity = 4.55052614/94.6822128 secs/batch = 0.6086s, grad.norm=0.62298679\n",
      " 19743: 14 [ 1165/ 1327], train_loss/perplexity = 4.70326567/110.3068085 secs/batch = 0.6140s, grad.norm=0.66597217\n",
      " 19748: 14 [ 1170/ 1327], train_loss/perplexity = 4.48198080/88.4096222 secs/batch = 0.6211s, grad.norm=0.65283018\n",
      " 19753: 14 [ 1175/ 1327], train_loss/perplexity = 4.30571651/74.1223068 secs/batch = 0.6070s, grad.norm=0.69654441\n",
      " 19758: 14 [ 1180/ 1327], train_loss/perplexity = 4.29209900/73.1197891 secs/batch = 0.6165s, grad.norm=0.67312825\n",
      " 19763: 14 [ 1185/ 1327], train_loss/perplexity = 4.47823143/88.0787582 secs/batch = 0.6193s, grad.norm=0.65285337\n",
      " 19768: 14 [ 1190/ 1327], train_loss/perplexity = 4.57981014/97.4958801 secs/batch = 0.6118s, grad.norm=0.65062338\n",
      " 19773: 14 [ 1195/ 1327], train_loss/perplexity = 4.40679979/82.0066071 secs/batch = 0.6278s, grad.norm=0.64148360\n",
      " 19778: 14 [ 1200/ 1327], train_loss/perplexity = 4.38165331/79.9701385 secs/batch = 0.6026s, grad.norm=0.66876853\n",
      " 19783: 14 [ 1205/ 1327], train_loss/perplexity = 4.37412977/79.3707352 secs/batch = 0.6055s, grad.norm=0.66228092\n",
      " 19788: 14 [ 1210/ 1327], train_loss/perplexity = 4.11226225/61.0847511 secs/batch = 0.6088s, grad.norm=0.69910121\n",
      " 19793: 14 [ 1215/ 1327], train_loss/perplexity = 4.33406019/76.2532654 secs/batch = 0.6108s, grad.norm=0.63079172\n",
      " 19798: 14 [ 1220/ 1327], train_loss/perplexity = 4.40069008/81.5070953 secs/batch = 0.6115s, grad.norm=0.65967089\n",
      " 19803: 14 [ 1225/ 1327], train_loss/perplexity = 4.23036766/68.7425003 secs/batch = 0.6041s, grad.norm=0.69984055\n",
      " 19808: 14 [ 1230/ 1327], train_loss/perplexity = 4.43364191/84.2376480 secs/batch = 0.6125s, grad.norm=0.64738798\n",
      " 19813: 14 [ 1235/ 1327], train_loss/perplexity = 4.38205481/80.0022507 secs/batch = 0.6030s, grad.norm=0.62068403\n",
      " 19818: 14 [ 1240/ 1327], train_loss/perplexity = 4.64572239/104.1385651 secs/batch = 0.6131s, grad.norm=0.67808002\n",
      " 19823: 14 [ 1245/ 1327], train_loss/perplexity = 4.53828764/93.5305023 secs/batch = 0.6210s, grad.norm=0.65280354\n",
      " 19828: 14 [ 1250/ 1327], train_loss/perplexity = 4.58358097/97.8642197 secs/batch = 0.6079s, grad.norm=0.60829788\n",
      " 19833: 14 [ 1255/ 1327], train_loss/perplexity = 4.60623932/100.1069717 secs/batch = 0.6059s, grad.norm=0.62685078\n",
      " 19838: 14 [ 1260/ 1327], train_loss/perplexity = 4.46644545/87.0467606 secs/batch = 0.6042s, grad.norm=0.66201311\n",
      " 19843: 14 [ 1265/ 1327], train_loss/perplexity = 4.61336803/100.8231506 secs/batch = 0.6066s, grad.norm=0.65180236\n",
      " 19848: 14 [ 1270/ 1327], train_loss/perplexity = 4.43334723/84.2128220 secs/batch = 0.6153s, grad.norm=0.64834088\n",
      " 19853: 14 [ 1275/ 1327], train_loss/perplexity = 4.57069349/96.6110840 secs/batch = 0.6126s, grad.norm=0.63892436\n",
      " 19858: 14 [ 1280/ 1327], train_loss/perplexity = 4.45595121/86.1380463 secs/batch = 0.6040s, grad.norm=0.66215342\n",
      " 19863: 14 [ 1285/ 1327], train_loss/perplexity = 4.41000843/82.2701569 secs/batch = 0.6144s, grad.norm=0.64250654\n",
      " 19868: 14 [ 1290/ 1327], train_loss/perplexity = 4.58259296/97.7675705 secs/batch = 0.6101s, grad.norm=0.64995193\n",
      " 19873: 14 [ 1295/ 1327], train_loss/perplexity = 4.58224773/97.7338257 secs/batch = 0.6204s, grad.norm=0.62013334\n",
      " 19878: 14 [ 1300/ 1327], train_loss/perplexity = 4.71581173/111.6994476 secs/batch = 0.6155s, grad.norm=0.63030255\n",
      " 19883: 14 [ 1305/ 1327], train_loss/perplexity = 4.82973719/125.1780548 secs/batch = 0.6007s, grad.norm=0.68029535\n",
      " 19888: 14 [ 1310/ 1327], train_loss/perplexity = 5.07705736/160.3016510 secs/batch = 0.6076s, grad.norm=0.64003235\n",
      " 19893: 14 [ 1315/ 1327], train_loss/perplexity = 4.92497396/137.6857605 secs/batch = 0.6062s, grad.norm=0.67122555\n",
      " 19898: 14 [ 1320/ 1327], train_loss/perplexity = 4.87342548/130.7680969 secs/batch = 0.6129s, grad.norm=0.68637854\n",
      " 19903: 14 [ 1325/ 1327], train_loss/perplexity = 4.80391359/121.9868927 secs/batch = 0.6064s, grad.norm=0.64685041\n",
      "Epoch training time: 812.6663839817047\n",
      "Saved char model cv/epoch014_4.6975.model\n",
      " 19910: 15 [    5/ 1327], train_loss/perplexity = 4.83868980/126.3037643 secs/batch = 0.6129s, grad.norm=0.65263945\n",
      " 19915: 15 [   10/ 1327], train_loss/perplexity = 4.38054609/79.8816452 secs/batch = 0.6099s, grad.norm=0.64248121\n",
      " 19920: 15 [   15/ 1327], train_loss/perplexity = 4.67075396/106.7782211 secs/batch = 0.6503s, grad.norm=0.62986726\n",
      " 19925: 15 [   20/ 1327], train_loss/perplexity = 4.81376648/123.1947556 secs/batch = 0.6105s, grad.norm=0.66676295\n",
      " 19930: 15 [   25/ 1327], train_loss/perplexity = 4.73471308/113.8307953 secs/batch = 0.6125s, grad.norm=0.64098799\n",
      " 19935: 15 [   30/ 1327], train_loss/perplexity = 4.69446516/109.3403168 secs/batch = 0.6063s, grad.norm=0.65671057\n",
      " 19940: 15 [   35/ 1327], train_loss/perplexity = 4.52699280/92.4800339 secs/batch = 0.6122s, grad.norm=0.65678525\n",
      " 19945: 15 [   40/ 1327], train_loss/perplexity = 4.62576532/102.0808716 secs/batch = 0.6044s, grad.norm=0.65766919\n",
      " 19950: 15 [   45/ 1327], train_loss/perplexity = 4.30748796/74.2537231 secs/batch = 0.6023s, grad.norm=0.64106029\n",
      " 19955: 15 [   50/ 1327], train_loss/perplexity = 4.52496958/92.2931213 secs/batch = 0.6097s, grad.norm=0.64346153\n",
      " 19960: 15 [   55/ 1327], train_loss/perplexity = 4.49783182/89.8221664 secs/batch = 0.6160s, grad.norm=0.65170479\n",
      " 19965: 15 [   60/ 1327], train_loss/perplexity = 4.76130486/116.8983612 secs/batch = 0.6060s, grad.norm=0.66809291\n",
      " 19970: 15 [   65/ 1327], train_loss/perplexity = 4.31185532/74.5787277 secs/batch = 0.6107s, grad.norm=0.63550717\n",
      " 19975: 15 [   70/ 1327], train_loss/perplexity = 4.23377895/68.9774017 secs/batch = 0.6137s, grad.norm=0.63111222\n",
      " 19980: 15 [   75/ 1327], train_loss/perplexity = 4.12815380/62.0632362 secs/batch = 0.6128s, grad.norm=0.63442540\n",
      " 19985: 15 [   80/ 1327], train_loss/perplexity = 4.52420282/92.2223816 secs/batch = 0.6107s, grad.norm=0.67097038\n",
      " 19990: 15 [   85/ 1327], train_loss/perplexity = 4.51105070/91.0174026 secs/batch = 0.6044s, grad.norm=0.64048070\n",
      " 19995: 15 [   90/ 1327], train_loss/perplexity = 4.52688456/92.4700241 secs/batch = 0.6087s, grad.norm=0.63227862\n",
      " 20000: 15 [   95/ 1327], train_loss/perplexity = 4.45997143/86.4850388 secs/batch = 0.6145s, grad.norm=0.66014946\n",
      " 20005: 15 [  100/ 1327], train_loss/perplexity = 4.67079639/106.7827530 secs/batch = 0.6105s, grad.norm=0.65209520\n",
      " 20010: 15 [  105/ 1327], train_loss/perplexity = 4.61034060/100.5183792 secs/batch = 0.6068s, grad.norm=0.74016345\n",
      " 20015: 15 [  110/ 1327], train_loss/perplexity = 4.40033007/81.4777603 secs/batch = 0.6105s, grad.norm=0.76410544\n",
      " 20020: 15 [  115/ 1327], train_loss/perplexity = 4.35342121/77.7439880 secs/batch = 0.6531s, grad.norm=0.64924479\n",
      " 20025: 15 [  120/ 1327], train_loss/perplexity = 4.46482229/86.9055862 secs/batch = 0.6016s, grad.norm=0.67260051\n",
      " 20030: 15 [  125/ 1327], train_loss/perplexity = 4.56369543/95.9373550 secs/batch = 0.6133s, grad.norm=0.66345710\n",
      " 20035: 15 [  130/ 1327], train_loss/perplexity = 4.46541977/86.9575272 secs/batch = 0.6101s, grad.norm=0.66872925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20040: 15 [  135/ 1327], train_loss/perplexity = 4.47160673/87.4971924 secs/batch = 0.6186s, grad.norm=0.64742029\n",
      " 20045: 15 [  140/ 1327], train_loss/perplexity = 4.77338505/118.3190842 secs/batch = 0.6130s, grad.norm=0.67070347\n",
      " 20050: 15 [  145/ 1327], train_loss/perplexity = 4.72221041/112.4164658 secs/batch = 0.6042s, grad.norm=0.67679882\n",
      " 20055: 15 [  150/ 1327], train_loss/perplexity = 4.67398071/107.1233215 secs/batch = 0.6089s, grad.norm=0.69391143\n",
      " 20060: 15 [  155/ 1327], train_loss/perplexity = 4.93033457/138.4258118 secs/batch = 0.6100s, grad.norm=0.64717883\n",
      " 20065: 15 [  160/ 1327], train_loss/perplexity = 4.54069471/93.7559128 secs/batch = 0.6060s, grad.norm=0.64341086\n",
      " 20070: 15 [  165/ 1327], train_loss/perplexity = 4.77765226/118.8250504 secs/batch = 0.6087s, grad.norm=0.66166836\n",
      " 20075: 15 [  170/ 1327], train_loss/perplexity = 4.51748276/91.6047134 secs/batch = 0.6111s, grad.norm=0.63888061\n",
      " 20080: 15 [  175/ 1327], train_loss/perplexity = 4.80885363/122.5910034 secs/batch = 0.6209s, grad.norm=0.63919383\n",
      " 20085: 15 [  180/ 1327], train_loss/perplexity = 4.68135166/107.9158401 secs/batch = 0.6088s, grad.norm=0.62370062\n",
      " 20090: 15 [  185/ 1327], train_loss/perplexity = 4.93074703/138.4829254 secs/batch = 0.6073s, grad.norm=0.64993602\n",
      " 20095: 15 [  190/ 1327], train_loss/perplexity = 4.51611948/91.4799194 secs/batch = 0.6103s, grad.norm=0.60141009\n",
      " 20100: 15 [  195/ 1327], train_loss/perplexity = 4.74356222/114.8425674 secs/batch = 0.6079s, grad.norm=0.62366432\n",
      " 20105: 15 [  200/ 1327], train_loss/perplexity = 4.58994102/98.4886246 secs/batch = 0.6093s, grad.norm=0.65817028\n",
      " 20110: 15 [  205/ 1327], train_loss/perplexity = 4.76527357/117.3632202 secs/batch = 0.6106s, grad.norm=0.65058726\n",
      " 20115: 15 [  210/ 1327], train_loss/perplexity = 4.66169071/105.8148346 secs/batch = 0.6179s, grad.norm=0.65890443\n",
      " 20120: 15 [  215/ 1327], train_loss/perplexity = 4.73865700/114.2806168 secs/batch = 0.6115s, grad.norm=0.63444483\n",
      " 20125: 15 [  220/ 1327], train_loss/perplexity = 4.75945044/116.6817856 secs/batch = 0.6102s, grad.norm=0.65212971\n",
      " 20130: 15 [  225/ 1327], train_loss/perplexity = 4.90116358/134.4461212 secs/batch = 0.6082s, grad.norm=0.64621955\n",
      " 20135: 15 [  230/ 1327], train_loss/perplexity = 4.71356916/111.4492340 secs/batch = 0.6083s, grad.norm=0.66892308\n",
      " 20140: 15 [  235/ 1327], train_loss/perplexity = 4.60419321/99.9023514 secs/batch = 0.6037s, grad.norm=0.65468878\n",
      " 20145: 15 [  240/ 1327], train_loss/perplexity = 4.44886112/85.5294800 secs/batch = 0.6124s, grad.norm=0.66510677\n",
      " 20150: 15 [  245/ 1327], train_loss/perplexity = 4.75068998/115.6640625 secs/batch = 0.6133s, grad.norm=0.64445162\n",
      " 20155: 15 [  250/ 1327], train_loss/perplexity = 4.47169495/87.5049133 secs/batch = 0.6257s, grad.norm=0.63465220\n",
      " 20160: 15 [  255/ 1327], train_loss/perplexity = 4.51855898/91.7033539 secs/batch = 0.6112s, grad.norm=0.61591977\n",
      " 20165: 15 [  260/ 1327], train_loss/perplexity = 4.77643013/118.6799240 secs/batch = 0.6307s, grad.norm=0.66993660\n",
      " 20170: 15 [  265/ 1327], train_loss/perplexity = 4.89119291/133.1122742 secs/batch = 0.6114s, grad.norm=0.63299006\n",
      " 20175: 15 [  270/ 1327], train_loss/perplexity = 4.92646360/137.8910065 secs/batch = 0.6150s, grad.norm=0.64809549\n",
      " 20180: 15 [  275/ 1327], train_loss/perplexity = 4.96576643/143.4184265 secs/batch = 0.6087s, grad.norm=0.64326692\n",
      " 20185: 15 [  280/ 1327], train_loss/perplexity = 4.69214678/109.0871124 secs/batch = 0.6087s, grad.norm=0.63893038\n",
      " 20190: 15 [  285/ 1327], train_loss/perplexity = 4.93814087/139.5106354 secs/batch = 0.6101s, grad.norm=0.64494365\n",
      " 20195: 15 [  290/ 1327], train_loss/perplexity = 4.73536587/113.9051285 secs/batch = 0.6139s, grad.norm=0.68020850\n",
      " 20200: 15 [  295/ 1327], train_loss/perplexity = 4.51927471/91.7690125 secs/batch = 0.6147s, grad.norm=0.63705552\n",
      " 20205: 15 [  300/ 1327], train_loss/perplexity = 4.07952833/59.1175804 secs/batch = 0.6058s, grad.norm=0.63123012\n",
      " 20210: 15 [  305/ 1327], train_loss/perplexity = 4.53557873/93.2774811 secs/batch = 0.6215s, grad.norm=0.64378816\n",
      " 20215: 15 [  310/ 1327], train_loss/perplexity = 4.59158421/98.6505890 secs/batch = 0.6231s, grad.norm=0.65740770\n",
      " 20220: 15 [  315/ 1327], train_loss/perplexity = 4.22095060/68.0981903 secs/batch = 0.6155s, grad.norm=0.63457137\n",
      " 20225: 15 [  320/ 1327], train_loss/perplexity = 4.14854097/63.3415146 secs/batch = 0.6100s, grad.norm=0.65764141\n",
      " 20230: 15 [  325/ 1327], train_loss/perplexity = 4.15677834/63.8654366 secs/batch = 0.6093s, grad.norm=0.64498085\n",
      " 20235: 15 [  330/ 1327], train_loss/perplexity = 4.63331509/102.8544693 secs/batch = 0.6076s, grad.norm=0.68785453\n",
      " 20240: 15 [  335/ 1327], train_loss/perplexity = 4.04323578/57.0105171 secs/batch = 0.6072s, grad.norm=0.63798922\n",
      " 20245: 15 [  340/ 1327], train_loss/perplexity = 4.80148554/121.6910629 secs/batch = 0.6065s, grad.norm=0.67853302\n",
      " 20250: 15 [  345/ 1327], train_loss/perplexity = 4.63454962/102.9815292 secs/batch = 0.6022s, grad.norm=0.63720405\n",
      " 20255: 15 [  350/ 1327], train_loss/perplexity = 4.66465902/106.1293945 secs/batch = 0.6113s, grad.norm=0.69802922\n",
      " 20260: 15 [  355/ 1327], train_loss/perplexity = 4.67510939/107.2443008 secs/batch = 0.6092s, grad.norm=0.64565361\n",
      " 20265: 15 [  360/ 1327], train_loss/perplexity = 4.85572910/128.4743347 secs/batch = 0.6112s, grad.norm=0.69523460\n",
      " 20270: 15 [  365/ 1327], train_loss/perplexity = 4.71842623/111.9918671 secs/batch = 0.6135s, grad.norm=0.62864554\n",
      " 20275: 15 [  370/ 1327], train_loss/perplexity = 4.79236174/120.5858231 secs/batch = 0.6117s, grad.norm=0.68450457\n",
      " 20280: 15 [  375/ 1327], train_loss/perplexity = 4.16000843/64.0720596 secs/batch = 0.6135s, grad.norm=0.62479514\n",
      " 20285: 15 [  380/ 1327], train_loss/perplexity = 4.30771637/74.2706909 secs/batch = 0.6152s, grad.norm=0.67435271\n",
      " 20290: 15 [  385/ 1327], train_loss/perplexity = 4.54559517/94.2164841 secs/batch = 0.6130s, grad.norm=0.65553993\n",
      " 20295: 15 [  390/ 1327], train_loss/perplexity = 4.61720896/101.2111511 secs/batch = 0.6096s, grad.norm=0.64602274\n",
      " 20300: 15 [  395/ 1327], train_loss/perplexity = 4.80050755/121.5721054 secs/batch = 0.6061s, grad.norm=0.64534044\n",
      " 20305: 15 [  400/ 1327], train_loss/perplexity = 4.56852055/96.4013824 secs/batch = 0.6172s, grad.norm=0.62802988\n",
      " 20310: 15 [  405/ 1327], train_loss/perplexity = 4.90773153/135.3320770 secs/batch = 0.6141s, grad.norm=0.68918741\n",
      " 20315: 15 [  410/ 1327], train_loss/perplexity = 4.57374859/96.9066925 secs/batch = 0.6089s, grad.norm=0.62371445\n",
      " 20320: 15 [  415/ 1327], train_loss/perplexity = 4.43650198/84.4789124 secs/batch = 0.6167s, grad.norm=0.65922576\n",
      " 20325: 15 [  420/ 1327], train_loss/perplexity = 4.21021891/67.3712845 secs/batch = 0.6101s, grad.norm=0.66248840\n",
      " 20330: 15 [  425/ 1327], train_loss/perplexity = 4.53097677/92.8492126 secs/batch = 0.6064s, grad.norm=0.66134322\n",
      " 20335: 15 [  430/ 1327], train_loss/perplexity = 4.72805166/113.0750427 secs/batch = 0.6137s, grad.norm=0.67873585\n",
      " 20340: 15 [  435/ 1327], train_loss/perplexity = 4.70975542/111.0250015 secs/batch = 0.6081s, grad.norm=0.68054330\n",
      " 20345: 15 [  440/ 1327], train_loss/perplexity = 4.32449007/75.5269928 secs/batch = 0.6060s, grad.norm=0.66553700\n",
      " 20350: 15 [  445/ 1327], train_loss/perplexity = 4.65525818/105.1363602 secs/batch = 0.6116s, grad.norm=0.68173748\n",
      " 20355: 15 [  450/ 1327], train_loss/perplexity = 4.54087734/93.7730331 secs/batch = 0.6094s, grad.norm=0.66710013\n",
      " 20360: 15 [  455/ 1327], train_loss/perplexity = 4.45506668/86.0618896 secs/batch = 0.6616s, grad.norm=0.69651896\n",
      " 20365: 15 [  460/ 1327], train_loss/perplexity = 4.57294798/96.8291397 secs/batch = 0.6064s, grad.norm=0.67786920\n",
      " 20370: 15 [  465/ 1327], train_loss/perplexity = 4.28661919/72.7201996 secs/batch = 0.6104s, grad.norm=0.67993724\n",
      " 20375: 15 [  470/ 1327], train_loss/perplexity = 4.86178207/129.2543335 secs/batch = 0.6145s, grad.norm=0.64956254\n",
      " 20380: 15 [  475/ 1327], train_loss/perplexity = 4.42640781/83.6304626 secs/batch = 0.6061s, grad.norm=0.60515076\n",
      " 20385: 15 [  480/ 1327], train_loss/perplexity = 4.59779692/99.2653885 secs/batch = 0.6122s, grad.norm=0.69783729\n",
      " 20390: 15 [  485/ 1327], train_loss/perplexity = 4.48248768/88.4544449 secs/batch = 0.6045s, grad.norm=0.64464188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20395: 15 [  490/ 1327], train_loss/perplexity = 4.42332506/83.3730469 secs/batch = 0.6126s, grad.norm=0.69931573\n",
      " 20400: 15 [  495/ 1327], train_loss/perplexity = 4.42308855/83.3533325 secs/batch = 0.6112s, grad.norm=0.65373534\n",
      " 20405: 15 [  500/ 1327], train_loss/perplexity = 4.67885065/107.6462784 secs/batch = 0.6008s, grad.norm=0.63422894\n",
      " 20410: 15 [  505/ 1327], train_loss/perplexity = 4.65496969/105.1060333 secs/batch = 0.6059s, grad.norm=0.63269711\n",
      " 20415: 15 [  510/ 1327], train_loss/perplexity = 5.03344250/153.4603882 secs/batch = 0.6132s, grad.norm=0.65906835\n",
      " 20420: 15 [  515/ 1327], train_loss/perplexity = 4.68388939/108.1900482 secs/batch = 0.6119s, grad.norm=0.62602854\n",
      " 20425: 15 [  520/ 1327], train_loss/perplexity = 4.84507895/127.1133194 secs/batch = 0.6117s, grad.norm=0.65902913\n",
      " 20430: 15 [  525/ 1327], train_loss/perplexity = 4.45887613/86.3903656 secs/batch = 0.6063s, grad.norm=0.66084099\n",
      " 20435: 15 [  530/ 1327], train_loss/perplexity = 4.52832508/92.6033249 secs/batch = 0.6079s, grad.norm=0.67366421\n",
      " 20440: 15 [  535/ 1327], train_loss/perplexity = 4.58382320/97.8879242 secs/batch = 0.6118s, grad.norm=0.63623965\n",
      " 20445: 15 [  540/ 1327], train_loss/perplexity = 4.70117712/110.0766678 secs/batch = 0.6083s, grad.norm=0.63394469\n",
      " 20450: 15 [  545/ 1327], train_loss/perplexity = 4.73297405/113.6330109 secs/batch = 0.6078s, grad.norm=0.65432101\n",
      " 20455: 15 [  550/ 1327], train_loss/perplexity = 4.69971800/109.9161682 secs/batch = 0.6147s, grad.norm=0.67589819\n",
      " 20460: 15 [  555/ 1327], train_loss/perplexity = 4.52269936/92.0838318 secs/batch = 0.6172s, grad.norm=0.64794570\n",
      " 20465: 15 [  560/ 1327], train_loss/perplexity = 4.57548618/97.0752258 secs/batch = 0.6111s, grad.norm=0.67314810\n",
      " 20470: 15 [  565/ 1327], train_loss/perplexity = 4.50946760/90.8734283 secs/batch = 0.6107s, grad.norm=0.66308647\n",
      " 20475: 15 [  570/ 1327], train_loss/perplexity = 4.48439646/88.6234436 secs/batch = 0.6062s, grad.norm=0.67250955\n",
      " 20480: 15 [  575/ 1327], train_loss/perplexity = 4.32638311/75.6700974 secs/batch = 0.6024s, grad.norm=0.67501009\n",
      " 20485: 15 [  580/ 1327], train_loss/perplexity = 4.69109774/108.9727402 secs/batch = 0.6085s, grad.norm=0.67321825\n",
      " 20490: 15 [  585/ 1327], train_loss/perplexity = 4.31665993/74.9379120 secs/batch = 0.6060s, grad.norm=0.64836991\n",
      " 20495: 15 [  590/ 1327], train_loss/perplexity = 4.65131903/104.7230301 secs/batch = 0.6039s, grad.norm=0.67301339\n",
      " 20500: 15 [  595/ 1327], train_loss/perplexity = 4.55506086/95.1125412 secs/batch = 0.6062s, grad.norm=0.67887437\n",
      " 20505: 15 [  600/ 1327], train_loss/perplexity = 4.86619997/129.8266296 secs/batch = 0.6082s, grad.norm=0.65503168\n",
      " 20510: 15 [  605/ 1327], train_loss/perplexity = 4.76863861/117.7588196 secs/batch = 0.6098s, grad.norm=0.65868282\n",
      " 20515: 15 [  610/ 1327], train_loss/perplexity = 4.87674522/131.2029266 secs/batch = 0.6079s, grad.norm=0.68666893\n",
      " 20520: 15 [  615/ 1327], train_loss/perplexity = 4.37606144/79.5242081 secs/batch = 0.6064s, grad.norm=0.64770985\n",
      " 20525: 15 [  620/ 1327], train_loss/perplexity = 4.73316860/113.6551208 secs/batch = 0.6080s, grad.norm=0.67017949\n",
      " 20530: 15 [  625/ 1327], train_loss/perplexity = 4.82383919/124.4419327 secs/batch = 0.6135s, grad.norm=0.66531146\n",
      " 20535: 15 [  630/ 1327], train_loss/perplexity = 4.85162687/127.9483795 secs/batch = 0.6099s, grad.norm=0.66327035\n",
      " 20540: 15 [  635/ 1327], train_loss/perplexity = 4.59153938/98.6461639 secs/batch = 0.6053s, grad.norm=0.65082765\n",
      " 20545: 15 [  640/ 1327], train_loss/perplexity = 4.64555645/104.1212845 secs/batch = 0.6097s, grad.norm=0.66478831\n",
      " 20550: 15 [  645/ 1327], train_loss/perplexity = 4.80000591/121.5111389 secs/batch = 0.6180s, grad.norm=0.68260235\n",
      " 20555: 15 [  650/ 1327], train_loss/perplexity = 4.41566372/82.7367401 secs/batch = 0.6083s, grad.norm=0.65136635\n",
      " 20560: 15 [  655/ 1327], train_loss/perplexity = 4.59218693/98.7100677 secs/batch = 0.6233s, grad.norm=0.66030085\n",
      " 20565: 15 [  660/ 1327], train_loss/perplexity = 4.47362423/87.6738968 secs/batch = 0.6103s, grad.norm=0.62457424\n",
      " 20570: 15 [  665/ 1327], train_loss/perplexity = 4.67052126/106.7533722 secs/batch = 0.6063s, grad.norm=0.67541200\n",
      " 20575: 15 [  670/ 1327], train_loss/perplexity = 4.53808117/93.5111923 secs/batch = 0.6068s, grad.norm=0.64920312\n",
      " 20580: 15 [  675/ 1327], train_loss/perplexity = 4.40104294/81.5358582 secs/batch = 0.6098s, grad.norm=0.68132395\n",
      " 20585: 15 [  680/ 1327], train_loss/perplexity = 4.60320902/99.8040771 secs/batch = 0.6130s, grad.norm=0.67865568\n",
      " 20590: 15 [  685/ 1327], train_loss/perplexity = 4.53031015/92.7873383 secs/batch = 0.6093s, grad.norm=0.67121434\n",
      " 20595: 15 [  690/ 1327], train_loss/perplexity = 4.79875517/121.3592529 secs/batch = 0.6041s, grad.norm=0.64771867\n",
      " 20600: 15 [  695/ 1327], train_loss/perplexity = 4.58577061/98.0787354 secs/batch = 0.6153s, grad.norm=0.63352627\n",
      " 20605: 15 [  700/ 1327], train_loss/perplexity = 4.81830788/123.7555008 secs/batch = 0.6078s, grad.norm=0.65669829\n",
      " 20610: 15 [  705/ 1327], train_loss/perplexity = 4.56037378/95.6192169 secs/batch = 0.6587s, grad.norm=0.66096431\n",
      " 20615: 15 [  710/ 1327], train_loss/perplexity = 4.46818924/87.1986847 secs/batch = 0.6151s, grad.norm=0.63075787\n",
      " 20620: 15 [  715/ 1327], train_loss/perplexity = 4.51032972/90.9518051 secs/batch = 0.6137s, grad.norm=0.65922982\n",
      " 20625: 15 [  720/ 1327], train_loss/perplexity = 4.49781704/89.8208389 secs/batch = 0.6018s, grad.norm=0.66382182\n",
      " 20630: 15 [  725/ 1327], train_loss/perplexity = 4.38539076/80.2695847 secs/batch = 0.6154s, grad.norm=0.67005551\n",
      " 20635: 15 [  730/ 1327], train_loss/perplexity = 4.55809832/95.4018860 secs/batch = 0.6162s, grad.norm=0.65448713\n",
      " 20640: 15 [  735/ 1327], train_loss/perplexity = 4.63543129/103.0723648 secs/batch = 0.6093s, grad.norm=0.66420859\n",
      " 20645: 15 [  740/ 1327], train_loss/perplexity = 4.09627104/60.1156998 secs/batch = 0.6134s, grad.norm=0.61228365\n",
      " 20650: 15 [  745/ 1327], train_loss/perplexity = 4.53214550/92.9577866 secs/batch = 0.6118s, grad.norm=0.63798624\n",
      " 20655: 15 [  750/ 1327], train_loss/perplexity = 4.44347954/85.0704346 secs/batch = 0.6125s, grad.norm=0.66605514\n",
      " 20660: 15 [  755/ 1327], train_loss/perplexity = 4.41358662/82.5650635 secs/batch = 0.6124s, grad.norm=0.63742691\n",
      " 20665: 15 [  760/ 1327], train_loss/perplexity = 4.26449490/71.1289825 secs/batch = 0.6123s, grad.norm=0.66059983\n",
      " 20670: 15 [  765/ 1327], train_loss/perplexity = 4.40892601/82.1811523 secs/batch = 0.6151s, grad.norm=0.66259181\n",
      " 20675: 15 [  770/ 1327], train_loss/perplexity = 4.33017206/75.9573517 secs/batch = 0.6136s, grad.norm=0.67341280\n",
      " 20680: 15 [  775/ 1327], train_loss/perplexity = 4.43010187/83.9399643 secs/batch = 0.6099s, grad.norm=0.64488727\n",
      " 20685: 15 [  780/ 1327], train_loss/perplexity = 4.77513933/118.5268250 secs/batch = 0.6025s, grad.norm=0.67165667\n",
      " 20690: 15 [  785/ 1327], train_loss/perplexity = 4.59926462/99.4111862 secs/batch = 0.6029s, grad.norm=0.68245351\n",
      " 20695: 15 [  790/ 1327], train_loss/perplexity = 4.38970423/80.6165695 secs/batch = 0.6141s, grad.norm=0.66327345\n",
      " 20700: 15 [  795/ 1327], train_loss/perplexity = 4.69318724/109.2006760 secs/batch = 0.6096s, grad.norm=0.65061289\n",
      " 20705: 15 [  800/ 1327], train_loss/perplexity = 4.65912724/105.5439301 secs/batch = 0.6130s, grad.norm=0.65844953\n",
      " 20710: 15 [  805/ 1327], train_loss/perplexity = 4.97195578/144.3088531 secs/batch = 0.6046s, grad.norm=0.65967441\n",
      " 20715: 15 [  810/ 1327], train_loss/perplexity = 4.57087231/96.6283646 secs/batch = 0.6129s, grad.norm=0.63162470\n",
      " 20720: 15 [  815/ 1327], train_loss/perplexity = 4.51701403/91.5617905 secs/batch = 0.6126s, grad.norm=0.66837943\n",
      " 20725: 15 [  820/ 1327], train_loss/perplexity = 4.21436501/67.6511917 secs/batch = 0.6074s, grad.norm=0.61838961\n",
      " 20730: 15 [  825/ 1327], train_loss/perplexity = 4.46711779/87.1053085 secs/batch = 0.6062s, grad.norm=0.63679034\n",
      " 20735: 15 [  830/ 1327], train_loss/perplexity = 4.27575541/71.9344559 secs/batch = 0.6144s, grad.norm=0.66219842\n",
      " 20740: 15 [  835/ 1327], train_loss/perplexity = 4.58778954/98.2769547 secs/batch = 0.6019s, grad.norm=0.70526153\n",
      " 20745: 15 [  840/ 1327], train_loss/perplexity = 4.62924242/102.4364319 secs/batch = 0.6073s, grad.norm=0.64706141\n",
      " 20750: 15 [  845/ 1327], train_loss/perplexity = 4.40659332/81.9896774 secs/batch = 0.6029s, grad.norm=0.65505153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20755: 15 [  850/ 1327], train_loss/perplexity = 4.54569054/94.2254715 secs/batch = 0.6624s, grad.norm=0.62719977\n",
      " 20760: 15 [  855/ 1327], train_loss/perplexity = 4.54137039/93.8192825 secs/batch = 0.6123s, grad.norm=0.68647909\n",
      " 20765: 15 [  860/ 1327], train_loss/perplexity = 4.24741793/69.9246292 secs/batch = 0.6114s, grad.norm=0.64350629\n",
      " 20770: 15 [  865/ 1327], train_loss/perplexity = 4.76995373/117.9137878 secs/batch = 0.6080s, grad.norm=0.68062288\n",
      " 20775: 15 [  870/ 1327], train_loss/perplexity = 4.60848427/100.3319550 secs/batch = 0.6035s, grad.norm=0.66100979\n",
      " 20780: 15 [  875/ 1327], train_loss/perplexity = 4.18061781/65.4062500 secs/batch = 0.6056s, grad.norm=0.65528250\n",
      " 20785: 15 [  880/ 1327], train_loss/perplexity = 4.41650724/82.8065567 secs/batch = 0.6052s, grad.norm=0.63371474\n",
      " 20790: 15 [  885/ 1327], train_loss/perplexity = 4.49511051/89.5780716 secs/batch = 0.6098s, grad.norm=0.63931656\n",
      " 20795: 15 [  890/ 1327], train_loss/perplexity = 4.68016720/107.7880936 secs/batch = 0.6068s, grad.norm=0.65283448\n",
      " 20800: 15 [  895/ 1327], train_loss/perplexity = 4.71893883/112.0492859 secs/batch = 0.6226s, grad.norm=0.65268713\n",
      " 20805: 15 [  900/ 1327], train_loss/perplexity = 4.63176394/102.6950531 secs/batch = 0.6107s, grad.norm=0.63421041\n",
      " 20810: 15 [  905/ 1327], train_loss/perplexity = 4.42638731/83.6287460 secs/batch = 0.6022s, grad.norm=0.63027769\n",
      " 20815: 15 [  910/ 1327], train_loss/perplexity = 4.54415846/94.0812225 secs/batch = 0.6172s, grad.norm=0.67966056\n",
      " 20820: 15 [  915/ 1327], train_loss/perplexity = 4.73420238/113.7726746 secs/batch = 0.6122s, grad.norm=0.66330266\n",
      " 20825: 15 [  920/ 1327], train_loss/perplexity = 4.90235996/134.6070709 secs/batch = 0.6118s, grad.norm=0.69324303\n",
      " 20830: 15 [  925/ 1327], train_loss/perplexity = 4.63888693/103.4291611 secs/batch = 0.6095s, grad.norm=0.69465446\n",
      " 20835: 15 [  930/ 1327], train_loss/perplexity = 4.59261942/98.7527695 secs/batch = 0.6058s, grad.norm=0.64430249\n",
      " 20840: 15 [  935/ 1327], train_loss/perplexity = 4.72871637/113.1502228 secs/batch = 0.6074s, grad.norm=0.64750451\n",
      " 20845: 15 [  940/ 1327], train_loss/perplexity = 4.66235161/105.8847885 secs/batch = 0.6001s, grad.norm=0.62582141\n",
      " 20850: 15 [  945/ 1327], train_loss/perplexity = 4.81883144/123.8203125 secs/batch = 0.6121s, grad.norm=0.63630426\n",
      " 20855: 15 [  950/ 1327], train_loss/perplexity = 4.57790661/97.3104706 secs/batch = 0.6582s, grad.norm=0.63781202\n",
      " 20860: 15 [  955/ 1327], train_loss/perplexity = 4.65154791/104.7470016 secs/batch = 0.6155s, grad.norm=0.62060124\n",
      " 20865: 15 [  960/ 1327], train_loss/perplexity = 4.91239071/135.9640808 secs/batch = 0.6061s, grad.norm=0.65895784\n",
      " 20870: 15 [  965/ 1327], train_loss/perplexity = 4.65368128/104.9707031 secs/batch = 0.6065s, grad.norm=0.66915292\n",
      " 20875: 15 [  970/ 1327], train_loss/perplexity = 4.89396334/133.4815521 secs/batch = 0.6112s, grad.norm=0.65461671\n",
      " 20880: 15 [  975/ 1327], train_loss/perplexity = 4.60446024/99.9290314 secs/batch = 0.6127s, grad.norm=0.64486831\n",
      " 20885: 15 [  980/ 1327], train_loss/perplexity = 4.42525721/83.5342865 secs/batch = 0.6102s, grad.norm=0.62975323\n",
      " 20890: 15 [  985/ 1327], train_loss/perplexity = 4.59022093/98.5161896 secs/batch = 0.6117s, grad.norm=0.64420706\n",
      " 20895: 15 [  990/ 1327], train_loss/perplexity = 4.77077389/118.0105362 secs/batch = 0.6097s, grad.norm=0.66609496\n",
      " 20900: 15 [  995/ 1327], train_loss/perplexity = 4.78186798/119.3270416 secs/batch = 0.6184s, grad.norm=0.65853781\n",
      " 20905: 15 [ 1000/ 1327], train_loss/perplexity = 4.28294420/72.4534454 secs/batch = 0.6154s, grad.norm=0.62916929\n",
      " 20910: 15 [ 1005/ 1327], train_loss/perplexity = 4.72038174/112.2110825 secs/batch = 0.6117s, grad.norm=0.65150237\n",
      " 20915: 15 [ 1010/ 1327], train_loss/perplexity = 4.27735233/72.0494232 secs/batch = 0.6119s, grad.norm=0.62954634\n",
      " 20920: 15 [ 1015/ 1327], train_loss/perplexity = 4.82140493/124.1393738 secs/batch = 0.6075s, grad.norm=0.64830506\n",
      " 20925: 15 [ 1020/ 1327], train_loss/perplexity = 4.92518187/137.7143860 secs/batch = 0.6165s, grad.norm=0.65778857\n",
      " 20930: 15 [ 1025/ 1327], train_loss/perplexity = 4.71886015/112.0404739 secs/batch = 0.6150s, grad.norm=0.63292384\n",
      " 20935: 15 [ 1030/ 1327], train_loss/perplexity = 4.58089256/97.6014709 secs/batch = 0.6177s, grad.norm=0.65589780\n",
      " 20940: 15 [ 1035/ 1327], train_loss/perplexity = 4.47612906/87.8937836 secs/batch = 0.6092s, grad.norm=0.65427816\n",
      " 20945: 15 [ 1040/ 1327], train_loss/perplexity = 4.73627806/114.0090790 secs/batch = 0.6098s, grad.norm=0.63641965\n",
      " 20950: 15 [ 1045/ 1327], train_loss/perplexity = 4.31266928/74.6394577 secs/batch = 0.6094s, grad.norm=0.63057905\n",
      " 20955: 15 [ 1050/ 1327], train_loss/perplexity = 4.38946486/80.5972748 secs/batch = 0.6130s, grad.norm=0.67363220\n",
      " 20960: 15 [ 1055/ 1327], train_loss/perplexity = 4.57794571/97.3142776 secs/batch = 0.6156s, grad.norm=0.67442197\n",
      " 20965: 15 [ 1060/ 1327], train_loss/perplexity = 4.18483973/65.6829758 secs/batch = 0.6084s, grad.norm=0.65788412\n",
      " 20970: 15 [ 1065/ 1327], train_loss/perplexity = 4.26305056/71.0263214 secs/batch = 0.6102s, grad.norm=0.64543456\n",
      " 20975: 15 [ 1070/ 1327], train_loss/perplexity = 4.64075994/103.6230621 secs/batch = 0.6157s, grad.norm=0.67177641\n",
      " 20980: 15 [ 1075/ 1327], train_loss/perplexity = 4.34127569/76.8054581 secs/batch = 0.6116s, grad.norm=0.65761703\n",
      " 20985: 15 [ 1080/ 1327], train_loss/perplexity = 4.33339310/76.2024078 secs/batch = 0.6069s, grad.norm=0.64826697\n",
      " 20990: 15 [ 1085/ 1327], train_loss/perplexity = 4.18966436/66.0006332 secs/batch = 0.6180s, grad.norm=0.63471991\n",
      " 20995: 15 [ 1090/ 1327], train_loss/perplexity = 4.41996431/83.0933228 secs/batch = 0.6107s, grad.norm=0.64175189\n",
      " 21000: 15 [ 1095/ 1327], train_loss/perplexity = 4.55861187/95.4508896 secs/batch = 0.6097s, grad.norm=0.68504107\n",
      " 21005: 15 [ 1100/ 1327], train_loss/perplexity = 4.31825733/75.0577164 secs/batch = 0.6123s, grad.norm=0.68699133\n",
      " 21010: 15 [ 1105/ 1327], train_loss/perplexity = 4.33450890/76.2874832 secs/batch = 0.6031s, grad.norm=0.68244541\n",
      " 21015: 15 [ 1110/ 1327], train_loss/perplexity = 4.72228193/112.4245071 secs/batch = 0.6141s, grad.norm=0.73144001\n",
      " 21020: 15 [ 1115/ 1327], train_loss/perplexity = 4.35388803/77.7802887 secs/batch = 0.6031s, grad.norm=0.61924261\n",
      " 21025: 15 [ 1120/ 1327], train_loss/perplexity = 4.58202744/97.7122955 secs/batch = 0.6124s, grad.norm=0.66565174\n",
      " 21030: 15 [ 1125/ 1327], train_loss/perplexity = 4.83463812/125.7930527 secs/batch = 0.6081s, grad.norm=0.66885138\n",
      " 21035: 15 [ 1130/ 1327], train_loss/perplexity = 4.47078609/87.4254227 secs/batch = 0.6084s, grad.norm=0.66061807\n",
      " 21040: 15 [ 1135/ 1327], train_loss/perplexity = 4.48286152/88.4875183 secs/batch = 0.6147s, grad.norm=0.62689072\n",
      " 21045: 15 [ 1140/ 1327], train_loss/perplexity = 4.73081350/113.3877640 secs/batch = 0.6030s, grad.norm=0.65886301\n",
      " 21050: 15 [ 1145/ 1327], train_loss/perplexity = 4.53332376/93.0673828 secs/batch = 0.6147s, grad.norm=0.65379155\n",
      " 21055: 15 [ 1150/ 1327], train_loss/perplexity = 4.50158739/90.1601334 secs/batch = 0.6146s, grad.norm=0.63864386\n",
      " 21060: 15 [ 1155/ 1327], train_loss/perplexity = 4.62517166/102.0202866 secs/batch = 0.6123s, grad.norm=0.66368306\n",
      " 21065: 15 [ 1160/ 1327], train_loss/perplexity = 4.55787945/95.3810043 secs/batch = 0.6092s, grad.norm=0.66830873\n",
      " 21070: 15 [ 1165/ 1327], train_loss/perplexity = 4.60137892/99.6215897 secs/batch = 0.6176s, grad.norm=0.68682069\n",
      " 21075: 15 [ 1170/ 1327], train_loss/perplexity = 4.43428373/84.2917252 secs/batch = 0.6107s, grad.norm=0.66022199\n",
      " 21080: 15 [ 1175/ 1327], train_loss/perplexity = 4.25885630/70.7290421 secs/batch = 0.6125s, grad.norm=0.69828159\n",
      " 21085: 15 [ 1180/ 1327], train_loss/perplexity = 4.27256441/71.7052841 secs/batch = 0.6087s, grad.norm=0.68723094\n",
      " 21090: 15 [ 1185/ 1327], train_loss/perplexity = 4.47204161/87.5352554 secs/batch = 0.6035s, grad.norm=0.64324325\n",
      " 21095: 15 [ 1190/ 1327], train_loss/perplexity = 4.55616856/95.2179565 secs/batch = 0.6115s, grad.norm=0.64948630\n",
      " 21100: 15 [ 1195/ 1327], train_loss/perplexity = 4.32699871/75.7166977 secs/batch = 0.6094s, grad.norm=0.67141157\n",
      " 21105: 15 [ 1200/ 1327], train_loss/perplexity = 4.31213713/74.5997467 secs/batch = 0.6150s, grad.norm=0.66857260\n",
      " 21110: 15 [ 1205/ 1327], train_loss/perplexity = 4.35198784/77.6326294 secs/batch = 0.6105s, grad.norm=0.66030365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21115: 15 [ 1210/ 1327], train_loss/perplexity = 4.02218485/55.8229370 secs/batch = 0.6147s, grad.norm=0.69330841\n",
      " 21120: 15 [ 1215/ 1327], train_loss/perplexity = 4.26810741/71.3864059 secs/batch = 0.6091s, grad.norm=0.64703071\n",
      " 21125: 15 [ 1220/ 1327], train_loss/perplexity = 4.31771708/75.0171738 secs/batch = 0.6128s, grad.norm=0.67604649\n",
      " 21130: 15 [ 1225/ 1327], train_loss/perplexity = 4.12093449/61.6167946 secs/batch = 0.6094s, grad.norm=0.67021382\n",
      " 21135: 15 [ 1230/ 1327], train_loss/perplexity = 4.38619089/80.3338318 secs/batch = 0.6069s, grad.norm=0.64775705\n",
      " 21140: 15 [ 1235/ 1327], train_loss/perplexity = 4.32122326/75.2806625 secs/batch = 0.6113s, grad.norm=0.63769078\n",
      " 21145: 15 [ 1240/ 1327], train_loss/perplexity = 4.55412722/95.0237808 secs/batch = 0.6153s, grad.norm=0.66965413\n",
      " 21150: 15 [ 1245/ 1327], train_loss/perplexity = 4.48357296/88.5504990 secs/batch = 0.6248s, grad.norm=0.67652541\n",
      " 21155: 15 [ 1250/ 1327], train_loss/perplexity = 4.53055382/92.8099442 secs/batch = 0.6115s, grad.norm=0.61048990\n",
      " 21160: 15 [ 1255/ 1327], train_loss/perplexity = 4.60283327/99.7665787 secs/batch = 0.6104s, grad.norm=0.65409398\n",
      " 21165: 15 [ 1260/ 1327], train_loss/perplexity = 4.43413639/84.2793121 secs/batch = 0.6061s, grad.norm=0.69969088\n",
      " 21170: 15 [ 1265/ 1327], train_loss/perplexity = 4.57662439/97.1857758 secs/batch = 0.6037s, grad.norm=0.67453837\n",
      " 21175: 15 [ 1270/ 1327], train_loss/perplexity = 4.38820362/80.4956894 secs/batch = 0.6101s, grad.norm=0.67340875\n",
      " 21180: 15 [ 1275/ 1327], train_loss/perplexity = 4.53179598/92.9253006 secs/batch = 0.6093s, grad.norm=0.68785876\n",
      " 21185: 15 [ 1280/ 1327], train_loss/perplexity = 4.39083338/80.7076492 secs/batch = 0.6043s, grad.norm=0.66548711\n",
      " 21190: 15 [ 1285/ 1327], train_loss/perplexity = 4.33852863/76.5947571 secs/batch = 0.6114s, grad.norm=0.65871263\n",
      " 21195: 15 [ 1290/ 1327], train_loss/perplexity = 4.57990122/97.5047607 secs/batch = 0.6103s, grad.norm=0.66145539\n",
      " 21200: 15 [ 1295/ 1327], train_loss/perplexity = 4.54536915/94.1951904 secs/batch = 0.6482s, grad.norm=0.62443018\n",
      " 21205: 15 [ 1300/ 1327], train_loss/perplexity = 4.67800236/107.5550003 secs/batch = 0.6198s, grad.norm=0.63400841\n",
      " 21210: 15 [ 1305/ 1327], train_loss/perplexity = 4.77306080/118.2807236 secs/batch = 0.6071s, grad.norm=0.66418970\n",
      " 21215: 15 [ 1310/ 1327], train_loss/perplexity = 4.99455452/147.6071777 secs/batch = 0.6114s, grad.norm=0.64929307\n",
      " 21220: 15 [ 1315/ 1327], train_loss/perplexity = 4.85088205/127.8531113 secs/batch = 0.6116s, grad.norm=0.67718655\n",
      " 21225: 15 [ 1320/ 1327], train_loss/perplexity = 4.79941320/121.4391403 secs/batch = 0.6100s, grad.norm=0.64895642\n",
      " 21230: 15 [ 1325/ 1327], train_loss/perplexity = 4.73450661/113.8072968 secs/batch = 0.6091s, grad.norm=0.66804296\n",
      "Epoch training time: 812.5405418872833\n",
      "Saved char model cv/epoch015_4.6482.model\n",
      " 21237: 16 [    5/ 1327], train_loss/perplexity = 4.81165266/122.9346161 secs/batch = 0.6123s, grad.norm=0.66348046\n",
      " 21242: 16 [   10/ 1327], train_loss/perplexity = 4.36286402/78.4815826 secs/batch = 0.6091s, grad.norm=0.64199281\n",
      " 21247: 16 [   15/ 1327], train_loss/perplexity = 4.63281679/102.8032303 secs/batch = 0.6270s, grad.norm=0.66128993\n",
      " 21252: 16 [   20/ 1327], train_loss/perplexity = 4.80641508/122.2924194 secs/batch = 0.6067s, grad.norm=0.68360430\n",
      " 21257: 16 [   25/ 1327], train_loss/perplexity = 4.68187380/107.9721985 secs/batch = 0.6104s, grad.norm=0.67068839\n",
      " 21262: 16 [   30/ 1327], train_loss/perplexity = 4.64295101/103.8503571 secs/batch = 0.6129s, grad.norm=0.67819893\n",
      " 21267: 16 [   35/ 1327], train_loss/perplexity = 4.49997711/90.0150681 secs/batch = 0.6074s, grad.norm=0.63877201\n",
      " 21272: 16 [   40/ 1327], train_loss/perplexity = 4.54588604/94.2438965 secs/batch = 0.6072s, grad.norm=0.66532457\n",
      " 21277: 16 [   45/ 1327], train_loss/perplexity = 4.25807142/70.6735535 secs/batch = 0.6071s, grad.norm=0.63737541\n",
      " 21282: 16 [   50/ 1327], train_loss/perplexity = 4.50802708/90.7426147 secs/batch = 0.6075s, grad.norm=0.65350395\n",
      " 21287: 16 [   55/ 1327], train_loss/perplexity = 4.43680096/84.5041733 secs/batch = 0.6079s, grad.norm=0.69424343\n",
      " 21292: 16 [   60/ 1327], train_loss/perplexity = 4.70936584/110.9817581 secs/batch = 0.6176s, grad.norm=0.68616754\n",
      " 21297: 16 [   65/ 1327], train_loss/perplexity = 4.30745554/74.2513199 secs/batch = 0.6140s, grad.norm=0.64608818\n",
      " 21302: 16 [   70/ 1327], train_loss/perplexity = 4.19516897/66.3649445 secs/batch = 0.6147s, grad.norm=0.66217589\n",
      " 21307: 16 [   75/ 1327], train_loss/perplexity = 4.08510494/59.4481773 secs/batch = 0.6058s, grad.norm=0.66473168\n",
      " 21312: 16 [   80/ 1327], train_loss/perplexity = 4.45636082/86.1733398 secs/batch = 0.6151s, grad.norm=0.65277535\n",
      " 21317: 16 [   85/ 1327], train_loss/perplexity = 4.50094461/90.1022034 secs/batch = 0.6057s, grad.norm=0.67347842\n",
      " 21322: 16 [   90/ 1327], train_loss/perplexity = 4.49658251/89.7100220 secs/batch = 0.6093s, grad.norm=0.66557229\n",
      " 21327: 16 [   95/ 1327], train_loss/perplexity = 4.38705683/80.4034271 secs/batch = 0.6103s, grad.norm=0.64161295\n",
      " 21332: 16 [  100/ 1327], train_loss/perplexity = 4.61384439/100.8711929 secs/batch = 0.6116s, grad.norm=0.67168605\n",
      " 21337: 16 [  105/ 1327], train_loss/perplexity = 4.55212736/94.8339386 secs/batch = 0.6116s, grad.norm=0.68829876\n",
      " 21342: 16 [  110/ 1327], train_loss/perplexity = 4.38901472/80.5610046 secs/batch = 0.6135s, grad.norm=0.69206339\n",
      " 21347: 16 [  115/ 1327], train_loss/perplexity = 4.30021811/73.7158661 secs/batch = 0.6511s, grad.norm=0.67734265\n",
      " 21352: 16 [  120/ 1327], train_loss/perplexity = 4.40299559/81.6952286 secs/batch = 0.6081s, grad.norm=0.71533960\n",
      " 21357: 16 [  125/ 1327], train_loss/perplexity = 4.56457376/96.0216599 secs/batch = 0.6065s, grad.norm=0.77813530\n",
      " 21362: 16 [  130/ 1327], train_loss/perplexity = 4.40317297/81.7097168 secs/batch = 0.6180s, grad.norm=0.70034891\n",
      " 21367: 16 [  135/ 1327], train_loss/perplexity = 4.43771648/84.5815811 secs/batch = 0.6150s, grad.norm=0.64959896\n",
      " 21372: 16 [  140/ 1327], train_loss/perplexity = 4.75962877/116.7025909 secs/batch = 0.6095s, grad.norm=0.66395062\n",
      " 21377: 16 [  145/ 1327], train_loss/perplexity = 4.69401121/109.2906876 secs/batch = 0.6106s, grad.norm=0.67670113\n",
      " 21382: 16 [  150/ 1327], train_loss/perplexity = 4.60375595/99.8586731 secs/batch = 0.6134s, grad.norm=0.66386610\n",
      " 21387: 16 [  155/ 1327], train_loss/perplexity = 4.87362862/130.7946625 secs/batch = 0.6120s, grad.norm=0.66379946\n",
      " 21392: 16 [  160/ 1327], train_loss/perplexity = 4.49885893/89.9144745 secs/batch = 0.6099s, grad.norm=0.61825728\n",
      " 21397: 16 [  165/ 1327], train_loss/perplexity = 4.73935843/114.3608093 secs/batch = 0.6099s, grad.norm=0.65581989\n",
      " 21402: 16 [  170/ 1327], train_loss/perplexity = 4.49702597/89.7498169 secs/batch = 0.6057s, grad.norm=0.66889167\n",
      " 21407: 16 [  175/ 1327], train_loss/perplexity = 4.78271294/119.4279099 secs/batch = 0.6134s, grad.norm=0.66220933\n",
      " 21412: 16 [  180/ 1327], train_loss/perplexity = 4.64233065/103.7859573 secs/batch = 0.6181s, grad.norm=0.64737874\n",
      " 21417: 16 [  185/ 1327], train_loss/perplexity = 4.92077351/137.1086273 secs/batch = 0.6149s, grad.norm=0.68515611\n",
      " 21422: 16 [  190/ 1327], train_loss/perplexity = 4.41182375/82.4196396 secs/batch = 0.6127s, grad.norm=0.62766731\n",
      " 21427: 16 [  195/ 1327], train_loss/perplexity = 4.69590521/109.4978790 secs/batch = 0.6210s, grad.norm=0.64936757\n",
      " 21432: 16 [  200/ 1327], train_loss/perplexity = 4.56379509/95.9469147 secs/batch = 0.6077s, grad.norm=0.64270151\n",
      " 21437: 16 [  205/ 1327], train_loss/perplexity = 4.71646261/111.7721710 secs/batch = 0.6152s, grad.norm=0.64774299\n",
      " 21442: 16 [  210/ 1327], train_loss/perplexity = 4.65780783/105.4047623 secs/batch = 0.6086s, grad.norm=0.65361887\n",
      " 21447: 16 [  215/ 1327], train_loss/perplexity = 4.69625664/109.5363693 secs/batch = 0.6124s, grad.norm=0.62757701\n",
      " 21452: 16 [  220/ 1327], train_loss/perplexity = 4.66550016/106.2186966 secs/batch = 0.6057s, grad.norm=0.65803385\n",
      " 21457: 16 [  225/ 1327], train_loss/perplexity = 4.87683821/131.2151337 secs/batch = 0.6168s, grad.norm=0.64263785\n",
      " 21462: 16 [  230/ 1327], train_loss/perplexity = 4.65208292/104.8030548 secs/batch = 0.6059s, grad.norm=0.68115121\n",
      " 21467: 16 [  235/ 1327], train_loss/perplexity = 4.61112165/100.5969238 secs/batch = 0.6199s, grad.norm=0.65686071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21472: 16 [  240/ 1327], train_loss/perplexity = 4.39841175/81.3216095 secs/batch = 0.6110s, grad.norm=0.69095069\n",
      " 21477: 16 [  245/ 1327], train_loss/perplexity = 4.71091413/111.1537247 secs/batch = 0.6195s, grad.norm=0.66370600\n",
      " 21482: 16 [  250/ 1327], train_loss/perplexity = 4.48393440/88.5825043 secs/batch = 0.6136s, grad.norm=0.64151597\n",
      " 21487: 16 [  255/ 1327], train_loss/perplexity = 4.44176102/84.9243622 secs/batch = 0.6166s, grad.norm=0.62784022\n",
      " 21492: 16 [  260/ 1327], train_loss/perplexity = 4.74840689/115.4002914 secs/batch = 0.6516s, grad.norm=0.70631623\n",
      " 21497: 16 [  265/ 1327], train_loss/perplexity = 4.85605240/128.5158691 secs/batch = 0.6012s, grad.norm=0.65750164\n",
      " 21502: 16 [  270/ 1327], train_loss/perplexity = 4.92131186/137.1824646 secs/batch = 0.6120s, grad.norm=0.64950651\n",
      " 21507: 16 [  275/ 1327], train_loss/perplexity = 4.88118601/131.7868652 secs/batch = 0.6069s, grad.norm=0.66745985\n",
      " 21512: 16 [  280/ 1327], train_loss/perplexity = 4.66244125/105.8942795 secs/batch = 0.6137s, grad.norm=0.67238235\n",
      " 21517: 16 [  285/ 1327], train_loss/perplexity = 4.89007425/132.9634399 secs/batch = 0.6081s, grad.norm=0.67229474\n",
      " 21522: 16 [  290/ 1327], train_loss/perplexity = 4.69129372/108.9940948 secs/batch = 0.6097s, grad.norm=0.70083946\n",
      " 21527: 16 [  295/ 1327], train_loss/perplexity = 4.47586060/87.8701859 secs/batch = 0.6078s, grad.norm=0.65182453\n",
      " 21532: 16 [  300/ 1327], train_loss/perplexity = 4.04520559/57.1229286 secs/batch = 0.6086s, grad.norm=0.62419522\n",
      " 21537: 16 [  305/ 1327], train_loss/perplexity = 4.51796579/91.6489716 secs/batch = 0.6068s, grad.norm=0.65611637\n",
      " 21542: 16 [  310/ 1327], train_loss/perplexity = 4.55580759/95.1835938 secs/batch = 0.6058s, grad.norm=0.66234994\n",
      " 21547: 16 [  315/ 1327], train_loss/perplexity = 4.17697334/65.1683121 secs/batch = 0.6140s, grad.norm=0.67077589\n",
      " 21552: 16 [  320/ 1327], train_loss/perplexity = 4.11346245/61.1581078 secs/batch = 0.6132s, grad.norm=0.67511910\n",
      " 21557: 16 [  325/ 1327], train_loss/perplexity = 4.08347225/59.3511963 secs/batch = 0.6129s, grad.norm=0.63474447\n",
      " 21562: 16 [  330/ 1327], train_loss/perplexity = 4.59136868/98.6293259 secs/batch = 0.6040s, grad.norm=0.67235875\n",
      " 21567: 16 [  335/ 1327], train_loss/perplexity = 4.03275967/56.4163857 secs/batch = 0.6100s, grad.norm=0.65555137\n",
      " 21572: 16 [  340/ 1327], train_loss/perplexity = 4.74504900/115.0134430 secs/batch = 0.6087s, grad.norm=0.65964615\n",
      " 21577: 16 [  345/ 1327], train_loss/perplexity = 4.61251163/100.7368469 secs/batch = 0.6104s, grad.norm=0.63045692\n",
      " 21582: 16 [  350/ 1327], train_loss/perplexity = 4.61867094/101.3592300 secs/batch = 0.6120s, grad.norm=0.69123155\n",
      " 21587: 16 [  355/ 1327], train_loss/perplexity = 4.59947348/99.4319458 secs/batch = 0.6160s, grad.norm=0.65785825\n",
      " 21592: 16 [  360/ 1327], train_loss/perplexity = 4.74711561/115.2513733 secs/batch = 0.6216s, grad.norm=0.68647492\n",
      " 21597: 16 [  365/ 1327], train_loss/perplexity = 4.66233778/105.8833237 secs/batch = 0.6165s, grad.norm=0.62778085\n",
      " 21602: 16 [  370/ 1327], train_loss/perplexity = 4.77621603/118.6545181 secs/batch = 0.6072s, grad.norm=0.70692599\n",
      " 21607: 16 [  375/ 1327], train_loss/perplexity = 4.11791420/61.4309769 secs/batch = 0.6116s, grad.norm=0.65646088\n",
      " 21612: 16 [  380/ 1327], train_loss/perplexity = 4.28195906/72.3821030 secs/batch = 0.6097s, grad.norm=0.68508315\n",
      " 21617: 16 [  385/ 1327], train_loss/perplexity = 4.46623135/87.0281219 secs/batch = 0.6054s, grad.norm=0.67002857\n",
      " 21622: 16 [  390/ 1327], train_loss/perplexity = 4.51512098/91.3886185 secs/batch = 0.6068s, grad.norm=0.64679146\n",
      " 21627: 16 [  395/ 1327], train_loss/perplexity = 4.70995474/111.0471344 secs/batch = 0.6159s, grad.norm=0.67862177\n",
      " 21632: 16 [  400/ 1327], train_loss/perplexity = 4.52918577/92.6830673 secs/batch = 0.6099s, grad.norm=0.65120703\n",
      " 21637: 16 [  405/ 1327], train_loss/perplexity = 4.89615154/133.7739716 secs/batch = 0.6107s, grad.norm=0.68895477\n",
      " 21642: 16 [  410/ 1327], train_loss/perplexity = 4.51624775/91.4916534 secs/batch = 0.6131s, grad.norm=0.63307881\n",
      " 21647: 16 [  415/ 1327], train_loss/perplexity = 4.39143705/80.7563858 secs/batch = 0.6184s, grad.norm=0.67101800\n",
      " 21652: 16 [  420/ 1327], train_loss/perplexity = 4.20013380/66.6952515 secs/batch = 0.6076s, grad.norm=0.67153418\n",
      " 21657: 16 [  425/ 1327], train_loss/perplexity = 4.46903324/87.2723083 secs/batch = 0.6126s, grad.norm=0.65595686\n",
      " 21662: 16 [  430/ 1327], train_loss/perplexity = 4.66965866/106.6613312 secs/batch = 0.6143s, grad.norm=0.68412644\n",
      " 21667: 16 [  435/ 1327], train_loss/perplexity = 4.67348003/107.0697021 secs/batch = 0.6119s, grad.norm=0.68544424\n",
      " 21672: 16 [  440/ 1327], train_loss/perplexity = 4.33021879/75.9609070 secs/batch = 0.6119s, grad.norm=0.66551125\n",
      " 21677: 16 [  445/ 1327], train_loss/perplexity = 4.62986040/102.4997559 secs/batch = 0.6111s, grad.norm=0.69628668\n",
      " 21682: 16 [  450/ 1327], train_loss/perplexity = 4.51492262/91.3704987 secs/batch = 0.6100s, grad.norm=0.67555082\n",
      " 21687: 16 [  455/ 1327], train_loss/perplexity = 4.44019508/84.7914810 secs/batch = 0.6227s, grad.norm=0.67172962\n",
      " 21692: 16 [  460/ 1327], train_loss/perplexity = 4.46419716/86.8512726 secs/batch = 0.6136s, grad.norm=0.65613836\n",
      " 21697: 16 [  465/ 1327], train_loss/perplexity = 4.28899240/72.8929825 secs/batch = 0.6147s, grad.norm=0.68810165\n",
      " 21702: 16 [  470/ 1327], train_loss/perplexity = 4.87508535/130.9853363 secs/batch = 0.6093s, grad.norm=0.66424888\n",
      " 21707: 16 [  475/ 1327], train_loss/perplexity = 4.34506083/77.0967255 secs/batch = 0.6098s, grad.norm=0.62331790\n",
      " 21712: 16 [  480/ 1327], train_loss/perplexity = 4.54653311/94.3048935 secs/batch = 0.6083s, grad.norm=0.68864936\n",
      " 21717: 16 [  485/ 1327], train_loss/perplexity = 4.50343323/90.3267136 secs/batch = 0.6078s, grad.norm=0.66234517\n",
      " 21722: 16 [  490/ 1327], train_loss/perplexity = 4.30807734/74.2975006 secs/batch = 0.6144s, grad.norm=0.69434416\n",
      " 21727: 16 [  495/ 1327], train_loss/perplexity = 4.37834644/79.7061234 secs/batch = 0.6164s, grad.norm=0.66625792\n",
      " 21732: 16 [  500/ 1327], train_loss/perplexity = 4.66539621/106.2076569 secs/batch = 0.6213s, grad.norm=0.64965576\n",
      " 21737: 16 [  505/ 1327], train_loss/perplexity = 4.63440323/102.9664536 secs/batch = 0.6420s, grad.norm=0.63171428\n",
      " 21742: 16 [  510/ 1327], train_loss/perplexity = 4.98019648/145.5029602 secs/batch = 0.6100s, grad.norm=0.65625048\n",
      " 21747: 16 [  515/ 1327], train_loss/perplexity = 4.64197493/103.7490387 secs/batch = 0.6071s, grad.norm=0.64385313\n",
      " 21752: 16 [  520/ 1327], train_loss/perplexity = 4.82328415/124.3728790 secs/batch = 0.6074s, grad.norm=0.69173706\n",
      " 21757: 16 [  525/ 1327], train_loss/perplexity = 4.42813301/83.7748642 secs/batch = 0.6128s, grad.norm=0.67354953\n",
      " 21762: 16 [  530/ 1327], train_loss/perplexity = 4.48675871/88.8330460 secs/batch = 0.6126s, grad.norm=0.67207235\n",
      " 21767: 16 [  535/ 1327], train_loss/perplexity = 4.61788082/101.2791748 secs/batch = 0.6088s, grad.norm=0.64768696\n",
      " 21772: 16 [  540/ 1327], train_loss/perplexity = 4.67741776/107.4921417 secs/batch = 0.6074s, grad.norm=0.64171231\n",
      " 21777: 16 [  545/ 1327], train_loss/perplexity = 4.68197155/107.9827576 secs/batch = 0.6115s, grad.norm=0.67583489\n",
      " 21782: 16 [  550/ 1327], train_loss/perplexity = 4.68177223/107.9612350 secs/batch = 0.6092s, grad.norm=0.67030948\n",
      " 21787: 16 [  555/ 1327], train_loss/perplexity = 4.49846935/89.8794556 secs/batch = 0.6095s, grad.norm=0.64444810\n",
      " 21792: 16 [  560/ 1327], train_loss/perplexity = 4.53808594/93.5116425 secs/batch = 0.6145s, grad.norm=0.71746898\n",
      " 21797: 16 [  565/ 1327], train_loss/perplexity = 4.48397160/88.5858002 secs/batch = 0.6134s, grad.norm=0.66588062\n",
      " 21802: 16 [  570/ 1327], train_loss/perplexity = 4.43825769/84.6273651 secs/batch = 0.6129s, grad.norm=0.67925286\n",
      " 21807: 16 [  575/ 1327], train_loss/perplexity = 4.27314758/71.7471085 secs/batch = 0.6145s, grad.norm=0.66936064\n",
      " 21812: 16 [  580/ 1327], train_loss/perplexity = 4.62768364/102.2768784 secs/batch = 0.6180s, grad.norm=0.68558371\n",
      " 21817: 16 [  585/ 1327], train_loss/perplexity = 4.24068546/69.4554443 secs/batch = 0.6132s, grad.norm=0.63945740\n",
      " 21822: 16 [  590/ 1327], train_loss/perplexity = 4.59816837/99.3022614 secs/batch = 0.6145s, grad.norm=0.68428224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21827: 16 [  595/ 1327], train_loss/perplexity = 4.50998783/90.9207153 secs/batch = 0.6175s, grad.norm=0.68956125\n",
      " 21832: 16 [  600/ 1327], train_loss/perplexity = 4.80248260/121.8124542 secs/batch = 0.6173s, grad.norm=0.69121289\n",
      " 21837: 16 [  605/ 1327], train_loss/perplexity = 4.74881887/115.4478455 secs/batch = 0.6545s, grad.norm=0.65481025\n",
      " 21842: 16 [  610/ 1327], train_loss/perplexity = 4.81174612/122.9461060 secs/batch = 0.6117s, grad.norm=0.68667269\n",
      " 21847: 16 [  615/ 1327], train_loss/perplexity = 4.38686466/80.3879776 secs/batch = 0.6175s, grad.norm=0.65373635\n",
      " 21852: 16 [  620/ 1327], train_loss/perplexity = 4.67983818/107.7526321 secs/batch = 0.6055s, grad.norm=0.66042912\n",
      " 21857: 16 [  625/ 1327], train_loss/perplexity = 4.77119350/118.0600662 secs/batch = 0.6069s, grad.norm=0.65939325\n",
      " 21862: 16 [  630/ 1327], train_loss/perplexity = 4.85489845/128.3676605 secs/batch = 0.6112s, grad.norm=0.65901059\n",
      " 21867: 16 [  635/ 1327], train_loss/perplexity = 4.56235504/95.8088455 secs/batch = 0.6098s, grad.norm=0.66667074\n",
      " 21872: 16 [  640/ 1327], train_loss/perplexity = 4.61116886/100.6016693 secs/batch = 0.6106s, grad.norm=0.66247612\n",
      " 21877: 16 [  645/ 1327], train_loss/perplexity = 4.74562597/115.0798187 secs/batch = 0.6154s, grad.norm=0.69407070\n",
      " 21882: 16 [  650/ 1327], train_loss/perplexity = 4.34773064/77.3028336 secs/batch = 0.6082s, grad.norm=0.65976459\n",
      " 21887: 16 [  655/ 1327], train_loss/perplexity = 4.53532982/93.2542648 secs/batch = 0.6134s, grad.norm=0.68291932\n",
      " 21892: 16 [  660/ 1327], train_loss/perplexity = 4.40580797/81.9253082 secs/batch = 0.6099s, grad.norm=0.64104193\n",
      " 21897: 16 [  665/ 1327], train_loss/perplexity = 4.60779428/100.2627563 secs/batch = 0.6090s, grad.norm=0.66634065\n",
      " 21902: 16 [  670/ 1327], train_loss/perplexity = 4.45919800/86.4181747 secs/batch = 0.6146s, grad.norm=0.65315080\n",
      " 21907: 16 [  675/ 1327], train_loss/perplexity = 4.32372570/75.4692841 secs/batch = 0.6111s, grad.norm=0.72831923\n",
      " 21912: 16 [  680/ 1327], train_loss/perplexity = 4.57181740/96.7197266 secs/batch = 0.6114s, grad.norm=0.68166989\n",
      " 21917: 16 [  685/ 1327], train_loss/perplexity = 4.44921684/85.5599136 secs/batch = 0.6077s, grad.norm=0.65698707\n",
      " 21922: 16 [  690/ 1327], train_loss/perplexity = 4.72686195/112.9405899 secs/batch = 0.6151s, grad.norm=0.66217422\n",
      " 21927: 16 [  695/ 1327], train_loss/perplexity = 4.54664612/94.3155518 secs/batch = 0.6106s, grad.norm=0.65545440\n",
      " 21932: 16 [  700/ 1327], train_loss/perplexity = 4.82109118/124.1004333 secs/batch = 0.6192s, grad.norm=0.68593991\n",
      " 21937: 16 [  705/ 1327], train_loss/perplexity = 4.51289129/91.1850815 secs/batch = 0.6139s, grad.norm=0.66552669\n",
      " 21942: 16 [  710/ 1327], train_loss/perplexity = 4.38638020/80.3490448 secs/batch = 0.6072s, grad.norm=0.65695786\n",
      " 21947: 16 [  715/ 1327], train_loss/perplexity = 4.43609190/84.4442825 secs/batch = 0.6063s, grad.norm=0.65341365\n",
      " 21952: 16 [  720/ 1327], train_loss/perplexity = 4.48322153/88.5193787 secs/batch = 0.6108s, grad.norm=0.69713235\n",
      " 21957: 16 [  725/ 1327], train_loss/perplexity = 4.34906530/77.4060745 secs/batch = 0.6104s, grad.norm=0.67461199\n",
      " 21962: 16 [  730/ 1327], train_loss/perplexity = 4.55288649/94.9059601 secs/batch = 0.6090s, grad.norm=0.67154652\n",
      " 21967: 16 [  735/ 1327], train_loss/perplexity = 4.61541939/101.0301895 secs/batch = 0.6049s, grad.norm=0.67700952\n",
      " 21972: 16 [  740/ 1327], train_loss/perplexity = 4.02997828/56.2596893 secs/batch = 0.6034s, grad.norm=0.61597830\n",
      " 21977: 16 [  745/ 1327], train_loss/perplexity = 4.47921896/88.1657867 secs/batch = 0.6092s, grad.norm=0.66958404\n",
      " 21982: 16 [  750/ 1327], train_loss/perplexity = 4.38563251/80.2889938 secs/batch = 0.6141s, grad.norm=0.68468773\n",
      " 21987: 16 [  755/ 1327], train_loss/perplexity = 4.36507416/78.6552353 secs/batch = 0.6064s, grad.norm=0.66238457\n",
      " 21992: 16 [  760/ 1327], train_loss/perplexity = 4.21118259/67.4362411 secs/batch = 0.6145s, grad.norm=0.64519173\n",
      " 21997: 16 [  765/ 1327], train_loss/perplexity = 4.33870125/76.6079788 secs/batch = 0.6059s, grad.norm=0.66815215\n",
      " 22002: 16 [  770/ 1327], train_loss/perplexity = 4.21532869/67.7164230 secs/batch = 0.6107s, grad.norm=0.67527831\n",
      " 22007: 16 [  775/ 1327], train_loss/perplexity = 4.39070272/80.6971054 secs/batch = 0.6168s, grad.norm=0.70412856\n",
      " 22012: 16 [  780/ 1327], train_loss/perplexity = 4.65101957/104.6916733 secs/batch = 0.6067s, grad.norm=0.70364225\n",
      " 22017: 16 [  785/ 1327], train_loss/perplexity = 4.52737808/92.5156708 secs/batch = 0.6056s, grad.norm=0.68504488\n",
      " 22022: 16 [  790/ 1327], train_loss/perplexity = 4.35544395/77.9014053 secs/batch = 0.6044s, grad.norm=0.69239026\n",
      " 22027: 16 [  795/ 1327], train_loss/perplexity = 4.65662575/105.2802429 secs/batch = 0.6094s, grad.norm=0.67002314\n",
      " 22032: 16 [  800/ 1327], train_loss/perplexity = 4.59330273/98.8202667 secs/batch = 0.6192s, grad.norm=0.68975651\n",
      " 22037: 16 [  805/ 1327], train_loss/perplexity = 4.93039417/138.4340668 secs/batch = 0.6081s, grad.norm=0.65587962\n",
      " 22042: 16 [  810/ 1327], train_loss/perplexity = 4.50393057/90.3716431 secs/batch = 0.6055s, grad.norm=0.64325178\n",
      " 22047: 16 [  815/ 1327], train_loss/perplexity = 4.44922733/85.5608063 secs/batch = 0.6062s, grad.norm=0.68088281\n",
      " 22052: 16 [  820/ 1327], train_loss/perplexity = 4.21448755/67.6594849 secs/batch = 0.6102s, grad.norm=0.62452066\n",
      " 22057: 16 [  825/ 1327], train_loss/perplexity = 4.42896461/83.8445587 secs/batch = 0.6124s, grad.norm=0.64130360\n",
      " 22062: 16 [  830/ 1327], train_loss/perplexity = 4.23126936/68.8045120 secs/batch = 0.6075s, grad.norm=0.67994201\n",
      " 22067: 16 [  835/ 1327], train_loss/perplexity = 4.47574806/87.8602982 secs/batch = 0.6089s, grad.norm=0.68807930\n",
      " 22072: 16 [  840/ 1327], train_loss/perplexity = 4.64318323/103.8744812 secs/batch = 0.6149s, grad.norm=0.67081499\n",
      " 22077: 16 [  845/ 1327], train_loss/perplexity = 4.37957716/79.8042831 secs/batch = 0.6076s, grad.norm=0.65647215\n",
      " 22082: 16 [  850/ 1327], train_loss/perplexity = 4.50896215/90.8275070 secs/batch = 0.6078s, grad.norm=0.63518065\n",
      " 22087: 16 [  855/ 1327], train_loss/perplexity = 4.51015520/90.9359283 secs/batch = 0.6093s, grad.norm=0.71506536\n",
      " 22092: 16 [  860/ 1327], train_loss/perplexity = 4.15902615/64.0091553 secs/batch = 0.6091s, grad.norm=0.64263290\n",
      " 22097: 16 [  865/ 1327], train_loss/perplexity = 4.69807386/109.7356033 secs/batch = 0.6137s, grad.norm=0.67820525\n",
      " 22102: 16 [  870/ 1327], train_loss/perplexity = 4.55677605/95.2758179 secs/batch = 0.6045s, grad.norm=0.67042977\n",
      " 22107: 16 [  875/ 1327], train_loss/perplexity = 4.14588785/63.1736870 secs/batch = 0.6102s, grad.norm=0.67164940\n",
      " 22112: 16 [  880/ 1327], train_loss/perplexity = 4.40846920/82.1436234 secs/batch = 0.6052s, grad.norm=0.63639081\n",
      " 22117: 16 [  885/ 1327], train_loss/perplexity = 4.46324444/86.7685699 secs/batch = 0.6060s, grad.norm=0.64380914\n",
      " 22122: 16 [  890/ 1327], train_loss/perplexity = 4.65469599/105.0772705 secs/batch = 0.6122s, grad.norm=0.66707134\n",
      " 22127: 16 [  895/ 1327], train_loss/perplexity = 4.65333843/104.9347153 secs/batch = 0.6295s, grad.norm=0.66463435\n",
      " 22132: 16 [  900/ 1327], train_loss/perplexity = 4.54040146/93.7284241 secs/batch = 0.6530s, grad.norm=0.65818000\n",
      " 22137: 16 [  905/ 1327], train_loss/perplexity = 4.33147621/76.0564804 secs/batch = 0.6092s, grad.norm=0.64086282\n",
      " 22142: 16 [  910/ 1327], train_loss/perplexity = 4.49529648/89.5947266 secs/batch = 0.6228s, grad.norm=0.66973668\n",
      " 22147: 16 [  915/ 1327], train_loss/perplexity = 4.62122965/101.6189117 secs/batch = 0.6110s, grad.norm=0.64699459\n",
      " 22152: 16 [  920/ 1327], train_loss/perplexity = 4.82437754/124.5089417 secs/batch = 0.6025s, grad.norm=0.67279077\n",
      " 22157: 16 [  925/ 1327], train_loss/perplexity = 4.58639812/98.1403046 secs/batch = 0.6126s, grad.norm=0.70374811\n",
      " 22162: 16 [  930/ 1327], train_loss/perplexity = 4.57195663/96.7331924 secs/batch = 0.6097s, grad.norm=0.64828354\n",
      " 22167: 16 [  935/ 1327], train_loss/perplexity = 4.63670397/103.2036209 secs/batch = 0.6200s, grad.norm=0.66471857\n",
      " 22172: 16 [  940/ 1327], train_loss/perplexity = 4.59836912/99.3222046 secs/batch = 0.6146s, grad.norm=0.65440881\n",
      " 22177: 16 [  945/ 1327], train_loss/perplexity = 4.78334427/119.5033340 secs/batch = 0.6202s, grad.norm=0.67533630\n",
      " 22182: 16 [  950/ 1327], train_loss/perplexity = 4.49416113/89.4930649 secs/batch = 0.6098s, grad.norm=0.61921757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22187: 16 [  955/ 1327], train_loss/perplexity = 4.62050724/101.5455246 secs/batch = 0.6054s, grad.norm=0.65879548\n",
      " 22192: 16 [  960/ 1327], train_loss/perplexity = 4.92002439/137.0059509 secs/batch = 0.6114s, grad.norm=0.68970180\n",
      " 22197: 16 [  965/ 1327], train_loss/perplexity = 4.60507298/99.9902802 secs/batch = 0.6149s, grad.norm=0.63094169\n",
      " 22202: 16 [  970/ 1327], train_loss/perplexity = 4.87729692/131.2753296 secs/batch = 0.6053s, grad.norm=0.65094095\n",
      " 22207: 16 [  975/ 1327], train_loss/perplexity = 4.56266975/95.8390045 secs/batch = 0.6099s, grad.norm=0.66320819\n",
      " 22212: 16 [  980/ 1327], train_loss/perplexity = 4.37678862/79.5820541 secs/batch = 0.6103s, grad.norm=0.65513480\n",
      " 22217: 16 [  985/ 1327], train_loss/perplexity = 4.56652641/96.2093353 secs/batch = 0.6104s, grad.norm=0.67987096\n",
      " 22222: 16 [  990/ 1327], train_loss/perplexity = 4.70859289/110.8960037 secs/batch = 0.6088s, grad.norm=0.68398112\n",
      " 22227: 16 [  995/ 1327], train_loss/perplexity = 4.72895002/113.1766663 secs/batch = 0.6144s, grad.norm=0.66245884\n",
      " 22232: 16 [ 1000/ 1327], train_loss/perplexity = 4.22307777/68.2432022 secs/batch = 0.6085s, grad.norm=0.61097836\n",
      " 22237: 16 [ 1005/ 1327], train_loss/perplexity = 4.64448881/104.0101852 secs/batch = 0.6115s, grad.norm=0.68138540\n",
      " 22242: 16 [ 1010/ 1327], train_loss/perplexity = 4.29975367/73.6816406 secs/batch = 0.6104s, grad.norm=0.63732308\n",
      " 22247: 16 [ 1015/ 1327], train_loss/perplexity = 4.77619314/118.6518021 secs/batch = 0.6095s, grad.norm=0.67883623\n",
      " 22252: 16 [ 1020/ 1327], train_loss/perplexity = 4.87318516/130.7366638 secs/batch = 0.6113s, grad.norm=0.67294514\n",
      " 22257: 16 [ 1025/ 1327], train_loss/perplexity = 4.68085670/107.8624420 secs/batch = 0.6089s, grad.norm=0.65710455\n",
      " 22262: 16 [ 1030/ 1327], train_loss/perplexity = 4.50405884/90.3832397 secs/batch = 0.6126s, grad.norm=0.64483744\n",
      " 22267: 16 [ 1035/ 1327], train_loss/perplexity = 4.45629501/86.1676636 secs/batch = 0.6139s, grad.norm=0.68774271\n",
      " 22272: 16 [ 1040/ 1327], train_loss/perplexity = 4.71405983/111.5039291 secs/batch = 0.6129s, grad.norm=0.66917270\n",
      " 22277: 16 [ 1045/ 1327], train_loss/perplexity = 4.29094505/73.0354614 secs/batch = 0.6498s, grad.norm=0.64301193\n",
      " 22282: 16 [ 1050/ 1327], train_loss/perplexity = 4.34481812/77.0780182 secs/batch = 0.6133s, grad.norm=0.66551363\n",
      " 22287: 16 [ 1055/ 1327], train_loss/perplexity = 4.50314331/90.3005295 secs/batch = 0.6102s, grad.norm=0.66853952\n",
      " 22292: 16 [ 1060/ 1327], train_loss/perplexity = 4.13208580/62.3077507 secs/batch = 0.6067s, grad.norm=0.67956209\n",
      " 22297: 16 [ 1065/ 1327], train_loss/perplexity = 4.23015451/68.7278519 secs/batch = 0.6074s, grad.norm=0.66002315\n",
      " 22302: 16 [ 1070/ 1327], train_loss/perplexity = 4.55651331/95.2507935 secs/batch = 0.6100s, grad.norm=0.67959476\n",
      " 22307: 16 [ 1075/ 1327], train_loss/perplexity = 4.33117056/76.0332336 secs/batch = 0.6110s, grad.norm=0.65768540\n",
      " 22312: 16 [ 1080/ 1327], train_loss/perplexity = 4.29443645/73.2909012 secs/batch = 0.6114s, grad.norm=0.64695799\n",
      " 22317: 16 [ 1085/ 1327], train_loss/perplexity = 4.18501234/65.6943130 secs/batch = 0.6160s, grad.norm=0.63681430\n",
      " 22322: 16 [ 1090/ 1327], train_loss/perplexity = 4.37750673/79.6392212 secs/batch = 0.6496s, grad.norm=0.65001476\n",
      " 22327: 16 [ 1095/ 1327], train_loss/perplexity = 4.47601986/87.8841858 secs/batch = 0.6165s, grad.norm=0.71320963\n",
      " 22332: 16 [ 1100/ 1327], train_loss/perplexity = 4.24361372/69.6591263 secs/batch = 0.6098s, grad.norm=0.70746058\n",
      " 22337: 16 [ 1105/ 1327], train_loss/perplexity = 4.29840946/73.5826645 secs/batch = 0.6010s, grad.norm=0.68179703\n",
      " 22342: 16 [ 1110/ 1327], train_loss/perplexity = 4.67020035/106.7191238 secs/batch = 0.6127s, grad.norm=0.72564524\n",
      " 22347: 16 [ 1115/ 1327], train_loss/perplexity = 4.25377750/70.3707352 secs/batch = 0.6025s, grad.norm=0.62839389\n",
      " 22352: 16 [ 1120/ 1327], train_loss/perplexity = 4.56665754/96.2219543 secs/batch = 0.6114s, grad.norm=0.66913491\n",
      " 22357: 16 [ 1125/ 1327], train_loss/perplexity = 4.71814156/111.9599915 secs/batch = 0.6078s, grad.norm=0.70196581\n",
      " 22362: 16 [ 1130/ 1327], train_loss/perplexity = 4.42619085/83.6123199 secs/batch = 0.6042s, grad.norm=0.66271341\n",
      " 22367: 16 [ 1135/ 1327], train_loss/perplexity = 4.42835951/83.7938385 secs/batch = 0.6077s, grad.norm=0.64143991\n",
      " 22372: 16 [ 1140/ 1327], train_loss/perplexity = 4.66184902/105.8315887 secs/batch = 0.6055s, grad.norm=0.67898214\n",
      " 22377: 16 [ 1145/ 1327], train_loss/perplexity = 4.50002527/90.0194092 secs/batch = 0.6497s, grad.norm=0.67820913\n",
      " 22382: 16 [ 1150/ 1327], train_loss/perplexity = 4.47083569/87.4297562 secs/batch = 0.6103s, grad.norm=0.65895796\n",
      " 22387: 16 [ 1155/ 1327], train_loss/perplexity = 4.58394098/97.8994522 secs/batch = 0.6090s, grad.norm=0.69535893\n",
      " 22392: 16 [ 1160/ 1327], train_loss/perplexity = 4.46554518/86.9684296 secs/batch = 0.6153s, grad.norm=0.65111041\n",
      " 22397: 16 [ 1165/ 1327], train_loss/perplexity = 4.58971739/98.4665985 secs/batch = 0.6103s, grad.norm=0.68920332\n",
      " 22402: 16 [ 1170/ 1327], train_loss/perplexity = 4.37071705/79.1003265 secs/batch = 0.6130s, grad.norm=0.65539718\n",
      " 22407: 16 [ 1175/ 1327], train_loss/perplexity = 4.17481565/65.0278473 secs/batch = 0.6119s, grad.norm=0.68065250\n",
      " 22412: 16 [ 1180/ 1327], train_loss/perplexity = 4.17837858/65.2599564 secs/batch = 0.6139s, grad.norm=0.67483079\n",
      " 22417: 16 [ 1185/ 1327], train_loss/perplexity = 4.41839695/82.9631882 secs/batch = 0.6105s, grad.norm=0.68436134\n",
      " 22422: 16 [ 1190/ 1327], train_loss/perplexity = 4.51336002/91.2278290 secs/batch = 0.6077s, grad.norm=0.70198345\n",
      " 22427: 16 [ 1195/ 1327], train_loss/perplexity = 4.30427599/74.0156097 secs/batch = 0.6050s, grad.norm=0.65621418\n",
      " 22432: 16 [ 1200/ 1327], train_loss/perplexity = 4.29145384/73.0726242 secs/batch = 0.6154s, grad.norm=0.67586589\n",
      " 22437: 16 [ 1205/ 1327], train_loss/perplexity = 4.26270771/71.0019760 secs/batch = 0.6171s, grad.norm=0.66570234\n",
      " 22442: 16 [ 1210/ 1327], train_loss/perplexity = 3.94324279/51.5856133 secs/batch = 0.6132s, grad.norm=0.66125900\n",
      " 22447: 16 [ 1215/ 1327], train_loss/perplexity = 4.23519278/69.0749969 secs/batch = 0.6155s, grad.norm=0.69605380\n",
      " 22452: 16 [ 1220/ 1327], train_loss/perplexity = 4.31650066/74.9259796 secs/batch = 0.6124s, grad.norm=0.68611354\n",
      " 22457: 16 [ 1225/ 1327], train_loss/perplexity = 4.07156372/58.6486015 secs/batch = 0.6079s, grad.norm=0.67719549\n",
      " 22462: 16 [ 1230/ 1327], train_loss/perplexity = 4.32046747/75.2237854 secs/batch = 0.6110s, grad.norm=0.63356960\n",
      " 22467: 16 [ 1235/ 1327], train_loss/perplexity = 4.27729368/72.0451965 secs/batch = 0.6050s, grad.norm=0.65945560\n",
      " 22472: 16 [ 1240/ 1327], train_loss/perplexity = 4.52051640/91.8830338 secs/batch = 0.6195s, grad.norm=0.68411249\n",
      " 22477: 16 [ 1245/ 1327], train_loss/perplexity = 4.44353247/85.0749359 secs/batch = 0.6122s, grad.norm=0.65349853\n",
      " 22482: 16 [ 1250/ 1327], train_loss/perplexity = 4.51329327/91.2217407 secs/batch = 0.6137s, grad.norm=0.62232512\n",
      " 22487: 16 [ 1255/ 1327], train_loss/perplexity = 4.50894356/90.8258133 secs/batch = 0.6175s, grad.norm=0.67053425\n",
      " 22492: 16 [ 1260/ 1327], train_loss/perplexity = 4.40431595/81.8031693 secs/batch = 0.6110s, grad.norm=0.70913005\n",
      " 22497: 16 [ 1265/ 1327], train_loss/perplexity = 4.52097702/91.9253693 secs/batch = 0.6117s, grad.norm=0.68040383\n",
      " 22502: 16 [ 1270/ 1327], train_loss/perplexity = 4.33669186/76.4542007 secs/batch = 0.6101s, grad.norm=0.68012482\n",
      " 22507: 16 [ 1275/ 1327], train_loss/perplexity = 4.51297903/91.1930847 secs/batch = 0.6193s, grad.norm=0.67110872\n",
      " 22512: 16 [ 1280/ 1327], train_loss/perplexity = 4.35614443/77.9559860 secs/batch = 0.6104s, grad.norm=0.66456681\n",
      " 22517: 16 [ 1285/ 1327], train_loss/perplexity = 4.27647066/71.9859314 secs/batch = 0.6182s, grad.norm=0.66351885\n",
      " 22522: 16 [ 1290/ 1327], train_loss/perplexity = 4.49444056/89.5180740 secs/batch = 0.6055s, grad.norm=0.65025115\n",
      " 22527: 16 [ 1295/ 1327], train_loss/perplexity = 4.43722296/84.5398483 secs/batch = 0.6082s, grad.norm=0.62518537\n",
      " 22532: 16 [ 1300/ 1327], train_loss/perplexity = 4.64201784/103.7534943 secs/batch = 0.6089s, grad.norm=0.67651200\n",
      " 22537: 16 [ 1305/ 1327], train_loss/perplexity = 4.72635889/112.8837891 secs/batch = 0.6109s, grad.norm=0.67368197\n",
      " 22542: 16 [ 1310/ 1327], train_loss/perplexity = 5.01560593/150.7474518 secs/batch = 0.6064s, grad.norm=0.66159111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22547: 16 [ 1315/ 1327], train_loss/perplexity = 4.81970882/123.9290009 secs/batch = 0.6129s, grad.norm=0.66378713\n",
      " 22552: 16 [ 1320/ 1327], train_loss/perplexity = 4.79762077/121.2216568 secs/batch = 0.6112s, grad.norm=0.65765280\n",
      " 22557: 16 [ 1325/ 1327], train_loss/perplexity = 4.66536188/106.2040100 secs/batch = 0.6092s, grad.norm=0.66260576\n",
      "Epoch training time: 812.98597407341\n",
      "Saved char model cv/epoch016_4.6040.model\n",
      " 22564: 17 [    5/ 1327], train_loss/perplexity = 4.75396299/116.0432510 secs/batch = 0.6030s, grad.norm=0.68925190\n",
      " 22569: 17 [   10/ 1327], train_loss/perplexity = 4.32033634/75.2139206 secs/batch = 0.6101s, grad.norm=0.65024364\n",
      " 22574: 17 [   15/ 1327], train_loss/perplexity = 4.59172535/98.6645126 secs/batch = 0.6273s, grad.norm=0.65584475\n",
      " 22579: 17 [   20/ 1327], train_loss/perplexity = 4.76041412/116.7942810 secs/batch = 0.6089s, grad.norm=0.67775625\n",
      " 22584: 17 [   25/ 1327], train_loss/perplexity = 4.66079330/105.7199173 secs/batch = 0.6153s, grad.norm=0.67640519\n",
      " 22589: 17 [   30/ 1327], train_loss/perplexity = 4.55002689/94.6349564 secs/batch = 0.6111s, grad.norm=0.67276627\n",
      " 22594: 17 [   35/ 1327], train_loss/perplexity = 4.49484110/89.5539398 secs/batch = 0.6112s, grad.norm=0.65854216\n",
      " 22599: 17 [   40/ 1327], train_loss/perplexity = 4.53474331/93.1995850 secs/batch = 0.6114s, grad.norm=0.69346040\n",
      " 22604: 17 [   45/ 1327], train_loss/perplexity = 4.25448608/70.4206161 secs/batch = 0.6196s, grad.norm=0.63464361\n",
      " 22609: 17 [   50/ 1327], train_loss/perplexity = 4.48055506/88.2836609 secs/batch = 0.6130s, grad.norm=0.66683269\n",
      " 22614: 17 [   55/ 1327], train_loss/perplexity = 4.40482044/81.8444443 secs/batch = 0.6132s, grad.norm=0.69579887\n",
      " 22619: 17 [   60/ 1327], train_loss/perplexity = 4.67147970/106.8557434 secs/batch = 0.6159s, grad.norm=0.69609141\n",
      " 22624: 17 [   65/ 1327], train_loss/perplexity = 4.25680399/70.5840378 secs/batch = 0.6139s, grad.norm=0.65316087\n",
      " 22629: 17 [   70/ 1327], train_loss/perplexity = 4.17098904/64.7794876 secs/batch = 0.6123s, grad.norm=0.65436882\n",
      " 22634: 17 [   75/ 1327], train_loss/perplexity = 4.03250360/56.4019432 secs/batch = 0.6097s, grad.norm=0.65267158\n",
      " 22639: 17 [   80/ 1327], train_loss/perplexity = 4.38294649/80.0736237 secs/batch = 0.6052s, grad.norm=0.68491554\n",
      " 22644: 17 [   85/ 1327], train_loss/perplexity = 4.43617296/84.4511261 secs/batch = 0.6079s, grad.norm=0.69020861\n",
      " 22649: 17 [   90/ 1327], train_loss/perplexity = 4.48152590/88.3694153 secs/batch = 0.6144s, grad.norm=0.71835881\n",
      " 22654: 17 [   95/ 1327], train_loss/perplexity = 4.37184763/79.1898117 secs/batch = 0.6080s, grad.norm=0.66304511\n",
      " 22659: 17 [  100/ 1327], train_loss/perplexity = 4.56353855/95.9223022 secs/batch = 0.6023s, grad.norm=0.66974795\n",
      " 22664: 17 [  105/ 1327], train_loss/perplexity = 4.49120760/89.2291336 secs/batch = 0.6069s, grad.norm=0.72053337\n",
      " 22669: 17 [  110/ 1327], train_loss/perplexity = 4.34803820/77.3266144 secs/batch = 0.6448s, grad.norm=0.68930888\n",
      " 22674: 17 [  115/ 1327], train_loss/perplexity = 4.27073193/71.5740051 secs/batch = 0.6089s, grad.norm=0.68403178\n",
      " 22679: 17 [  120/ 1327], train_loss/perplexity = 4.35305882/77.7158203 secs/batch = 0.6168s, grad.norm=0.69111431\n",
      " 22684: 17 [  125/ 1327], train_loss/perplexity = 4.55771160/95.3649979 secs/batch = 0.6249s, grad.norm=0.72342807\n",
      " 22689: 17 [  130/ 1327], train_loss/perplexity = 4.41590881/82.7570190 secs/batch = 0.5985s, grad.norm=0.72877151\n",
      " 22694: 17 [  135/ 1327], train_loss/perplexity = 4.38311481/80.0871048 secs/batch = 0.6058s, grad.norm=0.68035257\n",
      " 22699: 17 [  140/ 1327], train_loss/perplexity = 4.69767666/109.6920242 secs/batch = 0.6105s, grad.norm=0.67765599\n",
      " 22704: 17 [  145/ 1327], train_loss/perplexity = 4.63300657/102.8227463 secs/batch = 0.6077s, grad.norm=0.69860524\n",
      " 22709: 17 [  150/ 1327], train_loss/perplexity = 4.54409027/94.0748062 secs/batch = 0.6021s, grad.norm=0.68494350\n",
      " 22714: 17 [  155/ 1327], train_loss/perplexity = 4.87086439/130.4336090 secs/batch = 0.6063s, grad.norm=0.67552233\n",
      " 22719: 17 [  160/ 1327], train_loss/perplexity = 4.47167921/87.5035400 secs/batch = 0.6269s, grad.norm=0.63399202\n",
      " 22724: 17 [  165/ 1327], train_loss/perplexity = 4.68805027/108.6411514 secs/batch = 0.6096s, grad.norm=0.68809581\n",
      " 22729: 17 [  170/ 1327], train_loss/perplexity = 4.44188833/84.9351730 secs/batch = 0.6119s, grad.norm=0.66991633\n",
      " 22734: 17 [  175/ 1327], train_loss/perplexity = 4.70282459/110.2581711 secs/batch = 0.6162s, grad.norm=0.67042547\n",
      " 22739: 17 [  180/ 1327], train_loss/perplexity = 4.63382292/102.9067154 secs/batch = 0.6139s, grad.norm=0.66391432\n",
      " 22744: 17 [  185/ 1327], train_loss/perplexity = 4.81015778/122.7509842 secs/batch = 0.6112s, grad.norm=0.65789270\n",
      " 22749: 17 [  190/ 1327], train_loss/perplexity = 4.39633465/81.1528702 secs/batch = 0.6146s, grad.norm=0.62973452\n",
      " 22754: 17 [  195/ 1327], train_loss/perplexity = 4.63377380/102.9016647 secs/batch = 0.6128s, grad.norm=0.65848881\n",
      " 22759: 17 [  200/ 1327], train_loss/perplexity = 4.50783253/90.7249603 secs/batch = 0.6073s, grad.norm=0.67839438\n",
      " 22764: 17 [  205/ 1327], train_loss/perplexity = 4.68483829/108.2927628 secs/batch = 0.6175s, grad.norm=0.69205344\n",
      " 22769: 17 [  210/ 1327], train_loss/perplexity = 4.59802485/99.2880096 secs/batch = 0.7025s, grad.norm=0.65989321\n",
      " 22774: 17 [  215/ 1327], train_loss/perplexity = 4.71483088/111.5899353 secs/batch = 0.7577s, grad.norm=0.64265615\n",
      " 22779: 17 [  220/ 1327], train_loss/perplexity = 4.65438414/105.0445099 secs/batch = 0.6480s, grad.norm=0.68111163\n",
      " 22784: 17 [  225/ 1327], train_loss/perplexity = 4.84123850/126.6260834 secs/batch = 0.7401s, grad.norm=0.68475950\n",
      " 22789: 17 [  230/ 1327], train_loss/perplexity = 4.63980293/103.5239410 secs/batch = 0.7355s, grad.norm=0.69566172\n",
      " 22794: 17 [  235/ 1327], train_loss/perplexity = 4.56243372/95.8163834 secs/batch = 0.7469s, grad.norm=0.69504994\n",
      " 22799: 17 [  240/ 1327], train_loss/perplexity = 4.34479761/77.0764389 secs/batch = 0.6425s, grad.norm=0.67537695\n",
      " 22804: 17 [  245/ 1327], train_loss/perplexity = 4.65447140/105.0536728 secs/batch = 0.6483s, grad.norm=0.65997338\n",
      " 22809: 17 [  250/ 1327], train_loss/perplexity = 4.38167763/79.9720840 secs/batch = 0.6429s, grad.norm=0.63895905\n",
      " 22814: 17 [  255/ 1327], train_loss/perplexity = 4.44033384/84.8032455 secs/batch = 0.6314s, grad.norm=0.64291149\n",
      " 22819: 17 [  260/ 1327], train_loss/perplexity = 4.70241404/110.2129059 secs/batch = 0.6238s, grad.norm=0.67701757\n",
      " 22824: 17 [  265/ 1327], train_loss/perplexity = 4.83760881/126.1673050 secs/batch = 0.6220s, grad.norm=0.66482884\n",
      " 22829: 17 [  270/ 1327], train_loss/perplexity = 4.86867857/130.1488190 secs/batch = 0.6084s, grad.norm=0.68467414\n",
      " 22834: 17 [  275/ 1327], train_loss/perplexity = 4.84873772/127.5792465 secs/batch = 0.6267s, grad.norm=0.66724265\n",
      " 22839: 17 [  280/ 1327], train_loss/perplexity = 4.63568735/103.0987625 secs/batch = 0.6107s, grad.norm=0.65567845\n",
      " 22844: 17 [  285/ 1327], train_loss/perplexity = 4.88692713/132.5456543 secs/batch = 0.6099s, grad.norm=0.67672521\n",
      " 22849: 17 [  290/ 1327], train_loss/perplexity = 4.59519005/99.0069504 secs/batch = 0.6174s, grad.norm=0.71007460\n",
      " 22854: 17 [  295/ 1327], train_loss/perplexity = 4.38049412/79.8774948 secs/batch = 0.6197s, grad.norm=0.68859911\n",
      " 22859: 17 [  300/ 1327], train_loss/perplexity = 4.01435947/55.3878059 secs/batch = 0.6184s, grad.norm=0.65454668\n",
      " 22864: 17 [  305/ 1327], train_loss/perplexity = 4.47448730/87.7496033 secs/batch = 0.6257s, grad.norm=0.66298920\n",
      " 22869: 17 [  310/ 1327], train_loss/perplexity = 4.49373770/89.4551773 secs/batch = 0.6151s, grad.norm=0.68607289\n",
      " 22874: 17 [  315/ 1327], train_loss/perplexity = 4.10420036/60.5942726 secs/batch = 0.6800s, grad.norm=0.66700613\n",
      " 22879: 17 [  320/ 1327], train_loss/perplexity = 4.04595757/57.1659012 secs/batch = 0.6932s, grad.norm=0.69173640\n",
      " 22884: 17 [  325/ 1327], train_loss/perplexity = 4.03012514/56.2679520 secs/batch = 0.6496s, grad.norm=0.67115766\n",
      " 22889: 17 [  330/ 1327], train_loss/perplexity = 4.53230190/92.9723282 secs/batch = 0.6969s, grad.norm=0.69743556\n",
      " 22894: 17 [  335/ 1327], train_loss/perplexity = 3.96563292/52.7536469 secs/batch = 0.6499s, grad.norm=0.63263303\n",
      " 22899: 17 [  340/ 1327], train_loss/perplexity = 4.67661810/107.4062195 secs/batch = 0.7337s, grad.norm=0.68258929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22904: 17 [  345/ 1327], train_loss/perplexity = 4.54998398/94.6308899 secs/batch = 0.6838s, grad.norm=0.65178490\n",
      " 22909: 17 [  350/ 1327], train_loss/perplexity = 4.58312321/97.8194275 secs/batch = 0.6711s, grad.norm=0.67524099\n",
      " 22914: 17 [  355/ 1327], train_loss/perplexity = 4.58417749/97.9226151 secs/batch = 0.7295s, grad.norm=0.65750480\n",
      " 22919: 17 [  360/ 1327], train_loss/perplexity = 4.69108582/108.9714355 secs/batch = 0.6103s, grad.norm=0.68432397\n",
      " 22924: 17 [  365/ 1327], train_loss/perplexity = 4.63217020/102.7367783 secs/batch = 0.6874s, grad.norm=0.65962607\n",
      " 22929: 17 [  370/ 1327], train_loss/perplexity = 4.71683979/111.8143387 secs/batch = 0.6114s, grad.norm=0.71993673\n",
      " 22934: 17 [  375/ 1327], train_loss/perplexity = 4.12627745/61.9468918 secs/batch = 0.6451s, grad.norm=0.65077734\n",
      " 22939: 17 [  380/ 1327], train_loss/perplexity = 4.22449255/68.3398132 secs/batch = 0.6654s, grad.norm=0.68256313\n",
      " 22944: 17 [  385/ 1327], train_loss/perplexity = 4.44114017/84.8716583 secs/batch = 0.6511s, grad.norm=0.66256756\n",
      " 22949: 17 [  390/ 1327], train_loss/perplexity = 4.49639988/89.6936417 secs/batch = 0.6488s, grad.norm=0.65986520\n",
      " 22954: 17 [  395/ 1327], train_loss/perplexity = 4.70122051/110.0814438 secs/batch = 0.6179s, grad.norm=0.67029649\n",
      " 22959: 17 [  400/ 1327], train_loss/perplexity = 4.50880766/90.8134766 secs/batch = 0.6415s, grad.norm=0.68086511\n",
      " 22964: 17 [  405/ 1327], train_loss/perplexity = 4.81631041/123.5085526 secs/batch = 0.6210s, grad.norm=0.69601476\n",
      " 22969: 17 [  410/ 1327], train_loss/perplexity = 4.47003412/87.3597031 secs/batch = 0.6752s, grad.norm=0.66410172\n",
      " 22974: 17 [  415/ 1327], train_loss/perplexity = 4.35291910/77.7049637 secs/batch = 0.6394s, grad.norm=0.68344110\n",
      " 22979: 17 [  420/ 1327], train_loss/perplexity = 4.12021446/61.5724449 secs/batch = 0.6637s, grad.norm=0.67811626\n",
      " 22984: 17 [  425/ 1327], train_loss/perplexity = 4.42383528/83.4155960 secs/batch = 0.6159s, grad.norm=0.67842752\n",
      " 22989: 17 [  430/ 1327], train_loss/perplexity = 4.63177156/102.6958389 secs/batch = 0.6174s, grad.norm=0.68639630\n",
      " 22994: 17 [  435/ 1327], train_loss/perplexity = 4.66471672/106.1355133 secs/batch = 0.6200s, grad.norm=0.67269498\n",
      " 22999: 17 [  440/ 1327], train_loss/perplexity = 4.27121067/71.6082764 secs/batch = 0.6113s, grad.norm=0.68043351\n",
      " 23004: 17 [  445/ 1327], train_loss/perplexity = 4.58812761/98.3101807 secs/batch = 0.6537s, grad.norm=0.70942974\n",
      " 23009: 17 [  450/ 1327], train_loss/perplexity = 4.49412060/89.4894409 secs/batch = 0.6257s, grad.norm=0.69341338\n",
      " 23014: 17 [  455/ 1327], train_loss/perplexity = 4.38565779/80.2910233 secs/batch = 0.6134s, grad.norm=0.66296005\n",
      " 23019: 17 [  460/ 1327], train_loss/perplexity = 4.42903471/83.8504410 secs/batch = 0.6253s, grad.norm=0.70846927\n",
      " 23024: 17 [  465/ 1327], train_loss/perplexity = 4.25344181/70.3471146 secs/batch = 0.6208s, grad.norm=0.71070969\n",
      " 23029: 17 [  470/ 1327], train_loss/perplexity = 4.77798700/118.8648376 secs/batch = 0.6290s, grad.norm=0.67302150\n",
      " 23034: 17 [  475/ 1327], train_loss/perplexity = 4.37231112/79.2265244 secs/batch = 0.6174s, grad.norm=0.64517081\n",
      " 23039: 17 [  480/ 1327], train_loss/perplexity = 4.47178698/87.5129700 secs/batch = 0.6216s, grad.norm=0.69268715\n",
      " 23044: 17 [  485/ 1327], train_loss/perplexity = 4.44384050/85.1011429 secs/batch = 0.6210s, grad.norm=0.68532020\n",
      " 23049: 17 [  490/ 1327], train_loss/perplexity = 4.33705187/76.4817276 secs/batch = 0.6212s, grad.norm=0.71646583\n",
      " 23054: 17 [  495/ 1327], train_loss/perplexity = 4.35708904/78.0296631 secs/batch = 0.6259s, grad.norm=0.67631495\n",
      " 23059: 17 [  500/ 1327], train_loss/perplexity = 4.61666870/101.1564865 secs/batch = 0.6160s, grad.norm=0.66632605\n",
      " 23064: 17 [  505/ 1327], train_loss/perplexity = 4.58162642/97.6731186 secs/batch = 0.6283s, grad.norm=0.64287579\n",
      " 23069: 17 [  510/ 1327], train_loss/perplexity = 4.94717789/140.7771149 secs/batch = 0.6244s, grad.norm=0.66486138\n",
      " 23074: 17 [  515/ 1327], train_loss/perplexity = 4.62878942/102.3900375 secs/batch = 0.6496s, grad.norm=0.65973669\n",
      " 23079: 17 [  520/ 1327], train_loss/perplexity = 4.74425745/114.9224396 secs/batch = 0.6308s, grad.norm=0.68611485\n",
      " 23084: 17 [  525/ 1327], train_loss/perplexity = 4.40886497/82.1761398 secs/batch = 0.6220s, grad.norm=0.68098444\n",
      " 23089: 17 [  530/ 1327], train_loss/perplexity = 4.45174885/85.7768250 secs/batch = 0.6928s, grad.norm=0.70778739\n",
      " 23094: 17 [  535/ 1327], train_loss/perplexity = 4.56049919/95.6312027 secs/batch = 0.7149s, grad.norm=0.67717952\n",
      " 23099: 17 [  540/ 1327], train_loss/perplexity = 4.62421703/101.9229431 secs/batch = 0.7328s, grad.norm=0.64603209\n",
      " 23104: 17 [  545/ 1327], train_loss/perplexity = 4.63001204/102.5152969 secs/batch = 0.7793s, grad.norm=0.66079861\n",
      " 23109: 17 [  550/ 1327], train_loss/perplexity = 4.61011839/100.4960480 secs/batch = 0.6159s, grad.norm=0.69402188\n",
      " 23114: 17 [  555/ 1327], train_loss/perplexity = 4.43927288/84.7133255 secs/batch = 0.7417s, grad.norm=0.65004462\n",
      " 23119: 17 [  560/ 1327], train_loss/perplexity = 4.51709366/91.5690765 secs/batch = 0.6725s, grad.norm=0.68250811\n",
      " 23124: 17 [  565/ 1327], train_loss/perplexity = 4.45194530/85.7936783 secs/batch = 0.6674s, grad.norm=0.69201851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d8fbc6183fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlstm_char_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Char_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         c_g_def = graph_util.convert_variables_to_constants(\n",
      "\u001b[0;32m~/ERD/lstm_char_cnn.py\u001b[0m in \u001b[0;36mTrain_Char_Model\u001b[0;34m(session, train_model, train_reader, saver, summary_writer)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_rnn_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrnn_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             })\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as char_graph:\n",
    "    with tf.Session(graph=char_graph) as sess:\n",
    "        w2v = lstm_char_cnn.WordEmbedding(\n",
    "                    max_word_length = FLAGS.max_char_num , \n",
    "                    char_vocab_size = char_vocab.size, \n",
    "                    char_embed_size = FLAGS.char_embed_size, \n",
    "                    kernels = eval(FLAGS.kernels), \n",
    "                    kernel_features = eval(FLAGS.kernel_features), \n",
    "                    num_highway_layers = FLAGS.highway_layers,\n",
    "                    embedding_dim = FLAGS.embedding_dim\n",
    "                )\n",
    "        lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.num_unroll_steps,\n",
    "                    rnn_size = 300, \n",
    "                    num_rnn_layers = FLAGS.rnn_layers, \n",
    "                    word_vocab_size = word_vocab.size\n",
    "                )\n",
    "        char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                            w2v, lstm_lm, \n",
    "                            batch_size = FLAGS.batch_size, \n",
    "                            num_unroll_steps = FLAGS.num_unroll_steps, \n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            learning_rate = FLAGS.learning_rate,\n",
    "                            max_grad_norm = FLAGS.max_grad_norm\n",
    "                         )\n",
    "        t_vals = tf.trainable_variables()\n",
    "        g_vals = tf.global_variables()\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)\n",
    "        lstm_char_cnn.Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)\n",
    "        \n",
    "        c_g_def = graph_util.convert_variables_to_constants(\n",
    "        sess,\n",
    "        char_graph.as_graph_def(),\n",
    "        [\"lstm_word_embedding\", \"lstm_sent_embedding\"],\n",
    "        variable_names_whitelist=None,\n",
    "        variable_names_blacklist=t_vals)\n",
    "        c_g_def = char_graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Embedding/char_embedding:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/dense/kernel:0' shape=(1100, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/dense/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/WordEmbedding/SimpleLinear/Matrix:0' shape=(10000, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/WordEmbedding/SimpleLinear/Bias:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Embedding/char_embedding:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/dense/kernel:0' shape=(1100, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/dense/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/WordEmbedding/SimpleLinear/Matrix:0' shape=(10000, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/WordEmbedding/SimpleLinear/Bias:0' shape=(10000,) dtype=float32_ref>,\n",
       " <tf.Variable 'global_step:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'SGD_Training/learning_rate:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.losses import Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as senti_graph:\n",
    "    with tf.Session(graph=senti_graph) as sess:\n",
    "        s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "#         sent_x = tf.placeholder(tf.int32, shape = [None, 31, 21])\n",
    "        sent_y = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "#         words2vec = char_model(sent_x) #[None, max_word_num, kernerl_size]\n",
    "#         sentence = tf.reshape(words2vec, [-1, max_word_num, embedding_dim])\n",
    "        sentence = tf.placeholder(tf.float32, shape = [None, 31, 300], name = \"sentence_input\")\n",
    "#         sentence, = tf.import_graph_def(c_g_def, input_map={\"input:0\":sent_x}, return_elements=[\"lstm_word_embedding\"])\n",
    "        senti_features = s_model(sentence)\n",
    "        senti_out = tf.identity(senti_features, \"senti_feature_map\")\n",
    "        classifier = tf.layers.Dense(3, activation=tf.nn.relu, trainable=True)\n",
    "        senti_rst = classifier(senti_features)\n",
    "        sent_scores = tf.nn.softmax(senti_rst, axis=1)\n",
    "        sent_pred = tf.argmax(sent_scores, 1, name=\"predictions\")\n",
    "        sent_loss = tf.losses.softmax_cross_entropy(\n",
    "                            sent_y,\n",
    "                            senti_rst,\n",
    "                            weights=1.0,\n",
    "                            label_smoothing=0,\n",
    "                            scope=None,\n",
    "                            loss_collection=tf.GraphKeys.LOSSES,\n",
    "                            reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "                        )\n",
    "        sent_correct_predictions = tf.equal(sent_pred, tf.argmax(sent_y, 1))\n",
    "        sent_acc = tf.reduce_mean(tf.cast(sent_correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        sent_global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        sent_train_op = tf.train.AdagradOptimizer(0.01).minimize(sent_loss, sent_global_step)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)\n",
    "#         lstm_char_cnn.Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)\n",
    "        t_vals = tf.trainable_variables()\n",
    "#         s_g_def = graph_util.convert_variables_to_constants(\n",
    "#         sess,\n",
    "#         senti_graph.as_graph_def(),\n",
    "#         [\"senti_feature_map\"],\n",
    "#         variable_names_whitelist=None,\n",
    "#         variable_names_blacklist=t_vals)\n",
    "        s_g_def = senti_graph.as_graph_def()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt_vals: []\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value import/Embedding/TDNN/kernel_6/b\n\t [[node import/Embedding/TDNN/kernel_6/b/read (defined at <ipython-input-15-1e464ab14f9e>:5) ]]\n\nOriginal stack trace for 'import/Embedding/TDNN/kernel_6/b/read':\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-1e464ab14f9e>\", line 5, in <module>\n    sentence, = tf.import_graph_def(c_g_def, input_map={\"input:0\":sent_x, \"LSTM/Dropout:0\":dp}, return_elements=[\"lstm_word_embedding:0\"])\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value import/Embedding/TDNN/kernel_6/b\n\t [[{{node import/Embedding/TDNN/kernel_6/b/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1e464ab14f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tt_vals:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         scores = sess.run(senti_F,\n\u001b[0;32m---> 15\u001b[0;31m                feed_dict={sent_x: data_X, dp:0.8})\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value import/Embedding/TDNN/kernel_6/b\n\t [[node import/Embedding/TDNN/kernel_6/b/read (defined at <ipython-input-15-1e464ab14f9e>:5) ]]\n\nOriginal stack trace for 'import/Embedding/TDNN/kernel_6/b/read':\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-1e464ab14f9e>\", line 5, in <module>\n    sentence, = tf.import_graph_def(c_g_def, input_map={\"input:0\":sent_x, \"LSTM/Dropout:0\":dp}, return_elements=[\"lstm_word_embedding:0\"])\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g_combined:\n",
    "    with tf.Session(graph=g_combined) as sess:\n",
    "        sent_x = tf.placeholder(tf.int32, shape = [20, 31, 21], name = \"sent_input\")\n",
    "        dp = tf.placeholder(tf.float32)\n",
    "        sentence, = tf.import_graph_def(c_g_def, input_map={\"input:0\":sent_x, \"LSTM/Dropout:0\":dp}, return_elements=[\"lstm_word_embedding:0\"])\n",
    "        senti_F, = tf.import_graph_def(s_g_def, input_map={\"sentence_input:0\":sentence}, return_elements=[\"senti_feature_map:0\"])\n",
    "        data_X, data_Y = sentiReader.GetTrainingBatch(\n",
    "                                            0, \n",
    "                                            20\n",
    "                            )\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tt_vals = tf.trainable_variables()\n",
    "        print(\"tt_vals:\", tt_vals)\n",
    "        scores = sess.run(senti_F,\n",
    "               feed_dict={sent_x: data_X, dp:0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value import/Embedding/CNN_OUT/highway_lin_1/Bias\n\t [[node import/Embedding/CNN_OUT/highway_lin_1/Bias/read (defined at <ipython-input-11-dd1c849ccdd5>:3) ]]\n\nOriginal stack trace for 'import/Embedding/CNN_OUT/highway_lin_1/Bias/read':\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-dd1c849ccdd5>\", line 3, in <module>\n    tf.saved_model.loader.load(sess, [\"serve\"], \"./testdir\")\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 269, in load\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 422, in load\n    **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 352, in load_graph\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 1473, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 857, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value import/Embedding/CNN_OUT/highway_lin_1/Bias\n\t [[{{node import/Embedding/CNN_OUT/highway_lin_1/Bias/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dd1c849ccdd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtt_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     scores = sess.run(y,\n\u001b[0;32m---> 15\u001b[0;31m            feed_dict={x: data_X, dp:0.8})\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value import/Embedding/CNN_OUT/highway_lin_1/Bias\n\t [[node import/Embedding/CNN_OUT/highway_lin_1/Bias/read (defined at <ipython-input-11-dd1c849ccdd5>:3) ]]\n\nOriginal stack trace for 'import/Embedding/CNN_OUT/highway_lin_1/Bias/read':\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-dd1c849ccdd5>\", line 3, in <module>\n    tf.saved_model.loader.load(sess, [\"serve\"], \"./testdir\")\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 269, in load\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 422, in load\n    **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 352, in load_graph\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 1473, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 857, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/home/hadoop/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.saved_model.loader.load(sess, [\"serve\"], \"./testdir\")\n",
    "    graph = tf.get_default_graph()\n",
    "    x = sess.graph.get_tensor_by_name('sent_input:0')\n",
    "    y = sess.graph.get_tensor_by_name('senti_map:0')\n",
    "    dp = sess.graph.get_tensor_by_name('import/LSTM/Dropout:0')\n",
    "    data_X, data_Y = sentiReader.GetTrainingBatch(\n",
    "                                            0, \n",
    "                                            20\n",
    "                            )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tt_vals = tf.trainable_variables()\n",
    "    scores = sess.run(y,\n",
    "           feed_dict={x: data_X, dp:0.8})\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = lstm_char_cnn.WordEmbedding(\n",
    "                max_word_length = FLAGS.max_char_num , \n",
    "                char_vocab_size = char_vocab.size, \n",
    "                char_embed_size = FLAGS.char_embed_size, \n",
    "                kernels = eval(FLAGS.kernels), \n",
    "                kernel_features = eval(FLAGS.kernel_features), \n",
    "                num_highway_layers = FLAGS.highway_layers,\n",
    "                embedding_dim = FLAGS.embedding_dim\n",
    "            )\n",
    "lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "            batch_size = FLAGS.batch_size, \n",
    "            num_unroll_steps = FLAGS.num_unroll_steps, \n",
    "            rnn_size = FLAGS.rnn_size, \n",
    "            num_rnn_layers = FLAGS.rnn_layers, \n",
    "            word_vocab_size = word_vocab.size\n",
    "        )\n",
    "\n",
    "char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                    w2v, lstm_lm, \n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.num_unroll_steps, \n",
    "                    max_word_length = FLAGS.max_char_num, \n",
    "                    learning_rate = FLAGS.learning_rate,\n",
    "                    max_grad_norm = FLAGS.max_grad_norm\n",
    "                 )\n",
    "#sentiment analysis model\n",
    "s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "senti_train_graph = model.InferSentiTrainGraph(\n",
    "                        w2v, \n",
    "                        s_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        sent_num = FLAGS.sent_num,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "# df model\n",
    "rdm_model = model.RDM_Model(\n",
    "                max_seq_len = FLAGS.max_seq_len, \n",
    "                max_word_num = FLAGS.max_sent_len, \n",
    "                embedding_dim = FLAGS.embedding_dim, \n",
    "                hidden_dim = FLAGS.hidden_dim\n",
    "            )\n",
    "rdm_train_graph = model.InferRDMTrainGraph(\n",
    "                        w2v, s_model, rdm_model, \n",
    "                        max_seq_len = FLAGS.max_seq_len, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        embedding_dim = FLAGS.embedding_dim,\n",
    "                        class_num = FLAGS.class_num\n",
    "                )\n",
    "\n",
    "# rl model\n",
    "cm_model = model.CM_Model(\n",
    "                    max_word_num = FLAGS.max_sent_len, \n",
    "                    embedding_dim = FLAGS.embedding_dim, \n",
    "                    hidden_dim = FLAGS.hidden_dim, \n",
    "                    action_num = FLAGS.action_num\n",
    "            )\n",
    "cm_train_graph = model.InferCMTrainGraph(\n",
    "                        w2v, s_model, rdm_model, cm_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        embedding_dim = FLAGS.embedding_dim, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        action_num = FLAGS.action_num\n",
    "                    )\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
