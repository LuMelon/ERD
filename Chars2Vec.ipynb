{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.losses import Reduction\n",
    "\n",
    "class RL_GRU2:\n",
    "    def __init__(self, input_dim, hidden_dim, max_seq_len, max_word_len, class_num, action_num, sent_num):\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, max_seq_len, max_word_len, input_dim], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, class_num], name=\"input_y\")\n",
    "        self.x_len = tf.placeholder(tf.int32, [None], name=\"x_len\")\n",
    "\n",
    "        self.sent_x = tf.placeholder(tf.float32, [None, max_word_len, input_dim], name=\"sent_x\")\n",
    "        self.sent_y = tf.placeholder(tf.float32, [None, sent_num], name=\"sent_y\")\n",
    "        \n",
    "        self.init_states = tf.placeholder(tf.float32, [None, hidden_dim], name=\"topics\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        self.rl_state = tf.placeholder(tf.float32, [None, hidden_dim], name=\"rl_states\")\n",
    "        self.rl_input = tf.placeholder(tf.float32, [None, max_word_len, input_dim], name=\"rl_input\")\n",
    "        self.action = tf.placeholder(tf.float32, [None, action_num], name=\"action\")\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "\n",
    "        output_dim = hidden_dim\n",
    "\n",
    "        # shared pooling layer\n",
    "        self.w_t = tf.Variable(tf.random_uniform([input_dim, output_dim], -1.0, 1.0), name=\"w_t\")\n",
    "        self.b_t = tf.Variable(tf.constant(0.01, shape=[output_dim]), name=\"b_t\")\n",
    "        #[batchsize, max_seq_len, max_word_len, input_dim] --> [batchsize, max_seq_len, output_dim]\n",
    "        pooled_input_x = self.shared_pooling_layer(self.input_x, input_dim, max_seq_len, max_word_len, output_dim) # replace the shared_pooling_layer with a sentiment analysis model\n",
    "        pooled_rl_input = self.shared_pooling_layer(self.rl_input, input_dim, 1, max_word_len, output_dim)\n",
    "        pooled_rl_input = tf.reshape(pooled_rl_input, [-1, output_dim])\n",
    "\n",
    "        # dropout layer\n",
    "        pooled_input_x_dp = tf.nn.dropout(pooled_input_x, self.dropout_keep_prob)\n",
    "\n",
    "        # df model\n",
    "        df_cell = rnn.GRUCell(output_dim)\n",
    "        df_cell = rnn.DropoutWrapper(df_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "        w_tp = tf.constant(0.0, shape=[hidden_dim, output_dim], name=\"w_tp\")\n",
    "        self.df_state = tf.matmul(self.init_states, w_tp, name=\"df_state\") # w_tp is not an Variable?\n",
    "\n",
    "        df_outputs, df_last_state = tf.nn.dynamic_rnn(df_cell, pooled_input_x_dp, self.x_len, initial_state=self.df_state, dtype=tf.float32)\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        w_ps = tf.Variable(tf.truncated_normal([output_dim, class_num], stddev=0.1)) #\n",
    "        b_ps = tf.Variable(tf.constant(0.01, shape=[class_num])) #\n",
    "        l2_loss += tf.nn.l2_loss(w_ps) \n",
    "        l2_loss += tf.nn.l2_loss(b_ps) \n",
    "\n",
    "        self.pre_scores = tf.nn.xw_plus_b(df_last_state, w_ps, b_ps, name=\"p_scores\")\n",
    "        self.predictions = tf.argmax(self.pre_scores, 1, name=\"predictions\")\n",
    "\n",
    "        r_outputs = tf.reshape(df_outputs, [-1, output_dim]) #[batchsize*max_seq_len, output_dim]\n",
    "        scores_seq = tf.nn.softmax(tf.nn.xw_plus_b(r_outputs, w_ps, b_ps)) # [batchsize * max_seq_len, class_num] \n",
    "        self.out_seq = tf.reshape(scores_seq, [-1, max_seq_len, class_num], name=\"out_seq\") #[batchsize, max_seq_len, class_num]\n",
    "\n",
    "        df_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.pre_scores, labels=self.input_y)\n",
    "        self.loss = tf.reduce_mean(df_losses) + 0.1 * l2_loss\n",
    "\n",
    "        correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "        # rl model\n",
    "        self.rl_output, self.rl_new_state = df_cell(pooled_rl_input, self.rl_state)\n",
    "\n",
    "        w_ss1 = tf.Variable(tf.truncated_normal([output_dim, 64], stddev=0.01))\n",
    "        b_ss1 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "        rl_h1 = tf.nn.relu(tf.nn.xw_plus_b(self.rl_state, w_ss1, b_ss1))  # replace the process here\n",
    "\n",
    "        w_ss2 = tf.Variable(tf.truncated_normal([64, action_num], stddev=0.01))\n",
    "        b_ss2 = tf.Variable(tf.constant(0.01, shape=[action_num]))\n",
    "\n",
    "        self.stopScore = tf.nn.xw_plus_b(rl_h1, w_ss2, b_ss2, name=\"stopScore\")\n",
    "\n",
    "        self.isStop = tf.argmax(self.stopScore, 1, name=\"isStop\")\n",
    "\n",
    "        out_action = tf.reduce_sum(tf.multiply(self.stopScore, self.action), reduction_indices=1)\n",
    "        self.rl_cost = tf.reduce_mean(tf.square(self.reward - out_action), name=\"rl_cost\")\n",
    "\n",
    "        \n",
    "        # Sentiment Analysis Task\n",
    "        self.pooled_feat = self.SentCNN(self.sent_x)\n",
    "        classifier = tf.layers.Dense(sent_num, activation= tf.nn.relu, trainable=True)\n",
    "        self.sent_scores = tf.nn.softmax(classifier(self.pooled_feat), axis=1)\n",
    "        self.sent_pred = tf.argmax(self.sent_scores, 1, name=\"predictions\")\n",
    "        self.sent_loss = tf.losses.softmax_cross_entropy(\n",
    "                        self.sent_y,\n",
    "                        self.sent_scores,\n",
    "                        weights=1.0,\n",
    "                        label_smoothing=0,\n",
    "                        scope=None,\n",
    "                        loss_collection=tf.GraphKeys.LOSSES,\n",
    "                        reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "                    )\n",
    "        sent_correct_predictions = tf.equal(self.sent_pred, tf.argmax(self.sent_y, 1))\n",
    "        self.sent_acc = tf.reduce_mean(tf.cast(sent_correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def shared_pooling_layer(self, inputs, input_dim, max_seq_len, max_word_len, output_dim):\n",
    "        t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "        # t_h = tf.nn.xw_plus_b(t_inputs, self.w_t, self.b_t)\n",
    "        t_h = tf.matmul(t_inputs, self.w_t)\n",
    "        t_h = tf.reshape(t_h, [-1, max_word_len, output_dim])\n",
    "        t_h_expended = tf.expand_dims(t_h, -1)\n",
    "        pooled = tf.nn.max_pool(\n",
    "            t_h_expended,\n",
    "            ksize=[1, max_word_len, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        outs = tf.reshape(pooled, [-1, max_seq_len, output_dim])\n",
    "        return outs\n",
    "\n",
    "    def pooling_layer(self, inputs, input_dim, max_seq_len, max_word_len, output_dim):\n",
    "        t_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "        w = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n",
    "        b = tf.Variable(tf.constant(0.01, shape=[output_dim]))\n",
    "\n",
    "        h = tf.nn.xw_plus_b(t_inputs, w, b)\n",
    "        hs = tf.reshape(h, [-1, max_word_len, output_dim])\n",
    "\n",
    "        inputs_expended = tf.expand_dims(hs, -1)\n",
    "        # [seq, words, out] --> [seq, words, out, 1] --> [seq, 1, out, 1] --> [1, seq, out]\n",
    "        pooled = tf.nn.max_pool(\n",
    "            inputs_expended,\n",
    "            ksize=[1, max_word_len, 1, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"max_pool\"\n",
    "        )\n",
    "        cnn_outs = tf.reshape(pooled, [-1, max_seq_len, output_dim]) \n",
    "        return cnn_outs\n",
    "\n",
    "    def SentCNN(self, input_x):\n",
    "        num_filters = 256\n",
    "        kernel_size = 5\n",
    "        conv_input = tf.layers.conv1d(input_x, num_filters, kernel_size, strides=1, padding='valid', name='conv2', trainable=True)\n",
    "        feature_map = tf.nn.relu(conv_input) # [batchsize, conv_feats, filters]\n",
    "        pooled_feat = tf.reduce_max(feature_map, 1) #[batchsize, 1, filters]\n",
    "\n",
    "        return pooled_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "class Chars2Vec:\n",
    "    def __init__(self, emb_dim, char_to_ix):\n",
    "        if not isinstance(emb_dim, int) or emb_dim < 1:\n",
    "            raise TypeError(\"parameter 'emb_dim' must be a positive integer\")\n",
    "\n",
    "        if not isinstance(char_to_ix, dict):\n",
    "            raise TypeError(\"parameter 'char_to_ix' must be a dictionary\")\n",
    "            \n",
    "        self.char_to_ix = char_to_ix\n",
    "        self.ix_to_char = {char_to_ix[ch]: ch for ch in char_to_ix}\n",
    "        self.vocab_size = len(self.char_to_ix)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.cache = {}\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_rnn\")\n",
    "        self.init_state = tf.placeholder(tf.float32, [None, self.emb_dim], name='init_state')\n",
    "        \n",
    "    def embedding(self, input_x, x_len):\n",
    "        lstm_cell_1 = rnn.GRUCell(self.emb_dim, name = 'layer1')\n",
    "        lstm_cell_1 = rnn.DropoutWrapper(lstm_cell_1, output_keep_prob=self.dropout_keep_prob)\n",
    "        lstm_cell_2 = rnn.GRUCell(self.emb_dim, name = 'layer2')\n",
    "        lstm_cell_2 = rnn.DropoutWrapper(lstm_cell_2, output_keep_prob=self.dropout_keep_prob)\n",
    "        hiddens_1, hiddens_1_final = tf.nn.dynamic_rnn(lstm_cell_1, input_x, x_len, initial_state=self.init_state, dtype=tf.float32)\n",
    "        hiddens_2, hiddens_2_final = tf.nn.dynamic_rnn(lstm_cell_2, hiddens_1, x_len, initial_state=hiddens_1_final, dtype=tf.float32)\n",
    "        return hiddens_2_final\n",
    "    \n",
    "    def PredSimilar(self, input1, input2, x1_len, x2_len):\n",
    "        embed_1 = self.embedding(input1, x1_len)\n",
    "        embed_2 = self.embedding(input2, x2_len)\n",
    "        sub = embed_1 - embed_2\n",
    "        sub = sub*sub\n",
    "        RegLayer = tf.layers.Dense(1, activation= tf.nn.sigmoid, trainable=True)\n",
    "        pred = RegLayer(sub)\n",
    "        return pred\n",
    "    \n",
    "    def TrainModel(self, word_pairs, targets, max_epochs, patience, validation_split, batch_size):\n",
    "        if not isinstance(word_pairs, list) and not isinstance(word_pairs, np.ndarray):\n",
    "            raise TypeError(\"parameters 'word_pairs' must be a list or numpy.ndarray\")\n",
    "\n",
    "        if not isinstance(targets, list) and not isinstance(targets, np.ndarray):\n",
    "            raise TypeError(\"parameters 'targets' must be a list or numpy.ndarray\")\n",
    "    \n",
    "        assert len(word_pairs) == len(targets)\n",
    "        if isinstance(targets, list) and not isinstance(targets, np.ndarray):\n",
    "            targets = np.array(targets)\n",
    "    \n",
    "        def word2emb_list(word):\n",
    "            emb_list = []\n",
    "            for t in range(len(word)):\n",
    "                if word[t] in self.char_to_ix:\n",
    "                    x = np.zeros(self.vocab_size).tolist()\n",
    "                    x[self.char_to_ix[word[t]]] = 1\n",
    "                    emb_list.append(x)\n",
    "                else:\n",
    "                    emb_list.append(np.zeros(self.vocab_size).tolist())\n",
    "            return emb_list\n",
    "        \n",
    "        x_1, x_2 = [], []\n",
    "        for pair_words in word_pairs:\n",
    "            if not isinstance(pair_words[0], str) or not isinstance(pair_words[1], str):\n",
    "                raise TypeError(\"word must be a string\")\n",
    "            first_word = pair_words[0].lower()\n",
    "            second_word = pair_words[1].lower()\n",
    "            emb_list_1 = word2emb_list(first_word)\n",
    "            emb_list_2 = word2emb_list(second_word)\n",
    "            x_1.append(np.array(emb_list_1))\n",
    "            x_2.append(np.array(emb_list_2))\n",
    "        x1_len = np.array([len(word) for word in x_1])\n",
    "        x2_len = np.array([len(word) for word in x_2])\n",
    "        max_word_len = max(max(x1_len), max(x2_len))\n",
    "        x_1 = keras.preprocessing.sequence.pad_sequences(x_1, maxlen=max_word_len, dtype='int32', padding='pro', truncating='pre', value=0.0)\n",
    "        x_2 = keras.preprocessing.sequence.pad_sequences(x_2, maxlen=max_word_len, dtype='int32', padding='pro', truncating='pre', value=0.0)\n",
    "        \n",
    "        #shuffle the data\n",
    "        data_size = len(targets)\n",
    "        idxs = random.sample(range(data_size), data_size)\n",
    "        x_1 = x_1[idxs]\n",
    "        x_2 = x_2[idxs]\n",
    "        x1_len = x1_len[idxs]\n",
    "        x2_len = x2_len[idxs]\n",
    "        targets = targets[idxs]\n",
    "        \n",
    "        # train:validation:test = 5:1:2\n",
    "        split_1 = int((5*data_size)/8)\n",
    "        split_2 = int((6*data_size)/8)\n",
    "        train_idxs = idxs[:split_1]\n",
    "        val_idxs = idxs[split_1:split_2]\n",
    "        test_idxs =  idxs[split_2:]\n",
    "        max_iter = int(split_1 / batch_size) + 1\n",
    "        \n",
    "        #Tensor Graph\n",
    "        batch_X1 = tf.placeholder(tf.float32, [None, max_word_len, self.vocab_size], name=\"X_1\")\n",
    "        batch_X2 = tf.placeholder(tf.float32, [None, max_word_len, self.vocab_size], name=\"X_2\")\n",
    "        X1_len = tf.placeholder(tf.int32, [None], name='x1_len')\n",
    "        X2_len = tf.placeholder(tf.int32, [None], name='x2_len')\n",
    "        batch_Y = tf.placeholder(tf.float32, [None])\n",
    "        preds = self.PredSimilar(batch_X1, batch_X2, X1_len, X2_len)\n",
    "        loss = tf.reduce_sum(tf.pow(batch_Y-preds, 2), axis=0)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        min_val_loss = 0\n",
    "        init_S = np.zeros([batch_size, self.emb_dim], dtype='float32')\n",
    "        for i in range(max_epochs):\n",
    "            for j in range(max_iter):\n",
    "                batch_idxs = [train_idxs[j*batch_size + k] if (j*batch_size + k)<split_1 else train_idxs[(j*batch_size + k)%split_1] for k in range(batch_size)]\n",
    "                feed_dict = {batch_X1:x_1[batch_idxs], batch_X2:x_2[batch_idxs], batch_Y:targets[batch_idxs], self.init_state:init_S}\n",
    "                _, batch_loss = sess.run([train_op, loss], feed_dict)\n",
    "                print(\" Step: \" + str(j) + \" Training loss: \" + batch_loss)\n",
    "            \n",
    "            val_loss = 0\n",
    "            for j in range(split_1, split_2, batch_size):\n",
    "                batch_idxs = list(range(j, max(j+batch_size, split_2), 1))\n",
    "                feed_dict = {batch_X1:x_1[batch_idxs], batch_X2:x_2[batch_idxs], batch_Y:targets[batch_idxs], self.init_state:init_S[:len(batch_idxs)]}\n",
    "                batch_loss = sess.run(loss, feed_dict)\n",
    "                val_loss += batch_loss\n",
    "            print(\"Epochs: \" + str(i) + \"Validation loss: \" + batch_loss)\n",
    "            \n",
    "            if i == 1:\n",
    "                min_val_loss = val_loss\n",
    "            else:\n",
    "                if min_val_loss > val_loss:\n",
    "                    saver.save(sess, \"char2vec_saved/model\"+str(i))\n",
    "                    print(\"char2vec_saved/model \"+str(i)+\" saved\")\n",
    "        # test loss\n",
    "        for j in range(split_2, data_size, batch_size):\n",
    "            batch_idxs = list(range(j, max(j+batch_size, data_size), 1))\n",
    "            feed_dict = {batch_X1:x_1[batch_idxs], batch_X2:x_2[batch_idxs], batch_Y:targets[batch_idxs], self.init_state:init_S[:len(batch_idxs)]}\n",
    "            batch_loss = sess.run(loss, feed_dict)\n",
    "            val_loss += batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['f', 'k', 'n', '7', '’', '8', 'c', '9', 'b', ')', '(', 's', 'm', 'e', 'g', '4', ',', 'j', '”', '1', 'z', 't', '2', ' ', 'i', '–', 'o', 'l', '.', '!', 'd', 'u', 'a', '0', 'y', '-', 'x', 'w', '“', 'v', 'q', '&', ':', '6', 'r', 'h', 'p', '3', '5']\n",
    "char_2_ix = {c:ix for (ix, c) in enumerate(chars)}\n",
    "\n",
    "model = Chars2Vec(300, char_2_ix)\n",
    "\n",
    "zs = np.zeros([3, 9, len(chars)], dtype='float32')\n",
    "for i in range(len(zs)):\n",
    "    for j in range(zs.shape[1]):\n",
    "        zs[i][j][random.randint(0,9)]=1\n",
    "\n",
    "X_S1 = tf.convert_to_tensor(zs, name='X_S1')\n",
    "X_S2 = tf.convert_to_tensor(zs, name='X_S2')\n",
    "x_len = tf.placeholder(tf.int32, [None], name= 'x_len')\n",
    "# bs = tf.placeholder(tf.int32, name = 'batchsize')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
