{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import MyLogger\n",
    "import time\n",
    "import SubjObjLoader\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from collections import deque\n",
    "# import dataloader\n",
    "from BertRDMLoader import *\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import importlib\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import tsentiLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### RDM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class pooling_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(pooling_layer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        assert(inputs.ndim == 4 ) # [batchsize, max_seq_len, max_word_num, input_dim] \n",
    "        batch_size, max_seq_len, max_word_num, input_dim = inputs.shape\n",
    "        assert(input_dim == self.input_dim)\n",
    "        t_inputs = inputs.reshape([-1, self.input_dim])\n",
    "        return self.linear(t_inputs).reshape(\n",
    "            \n",
    "            [-1, max_word_num, self.output_dim]\n",
    "        \n",
    "        ).max(axis=1)[0].reshape(\n",
    "        \n",
    "            [-1, max_seq_len, self.output_dim]\n",
    "        \n",
    "        )\n",
    "\n",
    "class RDM_Model(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, hidden_dim, dropout_prob):\n",
    "        super(RDM_Model, self).__init__()\n",
    "        self.embedding_dim = sent_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_model = nn.GRU(word_embedding_dim, \n",
    "                                self.hidden_dim, \n",
    "                                batch_first=True, \n",
    "                                dropout=dropout_prob\n",
    "                            )\n",
    "        self.DropLayer = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x_emb, x_len, init_states): \n",
    "        \"\"\"\n",
    "        input_x: [batchsize, max_seq_len, sentence_embedding_dim] \n",
    "        x_emb: [batchsize, max_seq_len, 1, embedding_dim]\n",
    "        x_len: [batchsize]\n",
    "        init_states: [batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        batchsize, max_seq_len, _ , emb_dim = x_emb.shape\n",
    "        pool_feature = x_emb.reshape(\n",
    "                [-1, max_seq_len, emb_dim]\n",
    "        )\n",
    "        df_outputs, df_last_state = self.gru_model(pool_feature, init_states)\n",
    "        hidden_outs = [df_outputs[i][:x_len[i]] for i in range(batchsize)]\n",
    "        final_outs = [df_outputs[i][x_len[i]-1] for i in range(batchsize)]\n",
    "        return hidden_outs, final_outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### CM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CM_Model(nn.Module):\n",
    "    def __init__(self, sentence_embedding_dim, hidden_dim, action_num):\n",
    "        super(CM_Model, self).__init__()\n",
    "        self.sentence_embedding_dim = sentence_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_num = action_num\n",
    "#         self.PoolLayer = pooling_layer(self.embedding_dim, \n",
    "#                                             self.hidden_dim)\n",
    "        self.DenseLayer = nn.Linear(self.hidden_dim, 64)\n",
    "        self.Classifier = nn.Linear(64, self.action_num)\n",
    "        \n",
    "    def forward(self, rdm_model, rl_input, rl_state):\n",
    "        \"\"\"\n",
    "        rl_input: [batchsize, max_word_num, sentence_embedding_dim]\n",
    "        rl_state: [1, batchsize, hidden_dim]\n",
    "        \"\"\"\n",
    "        assert(rl_input.ndim==3)\n",
    "        batchsize, max_word_num, embedding_dim = rl_input.shape\n",
    "        rl_output, rl_new_state = rdm_model.gru_model(\n",
    "                                            rl_input, \n",
    "                                            rl_state\n",
    "                                        )\n",
    "        rl_h1 = nn.functional.relu(\n",
    "            self.DenseLayer(\n",
    "#                 rl_state.reshape([len(rl_input), self.hidden_dim]) #it is not sure to take rl_state , rather than rl_output, as the feature\n",
    "                rl_output.reshape(\n",
    "                    [len(rl_input), self.hidden_dim]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stopScore = self.Classifier(rl_h1)\n",
    "        isStop = stopScore.argmax(axis=1)\n",
    "        return stopScore, isStop, rl_new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer2seq(bert, layer, cuda=False):\n",
    "    if cuda:\n",
    "        outs = [bert( torch.tensor([input_]).cuda())\n",
    "                for input_ in layer]   \n",
    "    else: \n",
    "        outs = [bert( torch.tensor([input_]))\n",
    "                    for input_ in layer]\n",
    "    states = [item[1] for item in outs]\n",
    "    return rnn_utils.pad_sequence(states, batch_first=True)\n",
    "\n",
    "def Word_ids2SeqStates(word_ids, bert, ndim, cuda=False):\n",
    "    assert(ndim == 3)\n",
    "    if cuda:\n",
    "        embedding = [layer2seq(bert, layer, cuda) for layer in word_ids]\n",
    "    else:\n",
    "        embedding = [layer2seq(bert, layer) for layer in word_ids]\n",
    "    return padding_sequence(embedding)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def Count_Accs(ylabel, preds):\n",
    "    correct_preds = np.array(\n",
    "        [1 if y1==y2 else 0 \n",
    "        for (y1, y2) in zip(ylabel, preds)]\n",
    "    )\n",
    "    y_idxs = [idx if yl >0 else idx - len(ylabel) \n",
    "            for (idx, yl) in enumerate(ylabel)]\n",
    "    pos_idxs = list(filter(lambda x: x >= 0, y_idxs))\n",
    "    neg_idxs = list(filter(lambda x: x < 0, y_idxs))\n",
    "    acc = sum(correct_preds) / (1.0 * len(ylabel))\n",
    "    if len(pos_idxs) > 0:\n",
    "        pos_acc = sum(correct_preds[pos_idxs])/(1.0*len(pos_idxs))\n",
    "    else:\n",
    "        pos_acc = 0\n",
    "    if len(neg_idxs) > 0:\n",
    "        neg_acc = sum(correct_preds[neg_idxs])/(1.0*len(neg_idxs))\n",
    "    else:\n",
    "        neg_acc = 0\n",
    "    return acc, pos_acc, neg_acc, y_idxs, pos_idxs, neg_idxs, correct_preds\n",
    "\n",
    "def Loss_Fn(ylabel, pred_scores):\n",
    "    diff = ((ylabel - pred_scores)*(ylabel - pred_scores)).mean(axis=1)\n",
    "#     pos_neg = (1.0*sum(ylabel.argmax(axis=1)))/(1.0*(len(ylabel) - sum(ylabel.argmax(axis=1))))\n",
    "    pos_neg = 0\n",
    "    if pos_neg > 0:\n",
    "        print(\"unbalanced data\")\n",
    "        weight = torch.ones(len(ylabel)).cuda() + (ylabel.argmax(axis=1).to(torch.float32)/(1.0*pos_neg)) - ylabel.argmax(axis=1).to(torch.float32)\n",
    "        return (weight *diff).mean()\n",
    "    else:\n",
    "        print(\"totally unbalanced data\")\n",
    "        return diff.mean()\n",
    "\n",
    "def get_new_len(rdm_model, cm_model, bert, tokenizer, FLAGS, cuda):\n",
    "    # 因为计算句子向量的方式不一致，所以在这个地方的函数也要在各种方式内部定义\n",
    "    new_x_len = np.zeros([len(data_ID)], dtype=np.int32)\n",
    "    for i in range(len(data_ID)):\n",
    "        init_state = torch.zeros([1, 1, FLAGS.hidden_dim], dtype=torch.float32) if not cuda else torch.zeros([1, 1, FLAGS.hidden_dim], dtype=torch.float32).cuda()\n",
    "        for j in range(data_len[i]):\n",
    "            input_ids = tokenizer.encode(\n",
    "                        data[data_ID[i]]['text'][j],\n",
    "                        add_special_tokens=True\n",
    "                    )\n",
    "            input_ids = torch.tensor([input_ids]).cuda() if cuda else torch.tensor([input_ids])\n",
    "            sentence_emb = outs[0][0][0].reshape([1, 1,-1])\n",
    "            stopScore, isStop, rl_new_state = cm_model(rdm_model, sentence_emb, init_state)\n",
    "            print(\"params:\", isStop[0], isStop)\n",
    "            init_state = rl_new_state\n",
    "            if isStop[0] == 1:\n",
    "                new_x_len[i] = j+1\n",
    "                break\n",
    "        if new_x_len[i] == 0 or new_x_len[i] > data_len[i]:\n",
    "            new_x_len[i] = data_len[i]\n",
    "    return new_x_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 训练RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def TrainRDMModel(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, t_steps, new_data_len=[], logger=None, \n",
    "                        log_dir=\"RDMBertTrain\"):\n",
    "    batch_size = 20 \n",
    "    max_gpu_batch = 2 #cannot load a larger batch into the limited memory, but we could  accumulates grads\n",
    "    splits = int(batch_size/max_gpu_batch)\n",
    "    assert(batch_size%max_gpu_batch == 0)\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "    t_acc = 0.9\n",
    "    ret_acc = 0.0\n",
    "    init_states = torch.zeros([1, max_gpu_batch, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    weight = torch.tensor([2.0, 1.0], dtype=torch.float32).cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "    optim = torch.optim.Adagrad([\n",
    "                                {'params': bert.parameters(), 'lr':5e-5},\n",
    "                                {'params': rdm_classifier.parameters(), 'lr': 5e-3},\n",
    "                                {'params': rdm_model.parameters(), 'lr': 5e-3}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(log_dir)\n",
    "    acc_l = np.zeros(splits)\n",
    "    loss_l = np.zeros(splits)\n",
    "    for step in range(t_steps):\n",
    "        optim.zero_grad()\n",
    "        try:\n",
    "            for j in range(splits):\n",
    "                if len(new_data_len) > 0:\n",
    "                    x, x_len, y = get_df_batch(step*splits+j, max_gpu_batch, new_data_len, tokenizer=tokenizer)\n",
    "                else:\n",
    "                    x, x_len, y = get_df_batch(step, max_gpu_batch, tokenizer=tokenizer)\n",
    "                x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "                batchsize, max_seq_len, max_sent_len, emb_dim = x_emb.shape\n",
    "                rdm_hiddens, rdm_outs = rdm_model(x_emb, x_len, init_states)\n",
    "                rdm_scores = rdm_classifier(\n",
    "                    torch.cat(\n",
    "                        rdm_outs # a list of tensor, where the ndim of tensor is 1 and the shape of tensor is [hidden_size]\n",
    "                    ).reshape(\n",
    "                        [-1, rdm_model.hidden_dim]\n",
    "                    )\n",
    "                )\n",
    "                rdm_preds = rdm_scores.argmax(axis=1)\n",
    "                y_label = y.argmax(axis=1)\n",
    "                acc_l[j], _, _, _, _, _, _ = Count_Accs(y_label, rdm_preds)\n",
    "                loss = loss_fn(rdm_scores, torch.tensor(y_label).cuda())\n",
    "                loss.backward()\n",
    "                loss_l[j] = float(loss)\n",
    "#                 print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "        except RuntimeError as exception:\n",
    "            if \"out of memory\" in str(exception):\n",
    "                print(\"WARNING: out of memory\")\n",
    "                print(\"%d, %d | x_len:\"%(step, j), x_len)\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "#                     time.sleep(5)\n",
    "                raise exception\n",
    "            else:   \n",
    "                raise exception\n",
    "\n",
    "        optim.step()        \n",
    "        writer.add_scalar('Train Loss', loss_l.mean(), step)\n",
    "        writer.add_scalar('Train Accuracy', acc_l.mean(), step)\n",
    "\n",
    "        sum_loss += loss_l.mean()\n",
    "        sum_acc += acc_l.mean()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if step % 10 == 9:\n",
    "            sum_loss = sum_loss / 10\n",
    "            sum_acc = sum_acc / 10\n",
    "            print('%3d | %d , train_loss/accuracy = %6.8f/%6.7f'             % (step, t_steps, \n",
    "                sum_loss, sum_acc,\n",
    "                ))\n",
    "            logger.info('%3d | %d , train_loss/accuracy = %6.8f/%6.7f'             % (step, t_steps, \n",
    "                sum_loss, sum_acc,\n",
    "                ))\n",
    "            if step%500 == 499:\n",
    "                rdm_save_as = '/home/hadoop/ERD/%s/rdmModel_epoch%03d.pkl'                                    % (log_dir, step/500)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"bert\":bert.state_dict(),\n",
    "                        \"rmdModel\":rdm_model.state_dict(),\n",
    "                        \"rdm_classifier\": rdm_classifier.state_dict()\n",
    "                    },\n",
    "                    rdm_save_as\n",
    "                )\n",
    "#                 rdm_model, bert, sentiModel, rdm_classifier\n",
    "            sum_acc = 0.0\n",
    "            sum_loss = 0.0\n",
    "    print(get_curtime() + \" Train df Model End.\")\n",
    "    logger.info(get_curtime() + \" Train df Model End.\")\n",
    "    return ret_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, log_dir, logger, FLAGS, cuda=False):\n",
    "    batch_size = 20\n",
    "    t_acc = 0.9\n",
    "    ids = np.array(range(batch_size), dtype=np.int32)\n",
    "    seq_states = np.zeros([batch_size], dtype=np.int32)\n",
    "    isStop = np.zeros([batch_size], dtype=np.int32)\n",
    "    max_id = batch_size\n",
    "    df_init_states = torch.zeros([1, batch_size, rdm_model.hidden_dim], dtype=torch.float32).cuda()\n",
    "    writer = SummaryWriter(log_dir, filename_suffix=\"_CM\")\n",
    "    state = df_init_states\n",
    "    D = deque()\n",
    "    ssq = []\n",
    "    print(\"in RL the begining\")\n",
    "    rdm_optim = torch.optim.Adagrad([\n",
    "                            {'params': bert.parameters(), 'lr':1e-6},\n",
    "                            {'params': rdm_classifier.parameters(), 'lr': 5e-5},\n",
    "                            {'params': rdm_model.parameters(), 'lr': 5e-5}\n",
    "                         ],\n",
    "                            weight_decay = 0.2\n",
    "    )\n",
    "    rl_optim = torch.optim.Adam([{'params':cm_model.parameters(), 'lr':1e-5}])\n",
    "    # get_new_len(sess, mm)\n",
    "    data_ID = get_data_ID()\n",
    "\n",
    "    if len(data_ID) % batch_size == 0: # the total number of events\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size)\n",
    "    else:\n",
    "        flags = int(len(data_ID) / FLAGS.batch_size) + 1\n",
    "    for i in range(flags):\n",
    "        with torch.no_grad():\n",
    "            x, x_len, y = get_df_batch(i, batch_size, tokenizer=tokenizer)\n",
    "            x_emb = Word_ids2SeqStates(x, bert, 3, cuda=True) \n",
    "            batchsize, max_seq_len, max_sent_len, emb_dim = x_emb.shape\n",
    "            rdm_hiddens, rdm_outs = rdm_model(x_emb, x_len, df_init_states)\n",
    "            print(\"batch %d\"%i)\n",
    "            if len(ssq) > 0:\n",
    "                ssq.extend([rdm_classifier(h) for h in rdm_hiddens])\n",
    "            else:\n",
    "                ssq = [rdm_classifier(h) for h in rdm_hiddens]\n",
    "                \n",
    "#     # cache ssq for development\n",
    "#     with open('./RDMBertTrain/cached_ssq.pkl', 'wb') as handle:\n",
    "#         pickle.dump(ssq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #load the cached ssq\n",
    "#     with open(\"./RDMBertTrain/cached_ssq.pkl\", 'rb') as handle:\n",
    "#         ssq = pickle.load(handle)\n",
    "\n",
    "    print(get_curtime() + \" Now Start RL training ...\")\n",
    "    counter = 0\n",
    "    sum_rw = 0.0 # sum of rewards\n",
    "\n",
    "    data_len = get_data_len()\n",
    "\n",
    "    while True:\n",
    "        if counter > FLAGS.OBSERVE:\n",
    "            sum_rw += rw.mean()\n",
    "            if counter % 200 == 0:\n",
    "                sum_rw = float(sum_rw) / 200\n",
    "                print( get_curtime() + \" Step: \" + str(step) \n",
    "                       + \" REWARD IS \" + str(sum_rw) \n",
    "                     )\n",
    "                if sum_rw > t_rw:\n",
    "                    print(\"Retch The Target Reward\")\n",
    "                    break\n",
    "                if counter > t_steps:\n",
    "                    print(\"Retch The Target Steps\")\n",
    "                    break\n",
    "                sum_rw = 0.0\n",
    "            s_state, s_x, s_isStop, s_rw = get_RL_Train_batch(D, FLAGS, cuda)\n",
    "            x_emb = layer2seq(bert, s_x, cuda=True)\n",
    "            stopScore, isStop, rl_new_state = cm_model(rdm_model, x_emb, s_state)\n",
    "            out_action = (stopScore*s_isStop).sum(axis=1)\n",
    "            rl_cost = torch.mean((s_rw - out_action)*(s_rw - out_action))\n",
    "            rl_cost.backward()\n",
    "            rl_optim.step()\n",
    "            print(\"RL Cost:\", rl_cost, \" | RL Reward:\", rw.mean())\n",
    "            writer.add_scalar('RL Cost', rl_cost, counter - FLAGS.OBSERVE)\n",
    "            writer.add_scalar('RL Reward', rw.mean(), counter - FLAGS.OBSERVE)\n",
    "\n",
    "        input_x, input_y, ids, seq_states, max_id = get_rl_batch(ids, seq_states, isStop, max_id, 0, FLAGS, tokenizer=tokenizer)\n",
    "        with torch.no_grad():\n",
    "            x_emb = layer2seq(bert, input_x, cuda=True)\n",
    "            batchsize, max_sent_len, emb_dim = x_emb.shape\n",
    "            mss, isStop, mNewState = cm_model(rdm_model, x_emb, state)\n",
    "\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if random.random() < FLAGS.random_rate:\n",
    "                isStop[j] = int(torch.rand(2).argmax())\n",
    "            if seq_states[j] == data_len[ids[j]]:\n",
    "                isStop[j] = 1\n",
    "\n",
    "        # eval\n",
    "        rw = get_reward(isStop, mss, ssq, ids, seq_states)\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            D.append((state[0][j], input_x[j], isStop[j], rw[j]))\n",
    "            if len(D) > FLAGS.max_memory:\n",
    "                D.popleft()\n",
    "\n",
    "        state = mNewState\n",
    "        for j in range(FLAGS.batch_size):\n",
    "            if isStop[j] == 1:\n",
    "                state[0][j].fill_(0)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入数据和设置配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as cr:\n",
    "    dic = json.load(cr)\n",
    "\n",
    "class adict(dict):\n",
    "    ''' Attribute dictionary - a convenience data structure, similar to SimpleNamespace in python 3.3\n",
    "        One can use attributes to read/write dictionary content.\n",
    "    '''\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "FLAGS = adict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_logger = MyLogger(\"CMTest\")\n",
    "rdm_logger = MyLogger(\"RDMLogger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 187 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./bertModel/\")\n",
    "bert = BertModel.from_pretrained(\"./bertModel/\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.conda/envs/py37_torch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "rdm_model = RDM_Model(768, 300, 256, 0.2).cuda()\n",
    "cm_model = CM_Model(300, 256, 2).cuda()\n",
    "rdm_classifier = nn.Linear(256, 2).cuda()\n",
    "cm_log_dir=\"CMBertTrain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入模型预训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_file = \"./RDMBertTrain/rdmModel_epoch019.pkl\"\n",
    "checkpoint = torch.load(pretrained_file)\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "rdm_model.load_state_dict(checkpoint[\"rmdModel\"])\n",
    "rdm_classifier.load_state_dict(checkpoint[\"rdm_classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练RDM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainRDMModel(rdm_model, bert, rdm_classifier, \n",
    "                    tokenizer, 10000, new_data_len=[], logger=rdm_logger, \n",
    "                        log_dir=\"RDMBertTrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练CM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in RL the begining\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "batch 20\n",
      "batch 21\n",
      "batch 22\n",
      "batch 23\n",
      "batch 24\n",
      "2019-10-08 22:43:34 Now Start RL training ...\n",
      "RL Cost: tensor(510.7690, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3882)\n",
      "RL Cost: tensor(2515.8945, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.0808)\n",
      "RL Cost: tensor(4014.3755, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.5416)\n",
      "RL Cost: tensor(1010.3217, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.2342)\n",
      "RL Cost: tensor(2012.1571, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.2341)\n",
      "RL Cost: tensor(1509.2681, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3877)\n",
      "RL Cost: tensor(1012.5688, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.2339)\n",
      "RL Cost: tensor(2012.7330, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.5412)\n",
      "RL Cost: tensor(1012.8701, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.3795)\n",
      "RL Cost: tensor(2012.5004, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-22.6947)\n",
      "RL Cost: tensor(2010.6371, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.3800)\n",
      "RL Cost: tensor(8.3066, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3872)\n",
      "RL Cost: tensor(2013.9086, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3871)\n",
      "RL Cost: tensor(3513.7195, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3870)\n",
      "RL Cost: tensor(2010.2758, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.5406)\n",
      "RL Cost: tensor(1508.1300, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-33.3233)\n",
      "RL Cost: tensor(1010.0918, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-33.0016)\n",
      "RL Cost: tensor(1511.5176, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3867)\n",
      "RL Cost: tensor(2007.6129, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.6812)\n",
      "RL Cost: tensor(2015.3500, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.5419)\n",
      "RL Cost: tensor(2007.9905, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.3864)\n",
      "RL Cost: tensor(2015.0023, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-43.4699)\n",
      "RL Cost: tensor(506.5019, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.5400)\n",
      "RL Cost: tensor(508.6774, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.7056)\n",
      "RL Cost: tensor(3010.2922, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.5524)\n",
      "RL Cost: tensor(2510.9158, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.5969)\n",
      "RL Cost: tensor(1007.9286, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.5359)\n",
      "RL Cost: tensor(1011.1945, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.3830)\n",
      "RL Cost: tensor(2012.2489, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.6997)\n",
      "RL Cost: tensor(1008.4321, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.5995)\n",
      "RL Cost: tensor(2015.2513, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.8613)\n",
      "RL Cost: tensor(1007.7545, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.5584)\n",
      "RL Cost: tensor(3007.8289, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.6095)\n",
      "RL Cost: tensor(1507.2406, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.8486)\n",
      "RL Cost: tensor(508.7088, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.2372)\n",
      "RL Cost: tensor(1508.0505, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.6985)\n",
      "RL Cost: tensor(1506.4471, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-17.8540)\n",
      "RL Cost: tensor(1511.2297, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.5328)\n",
      "RL Cost: tensor(1507.1605, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.7149)\n",
      "RL Cost: tensor(1011.6442, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.0282)\n",
      "RL Cost: tensor(1501.9612, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.2979)\n",
      "RL Cost: tensor(2501.8135, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-23.4485)\n",
      "RL Cost: tensor(2504.2166, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-18.6051)\n",
      "RL Cost: tensor(1508.8046, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-18.2871)\n",
      "RL Cost: tensor(1504.7090, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-23.2952)\n",
      "RL Cost: tensor(1506.4999, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-18.4670)\n",
      "RL Cost: tensor(2507.3164, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.4617)\n",
      "RL Cost: tensor(2506.6121, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.6747)\n",
      "RL Cost: tensor(1006.0536, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.8442)\n",
      "RL Cost: tensor(3505.2141, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.8384)\n",
      "RL Cost: tensor(2001.0588, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-12.9876)\n",
      "RL Cost: tensor(1503.0647, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-13.7602)\n",
      "RL Cost: tensor(1007.1958, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.8377)\n",
      "RL Cost: tensor(2006.4641, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-18.7741)\n",
      "RL Cost: tensor(510.2074, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.1550)\n",
      "RL Cost: tensor(2509.9167, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-18.4589)\n",
      "RL Cost: tensor(1511.7278, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-13.1398)\n",
      "RL Cost: tensor(1010.8427, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-13.3176)\n",
      "RL Cost: tensor(2006.7004, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.3670)\n",
      "RL Cost: tensor(2007.0758, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.9865)\n",
      "RL Cost: tensor(1507.7114, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-7.8354)\n",
      "RL Cost: tensor(2007.1854, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.8649)\n",
      "RL Cost: tensor(1005.8942, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.2852)\n",
      "RL Cost: tensor(1011.2745, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.4489)\n",
      "RL Cost: tensor(1011.3966, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.9762)\n",
      "RL Cost: tensor(3009.3997, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(2.0119)\n",
      "RL Cost: tensor(1506.9788, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.0649)\n",
      "RL Cost: tensor(2510.8010, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.3212)\n",
      "RL Cost: tensor(507.8887, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.7921)\n",
      "RL Cost: tensor(1509.9203, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.7858)\n",
      "RL Cost: tensor(2997.9861, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-2.9895)\n",
      "RL Cost: tensor(2007.7766, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.3050)\n",
      "RL Cost: tensor(2508.1919, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.7526)\n",
      "RL Cost: tensor(1005.8083, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-19.2101)\n",
      "RL Cost: tensor(3009.4490, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-13.8881)\n",
      "RL Cost: tensor(1511.3311, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.1445)\n",
      "RL Cost: tensor(2502.1360, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.1513)\n",
      "RL Cost: tensor(2498.2849, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.0679)\n",
      "RL Cost: tensor(3504.4414, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.9449)\n",
      "RL Cost: tensor(3004.8333, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.2360)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Cost: tensor(1508.5826, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.3709)\n",
      "RL Cost: tensor(1008.0008, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.0508)\n",
      "RL Cost: tensor(1503.7678, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-13.7416)\n",
      "RL Cost: tensor(1506.3984, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-14.5319)\n",
      "RL Cost: tensor(3002.8293, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-14.3333)\n",
      "RL Cost: tensor(1000.3586, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.0141)\n",
      "RL Cost: tensor(1009.6281, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.1249)\n",
      "RL Cost: tensor(1010.0114, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-19.3496)\n",
      "RL Cost: tensor(3502.0376, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-14.3758)\n",
      "RL Cost: tensor(509.1007, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-14.0695)\n",
      "RL Cost: tensor(2495.4304, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.7339)\n",
      "RL Cost: tensor(1502.7516, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.8971)\n",
      "RL Cost: tensor(1004.3063, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-8.7420)\n",
      "RL Cost: tensor(1500.4875, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.6799)\n",
      "RL Cost: tensor(503.9085, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.1978)\n",
      "RL Cost: tensor(2001.7855, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.0649)\n",
      "RL Cost: tensor(1506.1038, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.0548)\n",
      "RL Cost: tensor(1501.0387, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.5816)\n",
      "RL Cost: tensor(2502.1873, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.1185)\n",
      "RL Cost: tensor(1002.1599, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.8860)\n",
      "RL Cost: tensor(2001.8065, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-3.7435)\n",
      "RL Cost: tensor(1504.3931, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.2025)\n",
      "RL Cost: tensor(2497.7876, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6404)\n",
      "RL Cost: tensor(2997.3904, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4892)\n",
      "RL Cost: tensor(3503.3586, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.3634)\n",
      "RL Cost: tensor(1499.0608, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.5053)\n",
      "RL Cost: tensor(3493.4888, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.1127)\n",
      "RL Cost: tensor(1004.7560, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.9708)\n",
      "RL Cost: tensor(1008.5200, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8043)\n",
      "RL Cost: tensor(1005.0641, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8136)\n",
      "RL Cost: tensor(1003.4431, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.5257)\n",
      "RL Cost: tensor(1498.3979, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8435)\n",
      "RL Cost: tensor(508.6263, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3823)\n",
      "RL Cost: tensor(508.0670, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.2872)\n",
      "RL Cost: tensor(2003.3617, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.2738)\n",
      "RL Cost: tensor(1508.2646, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.5238)\n",
      "RL Cost: tensor(508.1972, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6385)\n",
      "RL Cost: tensor(1505.0107, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6743)\n",
      "RL Cost: tensor(999.4971, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6529)\n",
      "RL Cost: tensor(1008.1325, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.1957)\n",
      "RL Cost: tensor(2498.3931, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.7930)\n",
      "RL Cost: tensor(1502.3882, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8154)\n",
      "RL Cost: tensor(2995.8770, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.1885)\n",
      "RL Cost: tensor(3491.2212, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8090)\n",
      "RL Cost: tensor(1504.1361, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6552)\n",
      "RL Cost: tensor(1500.7250, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.7892)\n",
      "RL Cost: tensor(1503.4719, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.3955)\n",
      "RL Cost: tensor(1993.5541, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3250)\n",
      "RL Cost: tensor(1501.7516, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.7796)\n",
      "RL Cost: tensor(1498.4442, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.2404)\n",
      "RL Cost: tensor(1009.1675, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.3590)\n",
      "RL Cost: tensor(2495.7778, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.1126)\n",
      "RL Cost: tensor(2003.2555, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8187)\n",
      "RL Cost: tensor(2495.8574, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8083)\n",
      "RL Cost: tensor(3990.6138, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.7745)\n",
      "RL Cost: tensor(1002.9274, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.9363)\n",
      "RL Cost: tensor(503.5655, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(1.1157)\n",
      "RL Cost: tensor(999.8755, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1929)\n",
      "RL Cost: tensor(2494.8516, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.2046)\n",
      "RL Cost: tensor(2990.3799, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4791)\n",
      "RL Cost: tensor(507.1102, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1627)\n",
      "RL Cost: tensor(2485.7803, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4687)\n",
      "RL Cost: tensor(3482.5935, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6209)\n",
      "RL Cost: tensor(2001.0594, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6235)\n",
      "RL Cost: tensor(2004.4563, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6647)\n",
      "RL Cost: tensor(2002.3746, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8140)\n",
      "RL Cost: tensor(1500.1973, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.3676)\n",
      "RL Cost: tensor(9.8316, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.5036)\n",
      "RL Cost: tensor(1991.8848, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.3569)\n",
      "RL Cost: tensor(2495.4216, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3347)\n",
      "RL Cost: tensor(2997.8960, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4816)\n",
      "RL Cost: tensor(2496.1521, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.6643)\n",
      "RL Cost: tensor(1003.2872, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.0107)\n",
      "RL Cost: tensor(508.2518, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.6742)\n",
      "RL Cost: tensor(1505.2168, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3307)\n",
      "RL Cost: tensor(508.9083, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.3479)\n",
      "RL Cost: tensor(1004.9149, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4941)\n",
      "RL Cost: tensor(507.9851, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8338)\n",
      "RL Cost: tensor(1504.4788, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.5302)\n",
      "RL Cost: tensor(507.4273, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.5021)\n",
      "RL Cost: tensor(2496.9402, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3395)\n",
      "RL Cost: tensor(1492.7783, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1717)\n",
      "RL Cost: tensor(2497.1814, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.6768)\n",
      "RL Cost: tensor(2994.0110, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4578)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Cost: tensor(1997.4042, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.9206)\n",
      "RL Cost: tensor(1002.5068, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1423)\n",
      "RL Cost: tensor(10.2327, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-10.0417)\n",
      "RL Cost: tensor(1978.1971, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.8202)\n",
      "RL Cost: tensor(1493.6461, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4444)\n",
      "RL Cost: tensor(2985.9626, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.5919)\n",
      "RL Cost: tensor(501.7199, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1593)\n",
      "RL Cost: tensor(1501.0096, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.4762)\n",
      "RL Cost: tensor(1001.3693, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.5399)\n",
      "RL Cost: tensor(2983.0156, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.8034)\n",
      "RL Cost: tensor(998.9675, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1713)\n",
      "RL Cost: tensor(2485.5083, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3257)\n",
      "RL Cost: tensor(1492.2305, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.6092)\n",
      "RL Cost: tensor(1985.5140, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1481)\n",
      "RL Cost: tensor(1496.6356, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4715)\n",
      "RL Cost: tensor(1501.3274, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.7057)\n",
      "RL Cost: tensor(505.6626, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1613)\n",
      "RL Cost: tensor(3484.3494, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3453)\n",
      "RL Cost: tensor(2983.9575, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.4680)\n",
      "RL Cost: tensor(1000.1489, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1964)\n",
      "RL Cost: tensor(3490.2336, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.9317)\n",
      "RL Cost: tensor(507.5905, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1952)\n",
      "RL Cost: tensor(2493.7278, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.0273)\n",
      "RL Cost: tensor(2482.6057, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-0.0230)\n",
      "RL Cost: tensor(501.6450, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-9.8511)\n",
      "RL Cost: tensor(1498.4944, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1714)\n",
      "RL Cost: tensor(1000.4268, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1637)\n",
      "RL Cost: tensor(1003.2806, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3462)\n",
      "RL Cost: tensor(999.4197, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1933)\n",
      "RL Cost: tensor(1986.5687, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3242)\n",
      "RL Cost: tensor(1501.2599, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-4.6692)\n",
      "RL Cost: tensor(1495.3217, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.0224)\n",
      "RL Cost: tensor(1499.5771, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.3319)\n",
      "RL Cost: tensor(1993.4941, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(0.1480)\n",
      "RL Cost: tensor(1003.2783, device='cuda:0', grad_fn=<MeanBackward0>)  | RL Reward: tensor(-14.9958)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cdbe46041395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainCMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RDMBertTrain/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCM_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-c7f8c1744abc>\u001b[0m in \u001b[0;36mTrainCMModel\u001b[0;34m(bert, rdm_model, rdm_classifier, cm_model, tokenizer, log_dir, logger, FLAGS, cuda)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0msum_rw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_rw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 print( get_curtime() + \" Step: \" + str(step) \n\u001b[0;32m---> 60\u001b[0;31m                        \u001b[0;34m+\u001b[0m \u001b[0;34m\" REWARD IS \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_rw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                      )\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msum_rw\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mt_rw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'step' is not defined"
     ]
    }
   ],
   "source": [
    "TrainCMModel(bert, rdm_model, rdm_classifier, cm_model, tokenizer, \"RDMBertTrain/\", CM_logger, FLAGS, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
