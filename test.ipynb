{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 21:49:09.941854 140607751878464 deprecation_wrapper.py:119] From /home/hadoop/ERD/model.py:6: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import tensorflow as tf\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from collections import deque\n",
    "import model\n",
    "from dataUtils import *\n",
    "from logger import MyLogger\n",
    "import sys\n",
    "import PTB_data_reader\n",
    "import time\n",
    "import numpy as np\n",
    "import lstm_char_cnn\n",
    "import pickle\n",
    "import dataloader\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "logger = MyLogger(\"ERDMain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent: 31 ,  max_seq_len: 101\n",
      "5802 data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0830 21:50:19.443750 140607751878464 logger.py:24] (300, 200, 101, 31, 2, 2)\n",
      "I0830 21:50:19.447273 140607751878464 logger.py:24] 2019-08-30 21:50:19 Data loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 200 101 31 2 2\n",
      "2019-08-30 21:50:19 Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# load twitter data\n",
    "# load_data(FLAGS.data_file_path)\n",
    "load_data_fast()\n",
    "\n",
    "#load PTB data\n",
    "# word_vocab, char_vocab, word_tensors, char_tensors, max_word_length = \\\n",
    "#     PTB_data_reader.load_data(FLAGS.data_dir, FLAGS.max_word_length, char_vocab, eos=FLAGS.EOS)\n",
    "word_vocab, char_vocab, word_tensors, char_tensors = \\\n",
    "    PTB_data_reader.load_data_fast()\n",
    "max_word_length = FLAGS.max_word_length\n",
    "train_reader = PTB_data_reader.DataReader(word_tensors['train'], char_tensors['train'],\n",
    "                          FLAGS.batch_size, FLAGS.max_sent_len) \n",
    "\n",
    "#load sentiment analysis data\n",
    "sentiReader = dataloader.SentiDataLoader(\n",
    "                                        dirpath = '/home/hadoop/trainingandtestdata',\n",
    "                                        trainfile = 'training.1600000.processed.noemoticon.csv', \n",
    "                                        testfile = 'testdata.manual.2009.06.14.csv', \n",
    "                                        charVocab = char_vocab\n",
    "                        )\n",
    "# sentiReader.load_data()\n",
    "sentiReader.load_data_fast(\n",
    "                        '/home/hadoop/ERD/data/senti_train_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_train_label.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_data.pickle',\n",
    "                        '/home/hadoop/ERD/data/senti_test_label.pickle'\n",
    "                          )\n",
    "\n",
    "\n",
    "# (self, input_dim, hidden_dim, max_seq_len, max_word_num, class_num, action_num):\n",
    "print(  FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "            FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                FLAGS.class_num, FLAGS.action_num   )\n",
    "logger.info(    (FLAGS.embedding_dim, FLAGS.hidden_dim, \n",
    "                    FLAGS.max_seq_len, FLAGS.max_sent_len, \n",
    "                        FLAGS.class_num, FLAGS.action_num)  )\n",
    "\n",
    "print(get_curtime() + \" Data loaded.\")\n",
    "logger.info(get_curtime() + \" Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the Twitter data\n",
    "# data = get_data()\n",
    "# with open('data/data_dict.txt', 'wb') as handle:\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # save the PTB data\n",
    "# with open('data/char_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_tensors.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_tensors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/char_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(char_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/word_vocab.txt', 'wb') as handle:\n",
    "#     pickle.dump(word_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save the senti data\n",
    "# with open('data/senti_train_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_train_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.train_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('data/senti_test_data.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/senti_test_label.pickle', 'wb') as handle:\n",
    "#     pickle.dump(sentiReader.test_label, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/hadoop/ERD/model.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(lstm_char_cnn)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32)\n",
      "input_: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(?, 1100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w2v = lstm_char_cnn.WordEmbedding(\n",
    "                max_word_length = FLAGS.max_char_num , \n",
    "                char_vocab_size = char_vocab.size, \n",
    "                char_embed_size = FLAGS.char_embed_size, \n",
    "                kernels = eval(FLAGS.kernels), \n",
    "                kernel_features = eval(FLAGS.kernel_features), \n",
    "                num_highway_layers = FLAGS.highway_layers,\n",
    "                embedding_dim = FLAGS.embedding_dim\n",
    "            )\n",
    "lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "            batch_size = FLAGS.batch_size, \n",
    "            num_unroll_steps = FLAGS.max_sent_len, \n",
    "            rnn_size = FLAGS.embedding_dim, \n",
    "            num_rnn_layers = FLAGS.rnn_layers, \n",
    "            word_vocab_size = word_vocab.size\n",
    "        )\n",
    "\n",
    "char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                    w2v, lstm_lm, \n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    max_word_length = FLAGS.max_char_num, \n",
    "                    learning_rate = FLAGS.learning_rate,\n",
    "                    max_grad_norm = FLAGS.max_grad_norm\n",
    "                 )\n",
    "#sentiment analysis model\n",
    "s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "senti_train_graph = model.InferSentiTrainGraph(\n",
    "                        w2v, \n",
    "                        lstm_lm, \n",
    "                        s_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        sent_num = FLAGS.sent_num,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(reader):\n",
    "    return reader.iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Char_Model(session, train_model, train_reader, saver, summary_writer):\n",
    "    best_valid_loss = None\n",
    "    rnn_state = session.run(train_model.initial_rnn_state)\n",
    "    cnt = 10\n",
    "#     for epoch in range(FLAGS.max_epochs):\n",
    "    for epoch in range(1):\n",
    "        epoch_start_time = time.time()\n",
    "        avg_train_loss = 0.0\n",
    "        count = 0\n",
    "        for x, y in train_reader.iter():\n",
    "            count += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            loss, _, rnn_state, gradient_norm, step, _ = session.run([\n",
    "                train_model.loss,\n",
    "                train_model.train_op,\n",
    "                train_model.final_rnn_state,\n",
    "                train_model.global_norm,\n",
    "                train_model.global_step,\n",
    "                train_model.clear_char_embedding_padding\n",
    "            ], {\n",
    "                train_model.input: x,\n",
    "                train_model.targets: y,\n",
    "                train_model.drop_out: 0.8,\n",
    "                train_model.initial_rnn_state: rnn_state\n",
    "            })\n",
    "\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"step_train_loss\", simple_value=loss),\n",
    "                tf.Summary.Value(tag=\"step_train_perplexity\", simple_value=np.exp(loss)),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "            avg_train_loss += 0.05 * (loss - avg_train_loss)\n",
    "\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            if count % FLAGS.print_every == 0:\n",
    "                print('%6d: %d [%5d/%5d], train_loss/perplexity = %6.8f/%6.7f secs/batch = %.4fs, grad.norm=%6.8f' % (step,\n",
    "                                                        epoch, count,\n",
    "                                                        train_reader.length,\n",
    "                                                        loss, np.exp(loss),\n",
    "                                                        time_elapsed,\n",
    "                                                        gradient_norm))\n",
    "                cnt += 1\n",
    "                if cnt == 10:\n",
    "                    break\n",
    "        print('Epoch training time:', time.time()-epoch_start_time)\n",
    "        save_as = '%s/epoch%03d_%.4f.model' % (FLAGS.train_dir, epoch, avg_train_loss)\n",
    "        saver.save(session, save_as)\n",
    "        print('Saved char model', save_as)\n",
    "\n",
    "def TrainSentiModel(sess, saver, logger, train_model, senti_reader, train_batch, test_batch):\n",
    "    train_iter = 100\n",
    "    for t_epoch in range(1): \n",
    "        # for validation\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for t_iter in range(train_iter):\n",
    "            data_X, data_Y = senti_reader.GetTrainingBatch(\n",
    "                                            t_iter, \n",
    "                                            train_batch\n",
    "                            )\n",
    "            feed_dic = {\n",
    "                        train_model.sent_x: data_X, \n",
    "                        train_model.sent_y: data_Y\n",
    "            }\n",
    "            _, step, loss, acc = sess.run(\n",
    "                                        [train_model.sent_train_op, \n",
    "                                         train_model.sent_global_step, \n",
    "                                         train_model.sent_loss, \n",
    "                                         train_model.sent_acc], \n",
    "                                        feed_dic)\n",
    "            sum_loss += loss\n",
    "            sum_acc += acc\n",
    "            if t_iter % 10 == 9:\n",
    "                sum_loss = sum_loss / 10\n",
    "                sum_acc = sum_acc / 10\n",
    "                ret_acc = sum_acc\n",
    "                print(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                logger.info(get_curtime() + \" Step: \" + str(step) + \" Training loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "                sum_acc = 0.0\n",
    "                sum_loss = 0.0\n",
    "                break\n",
    "        # for validation\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "        for t_iter in range(100):\n",
    "            data_X, data_Y = senti_reader.GetTestData(t_iter, test_batch)\n",
    "            feed_dic = {train_model.sent_x: data_X, train_model.sent_y: data_Y}\n",
    "            loss, acc = sess.run([train_model.sent_loss, train_model.sent_acc], feed_dic)\n",
    "            sum_loss += loss\n",
    "            sum_acc += acc    \n",
    "        sum_loss = sum_loss / 100\n",
    "        sum_acc = sum_acc / 100\n",
    "        ret_acc = sum_acc\n",
    "        print(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        logger.info(get_curtime() + \" Step: \" + str(step) + \" validation loss: \" + str(sum_loss) + \" accuracy: \" + str(sum_acc))\n",
    "        sum_acc = 0.0\n",
    "        sum_loss = 0.0\n",
    "\n",
    "        saver.save(sess, \"df_saved/sent_model\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32)\n",
      "     5: 0 [    5/ 1499], train_loss/perplexity = 9.15730667/9483.4804688 secs/batch = 1.2378s, grad.norm=0.12444942\n",
      "    10: 0 [   10/ 1499], train_loss/perplexity = 9.08647346/8834.9746094 secs/batch = 1.1685s, grad.norm=0.13129051\n"
     ]
    }
   ],
   "source": [
    "# train char model\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        w2v = lstm_char_cnn.WordEmbedding(\n",
    "                        max_word_length = FLAGS.max_char_num , \n",
    "                        char_vocab_size = char_vocab.size, \n",
    "                        char_embed_size = FLAGS.char_embed_size, \n",
    "                        kernels = eval(FLAGS.kernels), \n",
    "                        kernel_features = eval(FLAGS.kernel_features), \n",
    "                        num_highway_layers = FLAGS.highway_layers,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "        lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    rnn_size = FLAGS.embedding_dim, \n",
    "                    num_rnn_layers = FLAGS.rnn_layers, \n",
    "                    word_vocab_size = word_vocab.size\n",
    "                )\n",
    "\n",
    "        char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                            w2v, lstm_lm, \n",
    "                            batch_size = FLAGS.batch_size, \n",
    "                            num_unroll_steps = FLAGS.max_sent_len, \n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            learning_rate = FLAGS.learning_rate,\n",
    "                            max_grad_norm = FLAGS.max_grad_norm\n",
    "                         )\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(\"testdir/\", graph=sess.graph)\n",
    "        Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)\n",
    "        \n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        w2v = lstm_char_cnn.WordEmbedding(\n",
    "                        max_word_length = FLAGS.max_char_num , \n",
    "                        char_vocab_size = char_vocab.size, \n",
    "                        char_embed_size = FLAGS.char_embed_size, \n",
    "                        kernels = eval(FLAGS.kernels), \n",
    "                        kernel_features = eval(FLAGS.kernel_features), \n",
    "                        num_highway_layers = FLAGS.highway_layers,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "        lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    rnn_size = FLAGS.embedding_dim, \n",
    "                    num_rnn_layers = FLAGS.rnn_layers, \n",
    "                    word_vocab_size = word_vocab.size\n",
    "                )\n",
    "\n",
    "        char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                            w2v, lstm_lm, \n",
    "                            batch_size = FLAGS.batch_size, \n",
    "                            num_unroll_steps = FLAGS.max_sent_len, \n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            learning_rate = FLAGS.learning_rate,\n",
    "                            max_grad_norm = FLAGS.max_grad_norm\n",
    "                         )\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"testdir/\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        summary_writer = tf.summary.FileWriter(\"testdir/\", graph=sess.graph)\n",
    "        Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_: Tensor(\"input:0\", shape=(20, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_1/CNN_OUT/add_7:0\", shape=(620, 1100), dtype=float32)\n",
      "input_: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "input_cnn: Tensor(\"Embedding_2/CNN_OUT/add_7:0\", shape=(?, 1100), dtype=float32)\n",
      "un_init_var: <tf.Variable 'SentiModel/conv2/kernel:0' shape=(5, 300, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'SentiModel/conv2/bias:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'dense/kernel:0' shape=(200, 3) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'global_step_1:0' shape=() dtype=int32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/char_embedding/Adagrad:0' shape=(72, 15) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_1/w/Adagrad:0' shape=(1, 1, 15, 50) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_1/b/Adagrad:0' shape=(50,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_2/w/Adagrad:0' shape=(1, 2, 15, 100) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_2/b/Adagrad:0' shape=(100,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_3/w/Adagrad:0' shape=(1, 3, 15, 150) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_3/b/Adagrad:0' shape=(150,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_4/w/Adagrad:0' shape=(1, 4, 15, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_4/b/Adagrad:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_5/w/Adagrad:0' shape=(1, 5, 15, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_5/b/Adagrad:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_6/w/Adagrad:0' shape=(1, 6, 15, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_6/b/Adagrad:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_7/w/Adagrad:0' shape=(1, 7, 15, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/TDNN/kernel_7/b/Adagrad:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adagrad:0' shape=(1400, 1200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adagrad:0' shape=(600, 1200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'SentiModel/conv2/kernel/Adagrad:0' shape=(5, 300, 200) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'SentiModel/conv2/bias/Adagrad:0' shape=(200,) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'dense/kernel/Adagrad:0' shape=(200, 3) dtype=float32_ref>\n",
      "un_init_var: <tf.Variable 'dense/bias/Adagrad:0' shape=(3,) dtype=float32_ref>\n",
      "[]\n",
      "     5: 0 [    5/ 1499], train_loss/perplexity = 9.16200352/9528.1279297 secs/batch = 1.1605s, grad.norm=0.12303412\n",
      "Epoch training time: 7.493838548660278\n",
      "Saved char model cv/epoch000_2.0782.model\n"
     ]
    }
   ],
   "source": [
    "# reuse model\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        w2v = lstm_char_cnn.WordEmbedding(\n",
    "                        max_word_length = FLAGS.max_char_num , \n",
    "                        char_vocab_size = char_vocab.size, \n",
    "                        char_embed_size = FLAGS.char_embed_size, \n",
    "                        kernels = eval(FLAGS.kernels), \n",
    "                        kernel_features = eval(FLAGS.kernel_features), \n",
    "                        num_highway_layers = FLAGS.highway_layers,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "        lstm_lm = lstm_char_cnn.LSTM_LM(\n",
    "                    batch_size = FLAGS.batch_size, \n",
    "                    num_unroll_steps = FLAGS.max_sent_len, \n",
    "                    rnn_size = FLAGS.embedding_dim, \n",
    "                    num_rnn_layers = FLAGS.rnn_layers, \n",
    "                    word_vocab_size = word_vocab.size\n",
    "                )\n",
    "\n",
    "        char_train_graph = lstm_char_cnn.infer_train_model(\n",
    "                            w2v, lstm_lm, \n",
    "                            batch_size = FLAGS.batch_size, \n",
    "                            num_unroll_steps = FLAGS.max_sent_len, \n",
    "                            max_word_length = FLAGS.max_char_num, \n",
    "                            learning_rate = FLAGS.learning_rate,\n",
    "                            max_grad_norm = FLAGS.max_grad_norm\n",
    "                         )\n",
    "        val_list1 = tf.global_variables()\n",
    "        saver = tf.train.Saver(val_list1, max_to_keep=4)\n",
    "        sess.run(tf.variables_initializer(val_list1))\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"testdir/\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "#         Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)\n",
    "        #sentiment analysis model\n",
    "        s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "        senti_train_graph = model.InferSentiTrainGraph(\n",
    "                                w2v, \n",
    "                                lstm_lm, \n",
    "                                s_model, \n",
    "                                max_word_num = FLAGS.max_sent_len, \n",
    "                                max_char_num = FLAGS.max_char_num, \n",
    "                                hidden_dim = FLAGS.hidden_dim, \n",
    "                                sent_num = FLAGS.sent_num,\n",
    "                                embedding_dim = FLAGS.embedding_dim\n",
    "                            )\n",
    "        \n",
    "        uninitialized_vars = []\n",
    "        for var in tf.global_variables():\n",
    "            try:\n",
    "                sess.run(var)\n",
    "            except:\n",
    "                uninitialized_vars.append(var)\n",
    "                print(\"un_init_var:\", var)\n",
    "        print(unitialized_vars)\n",
    "        sess.run(tf.variables_initializer(uninitialized_vars))\n",
    "        summary_writer = tf.summary.FileWriter(\"testdir/\", graph=sess.graph)\n",
    "        Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Embedding/char_embedding:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1400, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'WordEmbedding/SimpleLinear/Matrix:0' shape=(10000, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'WordEmbedding/SimpleLinear/Bias:0' shape=(10000,) dtype=float32_ref>,\n",
       " <tf.Variable 'global_step:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'SGD_Training/learning_rate:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/kernel:0' shape=(5, 300, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(200, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'global_step_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'Embedding/char_embedding/Adagrad:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w/Adagrad:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b/Adagrad:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w/Adagrad:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b/Adagrad:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w/Adagrad:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b/Adagrad:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w/Adagrad:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w/Adagrad:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w/Adagrad:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w/Adagrad:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adagrad:0' shape=(1400, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adagrad:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/kernel/Adagrad:0' shape=(5, 300, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/bias/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adagrad:0' shape=(200, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adagrad:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'SentiModel/conv2/kernel:0' shape=(5, 300, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(200, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'global_step_1:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'Embedding/char_embedding/Adagrad:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w/Adagrad:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b/Adagrad:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w/Adagrad:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b/Adagrad:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w/Adagrad:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b/Adagrad:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w/Adagrad:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w/Adagrad:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w/Adagrad:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w/Adagrad:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix/Adagrad:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias/Adagrad:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adagrad:0' shape=(1400, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adagrad:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adagrad:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/kernel/Adagrad:0' shape=(5, 300, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'SentiModel/conv2/bias/Adagrad:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel/Adagrad:0' shape=(200, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias/Adagrad:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( filter(lambda var: var not in val_list1, val_list2) )\n",
    "# val_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Embedding/char_embedding:0' shape=(72, 15) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/w:0' shape=(1, 1, 15, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_1/b:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/w:0' shape=(1, 2, 15, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_2/b:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/w:0' shape=(1, 3, 15, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_3/b:0' shape=(150,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/w:0' shape=(1, 4, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_4/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/w:0' shape=(1, 5, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_5/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/w:0' shape=(1, 6, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_6/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/w:0' shape=(1, 7, 15, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/TDNN/kernel_7/b:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_0/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_lin_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Matrix:0' shape=(1100, 1100) dtype=float32_ref>,\n",
       " <tf.Variable 'Embedding/CNN_OUT/highway_gate_1/Bias:0' shape=(1100,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1400, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(600, 1200) dtype=float32_ref>,\n",
       " <tf.Variable 'LSTM/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1200,) dtype=float32_ref>,\n",
       " <tf.Variable 'WordEmbedding/SimpleLinear/Matrix:0' shape=(10000, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'WordEmbedding/SimpleLinear/Bias:0' shape=(10000,) dtype=float32_ref>,\n",
       " <tf.Variable 'global_step:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'SGD_Training/learning_rate:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis model\n",
    "s_model = model.SentiModel(FLAGS.hidden_dim, 5)\n",
    "senti_train_graph = model.InferSentiTrainGraph(\n",
    "                        w2v, \n",
    "                        lstm_lm, \n",
    "                        s_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        sent_num = FLAGS.sent_num,\n",
    "                        embedding_dim = FLAGS.embedding_dim\n",
    "                    )\n",
    "# df model\n",
    "rdm_model = model.RDM_Model(\n",
    "                max_seq_len = FLAGS.max_seq_len, \n",
    "                max_word_num = FLAGS.max_sent_len, \n",
    "                embedding_dim = FLAGS.embedding_dim, \n",
    "                hidden_dim = FLAGS.hidden_dim\n",
    "            )\n",
    "rdm_train_graph = model.InferRDMTrainGraph(\n",
    "                        w2v, lstm_lm, s_model, rdm_model, \n",
    "                        batchsize=FLAGS.batch_size,\n",
    "                        max_seq_len = FLAGS.max_seq_len, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        max_char_num = FLAGS.max_char_num, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        embedding_dim = FLAGS.embedding_dim,\n",
    "                        class_num = FLAGS.class_num\n",
    "                )\n",
    "\n",
    "# rl model\n",
    "cm_model = model.CM_Model(\n",
    "                    max_word_num = FLAGS.max_sent_len, \n",
    "                    embedding_dim = FLAGS.embedding_dim, \n",
    "                    hidden_dim = FLAGS.hidden_dim, \n",
    "                    action_num = FLAGS.action_num\n",
    "            )\n",
    "cm_train_graph = model.InferCMTrainGraph(\n",
    "                        w2v, s_model, rdm_model, cm_model, \n",
    "                        max_word_num = FLAGS.max_sent_len, \n",
    "                        embedding_dim = FLAGS.embedding_dim, \n",
    "                        hidden_dim = FLAGS.hidden_dim, \n",
    "                        action_num = FLAGS.action_num\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=4)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lstm_char_cnn' from '/home/hadoop/ERD/lstm_char_cnn.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(lstm_char_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     5: 0 [    5/ 1499], train_loss/perplexity = 9.15729713/9483.3896484 secs/batch = 0.5930s, grad.norm=0.12490036\n",
      "    10: 0 [   10/ 1499], train_loss/perplexity = 9.08498096/8821.7978516 secs/batch = 0.6202s, grad.norm=0.13333045\n",
      "    15: 0 [   15/ 1499], train_loss/perplexity = 8.98046017/7946.2880859 secs/batch = 0.7101s, grad.norm=0.18570276\n",
      "    20: 0 [   20/ 1499], train_loss/perplexity = 8.63869190/5645.9394531 secs/batch = 0.5504s, grad.norm=0.43776885\n",
      "    25: 0 [   25/ 1499], train_loss/perplexity = 7.53612280/1874.5479736 secs/batch = 0.6260s, grad.norm=0.47992179\n",
      "    30: 0 [   30/ 1499], train_loss/perplexity = 7.29143143/1467.6700439 secs/batch = 0.5361s, grad.norm=1.12251890\n",
      "    35: 0 [   35/ 1499], train_loss/perplexity = 7.44433117/1710.1411133 secs/batch = 0.5397s, grad.norm=1.29667437\n",
      "    40: 0 [   40/ 1499], train_loss/perplexity = 7.20252466/1342.8166504 secs/batch = 0.5403s, grad.norm=1.38146412\n",
      "    45: 0 [   45/ 1499], train_loss/perplexity = 7.23605251/1388.6016846 secs/batch = 0.5417s, grad.norm=0.60844481\n",
      "    50: 0 [   50/ 1499], train_loss/perplexity = 6.85240221/946.1510620 secs/batch = 0.5473s, grad.norm=0.86468434\n",
      "    55: 0 [   55/ 1499], train_loss/perplexity = 7.01791763/1116.4593506 secs/batch = 0.5437s, grad.norm=0.94763744\n",
      "    60: 0 [   60/ 1499], train_loss/perplexity = 7.07743979/1184.9309082 secs/batch = 0.5955s, grad.norm=0.66798860\n",
      "    65: 0 [   65/ 1499], train_loss/perplexity = 6.84144497/935.8403931 secs/batch = 0.5381s, grad.norm=0.82884061\n",
      "    70: 0 [   70/ 1499], train_loss/perplexity = 6.95450068/1047.8552246 secs/batch = 0.5412s, grad.norm=0.69485897\n",
      "    75: 0 [   75/ 1499], train_loss/perplexity = 6.74375486/848.7416992 secs/batch = 0.5385s, grad.norm=0.65387583\n",
      "    80: 0 [   80/ 1499], train_loss/perplexity = 6.66470098/784.2289429 secs/batch = 0.5437s, grad.norm=0.80050129\n",
      "    85: 0 [   85/ 1499], train_loss/perplexity = 6.73466873/841.0648193 secs/batch = 0.5459s, grad.norm=0.68331861\n",
      "    90: 0 [   90/ 1499], train_loss/perplexity = 7.22064877/1367.3758545 secs/batch = 0.5444s, grad.norm=0.65160012\n",
      "    95: 0 [   95/ 1499], train_loss/perplexity = 6.86706161/960.1232300 secs/batch = 0.5418s, grad.norm=2.23444128\n",
      "   100: 0 [  100/ 1499], train_loss/perplexity = 6.78442764/883.9739990 secs/batch = 0.5418s, grad.norm=0.48371723\n",
      "   105: 0 [  105/ 1499], train_loss/perplexity = 6.59414434/730.8032837 secs/batch = 0.5450s, grad.norm=0.64565563\n",
      "   110: 0 [  110/ 1499], train_loss/perplexity = 6.79803514/896.0848999 secs/batch = 0.5452s, grad.norm=0.64021891\n",
      "   115: 0 [  115/ 1499], train_loss/perplexity = 6.71722078/826.5172729 secs/batch = 0.5436s, grad.norm=0.52544338\n",
      "   120: 0 [  120/ 1499], train_loss/perplexity = 6.80204439/899.6846924 secs/batch = 0.5386s, grad.norm=0.50281554\n",
      "   125: 0 [  125/ 1499], train_loss/perplexity = 6.84960556/943.5086670 secs/batch = 0.5871s, grad.norm=0.69262564\n",
      "   130: 0 [  130/ 1499], train_loss/perplexity = 6.66587210/785.1478882 secs/batch = 0.5418s, grad.norm=0.66292650\n",
      "   135: 0 [  135/ 1499], train_loss/perplexity = 6.69437218/807.8466187 secs/batch = 0.5372s, grad.norm=0.56834054\n",
      "   140: 0 [  140/ 1499], train_loss/perplexity = 6.94751072/1040.5562744 secs/batch = 0.5415s, grad.norm=2.66790605\n",
      "   145: 0 [  145/ 1499], train_loss/perplexity = 6.89192533/984.2946777 secs/batch = 0.5467s, grad.norm=0.56570035\n",
      "   150: 0 [  150/ 1499], train_loss/perplexity = 6.72081041/829.4894409 secs/batch = 0.5445s, grad.norm=0.63384122\n",
      "   155: 0 [  155/ 1499], train_loss/perplexity = 6.67456198/792.0004883 secs/batch = 0.5489s, grad.norm=0.53302711\n",
      "   160: 0 [  160/ 1499], train_loss/perplexity = 6.79824162/896.2698975 secs/batch = 0.5404s, grad.norm=0.51960140\n",
      "   165: 0 [  165/ 1499], train_loss/perplexity = 6.54727268/697.3397217 secs/batch = 0.5411s, grad.norm=0.81722927\n",
      "   170: 0 [  170/ 1499], train_loss/perplexity = 6.68854046/803.1491699 secs/batch = 0.5394s, grad.norm=0.46655107\n",
      "   175: 0 [  175/ 1499], train_loss/perplexity = 6.52728939/683.5429077 secs/batch = 0.5377s, grad.norm=0.60712826\n",
      "   180: 0 [  180/ 1499], train_loss/perplexity = 6.84294033/937.2409058 secs/batch = 0.5404s, grad.norm=0.48056698\n",
      "   185: 0 [  185/ 1499], train_loss/perplexity = 6.61576891/746.7786865 secs/batch = 0.5419s, grad.norm=0.99304110\n",
      "   190: 0 [  190/ 1499], train_loss/perplexity = 6.87636948/969.1016235 secs/batch = 0.5519s, grad.norm=0.72139060\n",
      "   195: 0 [  195/ 1499], train_loss/perplexity = 6.72306252/831.3596802 secs/batch = 0.5514s, grad.norm=0.63136309\n",
      "   200: 0 [  200/ 1499], train_loss/perplexity = 6.72166538/830.1989746 secs/batch = 0.5374s, grad.norm=0.58111489\n",
      "   205: 0 [  205/ 1499], train_loss/perplexity = 6.84986305/943.7516479 secs/batch = 0.5409s, grad.norm=0.51003706\n",
      "   210: 0 [  210/ 1499], train_loss/perplexity = 6.58339691/722.9910889 secs/batch = 0.5372s, grad.norm=0.67135221\n",
      "   215: 0 [  215/ 1499], train_loss/perplexity = 6.66603374/785.2748413 secs/batch = 0.5393s, grad.norm=1.17223775\n",
      "   220: 0 [  220/ 1499], train_loss/perplexity = 6.74933624/853.4920654 secs/batch = 0.5442s, grad.norm=0.60290414\n",
      "   225: 0 [  225/ 1499], train_loss/perplexity = 6.80556345/902.8563232 secs/batch = 0.5374s, grad.norm=0.47272044\n",
      "   230: 0 [  230/ 1499], train_loss/perplexity = 6.70113182/813.3258667 secs/batch = 0.5396s, grad.norm=0.55341256\n",
      "   235: 0 [  235/ 1499], train_loss/perplexity = 6.91401625/1006.2805786 secs/batch = 0.5395s, grad.norm=0.49683353\n",
      "   240: 0 [  240/ 1499], train_loss/perplexity = 6.85481119/948.4330444 secs/batch = 0.5440s, grad.norm=0.53636760\n",
      "   245: 0 [  245/ 1499], train_loss/perplexity = 6.56626654/710.7114868 secs/batch = 0.5418s, grad.norm=0.44845966\n",
      "   250: 0 [  250/ 1499], train_loss/perplexity = 6.56709003/711.2969971 secs/batch = 0.5386s, grad.norm=0.42867157\n",
      "   255: 0 [  255/ 1499], train_loss/perplexity = 6.44371700/628.7395020 secs/batch = 0.5366s, grad.norm=0.42719221\n",
      "   260: 0 [  260/ 1499], train_loss/perplexity = 6.74554110/850.2590942 secs/batch = 0.5460s, grad.norm=0.45383269\n",
      "   265: 0 [  265/ 1499], train_loss/perplexity = 6.73745823/843.4142456 secs/batch = 0.5352s, grad.norm=1.08042061\n",
      "   270: 0 [  270/ 1499], train_loss/perplexity = 6.85704136/950.5505371 secs/batch = 0.5479s, grad.norm=0.51162636\n",
      "   275: 0 [  275/ 1499], train_loss/perplexity = 6.48904991/657.8980103 secs/batch = 0.5351s, grad.norm=0.43326911\n",
      "   280: 0 [  280/ 1499], train_loss/perplexity = 6.56894588/712.6182861 secs/batch = 0.5935s, grad.norm=0.62724084\n",
      "   285: 0 [  285/ 1499], train_loss/perplexity = 6.88734102/979.7927246 secs/batch = 0.5388s, grad.norm=0.42678773\n",
      "   290: 0 [  290/ 1499], train_loss/perplexity = 6.77013826/871.4323730 secs/batch = 0.5414s, grad.norm=0.41208357\n",
      "   295: 0 [  295/ 1499], train_loss/perplexity = 6.53362989/687.8906250 secs/batch = 0.5435s, grad.norm=0.42331758\n",
      "   300: 0 [  300/ 1499], train_loss/perplexity = 6.48760366/656.9472046 secs/batch = 0.5453s, grad.norm=0.46934214\n",
      "   305: 0 [  305/ 1499], train_loss/perplexity = 6.78201771/881.8462524 secs/batch = 0.5449s, grad.norm=0.56416482\n",
      "   310: 0 [  310/ 1499], train_loss/perplexity = 6.87790298/970.5888672 secs/batch = 0.5356s, grad.norm=0.39976019\n",
      "   315: 0 [  315/ 1499], train_loss/perplexity = 6.70414257/815.7782593 secs/batch = 0.5421s, grad.norm=0.41139778\n",
      "   320: 0 [  320/ 1499], train_loss/perplexity = 6.72836208/835.7772217 secs/batch = 0.5362s, grad.norm=0.46116558\n",
      "   325: 0 [  325/ 1499], train_loss/perplexity = 6.63636827/762.3214111 secs/batch = 0.5365s, grad.norm=0.50728679\n",
      "   330: 0 [  330/ 1499], train_loss/perplexity = 6.70524216/816.6757812 secs/batch = 0.5432s, grad.norm=1.39834237\n",
      "   335: 0 [  335/ 1499], train_loss/perplexity = 6.76682186/868.5471191 secs/batch = 0.5408s, grad.norm=0.67249143\n",
      "   340: 0 [  340/ 1499], train_loss/perplexity = 6.51874590/677.7279053 secs/batch = 0.5395s, grad.norm=0.41724956\n",
      "   345: 0 [  345/ 1499], train_loss/perplexity = 6.47745848/650.3160400 secs/batch = 0.5889s, grad.norm=0.51632887\n",
      "   350: 0 [  350/ 1499], train_loss/perplexity = 6.61782551/748.3161011 secs/batch = 0.5508s, grad.norm=0.41249710\n",
      "   355: 0 [  355/ 1499], train_loss/perplexity = 6.42453003/616.7908936 secs/batch = 0.5400s, grad.norm=0.40182540\n",
      "   360: 0 [  360/ 1499], train_loss/perplexity = 6.66702461/786.0532837 secs/batch = 0.5408s, grad.norm=0.52969933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   365: 0 [  365/ 1499], train_loss/perplexity = 6.63771820/763.3511963 secs/batch = 0.5380s, grad.norm=1.15613115\n",
      "   370: 0 [  370/ 1499], train_loss/perplexity = 6.57711983/718.4670410 secs/batch = 0.5411s, grad.norm=0.44019940\n",
      "   375: 0 [  375/ 1499], train_loss/perplexity = 6.43539858/623.5310669 secs/batch = 0.5414s, grad.norm=0.33791822\n",
      "   380: 0 [  380/ 1499], train_loss/perplexity = 6.64196396/766.5990601 secs/batch = 0.5434s, grad.norm=0.52224058\n",
      "   385: 0 [  385/ 1499], train_loss/perplexity = 6.61221409/744.1287842 secs/batch = 0.5400s, grad.norm=0.50121397\n",
      "   390: 0 [  390/ 1499], train_loss/perplexity = 6.81604147/912.3662109 secs/batch = 0.5370s, grad.norm=0.35354772\n",
      "   395: 0 [  395/ 1499], train_loss/perplexity = 6.80333662/900.8480835 secs/batch = 0.5420s, grad.norm=0.41359380\n",
      "   400: 0 [  400/ 1499], train_loss/perplexity = 6.71813107/827.2699585 secs/batch = 0.5535s, grad.norm=0.49285099\n",
      "   405: 0 [  405/ 1499], train_loss/perplexity = 6.58374310/723.2414551 secs/batch = 0.5451s, grad.norm=0.29595456\n",
      "   410: 0 [  410/ 1499], train_loss/perplexity = 6.44414663/629.0096436 secs/batch = 0.5399s, grad.norm=0.38925210\n",
      "   415: 0 [  415/ 1499], train_loss/perplexity = 6.71315145/823.1607056 secs/batch = 0.5382s, grad.norm=0.33190441\n",
      "   420: 0 [  420/ 1499], train_loss/perplexity = 6.59542894/731.7426758 secs/batch = 0.5413s, grad.norm=0.37358764\n",
      "   425: 0 [  425/ 1499], train_loss/perplexity = 6.78271008/882.4570312 secs/batch = 0.5414s, grad.norm=0.33966643\n",
      "   430: 0 [  430/ 1499], train_loss/perplexity = 6.52289391/680.5449829 secs/batch = 0.5376s, grad.norm=0.36335975\n",
      "   435: 0 [  435/ 1499], train_loss/perplexity = 6.59673595/732.6997070 secs/batch = 0.5426s, grad.norm=0.51651639\n",
      "   440: 0 [  440/ 1499], train_loss/perplexity = 6.59202242/729.2542114 secs/batch = 0.5418s, grad.norm=0.34399137\n",
      "   445: 0 [  445/ 1499], train_loss/perplexity = 6.61462164/745.9224243 secs/batch = 0.5483s, grad.norm=0.42661843\n",
      "   450: 0 [  450/ 1499], train_loss/perplexity = 6.64990139/772.7081299 secs/batch = 0.5356s, grad.norm=0.55940378\n",
      "   455: 0 [  455/ 1499], train_loss/perplexity = 6.66344261/783.2426758 secs/batch = 0.5352s, grad.norm=0.44745365\n",
      "   460: 0 [  460/ 1499], train_loss/perplexity = 6.79700184/895.1594238 secs/batch = 0.5410s, grad.norm=0.92568392\n",
      "   465: 0 [  465/ 1499], train_loss/perplexity = 6.67458868/792.0216064 secs/batch = 0.5384s, grad.norm=0.36509123\n",
      "   470: 0 [  470/ 1499], train_loss/perplexity = 6.79398727/892.4649658 secs/batch = 0.5370s, grad.norm=0.42790800\n",
      "   475: 0 [  475/ 1499], train_loss/perplexity = 6.59852171/734.0092773 secs/batch = 0.5353s, grad.norm=0.43085268\n",
      "   480: 0 [  480/ 1499], train_loss/perplexity = 6.60608721/739.5834961 secs/batch = 0.5413s, grad.norm=0.32275870\n",
      "   485: 0 [  485/ 1499], train_loss/perplexity = 6.66746092/786.3963623 secs/batch = 0.5431s, grad.norm=0.31361005\n",
      "   490: 0 [  490/ 1499], train_loss/perplexity = 6.62887478/756.6303101 secs/batch = 0.5473s, grad.norm=0.69130373\n",
      "   495: 0 [  495/ 1499], train_loss/perplexity = 6.64208412/766.6912231 secs/batch = 0.5380s, grad.norm=0.40845504\n",
      "   500: 0 [  500/ 1499], train_loss/perplexity = 6.73645163/842.5656738 secs/batch = 0.5916s, grad.norm=0.64608341\n",
      "   505: 0 [  505/ 1499], train_loss/perplexity = 6.46131802/639.9039307 secs/batch = 0.5403s, grad.norm=0.35556227\n",
      "   510: 0 [  510/ 1499], train_loss/perplexity = 6.75880146/861.6088867 secs/batch = 0.5429s, grad.norm=0.30980957\n",
      "   515: 0 [  515/ 1499], train_loss/perplexity = 6.57657194/718.0734863 secs/batch = 0.5408s, grad.norm=0.45310661\n",
      "   520: 0 [  520/ 1499], train_loss/perplexity = 6.79362631/892.1428833 secs/batch = 0.5444s, grad.norm=0.35592324\n",
      "   525: 0 [  525/ 1499], train_loss/perplexity = 6.91666603/1008.9505615 secs/batch = 0.5461s, grad.norm=0.50687110\n",
      "   530: 0 [  530/ 1499], train_loss/perplexity = 6.65585470/777.3220215 secs/batch = 0.5388s, grad.norm=0.49948555\n",
      "   535: 0 [  535/ 1499], train_loss/perplexity = 6.58111000/721.3395996 secs/batch = 0.5422s, grad.norm=0.41596952\n",
      "   540: 0 [  540/ 1499], train_loss/perplexity = 6.56899071/712.6502075 secs/batch = 0.5380s, grad.norm=0.46233657\n",
      "   545: 0 [  545/ 1499], train_loss/perplexity = 6.42729998/618.5017090 secs/batch = 0.5395s, grad.norm=0.65435636\n",
      "   550: 0 [  550/ 1499], train_loss/perplexity = 6.64267111/767.1413574 secs/batch = 0.5474s, grad.norm=0.43825257\n",
      "   555: 0 [  555/ 1499], train_loss/perplexity = 6.48377800/654.4387817 secs/batch = 0.5440s, grad.norm=0.41596040\n",
      "   560: 0 [  560/ 1499], train_loss/perplexity = 6.87060452/963.5308838 secs/batch = 0.5417s, grad.norm=0.61804181\n",
      "   565: 0 [  565/ 1499], train_loss/perplexity = 6.58342600/723.0121460 secs/batch = 0.5803s, grad.norm=0.32403567\n",
      "   570: 0 [  570/ 1499], train_loss/perplexity = 6.53397369/688.1271973 secs/batch = 0.5390s, grad.norm=0.28501704\n",
      "   575: 0 [  575/ 1499], train_loss/perplexity = 6.75026608/854.2860107 secs/batch = 0.5419s, grad.norm=1.09746063\n",
      "   580: 0 [  580/ 1499], train_loss/perplexity = 6.48975992/658.3652954 secs/batch = 0.5453s, grad.norm=0.37865958\n",
      "   585: 0 [  585/ 1499], train_loss/perplexity = 6.52872038/684.5217285 secs/batch = 0.5413s, grad.norm=0.42048693\n",
      "   590: 0 [  590/ 1499], train_loss/perplexity = 6.69236231/806.2245483 secs/batch = 0.5390s, grad.norm=0.64849412\n",
      "   595: 0 [  595/ 1499], train_loss/perplexity = 6.56055260/706.6621094 secs/batch = 0.5444s, grad.norm=0.26578006\n",
      "   600: 0 [  600/ 1499], train_loss/perplexity = 6.66821671/786.9909058 secs/batch = 0.5477s, grad.norm=0.49405050\n",
      "   605: 0 [  605/ 1499], train_loss/perplexity = 6.47536135/648.9536743 secs/batch = 0.5416s, grad.norm=0.34858578\n",
      "   610: 0 [  610/ 1499], train_loss/perplexity = 6.70364714/815.3742065 secs/batch = 0.5417s, grad.norm=0.44766417\n",
      "   615: 0 [  615/ 1499], train_loss/perplexity = 6.71979189/828.6450195 secs/batch = 0.5433s, grad.norm=0.48043135\n",
      "   620: 0 [  620/ 1499], train_loss/perplexity = 6.54954195/698.9239502 secs/batch = 0.5407s, grad.norm=0.30389974\n",
      "   625: 0 [  625/ 1499], train_loss/perplexity = 6.60607195/739.5722046 secs/batch = 0.5429s, grad.norm=0.36760679\n",
      "   630: 0 [  630/ 1499], train_loss/perplexity = 6.70804024/818.9641113 secs/batch = 0.5382s, grad.norm=0.36008716\n",
      "   635: 0 [  635/ 1499], train_loss/perplexity = 6.58663368/725.3350220 secs/batch = 0.5436s, grad.norm=0.70500648\n",
      "   640: 0 [  640/ 1499], train_loss/perplexity = 6.64935732/772.2878418 secs/batch = 0.5426s, grad.norm=0.30387124\n",
      "   645: 0 [  645/ 1499], train_loss/perplexity = 6.48686981/656.4652710 secs/batch = 0.5349s, grad.norm=0.31664306\n",
      "   650: 0 [  650/ 1499], train_loss/perplexity = 6.46839714/644.4499512 secs/batch = 0.5519s, grad.norm=0.53280395\n",
      "   655: 0 [  655/ 1499], train_loss/perplexity = 6.70073223/813.0009155 secs/batch = 0.5424s, grad.norm=0.47132611\n",
      "   660: 0 [  660/ 1499], train_loss/perplexity = 6.78019238/880.2380371 secs/batch = 0.5447s, grad.norm=0.36796808\n",
      "   665: 0 [  665/ 1499], train_loss/perplexity = 6.78560829/885.0182495 secs/batch = 0.5397s, grad.norm=0.35987845\n",
      "   670: 0 [  670/ 1499], train_loss/perplexity = 6.67328405/790.9890137 secs/batch = 0.5412s, grad.norm=0.40938026\n",
      "   675: 0 [  675/ 1499], train_loss/perplexity = 6.58391523/723.3659668 secs/batch = 0.5432s, grad.norm=0.35206249\n",
      "   680: 0 [  680/ 1499], train_loss/perplexity = 6.62061644/750.4075317 secs/batch = 0.5377s, grad.norm=0.41547358\n",
      "   685: 0 [  685/ 1499], train_loss/perplexity = 6.66847992/787.1981201 secs/batch = 0.5442s, grad.norm=0.37866428\n",
      "   690: 0 [  690/ 1499], train_loss/perplexity = 6.63233137/759.2501831 secs/batch = 0.5416s, grad.norm=0.42819425\n",
      "   695: 0 [  695/ 1499], train_loss/perplexity = 6.51923847/678.0618286 secs/batch = 0.5373s, grad.norm=0.38235751\n",
      "   700: 0 [  700/ 1499], train_loss/perplexity = 6.72567081/833.5309448 secs/batch = 0.5444s, grad.norm=0.34418362\n",
      "   705: 0 [  705/ 1499], train_loss/perplexity = 6.48960781/658.2651367 secs/batch = 0.5452s, grad.norm=0.36929053\n",
      "   710: 0 [  710/ 1499], train_loss/perplexity = 6.53926182/691.7757568 secs/batch = 0.5469s, grad.norm=0.26262283\n",
      "   715: 0 [  715/ 1499], train_loss/perplexity = 6.58596420/724.8496094 secs/batch = 0.5413s, grad.norm=0.37130007\n",
      "   720: 0 [  720/ 1499], train_loss/perplexity = 6.65871334/779.5473022 secs/batch = 0.5428s, grad.norm=0.31422433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   725: 0 [  725/ 1499], train_loss/perplexity = 6.52330303/680.8234863 secs/batch = 0.5357s, grad.norm=0.67174274\n",
      "   730: 0 [  730/ 1499], train_loss/perplexity = 6.60094213/735.7880859 secs/batch = 0.5373s, grad.norm=0.45806772\n",
      "   735: 0 [  735/ 1499], train_loss/perplexity = 6.57344103/715.8287964 secs/batch = 0.5337s, grad.norm=0.41944647\n",
      "   740: 0 [  740/ 1499], train_loss/perplexity = 6.57568359/717.4359131 secs/batch = 0.5410s, grad.norm=0.30755678\n",
      "   745: 0 [  745/ 1499], train_loss/perplexity = 6.48124552/652.7835083 secs/batch = 0.5372s, grad.norm=0.41048518\n",
      "   750: 0 [  750/ 1499], train_loss/perplexity = 6.62926912/756.9287720 secs/batch = 0.5467s, grad.norm=0.34821105\n",
      "   755: 0 [  755/ 1499], train_loss/perplexity = 6.50807333/670.5332642 secs/batch = 0.5429s, grad.norm=0.32785997\n",
      "   760: 0 [  760/ 1499], train_loss/perplexity = 6.28847456/538.3314819 secs/batch = 0.5420s, grad.norm=0.33857116\n",
      "   765: 0 [  765/ 1499], train_loss/perplexity = 6.53475618/688.6658325 secs/batch = 0.5373s, grad.norm=0.62272656\n",
      "   770: 0 [  770/ 1499], train_loss/perplexity = 6.59699631/732.8905029 secs/batch = 0.5846s, grad.norm=0.41805804\n",
      "   775: 0 [  775/ 1499], train_loss/perplexity = 6.79882669/896.7944336 secs/batch = 0.5398s, grad.norm=0.34699312\n",
      "   780: 0 [  780/ 1499], train_loss/perplexity = 6.47716522/650.1253662 secs/batch = 0.5428s, grad.norm=0.30478594\n",
      "   785: 0 [  785/ 1499], train_loss/perplexity = 6.56705618/711.2728882 secs/batch = 0.5556s, grad.norm=0.35426587\n",
      "   790: 0 [  790/ 1499], train_loss/perplexity = 6.37903118/589.3564453 secs/batch = 0.5374s, grad.norm=0.42432195\n",
      "   795: 0 [  795/ 1499], train_loss/perplexity = 6.63196516/758.9722290 secs/batch = 0.5839s, grad.norm=0.40839258\n",
      "   800: 0 [  800/ 1499], train_loss/perplexity = 6.51381111/674.3917236 secs/batch = 0.5438s, grad.norm=0.44693363\n",
      "   805: 0 [  805/ 1499], train_loss/perplexity = 6.58475256/723.9718628 secs/batch = 0.5463s, grad.norm=0.25474101\n",
      "   810: 0 [  810/ 1499], train_loss/perplexity = 6.61577129/746.7805176 secs/batch = 0.5392s, grad.norm=0.53808671\n",
      "   815: 0 [  815/ 1499], train_loss/perplexity = 6.61725283/747.8876953 secs/batch = 0.5422s, grad.norm=0.34571561\n",
      "   820: 0 [  820/ 1499], train_loss/perplexity = 6.40768576/606.4884644 secs/batch = 0.5449s, grad.norm=0.39519373\n",
      "   825: 0 [  825/ 1499], train_loss/perplexity = 6.62536049/753.9759521 secs/batch = 0.5393s, grad.norm=0.32359615\n",
      "   830: 0 [  830/ 1499], train_loss/perplexity = 6.59115744/728.6237183 secs/batch = 0.5401s, grad.norm=0.34478220\n",
      "   835: 0 [  835/ 1499], train_loss/perplexity = 6.56107807/707.0335083 secs/batch = 0.5372s, grad.norm=0.43927562\n",
      "   840: 0 [  840/ 1499], train_loss/perplexity = 6.48138285/652.8731689 secs/batch = 0.5392s, grad.norm=0.35346717\n",
      "   845: 0 [  845/ 1499], train_loss/perplexity = 6.52360678/681.0302734 secs/batch = 0.5400s, grad.norm=0.50119430\n",
      "   850: 0 [  850/ 1499], train_loss/perplexity = 6.56607103/710.5725098 secs/batch = 0.5381s, grad.norm=0.33522642\n",
      "   855: 0 [  855/ 1499], train_loss/perplexity = 6.66649818/785.6395874 secs/batch = 0.5384s, grad.norm=0.61077034\n",
      "   860: 0 [  860/ 1499], train_loss/perplexity = 6.32161283/556.4697876 secs/batch = 0.5343s, grad.norm=0.38085365\n",
      "   865: 0 [  865/ 1499], train_loss/perplexity = 6.54567003/696.2230225 secs/batch = 0.5351s, grad.norm=0.28946796\n",
      "   870: 0 [  870/ 1499], train_loss/perplexity = 6.43795347/625.1261597 secs/batch = 0.5365s, grad.norm=0.31877077\n",
      "   875: 0 [  875/ 1499], train_loss/perplexity = 6.58191824/721.9228516 secs/batch = 0.5407s, grad.norm=0.27921546\n",
      "   880: 0 [  880/ 1499], train_loss/perplexity = 6.44575262/630.0206909 secs/batch = 0.5447s, grad.norm=0.26821834\n",
      "   885: 0 [  885/ 1499], train_loss/perplexity = 6.38283062/591.5999146 secs/batch = 0.5419s, grad.norm=0.32863075\n",
      "   890: 0 [  890/ 1499], train_loss/perplexity = 6.54885054/698.4408569 secs/batch = 0.5431s, grad.norm=0.32203588\n",
      "   895: 0 [  895/ 1499], train_loss/perplexity = 6.52295732/680.5881348 secs/batch = 0.5409s, grad.norm=0.33574080\n",
      "   900: 0 [  900/ 1499], train_loss/perplexity = 6.53401709/688.1570435 secs/batch = 0.5420s, grad.norm=0.37119970\n",
      "   905: 0 [  905/ 1499], train_loss/perplexity = 6.56147718/707.3157349 secs/batch = 0.5392s, grad.norm=0.29058722\n",
      "   910: 0 [  910/ 1499], train_loss/perplexity = 6.70255566/814.4847412 secs/batch = 0.5446s, grad.norm=0.41764465\n",
      "   915: 0 [  915/ 1499], train_loss/perplexity = 6.43795824/625.1291504 secs/batch = 0.5406s, grad.norm=0.32782191\n",
      "   920: 0 [  920/ 1499], train_loss/perplexity = 6.29035234/539.3433228 secs/batch = 0.5442s, grad.norm=0.33519420\n",
      "   925: 0 [  925/ 1499], train_loss/perplexity = 6.65359592/775.5681763 secs/batch = 0.5461s, grad.norm=0.41569617\n",
      "   930: 0 [  930/ 1499], train_loss/perplexity = 6.57865810/719.5730591 secs/batch = 0.5376s, grad.norm=0.39121410\n",
      "   935: 0 [  935/ 1499], train_loss/perplexity = 6.61121225/743.3836670 secs/batch = 0.5407s, grad.norm=0.40881988\n",
      "   940: 0 [  940/ 1499], train_loss/perplexity = 6.34317446/568.5984497 secs/batch = 0.5421s, grad.norm=0.44135702\n",
      "   945: 0 [  945/ 1499], train_loss/perplexity = 6.61273527/744.5166626 secs/batch = 0.5418s, grad.norm=0.40684047\n",
      "   950: 0 [  950/ 1499], train_loss/perplexity = 6.59620047/732.3074951 secs/batch = 0.5366s, grad.norm=0.32767108\n",
      "   955: 0 [  955/ 1499], train_loss/perplexity = 6.67173576/789.7652588 secs/batch = 0.5499s, grad.norm=0.31711659\n",
      "   960: 0 [  960/ 1499], train_loss/perplexity = 6.54306316/694.4104004 secs/batch = 0.5406s, grad.norm=0.48935288\n",
      "   965: 0 [  965/ 1499], train_loss/perplexity = 6.51451683/674.8677979 secs/batch = 0.5410s, grad.norm=0.34123066\n",
      "   970: 0 [  970/ 1499], train_loss/perplexity = 6.72119379/829.8075562 secs/batch = 0.5442s, grad.norm=0.41939557\n",
      "   975: 0 [  975/ 1499], train_loss/perplexity = 6.57145977/714.4119873 secs/batch = 0.5397s, grad.norm=0.29637882\n",
      "   980: 0 [  980/ 1499], train_loss/perplexity = 6.57464266/716.6894531 secs/batch = 0.5403s, grad.norm=0.47710451\n",
      "   985: 0 [  985/ 1499], train_loss/perplexity = 6.41620827/611.6793823 secs/batch = 0.5430s, grad.norm=0.37993297\n",
      "   990: 0 [  990/ 1499], train_loss/perplexity = 6.57299614/715.5103760 secs/batch = 0.5347s, grad.norm=0.30635786\n",
      "   995: 0 [  995/ 1499], train_loss/perplexity = 6.73920584/844.8894653 secs/batch = 0.5403s, grad.norm=0.54763091\n",
      "  1000: 0 [ 1000/ 1499], train_loss/perplexity = 6.41316032/609.8178711 secs/batch = 0.5494s, grad.norm=0.30161265\n",
      "  1005: 0 [ 1005/ 1499], train_loss/perplexity = 6.71881628/827.8369751 secs/batch = 0.5453s, grad.norm=0.41859004\n",
      "  1010: 0 [ 1010/ 1499], train_loss/perplexity = 6.58723879/725.7741089 secs/batch = 0.5458s, grad.norm=0.30422357\n",
      "  1015: 0 [ 1015/ 1499], train_loss/perplexity = 6.44031096/626.6016235 secs/batch = 0.5408s, grad.norm=0.33926561\n",
      "  1020: 0 [ 1020/ 1499], train_loss/perplexity = 6.51318169/673.9673462 secs/batch = 0.5377s, grad.norm=0.46465868\n",
      "  1025: 0 [ 1025/ 1499], train_loss/perplexity = 6.44410896/628.9859619 secs/batch = 0.5416s, grad.norm=0.38345328\n",
      "  1030: 0 [ 1030/ 1499], train_loss/perplexity = 6.50059128/665.5350342 secs/batch = 0.5413s, grad.norm=0.30027229\n",
      "  1035: 0 [ 1035/ 1499], train_loss/perplexity = 6.86328840/956.5072632 secs/batch = 0.5409s, grad.norm=0.76152450\n",
      "  1040: 0 [ 1040/ 1499], train_loss/perplexity = 6.40522909/605.0003662 secs/batch = 0.5432s, grad.norm=0.31572387\n",
      "  1045: 0 [ 1045/ 1499], train_loss/perplexity = 6.60709333/740.3280029 secs/batch = 0.5388s, grad.norm=0.31082109\n",
      "  1050: 0 [ 1050/ 1499], train_loss/perplexity = 6.24729443/516.6132202 secs/batch = 0.5364s, grad.norm=0.39073661\n",
      "  1055: 0 [ 1055/ 1499], train_loss/perplexity = 6.55137396/700.2055664 secs/batch = 0.5440s, grad.norm=0.51979113\n",
      "  1060: 0 [ 1060/ 1499], train_loss/perplexity = 6.67375326/791.3602295 secs/batch = 0.5390s, grad.norm=0.32597333\n",
      "  1065: 0 [ 1065/ 1499], train_loss/perplexity = 6.46338463/641.2277222 secs/batch = 0.5929s, grad.norm=0.33962789\n",
      "  1070: 0 [ 1070/ 1499], train_loss/perplexity = 6.65898180/779.7565918 secs/batch = 0.5379s, grad.norm=0.42435664\n",
      "  1075: 0 [ 1075/ 1499], train_loss/perplexity = 6.56385469/708.9993896 secs/batch = 0.5410s, grad.norm=0.27427500\n",
      "  1080: 0 [ 1080/ 1499], train_loss/perplexity = 6.70603895/817.3267212 secs/batch = 0.5372s, grad.norm=0.32566512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1085: 0 [ 1085/ 1499], train_loss/perplexity = 6.68645906/801.4792480 secs/batch = 0.5370s, grad.norm=0.32147256\n",
      "  1090: 0 [ 1090/ 1499], train_loss/perplexity = 6.43140650/621.0468140 secs/batch = 0.5377s, grad.norm=0.34510538\n",
      "  1095: 0 [ 1095/ 1499], train_loss/perplexity = 6.62986803/757.3822021 secs/batch = 0.5432s, grad.norm=0.32318753\n",
      "  1100: 0 [ 1100/ 1499], train_loss/perplexity = 6.62116766/750.8212891 secs/batch = 0.5441s, grad.norm=0.29048184\n",
      "  1105: 0 [ 1105/ 1499], train_loss/perplexity = 6.53903818/691.6210327 secs/batch = 0.5483s, grad.norm=0.34881800\n",
      "  1110: 0 [ 1110/ 1499], train_loss/perplexity = 6.60372019/737.8349609 secs/batch = 0.5392s, grad.norm=0.28734133\n",
      "  1115: 0 [ 1115/ 1499], train_loss/perplexity = 6.36170435/579.2327271 secs/batch = 0.5519s, grad.norm=0.32726750\n",
      "  1120: 0 [ 1120/ 1499], train_loss/perplexity = 6.60577869/739.3553467 secs/batch = 0.5395s, grad.norm=0.40353474\n",
      "  1125: 0 [ 1125/ 1499], train_loss/perplexity = 6.54308033/694.4223633 secs/batch = 0.5429s, grad.norm=0.58018655\n",
      "  1130: 0 [ 1130/ 1499], train_loss/perplexity = 6.41749382/612.4662476 secs/batch = 0.5374s, grad.norm=0.33600625\n",
      "  1135: 0 [ 1135/ 1499], train_loss/perplexity = 6.74568415/850.3807373 secs/batch = 0.5324s, grad.norm=0.36664549\n",
      "  1140: 0 [ 1140/ 1499], train_loss/perplexity = 6.62472963/753.5004883 secs/batch = 0.5392s, grad.norm=0.31622428\n",
      "  1145: 0 [ 1145/ 1499], train_loss/perplexity = 6.57149601/714.4378662 secs/batch = 0.5457s, grad.norm=0.32059860\n",
      "  1150: 0 [ 1150/ 1499], train_loss/perplexity = 6.59649801/732.5253906 secs/batch = 0.5422s, grad.norm=0.35328507\n",
      "  1155: 0 [ 1155/ 1499], train_loss/perplexity = 6.37645006/587.8372192 secs/batch = 0.5482s, grad.norm=0.28386909\n",
      "  1160: 0 [ 1160/ 1499], train_loss/perplexity = 6.56001139/706.2797241 secs/batch = 0.5364s, grad.norm=0.40525278\n",
      "  1165: 0 [ 1165/ 1499], train_loss/perplexity = 6.38176060/590.9672241 secs/batch = 0.5424s, grad.norm=0.42688641\n",
      "  1170: 0 [ 1170/ 1499], train_loss/perplexity = 6.64684105/770.3469849 secs/batch = 0.5379s, grad.norm=0.32655734\n",
      "  1175: 0 [ 1175/ 1499], train_loss/perplexity = 6.33173609/562.1316528 secs/batch = 0.5390s, grad.norm=0.62986588\n",
      "  1180: 0 [ 1180/ 1499], train_loss/perplexity = 6.48464680/655.0075684 secs/batch = 0.5404s, grad.norm=0.42801502\n",
      "  1185: 0 [ 1185/ 1499], train_loss/perplexity = 6.45718956/637.2675171 secs/batch = 0.5382s, grad.norm=0.36655727\n",
      "  1190: 0 [ 1190/ 1499], train_loss/perplexity = 6.66610050/785.3272705 secs/batch = 0.5406s, grad.norm=0.31236479\n",
      "  1195: 0 [ 1195/ 1499], train_loss/perplexity = 6.59815741/733.7419434 secs/batch = 0.5435s, grad.norm=0.31136206\n",
      "  1200: 0 [ 1200/ 1499], train_loss/perplexity = 6.60423899/738.2178345 secs/batch = 0.5395s, grad.norm=0.42569974\n",
      "  1205: 0 [ 1205/ 1499], train_loss/perplexity = 6.48854017/657.5627441 secs/batch = 0.5462s, grad.norm=0.39152771\n",
      "  1210: 0 [ 1210/ 1499], train_loss/perplexity = 6.60719013/740.3996582 secs/batch = 0.5410s, grad.norm=0.60007781\n",
      "  1215: 0 [ 1215/ 1499], train_loss/perplexity = 6.22581577/505.6353455 secs/batch = 0.5421s, grad.norm=0.34048963\n",
      "  1220: 0 [ 1220/ 1499], train_loss/perplexity = 6.56840801/712.2350464 secs/batch = 0.5417s, grad.norm=0.33681569\n",
      "  1225: 0 [ 1225/ 1499], train_loss/perplexity = 6.28391886/535.8845825 secs/batch = 0.5857s, grad.norm=0.55357170\n",
      "  1230: 0 [ 1230/ 1499], train_loss/perplexity = 6.59212637/729.3300171 secs/batch = 0.5415s, grad.norm=0.42106202\n",
      "  1235: 0 [ 1235/ 1499], train_loss/perplexity = 6.40765715/606.4711304 secs/batch = 0.5383s, grad.norm=0.48425704\n",
      "  1240: 0 [ 1240/ 1499], train_loss/perplexity = 6.34663916/570.5718994 secs/batch = 0.5423s, grad.norm=0.34719381\n",
      "  1245: 0 [ 1245/ 1499], train_loss/perplexity = 6.57979059/720.3884888 secs/batch = 0.5378s, grad.norm=0.49055436\n",
      "  1250: 0 [ 1250/ 1499], train_loss/perplexity = 6.87499285/967.7684326 secs/batch = 0.5384s, grad.norm=0.31444463\n",
      "  1255: 0 [ 1255/ 1499], train_loss/perplexity = 6.50397158/667.7885742 secs/batch = 0.5439s, grad.norm=0.43614560\n",
      "  1260: 0 [ 1260/ 1499], train_loss/perplexity = 6.45196486/633.9466553 secs/batch = 0.5444s, grad.norm=0.33134216\n",
      "  1265: 0 [ 1265/ 1499], train_loss/perplexity = 6.60533905/739.0303955 secs/batch = 0.5478s, grad.norm=0.39092708\n",
      "  1270: 0 [ 1270/ 1499], train_loss/perplexity = 6.59262657/729.6949463 secs/batch = 0.5381s, grad.norm=0.36290774\n",
      "  1275: 0 [ 1275/ 1499], train_loss/perplexity = 6.58671856/725.3966064 secs/batch = 0.5461s, grad.norm=0.29321238\n",
      "  1280: 0 [ 1280/ 1499], train_loss/perplexity = 6.36700201/582.3094482 secs/batch = 0.5390s, grad.norm=0.49194983\n",
      "  1285: 0 [ 1285/ 1499], train_loss/perplexity = 6.60551119/739.1575928 secs/batch = 0.5440s, grad.norm=0.38744891\n",
      "  1290: 0 [ 1290/ 1499], train_loss/perplexity = 6.59258032/729.6611938 secs/batch = 0.5408s, grad.norm=0.36363137\n",
      "  1295: 0 [ 1295/ 1499], train_loss/perplexity = 6.39227295/597.2124634 secs/batch = 0.5361s, grad.norm=0.31721663\n",
      "  1300: 0 [ 1300/ 1499], train_loss/perplexity = 6.58470869/723.9401245 secs/batch = 0.5421s, grad.norm=0.41156068\n",
      "  1305: 0 [ 1305/ 1499], train_loss/perplexity = 6.52914667/684.8135986 secs/batch = 0.5496s, grad.norm=0.36026448\n",
      "  1310: 0 [ 1310/ 1499], train_loss/perplexity = 6.58108568/721.3220215 secs/batch = 0.5427s, grad.norm=0.34871238\n",
      "  1315: 0 [ 1315/ 1499], train_loss/perplexity = 6.43844271/625.4320679 secs/batch = 0.5355s, grad.norm=0.46074113\n",
      "  1320: 0 [ 1320/ 1499], train_loss/perplexity = 6.22717047/506.3208008 secs/batch = 0.5405s, grad.norm=0.29584298\n",
      "  1325: 0 [ 1325/ 1499], train_loss/perplexity = 6.52095318/679.2255249 secs/batch = 0.5428s, grad.norm=0.34046340\n",
      "  1330: 0 [ 1330/ 1499], train_loss/perplexity = 6.66480827/784.3130493 secs/batch = 0.5425s, grad.norm=0.30832905\n",
      "  1335: 0 [ 1335/ 1499], train_loss/perplexity = 6.46028948/639.2460938 secs/batch = 0.5894s, grad.norm=0.30788878\n",
      "  1340: 0 [ 1340/ 1499], train_loss/perplexity = 6.22136879/503.3917847 secs/batch = 0.5375s, grad.norm=0.41857082\n",
      "  1345: 0 [ 1345/ 1499], train_loss/perplexity = 6.31332874/551.8789673 secs/batch = 0.5385s, grad.norm=0.55141312\n",
      "  1350: 0 [ 1350/ 1499], train_loss/perplexity = 6.37138748/584.8687744 secs/batch = 0.5349s, grad.norm=0.45115727\n",
      "  1355: 0 [ 1355/ 1499], train_loss/perplexity = 6.16299009/474.8457947 secs/batch = 0.5535s, grad.norm=0.51879746\n",
      "  1360: 0 [ 1360/ 1499], train_loss/perplexity = 6.19069242/488.1840210 secs/batch = 0.5410s, grad.norm=0.62076747\n",
      "  1365: 0 [ 1365/ 1499], train_loss/perplexity = 6.26826429/527.5609131 secs/batch = 0.5371s, grad.norm=0.42707396\n",
      "  1370: 0 [ 1370/ 1499], train_loss/perplexity = 6.15751171/472.2515259 secs/batch = 0.5336s, grad.norm=0.33093607\n",
      "  1375: 0 [ 1375/ 1499], train_loss/perplexity = 6.22398520/504.7106018 secs/batch = 0.5388s, grad.norm=0.44423828\n",
      "  1380: 0 [ 1380/ 1499], train_loss/perplexity = 6.37190628/585.1722412 secs/batch = 0.5440s, grad.norm=0.45024595\n",
      "  1385: 0 [ 1385/ 1499], train_loss/perplexity = 6.34768248/571.1674805 secs/batch = 0.5419s, grad.norm=0.31128785\n",
      "  1390: 0 [ 1390/ 1499], train_loss/perplexity = 6.34310722/568.5602417 secs/batch = 0.5379s, grad.norm=0.58506179\n",
      "  1395: 0 [ 1395/ 1499], train_loss/perplexity = 6.39023113/595.9943237 secs/batch = 0.5349s, grad.norm=0.33352447\n",
      "  1400: 0 [ 1400/ 1499], train_loss/perplexity = 6.53998804/692.2783203 secs/batch = 0.5392s, grad.norm=0.52294397\n",
      "  1405: 0 [ 1405/ 1499], train_loss/perplexity = 6.32278252/557.1210327 secs/batch = 0.5507s, grad.norm=0.40532157\n",
      "  1410: 0 [ 1410/ 1499], train_loss/perplexity = 6.45905542/638.4577026 secs/batch = 0.5422s, grad.norm=0.44780421\n",
      "  1415: 0 [ 1415/ 1499], train_loss/perplexity = 6.37016582/584.1546631 secs/batch = 0.5375s, grad.norm=0.37836337\n",
      "  1420: 0 [ 1420/ 1499], train_loss/perplexity = 6.44845390/631.7248535 secs/batch = 0.5396s, grad.norm=0.38613677\n",
      "  1425: 0 [ 1425/ 1499], train_loss/perplexity = 6.37768459/588.5633545 secs/batch = 0.5372s, grad.norm=0.49662238\n",
      "  1430: 0 [ 1430/ 1499], train_loss/perplexity = 6.51781178/677.0951538 secs/batch = 0.5411s, grad.norm=0.57620710\n",
      "  1435: 0 [ 1435/ 1499], train_loss/perplexity = 6.26004457/523.2422485 secs/batch = 0.5393s, grad.norm=0.31711039\n",
      "  1440: 0 [ 1440/ 1499], train_loss/perplexity = 5.99387503/400.9653625 secs/batch = 0.5399s, grad.norm=0.33909395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1445: 0 [ 1445/ 1499], train_loss/perplexity = 6.31693411/553.8722534 secs/batch = 0.5748s, grad.norm=0.35198790\n",
      "  1450: 0 [ 1450/ 1499], train_loss/perplexity = 6.31810999/554.5239258 secs/batch = 0.5382s, grad.norm=0.29216608\n",
      "  1455: 0 [ 1455/ 1499], train_loss/perplexity = 6.54954863/698.9286499 secs/batch = 0.5433s, grad.norm=0.37018383\n",
      "  1460: 0 [ 1460/ 1499], train_loss/perplexity = 6.58060026/720.9719849 secs/batch = 0.5425s, grad.norm=0.38409078\n",
      "  1465: 0 [ 1465/ 1499], train_loss/perplexity = 6.81263685/909.2652588 secs/batch = 0.5421s, grad.norm=0.37625739\n",
      "  1470: 0 [ 1470/ 1499], train_loss/perplexity = 6.41194677/609.0782471 secs/batch = 0.5395s, grad.norm=0.46633723\n",
      "  1475: 0 [ 1475/ 1499], train_loss/perplexity = 6.37993383/589.8886719 secs/batch = 0.5380s, grad.norm=0.33547100\n",
      "  1480: 0 [ 1480/ 1499], train_loss/perplexity = 6.52528381/682.1733398 secs/batch = 0.5355s, grad.norm=0.33582419\n",
      "  1485: 0 [ 1485/ 1499], train_loss/perplexity = 6.23986483/512.7891846 secs/batch = 0.5363s, grad.norm=0.33655781\n",
      "  1490: 0 [ 1490/ 1499], train_loss/perplexity = 6.31130648/550.7640381 secs/batch = 0.5429s, grad.norm=0.37318173\n",
      "  1495: 0 [ 1495/ 1499], train_loss/perplexity = 6.59807110/733.6786499 secs/batch = 0.5365s, grad.norm=0.37567669\n",
      "Epoch training time: 818.0175364017487\n",
      "Saved char model cv/epoch000_6.4272.model\n",
      "  1504: 1 [    5/ 1499], train_loss/perplexity = 6.41237879/609.3414307 secs/batch = 0.5404s, grad.norm=0.34353438\n",
      "  1509: 1 [   10/ 1499], train_loss/perplexity = 6.47241545/647.0447388 secs/batch = 0.5390s, grad.norm=0.42311916\n",
      "  1514: 1 [   15/ 1499], train_loss/perplexity = 6.38949871/595.5579834 secs/batch = 0.5476s, grad.norm=0.31941402\n",
      "  1519: 1 [   20/ 1499], train_loss/perplexity = 6.31641340/553.5839233 secs/batch = 0.5465s, grad.norm=0.32075945\n",
      "  1524: 1 [   25/ 1499], train_loss/perplexity = 6.53500366/688.8363037 secs/batch = 0.5357s, grad.norm=0.33869284\n",
      "  1529: 1 [   30/ 1499], train_loss/perplexity = 6.50931168/671.3641357 secs/batch = 0.5371s, grad.norm=0.43728051\n",
      "  1534: 1 [   35/ 1499], train_loss/perplexity = 6.38650513/593.7777710 secs/batch = 0.5421s, grad.norm=0.38907242\n",
      "  1539: 1 [   40/ 1499], train_loss/perplexity = 6.45487738/635.7957764 secs/batch = 0.5376s, grad.norm=0.43733782\n",
      "  1544: 1 [   45/ 1499], train_loss/perplexity = 6.51425552/674.6914673 secs/batch = 0.5466s, grad.norm=0.43923029\n",
      "  1549: 1 [   50/ 1499], train_loss/perplexity = 6.27851295/532.9954834 secs/batch = 0.5435s, grad.norm=0.38089606\n",
      "  1554: 1 [   55/ 1499], train_loss/perplexity = 6.30651760/548.1328125 secs/batch = 0.5513s, grad.norm=0.38162139\n",
      "  1559: 1 [   60/ 1499], train_loss/perplexity = 6.32541609/558.5902100 secs/batch = 0.5415s, grad.norm=0.36381239\n",
      "  1564: 1 [   65/ 1499], train_loss/perplexity = 6.28550482/536.7351685 secs/batch = 0.5429s, grad.norm=0.39015490\n",
      "  1569: 1 [   70/ 1499], train_loss/perplexity = 6.28369713/535.7658081 secs/batch = 0.5469s, grad.norm=0.36839753\n",
      "  1574: 1 [   75/ 1499], train_loss/perplexity = 6.11838293/454.1297302 secs/batch = 0.5440s, grad.norm=0.36615428\n",
      "  1579: 1 [   80/ 1499], train_loss/perplexity = 6.16768551/477.0806274 secs/batch = 0.5345s, grad.norm=0.46023542\n",
      "  1584: 1 [   85/ 1499], train_loss/perplexity = 6.29615116/542.4799805 secs/batch = 0.5350s, grad.norm=0.34657952\n",
      "  1589: 1 [   90/ 1499], train_loss/perplexity = 6.36337042/580.1985474 secs/batch = 0.5417s, grad.norm=0.32423595\n",
      "  1594: 1 [   95/ 1499], train_loss/perplexity = 6.24862862/517.3029175 secs/batch = 0.5466s, grad.norm=0.60316974\n",
      "  1599: 1 [  100/ 1499], train_loss/perplexity = 6.24522495/515.5451660 secs/batch = 0.5387s, grad.norm=0.41163567\n",
      "  1604: 1 [  105/ 1499], train_loss/perplexity = 6.18180227/483.8632202 secs/batch = 0.5460s, grad.norm=0.39819500\n",
      "  1609: 1 [  110/ 1499], train_loss/perplexity = 6.13358593/461.0866089 secs/batch = 0.5466s, grad.norm=0.45565766\n",
      "  1614: 1 [  115/ 1499], train_loss/perplexity = 6.31076574/550.4663086 secs/batch = 0.5474s, grad.norm=0.35894278\n",
      "  1619: 1 [  120/ 1499], train_loss/perplexity = 6.19540977/490.4924011 secs/batch = 0.5427s, grad.norm=0.34805313\n",
      "  1624: 1 [  125/ 1499], train_loss/perplexity = 6.49443340/661.4493408 secs/batch = 0.5385s, grad.norm=0.56194562\n",
      "  1629: 1 [  130/ 1499], train_loss/perplexity = 6.29124832/539.8267822 secs/batch = 0.5386s, grad.norm=0.45132688\n",
      "  1634: 1 [  135/ 1499], train_loss/perplexity = 6.32408571/557.8475342 secs/batch = 0.5374s, grad.norm=0.33711112\n",
      "  1639: 1 [  140/ 1499], train_loss/perplexity = 6.31250715/551.4257202 secs/batch = 0.5373s, grad.norm=0.82540536\n",
      "  1644: 1 [  145/ 1499], train_loss/perplexity = 6.18595552/485.8770142 secs/batch = 0.5382s, grad.norm=0.42925227\n",
      "  1649: 1 [  150/ 1499], train_loss/perplexity = 6.38310957/591.7649536 secs/batch = 0.5384s, grad.norm=0.45615390\n",
      "  1654: 1 [  155/ 1499], train_loss/perplexity = 6.43258905/621.7816772 secs/batch = 0.5907s, grad.norm=0.43953520\n",
      "  1659: 1 [  160/ 1499], train_loss/perplexity = 6.52975225/685.2284546 secs/batch = 0.5410s, grad.norm=0.30693910\n",
      "  1664: 1 [  165/ 1499], train_loss/perplexity = 6.19223547/488.9378967 secs/batch = 0.5733s, grad.norm=0.49805364\n",
      "  1669: 1 [  170/ 1499], train_loss/perplexity = 6.49552727/662.1732788 secs/batch = 0.5398s, grad.norm=0.74555516\n",
      "  1674: 1 [  175/ 1499], train_loss/perplexity = 6.28403234/535.9454346 secs/batch = 0.5426s, grad.norm=0.37172604\n",
      "  1679: 1 [  180/ 1499], train_loss/perplexity = 6.55641079/703.7412720 secs/batch = 0.5381s, grad.norm=0.38325155\n",
      "  1684: 1 [  185/ 1499], train_loss/perplexity = 6.25449276/520.3453979 secs/batch = 0.5479s, grad.norm=0.47497094\n",
      "  1689: 1 [  190/ 1499], train_loss/perplexity = 6.61774921/748.2590332 secs/batch = 0.5445s, grad.norm=0.43040901\n",
      "  1694: 1 [  195/ 1499], train_loss/perplexity = 6.46728039/643.7306519 secs/batch = 0.5362s, grad.norm=0.41948330\n",
      "  1699: 1 [  200/ 1499], train_loss/perplexity = 6.48968410/658.3153687 secs/batch = 0.5406s, grad.norm=0.40494585\n",
      "  1704: 1 [  205/ 1499], train_loss/perplexity = 6.42513561/617.1644897 secs/batch = 0.5379s, grad.norm=0.30016392\n",
      "  1709: 1 [  210/ 1499], train_loss/perplexity = 6.21666050/501.0272522 secs/batch = 0.5484s, grad.norm=0.32051069\n",
      "  1714: 1 [  215/ 1499], train_loss/perplexity = 6.33491898/563.9237061 secs/batch = 0.5419s, grad.norm=0.41849229\n",
      "  1719: 1 [  220/ 1499], train_loss/perplexity = 6.25422430/520.2056885 secs/batch = 0.5413s, grad.norm=0.39090386\n",
      "  1724: 1 [  225/ 1499], train_loss/perplexity = 6.39437485/598.4690552 secs/batch = 0.5417s, grad.norm=0.48567331\n",
      "  1729: 1 [  230/ 1499], train_loss/perplexity = 6.35397768/574.7744141 secs/batch = 0.5403s, grad.norm=0.34521252\n",
      "  1734: 1 [  235/ 1499], train_loss/perplexity = 6.41809273/612.8331299 secs/batch = 0.5373s, grad.norm=0.51466984\n",
      "  1739: 1 [  240/ 1499], train_loss/perplexity = 6.48255730/653.6403809 secs/batch = 0.5399s, grad.norm=0.35949147\n",
      "  1744: 1 [  245/ 1499], train_loss/perplexity = 6.24616194/516.0284424 secs/batch = 0.5374s, grad.norm=0.42233419\n",
      "  1749: 1 [  250/ 1499], train_loss/perplexity = 6.28907728/538.6560669 secs/batch = 0.5383s, grad.norm=0.32567221\n",
      "  1754: 1 [  255/ 1499], train_loss/perplexity = 6.14484453/466.3071594 secs/batch = 0.5386s, grad.norm=0.37880716\n",
      "  1759: 1 [  260/ 1499], train_loss/perplexity = 6.40088987/602.3808594 secs/batch = 0.5383s, grad.norm=0.37388965\n",
      "  1764: 1 [  265/ 1499], train_loss/perplexity = 6.36900139/583.4748535 secs/batch = 0.5494s, grad.norm=0.55846167\n",
      "  1769: 1 [  270/ 1499], train_loss/perplexity = 6.48585606/655.8001099 secs/batch = 0.5437s, grad.norm=0.31238875\n",
      "  1774: 1 [  275/ 1499], train_loss/perplexity = 6.10617447/448.6192322 secs/batch = 0.5430s, grad.norm=0.39919320\n",
      "  1779: 1 [  280/ 1499], train_loss/perplexity = 6.23663139/511.1337891 secs/batch = 0.5384s, grad.norm=0.35409871\n",
      "  1784: 1 [  285/ 1499], train_loss/perplexity = 6.64016294/765.2196655 secs/batch = 0.5412s, grad.norm=0.37575129\n",
      "  1789: 1 [  290/ 1499], train_loss/perplexity = 6.57549620/717.3014526 secs/batch = 0.5383s, grad.norm=0.44355667\n",
      "  1794: 1 [  295/ 1499], train_loss/perplexity = 6.27798700/532.7152100 secs/batch = 0.5349s, grad.norm=0.39700335\n",
      "  1799: 1 [  300/ 1499], train_loss/perplexity = 6.16371632/475.1907654 secs/batch = 0.5463s, grad.norm=0.35092208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1804: 1 [  305/ 1499], train_loss/perplexity = 6.37815762/588.8418579 secs/batch = 0.5367s, grad.norm=0.47976202\n",
      "  1809: 1 [  310/ 1499], train_loss/perplexity = 6.56866121/712.4154053 secs/batch = 0.5357s, grad.norm=0.43348306\n",
      "  1814: 1 [  315/ 1499], train_loss/perplexity = 6.42409563/616.5230103 secs/batch = 0.5370s, grad.norm=0.40078473\n",
      "  1819: 1 [  320/ 1499], train_loss/perplexity = 6.45274687/634.4426270 secs/batch = 0.5451s, grad.norm=0.35786355\n",
      "  1824: 1 [  325/ 1499], train_loss/perplexity = 6.31093979/550.5621338 secs/batch = 0.5966s, grad.norm=0.42508742\n",
      "  1829: 1 [  330/ 1499], train_loss/perplexity = 6.24469614/515.2726440 secs/batch = 0.5464s, grad.norm=0.53714508\n",
      "  1834: 1 [  335/ 1499], train_loss/perplexity = 6.43552542/623.6101685 secs/batch = 0.5372s, grad.norm=0.43948698\n",
      "  1839: 1 [  340/ 1499], train_loss/perplexity = 6.12510490/457.1926575 secs/batch = 0.5387s, grad.norm=0.35895768\n",
      "  1844: 1 [  345/ 1499], train_loss/perplexity = 6.11412573/452.2005310 secs/batch = 0.5404s, grad.norm=0.50958914\n",
      "  1849: 1 [  350/ 1499], train_loss/perplexity = 6.31561995/553.1448975 secs/batch = 0.5370s, grad.norm=0.50424719\n",
      "  1854: 1 [  355/ 1499], train_loss/perplexity = 6.10525942/448.2088928 secs/batch = 0.5423s, grad.norm=0.44678313\n",
      "  1859: 1 [  360/ 1499], train_loss/perplexity = 6.28682327/537.4432983 secs/batch = 0.5387s, grad.norm=0.42424592\n",
      "  1864: 1 [  365/ 1499], train_loss/perplexity = 6.22603846/505.7479553 secs/batch = 0.5374s, grad.norm=1.09072065\n",
      "  1869: 1 [  370/ 1499], train_loss/perplexity = 6.20383596/494.6428528 secs/batch = 0.5390s, grad.norm=0.33605602\n",
      "  1874: 1 [  375/ 1499], train_loss/perplexity = 6.15926361/473.0795898 secs/batch = 0.5525s, grad.norm=0.41974729\n",
      "  1879: 1 [  380/ 1499], train_loss/perplexity = 6.38117647/590.6221313 secs/batch = 0.5451s, grad.norm=0.47637731\n",
      "  1884: 1 [  385/ 1499], train_loss/perplexity = 6.39430571/598.4276733 secs/batch = 0.5394s, grad.norm=0.35853603\n",
      "  1889: 1 [  390/ 1499], train_loss/perplexity = 6.59736109/733.1578979 secs/batch = 0.5318s, grad.norm=0.36643121\n",
      "  1894: 1 [  395/ 1499], train_loss/perplexity = 6.57641935/717.9639282 secs/batch = 0.5346s, grad.norm=0.33338967\n",
      "  1899: 1 [  400/ 1499], train_loss/perplexity = 6.48152494/652.9659424 secs/batch = 0.5390s, grad.norm=0.48339805\n",
      "  1904: 1 [  405/ 1499], train_loss/perplexity = 6.35302591/574.2276611 secs/batch = 0.5463s, grad.norm=0.41897696\n",
      "  1909: 1 [  410/ 1499], train_loss/perplexity = 6.06119013/428.8855591 secs/batch = 0.5415s, grad.norm=0.48271427\n",
      "  1914: 1 [  415/ 1499], train_loss/perplexity = 6.47336626/647.6602783 secs/batch = 0.5446s, grad.norm=0.36020434\n",
      "  1919: 1 [  420/ 1499], train_loss/perplexity = 6.28358412/535.7052612 secs/batch = 0.5369s, grad.norm=0.36525899\n",
      "  1924: 1 [  425/ 1499], train_loss/perplexity = 6.53251219/687.1222534 secs/batch = 0.5400s, grad.norm=0.36233675\n",
      "  1929: 1 [  430/ 1499], train_loss/perplexity = 6.19171238/488.6821899 secs/batch = 0.5380s, grad.norm=0.36112922\n",
      "  1934: 1 [  435/ 1499], train_loss/perplexity = 6.27797794/532.7103882 secs/batch = 0.5464s, grad.norm=0.62255710\n",
      "  1939: 1 [  440/ 1499], train_loss/perplexity = 6.38097191/590.5013428 secs/batch = 0.5425s, grad.norm=0.34075409\n",
      "  1944: 1 [  445/ 1499], train_loss/perplexity = 6.34723234/570.9104614 secs/batch = 0.5429s, grad.norm=0.36457750\n",
      "  1949: 1 [  450/ 1499], train_loss/perplexity = 6.26669645/526.7344360 secs/batch = 0.5430s, grad.norm=0.44282371\n",
      "  1954: 1 [  455/ 1499], train_loss/perplexity = 6.40294838/603.6221313 secs/batch = 0.5428s, grad.norm=0.41063157\n",
      "  1959: 1 [  460/ 1499], train_loss/perplexity = 6.43519974/623.4071045 secs/batch = 0.5495s, grad.norm=0.59032148\n",
      "  1964: 1 [  465/ 1499], train_loss/perplexity = 6.33558559/564.2997437 secs/batch = 0.5365s, grad.norm=0.40649563\n",
      "  1969: 1 [  470/ 1499], train_loss/perplexity = 6.45328140/634.7818604 secs/batch = 0.5458s, grad.norm=0.43878603\n",
      "  1974: 1 [  475/ 1499], train_loss/perplexity = 6.31069183/550.4255981 secs/batch = 0.5444s, grad.norm=0.46039465\n",
      "  1979: 1 [  480/ 1499], train_loss/perplexity = 6.33980417/566.6853027 secs/batch = 0.5415s, grad.norm=0.38458648\n",
      "  1984: 1 [  485/ 1499], train_loss/perplexity = 6.39305973/597.6825562 secs/batch = 0.5419s, grad.norm=0.42168778\n",
      "  1989: 1 [  490/ 1499], train_loss/perplexity = 6.22753477/506.5052795 secs/batch = 0.5382s, grad.norm=0.40309170\n",
      "  1994: 1 [  495/ 1499], train_loss/perplexity = 6.28988218/539.0897827 secs/batch = 0.5350s, grad.norm=0.44036070\n",
      "  1999: 1 [  500/ 1499], train_loss/perplexity = 6.39352655/597.9616089 secs/batch = 0.5379s, grad.norm=0.43770427\n",
      "  2004: 1 [  505/ 1499], train_loss/perplexity = 6.10725737/449.1052856 secs/batch = 0.5356s, grad.norm=0.42275888\n",
      "  2009: 1 [  510/ 1499], train_loss/perplexity = 6.48372412/654.4035034 secs/batch = 0.5370s, grad.norm=0.37940782\n",
      "  2014: 1 [  515/ 1499], train_loss/perplexity = 6.25222921/519.1688843 secs/batch = 0.5430s, grad.norm=0.41121438\n",
      "  2019: 1 [  520/ 1499], train_loss/perplexity = 6.54807138/697.8969116 secs/batch = 0.5449s, grad.norm=0.51346993\n",
      "  2024: 1 [  525/ 1499], train_loss/perplexity = 6.49158716/659.5693970 secs/batch = 0.5389s, grad.norm=0.39026901\n",
      "  2029: 1 [  530/ 1499], train_loss/perplexity = 6.36932802/583.6654663 secs/batch = 0.5422s, grad.norm=0.43672255\n",
      "  2034: 1 [  535/ 1499], train_loss/perplexity = 6.29295874/540.7509155 secs/batch = 0.5394s, grad.norm=0.40596136\n",
      "  2039: 1 [  540/ 1499], train_loss/perplexity = 6.11246443/451.4499207 secs/batch = 0.5367s, grad.norm=0.34986347\n",
      "  2044: 1 [  545/ 1499], train_loss/perplexity = 6.01938772/411.3266602 secs/batch = 0.5389s, grad.norm=0.72701967\n",
      "  2049: 1 [  550/ 1499], train_loss/perplexity = 6.39786005/600.5584717 secs/batch = 0.5449s, grad.norm=0.43297386\n",
      "  2054: 1 [  555/ 1499], train_loss/perplexity = 6.18072081/483.3402405 secs/batch = 0.5368s, grad.norm=0.39013284\n",
      "  2059: 1 [  560/ 1499], train_loss/perplexity = 6.53634739/689.7625122 secs/batch = 0.5342s, grad.norm=0.42131996\n",
      "  2064: 1 [  565/ 1499], train_loss/perplexity = 6.39367819/598.0523071 secs/batch = 0.5362s, grad.norm=0.46101013\n",
      "  2069: 1 [  570/ 1499], train_loss/perplexity = 6.28502703/536.4787598 secs/batch = 0.5498s, grad.norm=0.36509490\n",
      "  2074: 1 [  575/ 1499], train_loss/perplexity = 6.45761156/637.5364990 secs/batch = 0.5440s, grad.norm=0.91378045\n",
      "  2079: 1 [  580/ 1499], train_loss/perplexity = 6.11862278/454.2386780 secs/batch = 0.5405s, grad.norm=0.42634895\n",
      "  2084: 1 [  585/ 1499], train_loss/perplexity = 6.13016176/459.5104980 secs/batch = 0.5422s, grad.norm=0.38096055\n",
      "  2089: 1 [  590/ 1499], train_loss/perplexity = 6.39392996/598.2028809 secs/batch = 0.5421s, grad.norm=0.67683870\n",
      "  2094: 1 [  595/ 1499], train_loss/perplexity = 6.29371119/541.1579590 secs/batch = 0.5920s, grad.norm=0.36537528\n",
      "  2099: 1 [  600/ 1499], train_loss/perplexity = 6.05394983/425.7915039 secs/batch = 0.5325s, grad.norm=0.43834972\n",
      "  2104: 1 [  605/ 1499], train_loss/perplexity = 6.05496597/426.2243958 secs/batch = 0.5376s, grad.norm=0.35471302\n",
      "  2109: 1 [  610/ 1499], train_loss/perplexity = 6.37961817/589.7025146 secs/batch = 0.5615s, grad.norm=0.49130705\n",
      "  2114: 1 [  615/ 1499], train_loss/perplexity = 6.41325665/609.8765869 secs/batch = 0.5376s, grad.norm=0.57691550\n",
      "  2119: 1 [  620/ 1499], train_loss/perplexity = 6.13508320/461.7775269 secs/batch = 0.5919s, grad.norm=0.36740181\n",
      "  2124: 1 [  625/ 1499], train_loss/perplexity = 6.23488474/510.2417908 secs/batch = 0.5481s, grad.norm=0.46724570\n",
      "  2129: 1 [  630/ 1499], train_loss/perplexity = 6.34881163/571.8128052 secs/batch = 0.5462s, grad.norm=0.39036858\n",
      "  2134: 1 [  635/ 1499], train_loss/perplexity = 6.22497559/505.2106934 secs/batch = 0.5368s, grad.norm=0.74104726\n",
      "  2139: 1 [  640/ 1499], train_loss/perplexity = 6.34814739/571.4331055 secs/batch = 0.5462s, grad.norm=0.37380949\n",
      "  2144: 1 [  645/ 1499], train_loss/perplexity = 6.08944702/441.1773682 secs/batch = 0.5421s, grad.norm=0.35026628\n",
      "  2149: 1 [  650/ 1499], train_loss/perplexity = 6.07150698/433.3332214 secs/batch = 0.5445s, grad.norm=0.51243746\n",
      "  2154: 1 [  655/ 1499], train_loss/perplexity = 6.33205509/562.3110352 secs/batch = 0.5404s, grad.norm=0.40358728\n",
      "  2159: 1 [  660/ 1499], train_loss/perplexity = 6.49913931/664.5693970 secs/batch = 0.5478s, grad.norm=0.46860430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2164: 1 [  665/ 1499], train_loss/perplexity = 6.49079609/659.0477905 secs/batch = 0.5390s, grad.norm=0.40604243\n",
      "  2169: 1 [  670/ 1499], train_loss/perplexity = 6.37791872/588.7011719 secs/batch = 0.5393s, grad.norm=0.44745278\n",
      "  2174: 1 [  675/ 1499], train_loss/perplexity = 6.32105255/556.1580811 secs/batch = 0.5428s, grad.norm=0.34707427\n",
      "  2179: 1 [  680/ 1499], train_loss/perplexity = 6.34775639/571.2097168 secs/batch = 0.5458s, grad.norm=0.38400230\n",
      "  2184: 1 [  685/ 1499], train_loss/perplexity = 6.42425442/616.6209106 secs/batch = 0.5393s, grad.norm=0.52731264\n",
      "  2189: 1 [  690/ 1499], train_loss/perplexity = 6.30655146/548.1513672 secs/batch = 0.5392s, grad.norm=0.40353149\n",
      "  2194: 1 [  695/ 1499], train_loss/perplexity = 6.18162537/483.7776184 secs/batch = 0.5379s, grad.norm=0.42706722\n",
      "  2199: 1 [  700/ 1499], train_loss/perplexity = 6.43365002/622.4417114 secs/batch = 0.5449s, grad.norm=0.37177816\n",
      "  2204: 1 [  705/ 1499], train_loss/perplexity = 6.07863140/436.4314880 secs/batch = 0.5452s, grad.norm=0.39054787\n",
      "  2209: 1 [  710/ 1499], train_loss/perplexity = 6.27431154/530.7608643 secs/batch = 0.5415s, grad.norm=0.44969496\n",
      "  2214: 1 [  715/ 1499], train_loss/perplexity = 6.28527308/536.6107788 secs/batch = 0.5488s, grad.norm=0.38626161\n",
      "  2219: 1 [  720/ 1499], train_loss/perplexity = 6.32839012/560.2539062 secs/batch = 0.5404s, grad.norm=0.48531809\n",
      "  2224: 1 [  725/ 1499], train_loss/perplexity = 6.18302202/484.4537659 secs/batch = 0.5438s, grad.norm=0.66649657\n",
      "  2229: 1 [  730/ 1499], train_loss/perplexity = 6.30014944/544.6533203 secs/batch = 0.5351s, grad.norm=0.48209414\n",
      "  2234: 1 [  735/ 1499], train_loss/perplexity = 6.16212177/474.4336548 secs/batch = 0.5346s, grad.norm=0.37134394\n",
      "  2239: 1 [  740/ 1499], train_loss/perplexity = 6.20903492/497.2211609 secs/batch = 0.5387s, grad.norm=0.40398186\n",
      "  2244: 1 [  745/ 1499], train_loss/perplexity = 6.06181622/429.1541748 secs/batch = 0.5345s, grad.norm=0.49420217\n",
      "  2249: 1 [  750/ 1499], train_loss/perplexity = 6.32579374/558.8012085 secs/batch = 0.5379s, grad.norm=0.35332900\n",
      "  2254: 1 [  755/ 1499], train_loss/perplexity = 6.06314993/429.7269287 secs/batch = 0.5416s, grad.norm=0.39521906\n",
      "  2259: 1 [  760/ 1499], train_loss/perplexity = 5.81737280/336.0879211 secs/batch = 0.5384s, grad.norm=0.42869404\n",
      "  2264: 1 [  765/ 1499], train_loss/perplexity = 6.11026049/450.4560242 secs/batch = 0.5360s, grad.norm=0.49302483\n",
      "  2269: 1 [  770/ 1499], train_loss/perplexity = 6.22381592/504.6251831 secs/batch = 0.5494s, grad.norm=0.40310776\n",
      "  2274: 1 [  775/ 1499], train_loss/perplexity = 6.50408363/667.8634033 secs/batch = 0.5399s, grad.norm=0.40589100\n",
      "  2279: 1 [  780/ 1499], train_loss/perplexity = 6.16046238/473.6470337 secs/batch = 0.5367s, grad.norm=0.37631297\n",
      "  2284: 1 [  785/ 1499], train_loss/perplexity = 6.16824532/477.3477783 secs/batch = 0.5398s, grad.norm=0.37865597\n",
      "  2289: 1 [  790/ 1499], train_loss/perplexity = 5.84350157/344.9852295 secs/batch = 0.5396s, grad.norm=0.45145679\n",
      "  2294: 1 [  795/ 1499], train_loss/perplexity = 6.36695290/582.2808228 secs/batch = 0.5410s, grad.norm=0.61313093\n",
      "  2299: 1 [  800/ 1499], train_loss/perplexity = 6.11573935/452.9307861 secs/batch = 0.5444s, grad.norm=0.43572894\n",
      "  2304: 1 [  805/ 1499], train_loss/perplexity = 6.20394659/494.6975708 secs/batch = 0.5438s, grad.norm=0.32676879\n",
      "  2309: 1 [  810/ 1499], train_loss/perplexity = 6.18301439/484.4500732 secs/batch = 0.5392s, grad.norm=0.44073677\n",
      "  2314: 1 [  815/ 1499], train_loss/perplexity = 6.23164034/508.5890503 secs/batch = 0.5396s, grad.norm=0.36784181\n",
      "  2319: 1 [  820/ 1499], train_loss/perplexity = 5.92623615/374.7413940 secs/batch = 0.5457s, grad.norm=0.54166102\n",
      "  2324: 1 [  825/ 1499], train_loss/perplexity = 6.09773016/444.8468933 secs/batch = 0.5400s, grad.norm=0.37008491\n",
      "  2329: 1 [  830/ 1499], train_loss/perplexity = 6.25375366/519.9609375 secs/batch = 0.5670s, grad.norm=0.37371027\n",
      "  2334: 1 [  835/ 1499], train_loss/perplexity = 6.21028519/497.8432007 secs/batch = 0.5435s, grad.norm=0.49810684\n",
      "  2339: 1 [  840/ 1499], train_loss/perplexity = 6.11479473/452.5031433 secs/batch = 0.5383s, grad.norm=0.42811173\n",
      "  2344: 1 [  845/ 1499], train_loss/perplexity = 6.13181877/460.2725220 secs/batch = 0.5373s, grad.norm=0.56625110\n",
      "  2349: 1 [  850/ 1499], train_loss/perplexity = 6.21708393/501.2394409 secs/batch = 0.5442s, grad.norm=0.37850738\n",
      "  2354: 1 [  855/ 1499], train_loss/perplexity = 6.26522827/525.9616089 secs/batch = 0.5361s, grad.norm=0.42780602\n",
      "  2359: 1 [  860/ 1499], train_loss/perplexity = 5.99928093/403.1387939 secs/batch = 0.5385s, grad.norm=0.68698305\n",
      "  2364: 1 [  865/ 1499], train_loss/perplexity = 6.16805220/477.2556152 secs/batch = 0.5402s, grad.norm=0.46477434\n",
      "  2369: 1 [  870/ 1499], train_loss/perplexity = 6.07639360/435.4559326 secs/batch = 0.5417s, grad.norm=0.37976074\n",
      "  2374: 1 [  875/ 1499], train_loss/perplexity = 6.19881964/492.1677551 secs/batch = 0.5422s, grad.norm=0.44327927\n",
      "  2379: 1 [  880/ 1499], train_loss/perplexity = 6.06337881/429.8252869 secs/batch = 0.5420s, grad.norm=0.32692832\n",
      "  2384: 1 [  885/ 1499], train_loss/perplexity = 5.96637392/390.0886230 secs/batch = 0.5360s, grad.norm=0.40459540\n",
      "  2389: 1 [  890/ 1499], train_loss/perplexity = 6.17403078/480.1174622 secs/batch = 0.5782s, grad.norm=0.35333541\n",
      "  2394: 1 [  895/ 1499], train_loss/perplexity = 6.19928980/492.3992004 secs/batch = 0.5421s, grad.norm=0.39208835\n",
      "  2399: 1 [  900/ 1499], train_loss/perplexity = 6.16269779/474.7070007 secs/batch = 0.5403s, grad.norm=0.44879752\n",
      "  2404: 1 [  905/ 1499], train_loss/perplexity = 6.22094870/503.1803589 secs/batch = 0.5439s, grad.norm=0.56227273\n",
      "  2409: 1 [  910/ 1499], train_loss/perplexity = 6.36138344/579.0468750 secs/batch = 0.5363s, grad.norm=0.40965748\n",
      "  2414: 1 [  915/ 1499], train_loss/perplexity = 6.08311081/438.3908386 secs/batch = 0.5423s, grad.norm=0.40123242\n",
      "  2419: 1 [  920/ 1499], train_loss/perplexity = 5.85989809/350.6884155 secs/batch = 0.5409s, grad.norm=0.36198592\n",
      "  2424: 1 [  925/ 1499], train_loss/perplexity = 6.23217535/508.8612366 secs/batch = 0.5395s, grad.norm=0.49849918\n",
      "  2429: 1 [  930/ 1499], train_loss/perplexity = 6.17409849/480.1499634 secs/batch = 0.5445s, grad.norm=0.39702690\n",
      "  2434: 1 [  935/ 1499], train_loss/perplexity = 6.24240971/514.0958252 secs/batch = 0.5400s, grad.norm=0.40118763\n",
      "  2439: 1 [  940/ 1499], train_loss/perplexity = 5.89688492/363.9021301 secs/batch = 0.5373s, grad.norm=0.65659446\n",
      "  2444: 1 [  945/ 1499], train_loss/perplexity = 6.21931171/502.3573608 secs/batch = 0.5410s, grad.norm=0.40377674\n",
      "  2449: 1 [  950/ 1499], train_loss/perplexity = 6.15342093/470.3235779 secs/batch = 0.5437s, grad.norm=0.38103265\n",
      "  2454: 1 [  955/ 1499], train_loss/perplexity = 6.34608173/570.2539062 secs/batch = 0.5411s, grad.norm=0.37359843\n",
      "  2459: 1 [  960/ 1499], train_loss/perplexity = 6.12533426/457.2975464 secs/batch = 0.5413s, grad.norm=0.41670412\n",
      "  2464: 1 [  965/ 1499], train_loss/perplexity = 6.10348892/447.4160461 secs/batch = 0.5327s, grad.norm=0.45767942\n",
      "  2469: 1 [  970/ 1499], train_loss/perplexity = 6.41491890/610.8912354 secs/batch = 0.5411s, grad.norm=0.55273926\n",
      "  2474: 1 [  975/ 1499], train_loss/perplexity = 6.24352217/514.6680908 secs/batch = 0.5448s, grad.norm=0.43760553\n",
      "  2479: 1 [  980/ 1499], train_loss/perplexity = 6.27765226/532.5369263 secs/batch = 0.5427s, grad.norm=0.57607824\n",
      "  2484: 1 [  985/ 1499], train_loss/perplexity = 6.00550079/405.6540833 secs/batch = 0.5398s, grad.norm=0.41544682\n",
      "  2489: 1 [  990/ 1499], train_loss/perplexity = 6.13475227/461.6247253 secs/batch = 0.5389s, grad.norm=0.38731655\n",
      "  2494: 1 [  995/ 1499], train_loss/perplexity = 6.32811689/560.1008911 secs/batch = 0.5421s, grad.norm=0.48794636\n",
      "  2499: 1 [ 1000/ 1499], train_loss/perplexity = 6.00216389/404.3027039 secs/batch = 0.5372s, grad.norm=0.36731407\n",
      "  2504: 1 [ 1005/ 1499], train_loss/perplexity = 6.33321905/562.9658813 secs/batch = 0.5365s, grad.norm=0.47606307\n",
      "  2509: 1 [ 1010/ 1499], train_loss/perplexity = 6.23033714/507.9266968 secs/batch = 0.5388s, grad.norm=0.48298943\n",
      "  2514: 1 [ 1015/ 1499], train_loss/perplexity = 6.02616739/414.1248169 secs/batch = 0.5374s, grad.norm=0.41294244\n",
      "  2519: 1 [ 1020/ 1499], train_loss/perplexity = 6.13318348/460.9010925 secs/batch = 0.5441s, grad.norm=0.54546815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2524: 1 [ 1025/ 1499], train_loss/perplexity = 6.13661146/462.4837646 secs/batch = 0.5435s, grad.norm=0.51283818\n",
      "  2529: 1 [ 1030/ 1499], train_loss/perplexity = 6.14385366/465.8453369 secs/batch = 0.5430s, grad.norm=0.32038638\n",
      "  2534: 1 [ 1035/ 1499], train_loss/perplexity = 6.56315756/708.5053101 secs/batch = 0.5384s, grad.norm=0.63205379\n",
      "  2539: 1 [ 1040/ 1499], train_loss/perplexity = 5.99668694/402.0944214 secs/batch = 0.5445s, grad.norm=0.38887426\n",
      "  2544: 1 [ 1045/ 1499], train_loss/perplexity = 6.17535591/480.7540894 secs/batch = 0.5361s, grad.norm=0.42275581\n",
      "  2549: 1 [ 1050/ 1499], train_loss/perplexity = 5.85937023/350.5033264 secs/batch = 0.5822s, grad.norm=0.59220517\n",
      "  2554: 1 [ 1055/ 1499], train_loss/perplexity = 6.16229153/474.5141907 secs/batch = 0.5403s, grad.norm=0.49304575\n",
      "  2559: 1 [ 1060/ 1499], train_loss/perplexity = 6.36348391/580.2644043 secs/batch = 0.5377s, grad.norm=0.55010331\n",
      "  2564: 1 [ 1065/ 1499], train_loss/perplexity = 5.94577312/382.1346741 secs/batch = 0.5444s, grad.norm=0.38997859\n",
      "  2569: 1 [ 1070/ 1499], train_loss/perplexity = 6.35491753/575.3148804 secs/batch = 0.5367s, grad.norm=0.50880897\n",
      "  2574: 1 [ 1075/ 1499], train_loss/perplexity = 6.21004343/497.7228699 secs/batch = 0.5431s, grad.norm=0.36827722\n",
      "  2579: 1 [ 1080/ 1499], train_loss/perplexity = 6.31937027/555.2232666 secs/batch = 0.5465s, grad.norm=0.39601204\n",
      "  2584: 1 [ 1085/ 1499], train_loss/perplexity = 6.41765499/612.5649414 secs/batch = 0.5414s, grad.norm=0.40905374\n",
      "  2589: 1 [ 1090/ 1499], train_loss/perplexity = 6.04341412/421.3290405 secs/batch = 0.5354s, grad.norm=0.36460307\n",
      "  2594: 1 [ 1095/ 1499], train_loss/perplexity = 6.21537447/500.3833313 secs/batch = 0.5356s, grad.norm=0.38501018\n",
      "  2599: 1 [ 1100/ 1499], train_loss/perplexity = 6.24371767/514.7686768 secs/batch = 0.5407s, grad.norm=0.49441308\n",
      "  2604: 1 [ 1105/ 1499], train_loss/perplexity = 6.07742310/435.9044495 secs/batch = 0.5431s, grad.norm=0.46042976\n",
      "  2609: 1 [ 1110/ 1499], train_loss/perplexity = 6.19185257/488.7507019 secs/batch = 0.5401s, grad.norm=0.37835017\n",
      "  2614: 1 [ 1115/ 1499], train_loss/perplexity = 5.83723450/342.8299255 secs/batch = 0.5403s, grad.norm=0.43845496\n",
      "  2619: 1 [ 1120/ 1499], train_loss/perplexity = 6.11959219/454.6792297 secs/batch = 0.5386s, grad.norm=0.55718422\n",
      "  2624: 1 [ 1125/ 1499], train_loss/perplexity = 6.03371525/417.2623901 secs/batch = 0.5472s, grad.norm=0.49884689\n",
      "  2629: 1 [ 1130/ 1499], train_loss/perplexity = 5.93171167/376.7989197 secs/batch = 0.5403s, grad.norm=0.46895474\n",
      "  2634: 1 [ 1135/ 1499], train_loss/perplexity = 6.32758570/559.8034058 secs/batch = 0.5428s, grad.norm=0.43577445\n",
      "  2639: 1 [ 1140/ 1499], train_loss/perplexity = 6.21725845/501.3269348 secs/batch = 0.5387s, grad.norm=0.38251954\n",
      "  2644: 1 [ 1145/ 1499], train_loss/perplexity = 6.28736067/537.7321777 secs/batch = 0.5324s, grad.norm=0.42459053\n",
      "  2649: 1 [ 1150/ 1499], train_loss/perplexity = 6.24787760/516.9145508 secs/batch = 0.5446s, grad.norm=0.41230595\n",
      "  2654: 1 [ 1155/ 1499], train_loss/perplexity = 5.97511292/393.5125427 secs/batch = 0.5403s, grad.norm=0.53352267\n",
      "  2659: 1 [ 1160/ 1499], train_loss/perplexity = 6.18225956/484.0845337 secs/batch = 0.5956s, grad.norm=0.39547464\n",
      "  2664: 1 [ 1165/ 1499], train_loss/perplexity = 6.01671267/410.2278137 secs/batch = 0.5426s, grad.norm=0.43800321\n",
      "  2669: 1 [ 1170/ 1499], train_loss/perplexity = 6.29316568/540.8627930 secs/batch = 0.5424s, grad.norm=0.37484699\n",
      "  2674: 1 [ 1175/ 1499], train_loss/perplexity = 5.80542660/332.0968323 secs/batch = 0.5463s, grad.norm=0.41228184\n",
      "  2679: 1 [ 1180/ 1499], train_loss/perplexity = 6.07491732/434.8135376 secs/batch = 0.5452s, grad.norm=0.45774993\n",
      "  2684: 1 [ 1185/ 1499], train_loss/perplexity = 5.99729300/402.3381958 secs/batch = 0.5369s, grad.norm=0.52043730\n",
      "  2689: 1 [ 1190/ 1499], train_loss/perplexity = 6.21167803/498.5371094 secs/batch = 0.5390s, grad.norm=0.35142624\n",
      "  2694: 1 [ 1195/ 1499], train_loss/perplexity = 6.26754236/527.1801758 secs/batch = 0.5351s, grad.norm=0.42066705\n",
      "  2699: 1 [ 1200/ 1499], train_loss/perplexity = 6.14999580/468.7154236 secs/batch = 0.5441s, grad.norm=0.53004831\n",
      "  2704: 1 [ 1205/ 1499], train_loss/perplexity = 6.03062057/415.9730835 secs/batch = 0.5399s, grad.norm=0.53145278\n",
      "  2709: 1 [ 1210/ 1499], train_loss/perplexity = 6.12490940/457.1033020 secs/batch = 0.5441s, grad.norm=0.60254323\n",
      "  2714: 1 [ 1215/ 1499], train_loss/perplexity = 5.76253986/318.1553650 secs/batch = 0.5377s, grad.norm=0.58823568\n",
      "  2719: 1 [ 1220/ 1499], train_loss/perplexity = 6.17790890/481.9830322 secs/batch = 0.5415s, grad.norm=0.46575481\n",
      "  2724: 1 [ 1225/ 1499], train_loss/perplexity = 5.68804550/295.3158569 secs/batch = 0.5450s, grad.norm=0.58443999\n",
      "  2729: 1 [ 1230/ 1499], train_loss/perplexity = 6.17589855/481.0150452 secs/batch = 0.5452s, grad.norm=0.42274302\n",
      "  2734: 1 [ 1235/ 1499], train_loss/perplexity = 5.97631598/393.9862366 secs/batch = 0.5441s, grad.norm=0.57908171\n",
      "  2739: 1 [ 1240/ 1499], train_loss/perplexity = 5.92133570/372.9094849 secs/batch = 0.5398s, grad.norm=0.43001893\n",
      "  2744: 1 [ 1245/ 1499], train_loss/perplexity = 6.18779182/486.7700500 secs/batch = 0.5394s, grad.norm=0.44242528\n",
      "  2749: 1 [ 1250/ 1499], train_loss/perplexity = 6.46872711/644.6625977 secs/batch = 0.5400s, grad.norm=0.38731915\n",
      "  2754: 1 [ 1255/ 1499], train_loss/perplexity = 6.04278469/421.0639343 secs/batch = 0.5438s, grad.norm=0.42387566\n",
      "  2759: 1 [ 1260/ 1499], train_loss/perplexity = 6.07568216/435.1462402 secs/batch = 0.5409s, grad.norm=0.39904702\n",
      "  2764: 1 [ 1265/ 1499], train_loss/perplexity = 6.22129345/503.3538818 secs/batch = 0.5415s, grad.norm=0.54290998\n",
      "  2769: 1 [ 1270/ 1499], train_loss/perplexity = 6.24038219/513.0545654 secs/batch = 0.5447s, grad.norm=0.43270648\n",
      "  2774: 1 [ 1275/ 1499], train_loss/perplexity = 6.20158005/493.5282288 secs/batch = 0.5436s, grad.norm=0.43468091\n",
      "  2779: 1 [ 1280/ 1499], train_loss/perplexity = 5.88626194/360.0568542 secs/batch = 0.5439s, grad.norm=0.56259638\n",
      "  2784: 1 [ 1285/ 1499], train_loss/perplexity = 6.19639254/490.9746704 secs/batch = 0.5353s, grad.norm=0.47372892\n",
      "  2789: 1 [ 1290/ 1499], train_loss/perplexity = 6.17670059/481.4010010 secs/batch = 0.5379s, grad.norm=0.39568275\n",
      "  2794: 1 [ 1295/ 1499], train_loss/perplexity = 6.03064966/415.9851990 secs/batch = 0.5422s, grad.norm=0.45040253\n",
      "  2799: 1 [ 1300/ 1499], train_loss/perplexity = 6.26723433/527.0178223 secs/batch = 0.5372s, grad.norm=0.54350460\n",
      "  2804: 1 [ 1305/ 1499], train_loss/perplexity = 6.14618778/466.9339294 secs/batch = 0.5411s, grad.norm=0.58320898\n",
      "  2809: 1 [ 1310/ 1499], train_loss/perplexity = 6.18563652/485.7220459 secs/batch = 0.5435s, grad.norm=0.35527754\n",
      "  2814: 1 [ 1315/ 1499], train_loss/perplexity = 6.04078913/420.2245178 secs/batch = 0.5435s, grad.norm=0.41509095\n",
      "  2819: 1 [ 1320/ 1499], train_loss/perplexity = 5.77636623/322.5848694 secs/batch = 0.5475s, grad.norm=0.36514372\n",
      "  2824: 1 [ 1325/ 1499], train_loss/perplexity = 6.15281487/470.0386353 secs/batch = 0.5407s, grad.norm=0.37361526\n",
      "  2829: 1 [ 1330/ 1499], train_loss/perplexity = 6.27998066/533.7783203 secs/batch = 0.5420s, grad.norm=0.37275329\n",
      "  2834: 1 [ 1335/ 1499], train_loss/perplexity = 6.06063175/428.6461487 secs/batch = 0.5404s, grad.norm=0.40044454\n",
      "  2839: 1 [ 1340/ 1499], train_loss/perplexity = 5.64508963/282.8988953 secs/batch = 0.5422s, grad.norm=0.51083422\n",
      "  2844: 1 [ 1345/ 1499], train_loss/perplexity = 5.82395315/338.3067932 secs/batch = 0.5381s, grad.norm=0.50328350\n",
      "  2849: 1 [ 1350/ 1499], train_loss/perplexity = 5.90772200/367.8671875 secs/batch = 0.5377s, grad.norm=0.52758914\n",
      "  2854: 1 [ 1355/ 1499], train_loss/perplexity = 5.69596672/297.6643982 secs/batch = 0.5450s, grad.norm=0.51630628\n",
      "  2859: 1 [ 1360/ 1499], train_loss/perplexity = 5.70935726/301.6770935 secs/batch = 0.5362s, grad.norm=0.57278699\n",
      "  2864: 1 [ 1365/ 1499], train_loss/perplexity = 5.73740768/310.2590637 secs/batch = 0.5406s, grad.norm=0.36599162\n",
      "  2869: 1 [ 1370/ 1499], train_loss/perplexity = 5.65934134/286.9595642 secs/batch = 0.5331s, grad.norm=0.42662016\n",
      "  2874: 1 [ 1375/ 1499], train_loss/perplexity = 5.66909122/289.7710876 secs/batch = 0.5444s, grad.norm=0.46369776\n",
      "  2879: 1 [ 1380/ 1499], train_loss/perplexity = 6.05239248/425.1289368 secs/batch = 0.5418s, grad.norm=0.50955784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2884: 1 [ 1385/ 1499], train_loss/perplexity = 5.96261358/388.6245117 secs/batch = 0.5397s, grad.norm=0.51529950\n",
      "  2889: 1 [ 1390/ 1499], train_loss/perplexity = 5.95312214/384.9533386 secs/batch = 0.5415s, grad.norm=0.48251107\n",
      "  2894: 1 [ 1395/ 1499], train_loss/perplexity = 6.02153826/412.2121887 secs/batch = 0.5459s, grad.norm=0.37718141\n",
      "  2899: 1 [ 1400/ 1499], train_loss/perplexity = 6.05875349/427.8417969 secs/batch = 0.5370s, grad.norm=0.44711286\n",
      "  2904: 1 [ 1405/ 1499], train_loss/perplexity = 5.96048021/387.7962952 secs/batch = 0.5416s, grad.norm=0.48467934\n",
      "  2909: 1 [ 1410/ 1499], train_loss/perplexity = 6.11054468/450.5840759 secs/batch = 0.5383s, grad.norm=0.39748555\n",
      "  2914: 1 [ 1415/ 1499], train_loss/perplexity = 6.06038570/428.5406799 secs/batch = 0.5386s, grad.norm=0.57142246\n",
      "  2919: 1 [ 1420/ 1499], train_loss/perplexity = 6.12184954/455.7067566 secs/batch = 0.5406s, grad.norm=0.41772982\n",
      "  2924: 1 [ 1425/ 1499], train_loss/perplexity = 5.98669147/398.0953064 secs/batch = 0.5470s, grad.norm=0.55109268\n",
      "  2929: 1 [ 1430/ 1499], train_loss/perplexity = 6.14450312/466.1479797 secs/batch = 0.5801s, grad.norm=0.41273829\n",
      "  2934: 1 [ 1435/ 1499], train_loss/perplexity = 5.89332104/362.6075134 secs/batch = 0.5428s, grad.norm=0.41694385\n",
      "  2939: 1 [ 1440/ 1499], train_loss/perplexity = 5.57530737/263.8306274 secs/batch = 0.5442s, grad.norm=0.44814026\n",
      "  2944: 1 [ 1445/ 1499], train_loss/perplexity = 6.02066946/411.8542175 secs/batch = 0.5421s, grad.norm=0.43159011\n",
      "  2949: 1 [ 1450/ 1499], train_loss/perplexity = 5.95684958/386.3908997 secs/batch = 0.5410s, grad.norm=0.40419778\n",
      "  2954: 1 [ 1455/ 1499], train_loss/perplexity = 6.26388836/525.2573853 secs/batch = 0.5421s, grad.norm=0.41482934\n",
      "  2959: 1 [ 1460/ 1499], train_loss/perplexity = 6.36394262/580.5306396 secs/batch = 0.5347s, grad.norm=0.50609076\n",
      "  2964: 1 [ 1465/ 1499], train_loss/perplexity = 6.49912930/664.5627441 secs/batch = 0.5368s, grad.norm=0.41296878\n",
      "  2969: 1 [ 1470/ 1499], train_loss/perplexity = 6.11757183/453.7615356 secs/batch = 0.5373s, grad.norm=0.59998399\n",
      "  2974: 1 [ 1475/ 1499], train_loss/perplexity = 6.08593225/439.6294556 secs/batch = 0.5511s, grad.norm=0.45488870\n",
      "  2979: 1 [ 1480/ 1499], train_loss/perplexity = 6.17672396/481.4122314 secs/batch = 0.5361s, grad.norm=0.42847553\n",
      "  2984: 1 [ 1485/ 1499], train_loss/perplexity = 5.85168791/347.8209839 secs/batch = 0.5416s, grad.norm=0.40201899\n",
      "  2989: 1 [ 1490/ 1499], train_loss/perplexity = 5.99140787/399.9773254 secs/batch = 0.5381s, grad.norm=0.46418256\n",
      "  2994: 1 [ 1495/ 1499], train_loss/perplexity = 6.27928638/533.4078979 secs/batch = 0.5469s, grad.norm=0.52964014\n",
      "Epoch training time: 815.3354334831238\n",
      "Saved char model cv/epoch001_6.0870.model\n",
      "  3003: 2 [    5/ 1499], train_loss/perplexity = 6.11625624/453.1649780 secs/batch = 0.5373s, grad.norm=0.39551008\n",
      "  3008: 2 [   10/ 1499], train_loss/perplexity = 6.14985609/468.6499329 secs/batch = 0.5418s, grad.norm=0.47211760\n",
      "  3013: 2 [   15/ 1499], train_loss/perplexity = 6.04963493/423.9582214 secs/batch = 0.5432s, grad.norm=0.44561040\n",
      "  3018: 2 [   20/ 1499], train_loss/perplexity = 5.89330101/362.6002502 secs/batch = 0.5386s, grad.norm=0.34511730\n",
      "  3023: 2 [   25/ 1499], train_loss/perplexity = 6.24301386/514.4065552 secs/batch = 0.5430s, grad.norm=0.41977373\n",
      "  3028: 2 [   30/ 1499], train_loss/perplexity = 6.21502876/500.2103882 secs/batch = 0.5440s, grad.norm=0.41419071\n",
      "  3033: 2 [   35/ 1499], train_loss/perplexity = 6.03074312/416.0240784 secs/batch = 0.5439s, grad.norm=0.42778352\n",
      "  3038: 2 [   40/ 1499], train_loss/perplexity = 6.06953430/432.4792175 secs/batch = 0.5419s, grad.norm=0.46157807\n",
      "  3043: 2 [   45/ 1499], train_loss/perplexity = 6.12692499/458.0255737 secs/batch = 0.5398s, grad.norm=0.45331353\n",
      "  3048: 2 [   50/ 1499], train_loss/perplexity = 5.92929316/375.8887329 secs/batch = 0.5428s, grad.norm=0.46166569\n",
      "  3053: 2 [   55/ 1499], train_loss/perplexity = 5.98179531/396.1509399 secs/batch = 0.5422s, grad.norm=0.54602629\n",
      "  3058: 2 [   60/ 1499], train_loss/perplexity = 5.93714476/378.8516846 secs/batch = 0.5374s, grad.norm=0.43798426\n",
      "  3063: 2 [   65/ 1499], train_loss/perplexity = 5.89524698/363.3065491 secs/batch = 0.5437s, grad.norm=0.46609241\n",
      "  3068: 2 [   70/ 1499], train_loss/perplexity = 5.92416763/373.9670410 secs/batch = 0.5370s, grad.norm=0.63123184\n",
      "  3073: 2 [   75/ 1499], train_loss/perplexity = 5.72141123/305.3355103 secs/batch = 0.5366s, grad.norm=0.46599960\n",
      "  3078: 2 [   80/ 1499], train_loss/perplexity = 5.80127907/330.7223206 secs/batch = 0.5453s, grad.norm=0.40707240\n",
      "  3083: 2 [   85/ 1499], train_loss/perplexity = 5.91691160/371.2633362 secs/batch = 0.5428s, grad.norm=0.59253484\n",
      "  3088: 2 [   90/ 1499], train_loss/perplexity = 6.11186600/451.1798401 secs/batch = 0.5386s, grad.norm=0.60153067\n",
      "  3093: 2 [   95/ 1499], train_loss/perplexity = 5.86277390/351.6983643 secs/batch = 0.5422s, grad.norm=0.47411147\n",
      "  3098: 2 [  100/ 1499], train_loss/perplexity = 5.85311460/348.3175659 secs/batch = 0.5419s, grad.norm=0.40808788\n",
      "  3103: 2 [  105/ 1499], train_loss/perplexity = 5.78059864/323.9530640 secs/batch = 0.5380s, grad.norm=0.57867521\n",
      "  3108: 2 [  110/ 1499], train_loss/perplexity = 5.76831627/319.9984741 secs/batch = 0.5360s, grad.norm=0.45818987\n",
      "  3113: 2 [  115/ 1499], train_loss/perplexity = 5.95520306/385.7552185 secs/batch = 0.5457s, grad.norm=0.46054026\n",
      "  3118: 2 [  120/ 1499], train_loss/perplexity = 5.82056761/337.1633911 secs/batch = 0.5376s, grad.norm=0.49515939\n",
      "  3123: 2 [  125/ 1499], train_loss/perplexity = 6.08640623/439.8378906 secs/batch = 0.5386s, grad.norm=0.53125805\n",
      "  3128: 2 [  130/ 1499], train_loss/perplexity = 6.03909445/419.5129700 secs/batch = 0.5448s, grad.norm=0.56931651\n",
      "  3133: 2 [  135/ 1499], train_loss/perplexity = 5.99879217/402.9418030 secs/batch = 0.5520s, grad.norm=0.56025928\n",
      "  3138: 2 [  140/ 1499], train_loss/perplexity = 5.92170525/373.0473022 secs/batch = 0.5398s, grad.norm=0.58590043\n",
      "  3143: 2 [  145/ 1499], train_loss/perplexity = 5.85556698/349.1728210 secs/batch = 0.5407s, grad.norm=0.64224887\n",
      "  3148: 2 [  150/ 1499], train_loss/perplexity = 6.00369215/404.9210815 secs/batch = 0.5410s, grad.norm=0.55821449\n",
      "  3153: 2 [  155/ 1499], train_loss/perplexity = 6.07878017/436.4964294 secs/batch = 0.5430s, grad.norm=0.44080764\n",
      "  3158: 2 [  160/ 1499], train_loss/perplexity = 6.24155807/513.6582031 secs/batch = 0.5400s, grad.norm=0.41137591\n",
      "  3163: 2 [  165/ 1499], train_loss/perplexity = 5.84024334/343.8630066 secs/batch = 0.5368s, grad.norm=0.55240488\n",
      "  3168: 2 [  170/ 1499], train_loss/perplexity = 6.19349766/489.5554199 secs/batch = 0.5391s, grad.norm=0.71966577\n",
      "  3173: 2 [  175/ 1499], train_loss/perplexity = 5.91263533/369.6791077 secs/batch = 0.5447s, grad.norm=0.37649712\n",
      "  3178: 2 [  180/ 1499], train_loss/perplexity = 6.11384487/452.0735474 secs/batch = 0.5536s, grad.norm=0.43218005\n",
      "  3183: 2 [  185/ 1499], train_loss/perplexity = 5.90936756/368.4730530 secs/batch = 0.5458s, grad.norm=0.41394094\n",
      "  3188: 2 [  190/ 1499], train_loss/perplexity = 6.30552959/547.5914917 secs/batch = 0.5383s, grad.norm=0.58422923\n",
      "  3193: 2 [  195/ 1499], train_loss/perplexity = 6.17509937/480.6307983 secs/batch = 0.5401s, grad.norm=0.51187164\n",
      "  3198: 2 [  200/ 1499], train_loss/perplexity = 6.10485458/448.0274963 secs/batch = 0.5845s, grad.norm=0.39108226\n",
      "  3203: 2 [  205/ 1499], train_loss/perplexity = 6.06708622/431.4217834 secs/batch = 0.5447s, grad.norm=0.37462905\n",
      "  3208: 2 [  210/ 1499], train_loss/perplexity = 5.84566116/345.7310486 secs/batch = 0.5427s, grad.norm=0.37869024\n",
      "  3213: 2 [  215/ 1499], train_loss/perplexity = 5.97015667/391.5670166 secs/batch = 0.5372s, grad.norm=0.57270700\n",
      "  3218: 2 [  220/ 1499], train_loss/perplexity = 5.83645964/342.5643921 secs/batch = 0.5416s, grad.norm=0.41049579\n",
      "  3223: 2 [  225/ 1499], train_loss/perplexity = 5.99894762/403.0044556 secs/batch = 0.5938s, grad.norm=0.48488510\n",
      "  3228: 2 [  230/ 1499], train_loss/perplexity = 6.05482340/426.1636353 secs/batch = 0.5353s, grad.norm=0.44432268\n",
      "  3233: 2 [  235/ 1499], train_loss/perplexity = 5.96752691/390.5386353 secs/batch = 0.5412s, grad.norm=0.48764160\n",
      "  3238: 2 [  240/ 1499], train_loss/perplexity = 6.15568590/471.3900452 secs/batch = 0.5446s, grad.norm=0.41961038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3243: 2 [  245/ 1499], train_loss/perplexity = 5.85004902/347.2514038 secs/batch = 0.5453s, grad.norm=0.40856382\n",
      "  3248: 2 [  250/ 1499], train_loss/perplexity = 5.96549988/389.7478027 secs/batch = 0.5469s, grad.norm=0.49447820\n",
      "  3253: 2 [  255/ 1499], train_loss/perplexity = 5.80752420/332.7941895 secs/batch = 0.5422s, grad.norm=0.42155397\n",
      "  3258: 2 [  260/ 1499], train_loss/perplexity = 6.11057281/450.5967407 secs/batch = 0.5371s, grad.norm=0.53382552\n",
      "  3263: 2 [  265/ 1499], train_loss/perplexity = 6.00235653/404.3806152 secs/batch = 0.5352s, grad.norm=0.44103488\n",
      "  3268: 2 [  270/ 1499], train_loss/perplexity = 6.14621401/466.9461670 secs/batch = 0.5421s, grad.norm=0.56417680\n",
      "  3273: 2 [  275/ 1499], train_loss/perplexity = 5.65832615/286.6683960 secs/batch = 0.5437s, grad.norm=0.43526846\n",
      "  3278: 2 [  280/ 1499], train_loss/perplexity = 5.88303900/358.8982849 secs/batch = 0.5475s, grad.norm=0.61070144\n",
      "  3283: 2 [  285/ 1499], train_loss/perplexity = 6.29367256/541.1370239 secs/batch = 0.5482s, grad.norm=0.41550770\n",
      "  3288: 2 [  290/ 1499], train_loss/perplexity = 6.21551371/500.4530029 secs/batch = 0.5437s, grad.norm=0.54265684\n",
      "  3293: 2 [  295/ 1499], train_loss/perplexity = 5.91441774/370.3386230 secs/batch = 0.5371s, grad.norm=0.44354203\n",
      "  3298: 2 [  300/ 1499], train_loss/perplexity = 5.80747652/332.7783203 secs/batch = 0.5441s, grad.norm=0.41979396\n",
      "  3303: 2 [  305/ 1499], train_loss/perplexity = 6.07202625/433.5582886 secs/batch = 0.5714s, grad.norm=0.45641550\n",
      "  3308: 2 [  310/ 1499], train_loss/perplexity = 6.18774080/486.7452087 secs/batch = 0.5879s, grad.norm=0.47427887\n",
      "  3313: 2 [  315/ 1499], train_loss/perplexity = 6.08767033/440.3942261 secs/batch = 0.6870s, grad.norm=0.42239115\n",
      "  3318: 2 [  320/ 1499], train_loss/perplexity = 6.10603952/448.5586853 secs/batch = 0.5714s, grad.norm=0.43509138\n",
      "  3323: 2 [  325/ 1499], train_loss/perplexity = 5.96121264/388.0804443 secs/batch = 0.5660s, grad.norm=0.48778477\n",
      "  3328: 2 [  330/ 1499], train_loss/perplexity = 5.86754704/353.3810730 secs/batch = 0.6170s, grad.norm=0.45010513\n",
      "  3333: 2 [  335/ 1499], train_loss/perplexity = 6.03899240/419.4701538 secs/batch = 0.6007s, grad.norm=0.54857153\n",
      "  3338: 2 [  340/ 1499], train_loss/perplexity = 5.79347610/328.1517334 secs/batch = 0.5514s, grad.norm=0.61497116\n",
      "  3343: 2 [  345/ 1499], train_loss/perplexity = 5.73285723/308.8504639 secs/batch = 0.5429s, grad.norm=0.40768027\n",
      "  3348: 2 [  350/ 1499], train_loss/perplexity = 5.92311478/373.5735168 secs/batch = 0.5429s, grad.norm=0.46347815\n",
      "  3353: 2 [  355/ 1499], train_loss/perplexity = 5.70175838/299.3934021 secs/batch = 0.5367s, grad.norm=0.42477363\n",
      "  3358: 2 [  360/ 1499], train_loss/perplexity = 5.86914349/353.9456787 secs/batch = 0.5402s, grad.norm=0.51759255\n",
      "  3363: 2 [  365/ 1499], train_loss/perplexity = 5.71930742/304.6938171 secs/batch = 0.5461s, grad.norm=0.50661939\n",
      "  3368: 2 [  370/ 1499], train_loss/perplexity = 5.82668638/339.2327271 secs/batch = 0.5494s, grad.norm=0.49134460\n",
      "  3373: 2 [  375/ 1499], train_loss/perplexity = 5.84105206/344.1412048 secs/batch = 0.5633s, grad.norm=0.42670390\n",
      "  3378: 2 [  380/ 1499], train_loss/perplexity = 6.00607252/405.8860779 secs/batch = 0.5357s, grad.norm=0.50349826\n",
      "  3383: 2 [  385/ 1499], train_loss/perplexity = 6.12659502/457.8744507 secs/batch = 0.5396s, grad.norm=0.41922960\n",
      "  3388: 2 [  390/ 1499], train_loss/perplexity = 6.24220037/513.9882202 secs/batch = 0.5377s, grad.norm=0.50395674\n",
      "  3393: 2 [  395/ 1499], train_loss/perplexity = 6.27630758/531.8213501 secs/batch = 0.5387s, grad.norm=0.50271702\n",
      "  3398: 2 [  400/ 1499], train_loss/perplexity = 6.13268185/460.6699524 secs/batch = 0.5414s, grad.norm=0.55234206\n",
      "  3403: 2 [  405/ 1499], train_loss/perplexity = 6.05973721/428.2628784 secs/batch = 0.5486s, grad.norm=0.55169028\n",
      "  3408: 2 [  410/ 1499], train_loss/perplexity = 5.63246775/279.3506470 secs/batch = 0.5457s, grad.norm=0.54420799\n",
      "  3413: 2 [  415/ 1499], train_loss/perplexity = 6.13300180/460.8173828 secs/batch = 0.5408s, grad.norm=0.47114784\n",
      "  3418: 2 [  420/ 1499], train_loss/perplexity = 5.87013626/354.2972412 secs/batch = 0.5433s, grad.norm=0.45407534\n",
      "  3423: 2 [  425/ 1499], train_loss/perplexity = 6.21428919/499.8405762 secs/batch = 0.5467s, grad.norm=0.43568912\n",
      "  3428: 2 [  430/ 1499], train_loss/perplexity = 5.85783005/349.9639282 secs/batch = 0.5627s, grad.norm=0.42688552\n",
      "  3433: 2 [  435/ 1499], train_loss/perplexity = 5.86568069/352.7221680 secs/batch = 0.5385s, grad.norm=0.46272019\n",
      "  3438: 2 [  440/ 1499], train_loss/perplexity = 6.03793669/419.0275574 secs/batch = 0.5410s, grad.norm=0.56145293\n",
      "  3443: 2 [  445/ 1499], train_loss/perplexity = 6.01800632/410.7588501 secs/batch = 0.5420s, grad.norm=0.42429903\n",
      "  3448: 2 [  450/ 1499], train_loss/perplexity = 5.91539288/370.6999207 secs/batch = 0.5423s, grad.norm=0.53409088\n",
      "  3453: 2 [  455/ 1499], train_loss/perplexity = 6.07258654/433.8012695 secs/batch = 0.5369s, grad.norm=0.42034441\n",
      "  3458: 2 [  460/ 1499], train_loss/perplexity = 6.11584616/452.9791870 secs/batch = 0.5431s, grad.norm=0.59069014\n",
      "  3463: 2 [  465/ 1499], train_loss/perplexity = 6.02753496/414.6915283 secs/batch = 0.5429s, grad.norm=0.51176834\n",
      "  3468: 2 [  470/ 1499], train_loss/perplexity = 6.11637020/453.2166138 secs/batch = 0.5414s, grad.norm=0.53848457\n",
      "  3473: 2 [  475/ 1499], train_loss/perplexity = 5.92153692/372.9845276 secs/batch = 0.5422s, grad.norm=0.45352945\n",
      "  3478: 2 [  480/ 1499], train_loss/perplexity = 6.03813601/419.1110840 secs/batch = 0.5380s, grad.norm=0.45220593\n",
      "  3483: 2 [  485/ 1499], train_loss/perplexity = 6.04588318/422.3706360 secs/batch = 0.5436s, grad.norm=0.47754246\n",
      "  3488: 2 [  490/ 1499], train_loss/perplexity = 5.92677975/374.9451599 secs/batch = 0.5416s, grad.norm=0.56548256\n",
      "  3493: 2 [  495/ 1499], train_loss/perplexity = 5.94318104/381.1454468 secs/batch = 0.5364s, grad.norm=0.45035768\n",
      "  3498: 2 [  500/ 1499], train_loss/perplexity = 6.08627129/439.7785339 secs/batch = 0.5444s, grad.norm=0.44691131\n",
      "  3503: 2 [  505/ 1499], train_loss/perplexity = 5.78288078/324.6932068 secs/batch = 0.5387s, grad.norm=0.59097487\n",
      "  3508: 2 [  510/ 1499], train_loss/perplexity = 6.16705275/476.7788391 secs/batch = 0.5435s, grad.norm=0.45077640\n",
      "  3513: 2 [  515/ 1499], train_loss/perplexity = 5.87147093/354.7704468 secs/batch = 0.5460s, grad.norm=0.51978284\n",
      "  3518: 2 [  520/ 1499], train_loss/perplexity = 6.15852976/472.7325439 secs/batch = 0.5472s, grad.norm=0.53774655\n",
      "  3523: 2 [  525/ 1499], train_loss/perplexity = 6.16347313/475.0751953 secs/batch = 0.5343s, grad.norm=0.57170695\n",
      "  3528: 2 [  530/ 1499], train_loss/perplexity = 6.05801487/427.5259094 secs/batch = 0.5406s, grad.norm=0.54771531\n",
      "  3533: 2 [  535/ 1499], train_loss/perplexity = 5.95350170/385.0994873 secs/batch = 0.5334s, grad.norm=0.47353330\n",
      "  3538: 2 [  540/ 1499], train_loss/perplexity = 5.79176044/327.5892334 secs/batch = 0.5440s, grad.norm=0.43960688\n",
      "  3543: 2 [  545/ 1499], train_loss/perplexity = 5.63609648/280.3661499 secs/batch = 0.5427s, grad.norm=0.82844526\n",
      "  3548: 2 [  550/ 1499], train_loss/perplexity = 6.06041718/428.5541687 secs/batch = 0.5416s, grad.norm=0.56484610\n",
      "  3553: 2 [  555/ 1499], train_loss/perplexity = 5.82659245/339.2008667 secs/batch = 0.5412s, grad.norm=0.47793749\n",
      "  3558: 2 [  560/ 1499], train_loss/perplexity = 6.18672371/486.2503967 secs/batch = 0.5358s, grad.norm=0.45096806\n",
      "  3563: 2 [  565/ 1499], train_loss/perplexity = 6.03559637/418.0480347 secs/batch = 0.5490s, grad.norm=0.43599325\n",
      "  3568: 2 [  570/ 1499], train_loss/perplexity = 5.98770523/398.4990845 secs/batch = 0.5497s, grad.norm=0.40038389\n",
      "  3573: 2 [  575/ 1499], train_loss/perplexity = 6.03401136/417.3859558 secs/batch = 0.5362s, grad.norm=0.48095274\n",
      "  3578: 2 [  580/ 1499], train_loss/perplexity = 5.81308222/334.6489868 secs/batch = 0.5423s, grad.norm=0.52599245\n",
      "  3583: 2 [  585/ 1499], train_loss/perplexity = 5.72580099/306.6788025 secs/batch = 0.5389s, grad.norm=0.49377868\n",
      "  3588: 2 [  590/ 1499], train_loss/perplexity = 5.98447895/397.2154846 secs/batch = 0.5817s, grad.norm=0.52635795\n",
      "  3593: 2 [  595/ 1499], train_loss/perplexity = 5.91115236/369.1312866 secs/batch = 0.5429s, grad.norm=0.43188906\n",
      "  3598: 2 [  600/ 1499], train_loss/perplexity = 5.71807051/304.3171692 secs/batch = 0.5419s, grad.norm=0.45441660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3603: 2 [  605/ 1499], train_loss/perplexity = 5.72216511/305.5657959 secs/batch = 0.5413s, grad.norm=0.43465331\n",
      "  3608: 2 [  610/ 1499], train_loss/perplexity = 6.01751614/410.5575562 secs/batch = 0.5408s, grad.norm=0.54842395\n",
      "  3613: 2 [  615/ 1499], train_loss/perplexity = 6.01689196/410.3013916 secs/batch = 0.5374s, grad.norm=0.42902750\n",
      "  3618: 2 [  620/ 1499], train_loss/perplexity = 5.84651470/346.0262756 secs/batch = 0.5388s, grad.norm=0.46556053\n",
      "  3623: 2 [  625/ 1499], train_loss/perplexity = 5.86388540/352.0895081 secs/batch = 0.5428s, grad.norm=0.48113179\n",
      "  3628: 2 [  630/ 1499], train_loss/perplexity = 5.96937752/391.2620544 secs/batch = 0.5344s, grad.norm=0.45657605\n",
      "  3633: 2 [  635/ 1499], train_loss/perplexity = 5.80281544/331.2308044 secs/batch = 0.5414s, grad.norm=0.78509980\n",
      "  3638: 2 [  640/ 1499], train_loss/perplexity = 6.02418709/413.3055115 secs/batch = 0.5407s, grad.norm=0.43782437\n",
      "  3643: 2 [  645/ 1499], train_loss/perplexity = 5.68557405/294.5869141 secs/batch = 0.5418s, grad.norm=0.47737569\n",
      "  3648: 2 [  650/ 1499], train_loss/perplexity = 5.70778942/301.2044983 secs/batch = 0.5746s, grad.norm=0.62055385\n",
      "  3653: 2 [  655/ 1499], train_loss/perplexity = 6.05433369/425.9549866 secs/batch = 0.5376s, grad.norm=0.44546178\n",
      "  3658: 2 [  660/ 1499], train_loss/perplexity = 6.15242386/469.8548584 secs/batch = 0.5384s, grad.norm=0.48170149\n",
      "  3663: 2 [  665/ 1499], train_loss/perplexity = 6.11233616/451.3919983 secs/batch = 0.5966s, grad.norm=0.46369022\n",
      "  3668: 2 [  670/ 1499], train_loss/perplexity = 6.09024382/441.5290527 secs/batch = 0.5381s, grad.norm=0.48175317\n",
      "  3673: 2 [  675/ 1499], train_loss/perplexity = 6.00341702/404.8096619 secs/batch = 0.5413s, grad.norm=0.44050634\n",
      "  3678: 2 [  680/ 1499], train_loss/perplexity = 6.00988531/407.4365845 secs/batch = 0.5384s, grad.norm=0.49149758\n",
      "  3683: 2 [  685/ 1499], train_loss/perplexity = 6.07988977/436.9810181 secs/batch = 0.5440s, grad.norm=0.45205095\n",
      "  3688: 2 [  690/ 1499], train_loss/perplexity = 5.95798159/386.8285522 secs/batch = 0.5416s, grad.norm=0.44286293\n",
      "  3693: 2 [  695/ 1499], train_loss/perplexity = 5.87956476/357.6535339 secs/batch = 0.5397s, grad.norm=0.47411054\n",
      "  3698: 2 [  700/ 1499], train_loss/perplexity = 6.15101290/469.1923828 secs/batch = 0.5404s, grad.norm=0.50297260\n",
      "  3703: 2 [  705/ 1499], train_loss/perplexity = 5.68680429/294.9495544 secs/batch = 0.5384s, grad.norm=0.47775224\n",
      "  3708: 2 [  710/ 1499], train_loss/perplexity = 5.89983559/364.9774475 secs/batch = 0.5406s, grad.norm=0.41626719\n",
      "  3713: 2 [  715/ 1499], train_loss/perplexity = 5.91865873/371.9125366 secs/batch = 0.5449s, grad.norm=0.44588810\n",
      "  3718: 2 [  720/ 1499], train_loss/perplexity = 6.00511026/405.4956970 secs/batch = 0.5464s, grad.norm=0.57167411\n",
      "  3723: 2 [  725/ 1499], train_loss/perplexity = 5.81891537/336.6067505 secs/batch = 0.5404s, grad.norm=0.58585250\n",
      "  3728: 2 [  730/ 1499], train_loss/perplexity = 5.97583294/393.7959595 secs/batch = 0.5374s, grad.norm=0.58169812\n",
      "  3733: 2 [  735/ 1499], train_loss/perplexity = 5.83151913/340.8761292 secs/batch = 0.5428s, grad.norm=0.49734735\n",
      "  3738: 2 [  740/ 1499], train_loss/perplexity = 5.79572153/328.8894043 secs/batch = 0.5375s, grad.norm=0.43704948\n",
      "  3743: 2 [  745/ 1499], train_loss/perplexity = 5.64609623/283.1838074 secs/batch = 0.5405s, grad.norm=0.44887316\n",
      "  3748: 2 [  750/ 1499], train_loss/perplexity = 5.93596888/378.4064331 secs/batch = 0.5417s, grad.norm=0.46101820\n",
      "  3753: 2 [  755/ 1499], train_loss/perplexity = 5.67353916/291.0628357 secs/batch = 0.5405s, grad.norm=0.49849111\n",
      "  3758: 2 [  760/ 1499], train_loss/perplexity = 5.52539396/250.9851990 secs/batch = 0.5357s, grad.norm=0.84569734\n",
      "  3763: 2 [  765/ 1499], train_loss/perplexity = 5.78145075/324.2292175 secs/batch = 0.5430s, grad.norm=0.53856164\n",
      "  3768: 2 [  770/ 1499], train_loss/perplexity = 5.88795996/360.6687622 secs/batch = 0.5466s, grad.norm=0.47618717\n",
      "  3773: 2 [  775/ 1499], train_loss/perplexity = 6.19856501/492.0424500 secs/batch = 0.5415s, grad.norm=0.52421623\n",
      "  3778: 2 [  780/ 1499], train_loss/perplexity = 5.73590183/309.7922363 secs/batch = 0.5406s, grad.norm=0.41913730\n",
      "  3783: 2 [  785/ 1499], train_loss/perplexity = 5.87493515/356.0015869 secs/batch = 0.5401s, grad.norm=0.47874665\n",
      "  3788: 2 [  790/ 1499], train_loss/perplexity = 5.52969551/252.0671539 secs/batch = 0.5426s, grad.norm=0.64725757\n",
      "  3793: 2 [  795/ 1499], train_loss/perplexity = 5.97899342/395.0425110 secs/batch = 0.5418s, grad.norm=0.46130267\n",
      "  3798: 2 [  800/ 1499], train_loss/perplexity = 5.76535034/319.0508118 secs/batch = 0.5335s, grad.norm=0.50635850\n",
      "  3803: 2 [  805/ 1499], train_loss/perplexity = 5.85870361/350.2697754 secs/batch = 0.5372s, grad.norm=0.39564964\n",
      "  3808: 2 [  810/ 1499], train_loss/perplexity = 5.78219748/324.4714355 secs/batch = 0.5501s, grad.norm=0.42612460\n",
      "  3813: 2 [  815/ 1499], train_loss/perplexity = 5.87922335/357.5314636 secs/batch = 0.5553s, grad.norm=0.41124171\n",
      "  3818: 2 [  820/ 1499], train_loss/perplexity = 5.55344248/258.1246033 secs/batch = 0.5436s, grad.norm=0.52585781\n",
      "  3823: 2 [  825/ 1499], train_loss/perplexity = 5.67840624/292.4829102 secs/batch = 0.5405s, grad.norm=0.46477377\n",
      "  3828: 2 [  830/ 1499], train_loss/perplexity = 5.92140436/372.9350891 secs/batch = 0.5374s, grad.norm=0.48186326\n",
      "  3833: 2 [  835/ 1499], train_loss/perplexity = 5.90991592/368.6751709 secs/batch = 0.5399s, grad.norm=0.54262531\n",
      "  3838: 2 [  840/ 1499], train_loss/perplexity = 5.79143524/327.4826965 secs/batch = 0.5377s, grad.norm=0.49537909\n",
      "  3843: 2 [  845/ 1499], train_loss/perplexity = 5.73273420/308.8124695 secs/batch = 0.5362s, grad.norm=0.49395335\n",
      "  3848: 2 [  850/ 1499], train_loss/perplexity = 5.78398180/325.0509033 secs/batch = 0.5382s, grad.norm=0.45482701\n",
      "  3853: 2 [  855/ 1499], train_loss/perplexity = 5.94747925/382.7872009 secs/batch = 0.5407s, grad.norm=0.49856007\n",
      "  3858: 2 [  860/ 1499], train_loss/perplexity = 5.62788725/278.0740051 secs/batch = 0.5404s, grad.norm=0.74418497\n",
      "  3863: 2 [  865/ 1499], train_loss/perplexity = 5.78651619/325.8757629 secs/batch = 0.5452s, grad.norm=0.42432410\n",
      "  3868: 2 [  870/ 1499], train_loss/perplexity = 5.74941826/314.0079346 secs/batch = 0.5451s, grad.norm=0.49362567\n",
      "  3873: 2 [  875/ 1499], train_loss/perplexity = 5.80621862/332.3599548 secs/batch = 0.5408s, grad.norm=0.52104348\n",
      "  3878: 2 [  880/ 1499], train_loss/perplexity = 5.70225811/299.5430298 secs/batch = 0.5440s, grad.norm=0.49183989\n",
      "  3883: 2 [  885/ 1499], train_loss/perplexity = 5.63315725/279.5433044 secs/batch = 0.5367s, grad.norm=0.54879713\n",
      "  3888: 2 [  890/ 1499], train_loss/perplexity = 5.90568733/367.1194763 secs/batch = 0.5393s, grad.norm=0.41580606\n",
      "  3893: 2 [  895/ 1499], train_loss/perplexity = 5.91888285/371.9959106 secs/batch = 0.5403s, grad.norm=0.43277314\n",
      "  3898: 2 [  900/ 1499], train_loss/perplexity = 5.74378014/312.2424927 secs/batch = 0.5435s, grad.norm=0.44716769\n",
      "  3903: 2 [  905/ 1499], train_loss/perplexity = 5.79280615/327.9319458 secs/batch = 0.5504s, grad.norm=0.59084195\n",
      "  3908: 2 [  910/ 1499], train_loss/perplexity = 5.98623323/397.9129333 secs/batch = 0.5410s, grad.norm=0.49542695\n",
      "  3913: 2 [  915/ 1499], train_loss/perplexity = 5.72242928/305.6465149 secs/batch = 0.5386s, grad.norm=0.56661451\n",
      "  3918: 2 [  920/ 1499], train_loss/perplexity = 5.47048950/237.5764618 secs/batch = 0.5443s, grad.norm=0.42610285\n",
      "  3923: 2 [  925/ 1499], train_loss/perplexity = 5.91492701/370.5272522 secs/batch = 0.5406s, grad.norm=0.68731964\n",
      "  3928: 2 [  930/ 1499], train_loss/perplexity = 5.85458136/348.8288269 secs/batch = 0.5393s, grad.norm=0.51379740\n",
      "  3933: 2 [  935/ 1499], train_loss/perplexity = 5.89180756/362.0591431 secs/batch = 0.5816s, grad.norm=0.43283629\n",
      "  3938: 2 [  940/ 1499], train_loss/perplexity = 5.52682447/251.3444977 secs/batch = 0.5400s, grad.norm=0.51743495\n",
      "  3943: 2 [  945/ 1499], train_loss/perplexity = 5.90227699/365.8695984 secs/batch = 0.5365s, grad.norm=0.52150875\n",
      "  3948: 2 [  950/ 1499], train_loss/perplexity = 5.83819771/343.1603088 secs/batch = 0.5361s, grad.norm=0.48423833\n",
      "  3953: 2 [  955/ 1499], train_loss/perplexity = 5.97498608/393.4626160 secs/batch = 0.5392s, grad.norm=0.40736976\n",
      "  3958: 2 [  960/ 1499], train_loss/perplexity = 5.74666977/313.1460876 secs/batch = 0.5441s, grad.norm=0.48821676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3963: 2 [  965/ 1499], train_loss/perplexity = 5.83567142/342.2944946 secs/batch = 0.5433s, grad.norm=0.70064616\n",
      "  3968: 2 [  970/ 1499], train_loss/perplexity = 6.01706362/410.3718262 secs/batch = 0.5416s, grad.norm=0.45476413\n",
      "  3973: 2 [  975/ 1499], train_loss/perplexity = 5.86966133/354.1290283 secs/batch = 0.5447s, grad.norm=0.63425863\n",
      "  3978: 2 [  980/ 1499], train_loss/perplexity = 5.93623257/378.5062561 secs/batch = 0.5419s, grad.norm=0.58096004\n",
      "  3983: 2 [  985/ 1499], train_loss/perplexity = 5.62039375/275.9980469 secs/batch = 0.5452s, grad.norm=0.50643891\n",
      "  3988: 2 [  990/ 1499], train_loss/perplexity = 5.80965281/333.5033264 secs/batch = 0.5382s, grad.norm=0.46288157\n",
      "  3993: 2 [  995/ 1499], train_loss/perplexity = 6.00993824/407.4581604 secs/batch = 0.5429s, grad.norm=0.58836323\n",
      "  3998: 2 [ 1000/ 1499], train_loss/perplexity = 5.70917606/301.6224365 secs/batch = 0.5402s, grad.norm=0.50464469\n",
      "  4003: 2 [ 1005/ 1499], train_loss/perplexity = 6.02266216/412.6757507 secs/batch = 0.5397s, grad.norm=0.49785522\n",
      "  4008: 2 [ 1010/ 1499], train_loss/perplexity = 5.90657425/367.4452209 secs/batch = 0.5404s, grad.norm=0.44574574\n",
      "  4013: 2 [ 1015/ 1499], train_loss/perplexity = 5.66790199/289.4266663 secs/batch = 0.5491s, grad.norm=0.46328974\n",
      "  4018: 2 [ 1020/ 1499], train_loss/perplexity = 5.83124161/340.7815247 secs/batch = 0.5405s, grad.norm=0.67606091\n",
      "  4023: 2 [ 1025/ 1499], train_loss/perplexity = 5.75835037/316.8252563 secs/batch = 0.5414s, grad.norm=0.46572134\n",
      "  4028: 2 [ 1030/ 1499], train_loss/perplexity = 5.86656284/353.0334473 secs/batch = 0.5472s, grad.norm=0.43097636\n",
      "  4033: 2 [ 1035/ 1499], train_loss/perplexity = 6.19721746/491.3798523 secs/batch = 0.5455s, grad.norm=0.56477761\n",
      "  4038: 2 [ 1040/ 1499], train_loss/perplexity = 5.64716625/283.4869995 secs/batch = 0.5403s, grad.norm=0.44522315\n",
      "  4043: 2 [ 1045/ 1499], train_loss/perplexity = 5.75285864/315.0900879 secs/batch = 0.5420s, grad.norm=0.52413827\n",
      "  4048: 2 [ 1050/ 1499], train_loss/perplexity = 5.44678164/232.0102692 secs/batch = 0.5368s, grad.norm=0.48008266\n",
      "  4053: 2 [ 1055/ 1499], train_loss/perplexity = 5.82835102/339.7978821 secs/batch = 0.5385s, grad.norm=0.51440245\n",
      "  4058: 2 [ 1060/ 1499], train_loss/perplexity = 6.03547478/417.9972229 secs/batch = 0.5507s, grad.norm=0.57606983\n",
      "  4063: 2 [ 1065/ 1499], train_loss/perplexity = 5.58310890/265.8969727 secs/batch = 0.5483s, grad.norm=0.45145577\n",
      "  4068: 2 [ 1070/ 1499], train_loss/perplexity = 6.02747679/414.6674194 secs/batch = 0.5350s, grad.norm=0.44575030\n",
      "  4073: 2 [ 1075/ 1499], train_loss/perplexity = 5.88873100/360.9469604 secs/batch = 0.5407s, grad.norm=0.47348222\n",
      "  4078: 2 [ 1080/ 1499], train_loss/perplexity = 6.07809877/436.1990967 secs/batch = 0.5391s, grad.norm=0.51382440\n",
      "  4083: 2 [ 1085/ 1499], train_loss/perplexity = 6.15606403/471.5683289 secs/batch = 0.5447s, grad.norm=0.49837491\n",
      "  4088: 2 [ 1090/ 1499], train_loss/perplexity = 5.70686245/300.9254150 secs/batch = 0.5542s, grad.norm=0.43024874\n",
      "  4093: 2 [ 1095/ 1499], train_loss/perplexity = 5.92041731/372.5671692 secs/batch = 0.5362s, grad.norm=0.55217344\n",
      "  4098: 2 [ 1100/ 1499], train_loss/perplexity = 5.91647625/371.1017456 secs/batch = 0.5435s, grad.norm=0.56997138\n",
      "  4103: 2 [ 1105/ 1499], train_loss/perplexity = 5.66103220/287.4451904 secs/batch = 0.5403s, grad.norm=0.48721021\n",
      "  4108: 2 [ 1110/ 1499], train_loss/perplexity = 5.88013363/357.8570557 secs/batch = 0.5470s, grad.norm=0.50494605\n",
      "  4113: 2 [ 1115/ 1499], train_loss/perplexity = 5.51112318/247.4288788 secs/batch = 0.5413s, grad.norm=0.47447285\n",
      "  4118: 2 [ 1120/ 1499], train_loss/perplexity = 5.72367048/306.0261230 secs/batch = 0.5430s, grad.norm=0.50245309\n",
      "  4123: 2 [ 1125/ 1499], train_loss/perplexity = 5.70095396/299.1526489 secs/batch = 0.5465s, grad.norm=0.49507579\n",
      "  4128: 2 [ 1130/ 1499], train_loss/perplexity = 5.52279282/250.3331909 secs/batch = 0.5788s, grad.norm=0.51635098\n",
      "  4133: 2 [ 1135/ 1499], train_loss/perplexity = 6.07060194/432.9412231 secs/batch = 0.5437s, grad.norm=0.50958419\n",
      "  4138: 2 [ 1140/ 1499], train_loss/perplexity = 5.94220781/380.7746887 secs/batch = 0.5836s, grad.norm=0.48111612\n",
      "  4143: 2 [ 1145/ 1499], train_loss/perplexity = 6.01120806/407.9758911 secs/batch = 0.5458s, grad.norm=0.51508331\n",
      "  4148: 2 [ 1150/ 1499], train_loss/perplexity = 5.90809488/368.0043945 secs/batch = 0.5879s, grad.norm=0.54483199\n",
      "  4153: 2 [ 1155/ 1499], train_loss/perplexity = 5.61732006/275.1510010 secs/batch = 0.5450s, grad.norm=0.53718150\n",
      "  4158: 2 [ 1160/ 1499], train_loss/perplexity = 5.80097246/330.6209106 secs/batch = 0.5872s, grad.norm=0.48307297\n",
      "  4163: 2 [ 1165/ 1499], train_loss/perplexity = 5.67518568/291.5424805 secs/batch = 0.5637s, grad.norm=0.54276597\n",
      "  4168: 2 [ 1170/ 1499], train_loss/perplexity = 5.97617006/393.9287415 secs/batch = 0.6016s, grad.norm=0.50678605\n",
      "  4173: 2 [ 1175/ 1499], train_loss/perplexity = 5.42986250/228.1178741 secs/batch = 0.5392s, grad.norm=0.54133499\n",
      "  4178: 2 [ 1180/ 1499], train_loss/perplexity = 5.68193388/293.5165100 secs/batch = 0.6031s, grad.norm=0.47649294\n",
      "  4183: 2 [ 1185/ 1499], train_loss/perplexity = 5.58941889/267.5800781 secs/batch = 0.6145s, grad.norm=0.58068997\n",
      "  4188: 2 [ 1190/ 1499], train_loss/perplexity = 5.86691380/353.1573792 secs/batch = 0.5971s, grad.norm=0.45468459\n",
      "  4193: 2 [ 1195/ 1499], train_loss/perplexity = 5.89621496/363.6583862 secs/batch = 0.6139s, grad.norm=0.52477729\n",
      "  4198: 2 [ 1200/ 1499], train_loss/perplexity = 5.73682642/310.0787964 secs/batch = 0.6092s, grad.norm=0.44820631\n",
      "  4203: 2 [ 1205/ 1499], train_loss/perplexity = 5.71299362/302.7761230 secs/batch = 0.6671s, grad.norm=0.56905740\n",
      "  4208: 2 [ 1210/ 1499], train_loss/perplexity = 5.68980742/295.8366394 secs/batch = 0.5815s, grad.norm=0.54729646\n",
      "  4213: 2 [ 1215/ 1499], train_loss/perplexity = 5.34262753/209.0613098 secs/batch = 0.7173s, grad.norm=0.65393329\n",
      "  4218: 2 [ 1220/ 1499], train_loss/perplexity = 5.72492933/306.4116211 secs/batch = 0.6767s, grad.norm=0.46871895\n",
      "  4223: 2 [ 1225/ 1499], train_loss/perplexity = 5.23089361/186.9597931 secs/batch = 0.6574s, grad.norm=0.59197098\n",
      "  4228: 2 [ 1230/ 1499], train_loss/perplexity = 5.81119204/334.0170593 secs/batch = 0.5942s, grad.norm=0.62681222\n",
      "  4233: 2 [ 1235/ 1499], train_loss/perplexity = 5.54417372/255.7431793 secs/batch = 0.6148s, grad.norm=0.48458391\n",
      "  4238: 2 [ 1240/ 1499], train_loss/perplexity = 5.61239624/273.7995300 secs/batch = 0.6940s, grad.norm=0.48038024\n",
      "  4243: 2 [ 1245/ 1499], train_loss/perplexity = 5.89188910/362.0886536 secs/batch = 0.6082s, grad.norm=0.46762106\n",
      "  4248: 2 [ 1250/ 1499], train_loss/perplexity = 6.13537931/461.9142761 secs/batch = 0.5682s, grad.norm=0.47274497\n",
      "  4253: 2 [ 1255/ 1499], train_loss/perplexity = 5.65643215/286.1259766 secs/batch = 0.5884s, grad.norm=0.46915302\n",
      "  4258: 2 [ 1260/ 1499], train_loss/perplexity = 5.78090906/324.0536499 secs/batch = 0.6950s, grad.norm=0.47149244\n",
      "  4263: 2 [ 1265/ 1499], train_loss/perplexity = 5.82084560/337.2571106 secs/batch = 0.6714s, grad.norm=0.45651174\n",
      "  4268: 2 [ 1270/ 1499], train_loss/perplexity = 5.85719681/349.7423706 secs/batch = 0.5744s, grad.norm=0.45090386\n",
      "  4273: 2 [ 1275/ 1499], train_loss/perplexity = 6.01748562/410.5450439 secs/batch = 0.5960s, grad.norm=0.96912897\n",
      "  4278: 2 [ 1280/ 1499], train_loss/perplexity = 5.51600742/248.6403351 secs/batch = 0.5422s, grad.norm=0.56061816\n",
      "  4283: 2 [ 1285/ 1499], train_loss/perplexity = 5.86930561/354.0030823 secs/batch = 0.5416s, grad.norm=0.48928356\n",
      "  4288: 2 [ 1290/ 1499], train_loss/perplexity = 5.89491892/363.1873779 secs/batch = 0.5436s, grad.norm=0.55401111\n",
      "  4293: 2 [ 1295/ 1499], train_loss/perplexity = 5.68921375/295.6610718 secs/batch = 0.5981s, grad.norm=0.46754193\n",
      "  4298: 2 [ 1300/ 1499], train_loss/perplexity = 5.93050385/376.3440857 secs/batch = 0.6849s, grad.norm=0.46757746\n",
      "  4303: 2 [ 1305/ 1499], train_loss/perplexity = 5.77787352/323.0714417 secs/batch = 0.5748s, grad.norm=0.50979871\n",
      "  4308: 2 [ 1310/ 1499], train_loss/perplexity = 5.86522818/352.5625916 secs/batch = 0.6971s, grad.norm=0.47331417\n",
      "  4313: 2 [ 1315/ 1499], train_loss/perplexity = 5.72475529/306.3582764 secs/batch = 0.5929s, grad.norm=0.61983556\n",
      "  4318: 2 [ 1320/ 1499], train_loss/perplexity = 5.42209959/226.3538818 secs/batch = 0.5744s, grad.norm=0.45122465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4323: 2 [ 1325/ 1499], train_loss/perplexity = 5.77604628/322.4816589 secs/batch = 0.6355s, grad.norm=0.48283646\n",
      "  4328: 2 [ 1330/ 1499], train_loss/perplexity = 5.89789295/364.2691345 secs/batch = 0.5746s, grad.norm=0.45466971\n",
      "  4333: 2 [ 1335/ 1499], train_loss/perplexity = 5.70828009/301.3523254 secs/batch = 0.5436s, grad.norm=0.49445704\n",
      "  4338: 2 [ 1340/ 1499], train_loss/perplexity = 5.24642229/189.8856964 secs/batch = 0.5548s, grad.norm=0.50596249\n",
      "  4343: 2 [ 1345/ 1499], train_loss/perplexity = 5.40404892/222.3046875 secs/batch = 0.6403s, grad.norm=0.52940959\n",
      "  4348: 2 [ 1350/ 1499], train_loss/perplexity = 5.58526039/266.4696655 secs/batch = 0.5333s, grad.norm=0.63639933\n",
      "  4353: 2 [ 1355/ 1499], train_loss/perplexity = 5.30879450/202.1064453 secs/batch = 0.5415s, grad.norm=0.57513338\n",
      "  4358: 2 [ 1360/ 1499], train_loss/perplexity = 5.37441492/215.8135681 secs/batch = 0.5988s, grad.norm=0.53514719\n",
      "  4363: 2 [ 1365/ 1499], train_loss/perplexity = 5.35850668/212.4075165 secs/batch = 0.5432s, grad.norm=0.44452083\n",
      "  4368: 2 [ 1370/ 1499], train_loss/perplexity = 5.28412676/197.1819153 secs/batch = 0.5799s, grad.norm=0.49704826\n",
      "  4373: 2 [ 1375/ 1499], train_loss/perplexity = 5.32806778/206.0394745 secs/batch = 0.5763s, grad.norm=0.53794503\n",
      "  4378: 2 [ 1380/ 1499], train_loss/perplexity = 5.80502272/331.9627380 secs/batch = 0.6553s, grad.norm=0.55534834\n",
      "  4383: 2 [ 1385/ 1499], train_loss/perplexity = 5.59279776/268.4857178 secs/batch = 0.6316s, grad.norm=0.47267109\n",
      "  4388: 2 [ 1390/ 1499], train_loss/perplexity = 5.56439257/260.9666443 secs/batch = 0.6372s, grad.norm=0.46326602\n",
      "  4393: 2 [ 1395/ 1499], train_loss/perplexity = 5.70087004/299.1275330 secs/batch = 0.6081s, grad.norm=0.45518368\n",
      "  4398: 2 [ 1400/ 1499], train_loss/perplexity = 5.76870728/320.1236267 secs/batch = 0.6137s, grad.norm=0.51491094\n",
      "  4403: 2 [ 1405/ 1499], train_loss/perplexity = 5.56651068/261.5199890 secs/batch = 0.6316s, grad.norm=0.48366320\n",
      "  4408: 2 [ 1410/ 1499], train_loss/perplexity = 5.81367683/334.8480530 secs/batch = 0.5400s, grad.norm=0.43769479\n",
      "  4413: 2 [ 1415/ 1499], train_loss/perplexity = 5.69772196/298.1873474 secs/batch = 0.5434s, grad.norm=0.49929631\n",
      "  4418: 2 [ 1420/ 1499], train_loss/perplexity = 5.71289158/302.7452087 secs/batch = 0.5438s, grad.norm=0.45826352\n",
      "  4423: 2 [ 1425/ 1499], train_loss/perplexity = 5.63475084/279.9891357 secs/batch = 0.5394s, grad.norm=0.59170544\n",
      "  4428: 2 [ 1430/ 1499], train_loss/perplexity = 5.84628201/345.9457703 secs/batch = 0.5457s, grad.norm=0.48161644\n",
      "  4433: 2 [ 1435/ 1499], train_loss/perplexity = 5.56160355/260.2398071 secs/batch = 0.5445s, grad.norm=0.57937837\n",
      "  4438: 2 [ 1440/ 1499], train_loss/perplexity = 5.38863420/218.9042053 secs/batch = 0.5420s, grad.norm=0.84808308\n",
      "  4443: 2 [ 1445/ 1499], train_loss/perplexity = 5.66434622/288.3993835 secs/batch = 0.5535s, grad.norm=0.43996036\n",
      "  4448: 2 [ 1450/ 1499], train_loss/perplexity = 5.68957281/295.7672424 secs/batch = 0.5350s, grad.norm=0.45204261\n",
      "  4453: 2 [ 1455/ 1499], train_loss/perplexity = 6.00013018/403.4813232 secs/batch = 0.5450s, grad.norm=0.45799240\n",
      "  4458: 2 [ 1460/ 1499], train_loss/perplexity = 5.98465347/397.2848206 secs/batch = 0.5416s, grad.norm=0.46483889\n",
      "  4463: 2 [ 1465/ 1499], train_loss/perplexity = 6.14684439/467.2406311 secs/batch = 0.5398s, grad.norm=0.47606641\n",
      "  4468: 2 [ 1470/ 1499], train_loss/perplexity = 5.80771780/332.8586121 secs/batch = 0.5440s, grad.norm=0.60259962\n",
      "  4473: 2 [ 1475/ 1499], train_loss/perplexity = 5.79866457/329.8587646 secs/batch = 0.5423s, grad.norm=0.47644007\n",
      "  4478: 2 [ 1480/ 1499], train_loss/perplexity = 5.82750940/339.5120239 secs/batch = 0.5513s, grad.norm=0.45868671\n",
      "  4483: 2 [ 1485/ 1499], train_loss/perplexity = 5.54657507/256.3580322 secs/batch = 0.5446s, grad.norm=0.47494313\n",
      "  4488: 2 [ 1490/ 1499], train_loss/perplexity = 5.67105818/290.3416138 secs/batch = 0.5427s, grad.norm=0.47241637\n",
      "  4493: 2 [ 1495/ 1499], train_loss/perplexity = 5.93360996/377.5148621 secs/batch = 0.5403s, grad.norm=0.50085980\n",
      "Epoch training time: 832.8266475200653\n",
      "Saved char model cv/epoch002_5.7780.model\n",
      "  4502: 3 [    5/ 1499], train_loss/perplexity = 5.84393978/345.1364136 secs/batch = 0.5417s, grad.norm=0.45096976\n",
      "  4507: 3 [   10/ 1499], train_loss/perplexity = 5.86090708/351.0424194 secs/batch = 0.5411s, grad.norm=0.50958407\n",
      "  4512: 3 [   15/ 1499], train_loss/perplexity = 5.69251442/296.6385498 secs/batch = 0.5389s, grad.norm=0.53020269\n",
      "  4517: 3 [   20/ 1499], train_loss/perplexity = 5.53995275/254.6659698 secs/batch = 0.5435s, grad.norm=0.43480808\n",
      "  4522: 3 [   25/ 1499], train_loss/perplexity = 5.94380331/381.3826904 secs/batch = 0.5338s, grad.norm=0.49294233\n",
      "  4527: 3 [   30/ 1499], train_loss/perplexity = 5.91848660/371.8485413 secs/batch = 0.5387s, grad.norm=0.52424800\n",
      "  4532: 3 [   35/ 1499], train_loss/perplexity = 5.70510864/300.3981018 secs/batch = 0.5584s, grad.norm=0.47681960\n",
      "  4537: 3 [   40/ 1499], train_loss/perplexity = 5.77096367/320.8467712 secs/batch = 0.5622s, grad.norm=0.49107400\n",
      "  4542: 3 [   45/ 1499], train_loss/perplexity = 5.74984598/314.1422729 secs/batch = 0.5469s, grad.norm=0.48386607\n",
      "  4547: 3 [   50/ 1499], train_loss/perplexity = 5.63931704/281.2705688 secs/batch = 0.5418s, grad.norm=0.50987291\n",
      "  4552: 3 [   55/ 1499], train_loss/perplexity = 5.56853533/262.0499878 secs/batch = 0.5639s, grad.norm=0.47833726\n",
      "  4557: 3 [   60/ 1499], train_loss/perplexity = 5.55812693/259.3366394 secs/batch = 0.5453s, grad.norm=0.47084484\n",
      "  4562: 3 [   65/ 1499], train_loss/perplexity = 5.63698673/280.6158752 secs/batch = 0.5434s, grad.norm=0.57340133\n",
      "  4567: 3 [   70/ 1499], train_loss/perplexity = 5.57954502/264.9510193 secs/batch = 0.5440s, grad.norm=0.54529876\n",
      "  4572: 3 [   75/ 1499], train_loss/perplexity = 5.37154102/215.1942291 secs/batch = 0.5473s, grad.norm=0.56521577\n",
      "  4577: 3 [   80/ 1499], train_loss/perplexity = 5.53664780/253.8256989 secs/batch = 0.5419s, grad.norm=0.51733136\n",
      "  4582: 3 [   85/ 1499], train_loss/perplexity = 5.52520227/250.9370880 secs/batch = 0.5457s, grad.norm=0.60872847\n",
      "  4587: 3 [   90/ 1499], train_loss/perplexity = 5.69345331/296.9172058 secs/batch = 0.5401s, grad.norm=0.51918721\n",
      "  4592: 3 [   95/ 1499], train_loss/perplexity = 5.44399595/231.3648682 secs/batch = 0.5445s, grad.norm=0.53247452\n",
      "  4597: 3 [  100/ 1499], train_loss/perplexity = 5.52385902/250.6002502 secs/batch = 0.5525s, grad.norm=0.50348890\n",
      "  4602: 3 [  105/ 1499], train_loss/perplexity = 5.43691921/229.7333374 secs/batch = 0.5427s, grad.norm=0.49850574\n",
      "  4607: 3 [  110/ 1499], train_loss/perplexity = 5.45009613/232.7805481 secs/batch = 0.5357s, grad.norm=0.63452846\n",
      "  4612: 3 [  115/ 1499], train_loss/perplexity = 5.66085339/287.3937988 secs/batch = 0.5399s, grad.norm=0.50049704\n",
      "  4617: 3 [  120/ 1499], train_loss/perplexity = 5.46031570/235.1716614 secs/batch = 0.5402s, grad.norm=0.44484439\n",
      "  4622: 3 [  125/ 1499], train_loss/perplexity = 5.71795654/304.2825012 secs/batch = 0.5402s, grad.norm=0.56279260\n",
      "  4627: 3 [  130/ 1499], train_loss/perplexity = 5.66452789/288.4517822 secs/batch = 0.5367s, grad.norm=0.54919571\n",
      "  4632: 3 [  135/ 1499], train_loss/perplexity = 5.65197802/284.8543701 secs/batch = 0.5404s, grad.norm=0.49419636\n",
      "  4637: 3 [  140/ 1499], train_loss/perplexity = 5.65491581/285.6924438 secs/batch = 0.5375s, grad.norm=0.57487977\n",
      "  4642: 3 [  145/ 1499], train_loss/perplexity = 5.43420458/229.1105347 secs/batch = 0.5456s, grad.norm=0.57138515\n",
      "  4647: 3 [  150/ 1499], train_loss/perplexity = 5.67770147/292.2768555 secs/batch = 0.5421s, grad.norm=0.66613793\n",
      "  4652: 3 [  155/ 1499], train_loss/perplexity = 5.74172401/311.6011658 secs/batch = 0.5368s, grad.norm=0.45318919\n",
      "  4657: 3 [  160/ 1499], train_loss/perplexity = 5.99713135/402.2731628 secs/batch = 0.5437s, grad.norm=0.45736271\n",
      "  4662: 3 [  165/ 1499], train_loss/perplexity = 5.47657490/239.0266113 secs/batch = 0.5662s, grad.norm=0.46512410\n",
      "  4667: 3 [  170/ 1499], train_loss/perplexity = 5.91297054/369.8030396 secs/batch = 0.5438s, grad.norm=0.56130821\n",
      "  4672: 3 [  175/ 1499], train_loss/perplexity = 5.62915993/278.4281311 secs/batch = 0.5427s, grad.norm=0.45177200\n",
      "  4677: 3 [  180/ 1499], train_loss/perplexity = 5.82773399/339.5882874 secs/batch = 0.5373s, grad.norm=0.45986333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4682: 3 [  185/ 1499], train_loss/perplexity = 5.57537842/263.8493958 secs/batch = 0.5401s, grad.norm=0.46138558\n",
      "  4687: 3 [  190/ 1499], train_loss/perplexity = 5.85907793/350.4009094 secs/batch = 0.5362s, grad.norm=0.49702737\n",
      "  4692: 3 [  195/ 1499], train_loss/perplexity = 5.87798166/357.0877991 secs/batch = 0.5459s, grad.norm=0.48084006\n",
      "  4697: 3 [  200/ 1499], train_loss/perplexity = 5.83228970/341.1388855 secs/batch = 0.5455s, grad.norm=0.58503973\n",
      "  4702: 3 [  205/ 1499], train_loss/perplexity = 5.72810173/307.3852234 secs/batch = 0.5445s, grad.norm=0.44629699\n",
      "  4707: 3 [  210/ 1499], train_loss/perplexity = 5.52109480/249.9084930 secs/batch = 0.5395s, grad.norm=0.50454909\n",
      "  4712: 3 [  215/ 1499], train_loss/perplexity = 5.69145441/296.3242798 secs/batch = 0.5895s, grad.norm=0.55369788\n",
      "  4717: 3 [  220/ 1499], train_loss/perplexity = 5.51014423/247.1867828 secs/batch = 0.5530s, grad.norm=0.49501511\n",
      "  4722: 3 [  225/ 1499], train_loss/perplexity = 5.69811869/298.3056641 secs/batch = 0.5365s, grad.norm=0.53946352\n",
      "  4727: 3 [  230/ 1499], train_loss/perplexity = 5.75318861/315.1940918 secs/batch = 0.5455s, grad.norm=0.56809342\n",
      "  4732: 3 [  235/ 1499], train_loss/perplexity = 5.69197893/296.4797668 secs/batch = 0.5357s, grad.norm=0.51824772\n",
      "  4737: 3 [  240/ 1499], train_loss/perplexity = 5.84859037/346.7452698 secs/batch = 0.5424s, grad.norm=0.44840011\n",
      "  4742: 3 [  245/ 1499], train_loss/perplexity = 5.56874466/262.1048584 secs/batch = 0.5416s, grad.norm=0.65508479\n",
      "  4747: 3 [  250/ 1499], train_loss/perplexity = 5.60786581/272.5619202 secs/batch = 0.5412s, grad.norm=0.46557504\n",
      "  4752: 3 [  255/ 1499], train_loss/perplexity = 5.45622969/234.2127075 secs/batch = 0.5572s, grad.norm=0.49214926\n",
      "  4757: 3 [  260/ 1499], train_loss/perplexity = 5.75754976/316.5717163 secs/batch = 0.5432s, grad.norm=0.50884652\n",
      "  4762: 3 [  265/ 1499], train_loss/perplexity = 5.72186613/305.4744568 secs/batch = 0.5429s, grad.norm=0.55766481\n",
      "  4767: 3 [  270/ 1499], train_loss/perplexity = 5.78884077/326.6341553 secs/batch = 0.5320s, grad.norm=0.48475018\n",
      "  4772: 3 [  275/ 1499], train_loss/perplexity = 5.37679815/216.3285217 secs/batch = 0.5454s, grad.norm=0.58341932\n",
      "  4777: 3 [  280/ 1499], train_loss/perplexity = 5.55456161/258.4136658 secs/batch = 0.5478s, grad.norm=0.57590055\n",
      "  4782: 3 [  285/ 1499], train_loss/perplexity = 6.00100517/403.8345032 secs/batch = 0.5411s, grad.norm=0.49032256\n",
      "  4787: 3 [  290/ 1499], train_loss/perplexity = 5.87213850/355.0073547 secs/batch = 0.5360s, grad.norm=0.45396268\n",
      "  4792: 3 [  295/ 1499], train_loss/perplexity = 5.64637089/283.2615967 secs/batch = 0.5492s, grad.norm=0.48465732\n",
      "  4797: 3 [  300/ 1499], train_loss/perplexity = 5.56906080/262.1877441 secs/batch = 0.5424s, grad.norm=0.47735065\n",
      "  4802: 3 [  305/ 1499], train_loss/perplexity = 5.80253029/331.1363831 secs/batch = 0.5512s, grad.norm=0.51517409\n",
      "  4807: 3 [  310/ 1499], train_loss/perplexity = 5.91417313/370.2480164 secs/batch = 0.5370s, grad.norm=0.53225762\n",
      "  4812: 3 [  315/ 1499], train_loss/perplexity = 5.80016088/330.3526917 secs/batch = 0.5462s, grad.norm=0.44222081\n",
      "  4817: 3 [  320/ 1499], train_loss/perplexity = 5.82004833/336.9883423 secs/batch = 0.5415s, grad.norm=0.73226649\n",
      "  4822: 3 [  325/ 1499], train_loss/perplexity = 5.67512703/291.5253601 secs/batch = 0.5407s, grad.norm=0.54639030\n",
      "  4827: 3 [  330/ 1499], train_loss/perplexity = 5.60968351/273.0578003 secs/batch = 0.5389s, grad.norm=0.57557607\n",
      "  4832: 3 [  335/ 1499], train_loss/perplexity = 5.72461939/306.3166504 secs/batch = 0.5393s, grad.norm=0.56981182\n",
      "  4837: 3 [  340/ 1499], train_loss/perplexity = 5.43976736/230.3885803 secs/batch = 0.5401s, grad.norm=0.44962788\n",
      "  4842: 3 [  345/ 1499], train_loss/perplexity = 5.39837122/221.0460815 secs/batch = 0.5415s, grad.norm=0.46440655\n",
      "  4847: 3 [  350/ 1499], train_loss/perplexity = 5.63942337/281.3004761 secs/batch = 0.5576s, grad.norm=0.62144434\n",
      "  4852: 3 [  355/ 1499], train_loss/perplexity = 5.43121815/228.4273376 secs/batch = 0.5481s, grad.norm=0.55713493\n",
      "  4857: 3 [  360/ 1499], train_loss/perplexity = 5.53984213/254.6378021 secs/batch = 0.5381s, grad.norm=0.58684260\n",
      "  4862: 3 [  365/ 1499], train_loss/perplexity = 5.40216255/221.8857422 secs/batch = 0.5418s, grad.norm=0.49576452\n",
      "  4867: 3 [  370/ 1499], train_loss/perplexity = 5.46693277/236.7329712 secs/batch = 0.5383s, grad.norm=0.56083387\n",
      "  4872: 3 [  375/ 1499], train_loss/perplexity = 5.57897854/264.8009949 secs/batch = 0.5441s, grad.norm=0.53045303\n",
      "  4877: 3 [  380/ 1499], train_loss/perplexity = 5.75512600/315.8053284 secs/batch = 0.5469s, grad.norm=0.55688858\n",
      "  4882: 3 [  385/ 1499], train_loss/perplexity = 5.90082359/365.3382263 secs/batch = 0.5395s, grad.norm=0.50593191\n",
      "  4887: 3 [  390/ 1499], train_loss/perplexity = 5.96339464/388.9281616 secs/batch = 0.5430s, grad.norm=0.56090361\n",
      "  4892: 3 [  395/ 1499], train_loss/perplexity = 5.98615694/397.8825684 secs/batch = 0.5483s, grad.norm=0.53129333\n",
      "  4897: 3 [  400/ 1499], train_loss/perplexity = 5.86112118/351.1175842 secs/batch = 0.5473s, grad.norm=0.70004940\n",
      "  4902: 3 [  405/ 1499], train_loss/perplexity = 5.76845407/320.0426025 secs/batch = 0.5459s, grad.norm=0.46778947\n",
      "  4907: 3 [  410/ 1499], train_loss/perplexity = 5.30668211/201.6799622 secs/batch = 0.5325s, grad.norm=0.69694304\n",
      "  4912: 3 [  415/ 1499], train_loss/perplexity = 5.84419584/345.2248230 secs/batch = 0.5408s, grad.norm=0.51215214\n",
      "  4917: 3 [  420/ 1499], train_loss/perplexity = 5.54184008/255.1470642 secs/batch = 0.5399s, grad.norm=0.47232506\n",
      "  4922: 3 [  425/ 1499], train_loss/perplexity = 5.87242794/355.1101074 secs/batch = 0.5382s, grad.norm=0.49094877\n",
      "  4927: 3 [  430/ 1499], train_loss/perplexity = 5.58330297/265.9485779 secs/batch = 0.5329s, grad.norm=0.48489156\n",
      "  4932: 3 [  435/ 1499], train_loss/perplexity = 5.53763962/254.0775757 secs/batch = 0.5878s, grad.norm=0.46810594\n",
      "  4937: 3 [  440/ 1499], train_loss/perplexity = 5.62679768/277.7711792 secs/batch = 0.5405s, grad.norm=0.56308961\n",
      "  4942: 3 [  445/ 1499], train_loss/perplexity = 5.72802830/307.3626404 secs/batch = 0.5441s, grad.norm=0.51516259\n",
      "  4947: 3 [  450/ 1499], train_loss/perplexity = 5.65918779/286.9155273 secs/batch = 0.5415s, grad.norm=0.53125465\n",
      "  4952: 3 [  455/ 1499], train_loss/perplexity = 5.80496836/331.9447021 secs/batch = 0.5413s, grad.norm=0.46026886\n",
      "  4957: 3 [  460/ 1499], train_loss/perplexity = 5.76891804/320.1911011 secs/batch = 0.5959s, grad.norm=0.57507694\n",
      "  4962: 3 [  465/ 1499], train_loss/perplexity = 5.76464033/318.8243408 secs/batch = 0.5430s, grad.norm=0.61177456\n",
      "  4967: 3 [  470/ 1499], train_loss/perplexity = 5.82733965/339.4544067 secs/batch = 0.5463s, grad.norm=0.47968405\n",
      "  4972: 3 [  475/ 1499], train_loss/perplexity = 5.63322258/279.5615845 secs/batch = 0.5438s, grad.norm=0.51286316\n",
      "  4977: 3 [  480/ 1499], train_loss/perplexity = 5.75939846/317.1575012 secs/batch = 0.5424s, grad.norm=0.50668705\n",
      "  4982: 3 [  485/ 1499], train_loss/perplexity = 5.72825861/307.4334412 secs/batch = 0.5428s, grad.norm=0.48631239\n",
      "  4987: 3 [  490/ 1499], train_loss/perplexity = 5.62089586/276.1366577 secs/batch = 0.5458s, grad.norm=0.50803643\n",
      "  4992: 3 [  495/ 1499], train_loss/perplexity = 5.65831327/286.6647034 secs/batch = 0.5414s, grad.norm=0.56016713\n",
      "  4997: 3 [  500/ 1499], train_loss/perplexity = 5.73768616/310.3454895 secs/batch = 0.5434s, grad.norm=0.48287961\n",
      "  5002: 3 [  505/ 1499], train_loss/perplexity = 5.51749992/249.0117035 secs/batch = 0.5440s, grad.norm=0.62333244\n",
      "  5007: 3 [  510/ 1499], train_loss/perplexity = 5.87868738/357.3398743 secs/batch = 0.5500s, grad.norm=0.52035391\n",
      "  5012: 3 [  515/ 1499], train_loss/perplexity = 5.61230421/273.7743530 secs/batch = 0.5442s, grad.norm=0.66706026\n",
      "  5017: 3 [  520/ 1499], train_loss/perplexity = 5.82370806/338.2238770 secs/batch = 0.5414s, grad.norm=0.56390798\n",
      "  5022: 3 [  525/ 1499], train_loss/perplexity = 5.91369057/370.0693970 secs/batch = 0.5477s, grad.norm=0.56716275\n",
      "  5027: 3 [  530/ 1499], train_loss/perplexity = 5.68640614/294.8321228 secs/batch = 0.5429s, grad.norm=0.49570203\n",
      "  5032: 3 [  535/ 1499], train_loss/perplexity = 5.67684698/292.0272217 secs/batch = 0.5354s, grad.norm=0.50856400\n",
      "  5037: 3 [  540/ 1499], train_loss/perplexity = 5.53464222/253.3171387 secs/batch = 0.5411s, grad.norm=0.51333088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5042: 3 [  545/ 1499], train_loss/perplexity = 5.30772686/201.8907776 secs/batch = 0.5370s, grad.norm=0.55924791\n",
      "  5047: 3 [  550/ 1499], train_loss/perplexity = 5.75231600/314.9191589 secs/batch = 0.5431s, grad.norm=0.56208599\n",
      "  5052: 3 [  555/ 1499], train_loss/perplexity = 5.54132462/255.0155792 secs/batch = 0.5413s, grad.norm=0.51101333\n",
      "  5057: 3 [  560/ 1499], train_loss/perplexity = 5.93056488/376.3670654 secs/batch = 0.5470s, grad.norm=0.52870917\n",
      "  5062: 3 [  565/ 1499], train_loss/perplexity = 5.78970432/326.9163513 secs/batch = 0.5393s, grad.norm=0.50941259\n",
      "  5067: 3 [  570/ 1499], train_loss/perplexity = 5.75434971/315.5602722 secs/batch = 0.5432s, grad.norm=0.48101228\n",
      "  5072: 3 [  575/ 1499], train_loss/perplexity = 5.77391195/321.7941284 secs/batch = 0.5389s, grad.norm=0.50528854\n",
      "  5077: 3 [  580/ 1499], train_loss/perplexity = 5.55038595/257.3368530 secs/batch = 0.5426s, grad.norm=0.66021574\n",
      "  5082: 3 [  585/ 1499], train_loss/perplexity = 5.44460630/231.5061188 secs/batch = 0.5387s, grad.norm=0.49454716\n",
      "  5087: 3 [  590/ 1499], train_loss/perplexity = 5.66043282/287.2729492 secs/batch = 0.5375s, grad.norm=0.56404561\n",
      "  5092: 3 [  595/ 1499], train_loss/perplexity = 5.60562372/271.9515076 secs/batch = 0.5350s, grad.norm=0.47614950\n",
      "  5097: 3 [  600/ 1499], train_loss/perplexity = 5.45429039/233.7589417 secs/batch = 0.5451s, grad.norm=0.54408187\n",
      "  5102: 3 [  605/ 1499], train_loss/perplexity = 5.46996260/237.4513092 secs/batch = 0.5409s, grad.norm=0.50125962\n",
      "  5107: 3 [  610/ 1499], train_loss/perplexity = 5.72102976/305.2190552 secs/batch = 0.5459s, grad.norm=0.50572675\n",
      "  5112: 3 [  615/ 1499], train_loss/perplexity = 5.74135685/311.4867554 secs/batch = 0.5495s, grad.norm=0.46888542\n",
      "  5117: 3 [  620/ 1499], train_loss/perplexity = 5.57683802/264.2347717 secs/batch = 0.5440s, grad.norm=0.52136588\n",
      "  5122: 3 [  625/ 1499], train_loss/perplexity = 5.59020615/267.7908325 secs/batch = 0.5422s, grad.norm=0.54548019\n",
      "  5127: 3 [  630/ 1499], train_loss/perplexity = 5.69264889/296.6784363 secs/batch = 0.5417s, grad.norm=0.52574152\n",
      "  5132: 3 [  635/ 1499], train_loss/perplexity = 5.48720074/241.5800171 secs/batch = 0.5379s, grad.norm=0.67036712\n",
      "  5137: 3 [  640/ 1499], train_loss/perplexity = 5.73534441/309.6195984 secs/batch = 0.5366s, grad.norm=0.54864615\n",
      "  5142: 3 [  645/ 1499], train_loss/perplexity = 5.49059534/242.4014740 secs/batch = 0.5434s, grad.norm=0.80909634\n",
      "  5147: 3 [  650/ 1499], train_loss/perplexity = 5.42579842/227.1926727 secs/batch = 0.5430s, grad.norm=0.59563893\n",
      "  5152: 3 [  655/ 1499], train_loss/perplexity = 5.75456381/315.6278381 secs/batch = 0.5415s, grad.norm=0.47735581\n",
      "  5157: 3 [  660/ 1499], train_loss/perplexity = 5.91111469/369.1173706 secs/batch = 0.5497s, grad.norm=0.58360755\n",
      "  5162: 3 [  665/ 1499], train_loss/perplexity = 5.84355211/345.0026550 secs/batch = 0.5353s, grad.norm=0.59925032\n",
      "  5167: 3 [  670/ 1499], train_loss/perplexity = 5.83477831/341.9889221 secs/batch = 0.5438s, grad.norm=0.50897926\n",
      "  5172: 3 [  675/ 1499], train_loss/perplexity = 5.73521900/309.5807495 secs/batch = 0.5388s, grad.norm=0.52672660\n",
      "  5177: 3 [  680/ 1499], train_loss/perplexity = 5.70021009/298.9302063 secs/batch = 0.5405s, grad.norm=0.49955335\n",
      "  5182: 3 [  685/ 1499], train_loss/perplexity = 5.79779911/329.5733948 secs/batch = 0.5397s, grad.norm=0.47754478\n",
      "  5187: 3 [  690/ 1499], train_loss/perplexity = 5.72058153/305.0822754 secs/batch = 0.5416s, grad.norm=0.55118847\n",
      "  5192: 3 [  695/ 1499], train_loss/perplexity = 5.63823462/280.9662781 secs/batch = 0.5392s, grad.norm=0.56602478\n",
      "  5197: 3 [  700/ 1499], train_loss/perplexity = 5.85307598/348.3041077 secs/batch = 0.5371s, grad.norm=0.55915201\n",
      "  5202: 3 [  705/ 1499], train_loss/perplexity = 5.38980770/219.1612396 secs/batch = 0.5459s, grad.norm=0.54128683\n",
      "  5207: 3 [  710/ 1499], train_loss/perplexity = 5.65288401/285.1125488 secs/batch = 0.5440s, grad.norm=0.46699095\n",
      "  5212: 3 [  715/ 1499], train_loss/perplexity = 5.67018270/290.0875244 secs/batch = 0.5348s, grad.norm=0.52296519\n",
      "  5217: 3 [  720/ 1499], train_loss/perplexity = 5.72176170/305.4425354 secs/batch = 0.5416s, grad.norm=0.53436983\n",
      "  5222: 3 [  725/ 1499], train_loss/perplexity = 5.56291199/260.5805359 secs/batch = 0.5428s, grad.norm=0.60907412\n",
      "  5227: 3 [  730/ 1499], train_loss/perplexity = 5.72892475/307.6383057 secs/batch = 0.5855s, grad.norm=0.63890737\n",
      "  5232: 3 [  735/ 1499], train_loss/perplexity = 5.52736759/251.4810333 secs/batch = 0.5429s, grad.norm=0.51960576\n",
      "  5237: 3 [  740/ 1499], train_loss/perplexity = 5.51293612/247.8778534 secs/batch = 0.5460s, grad.norm=0.49316904\n",
      "  5242: 3 [  745/ 1499], train_loss/perplexity = 5.36959505/214.7758789 secs/batch = 0.5362s, grad.norm=0.53347123\n",
      "  5247: 3 [  750/ 1499], train_loss/perplexity = 5.64687490/283.4044189 secs/batch = 0.5414s, grad.norm=0.51979101\n",
      "  5252: 3 [  755/ 1499], train_loss/perplexity = 5.47931433/239.6823120 secs/batch = 0.5460s, grad.norm=0.63608670\n",
      "  5257: 3 [  760/ 1499], train_loss/perplexity = 5.13157272/169.2831421 secs/batch = 0.5487s, grad.norm=0.63938659\n",
      "  5262: 3 [  765/ 1499], train_loss/perplexity = 5.49632215/243.7936401 secs/batch = 0.5446s, grad.norm=0.60815495\n",
      "  5267: 3 [  770/ 1499], train_loss/perplexity = 5.62386084/276.9566040 secs/batch = 0.5409s, grad.norm=0.55827826\n",
      "  5272: 3 [  775/ 1499], train_loss/perplexity = 5.90879822/368.2633057 secs/batch = 0.5417s, grad.norm=0.55813891\n",
      "  5277: 3 [  780/ 1499], train_loss/perplexity = 5.42498446/227.0078125 secs/batch = 0.5449s, grad.norm=0.50218409\n",
      "  5282: 3 [  785/ 1499], train_loss/perplexity = 5.57860041/264.7008667 secs/batch = 0.5404s, grad.norm=0.53438079\n",
      "  5287: 3 [  790/ 1499], train_loss/perplexity = 5.19615221/180.5760803 secs/batch = 0.5426s, grad.norm=0.55111319\n",
      "  5292: 3 [  795/ 1499], train_loss/perplexity = 5.69443178/297.2078552 secs/batch = 0.5395s, grad.norm=0.56232852\n",
      "  5297: 3 [  800/ 1499], train_loss/perplexity = 5.53908110/254.4440765 secs/batch = 0.5387s, grad.norm=0.58450240\n",
      "  5302: 3 [  805/ 1499], train_loss/perplexity = 5.58711910/266.9654236 secs/batch = 0.5519s, grad.norm=0.52928454\n",
      "  5307: 3 [  810/ 1499], train_loss/perplexity = 5.52804852/251.6523285 secs/batch = 0.5437s, grad.norm=0.48987943\n",
      "  5312: 3 [  815/ 1499], train_loss/perplexity = 5.67691088/292.0458679 secs/batch = 0.5410s, grad.norm=0.51645964\n",
      "  5317: 3 [  820/ 1499], train_loss/perplexity = 5.23449659/187.6346283 secs/batch = 0.5367s, grad.norm=0.50188708\n",
      "  5322: 3 [  825/ 1499], train_loss/perplexity = 5.41628981/225.0426178 secs/batch = 0.5461s, grad.norm=0.66996479\n",
      "  5327: 3 [  830/ 1499], train_loss/perplexity = 5.65952396/287.0119934 secs/batch = 0.5408s, grad.norm=0.59365135\n",
      "  5332: 3 [  835/ 1499], train_loss/perplexity = 5.64029360/281.5453796 secs/batch = 0.5483s, grad.norm=0.61159402\n",
      "  5337: 3 [  840/ 1499], train_loss/perplexity = 5.54458141/255.8474579 secs/batch = 0.5366s, grad.norm=0.54753846\n",
      "  5342: 3 [  845/ 1499], train_loss/perplexity = 5.45802593/234.6337891 secs/batch = 0.5370s, grad.norm=0.66009635\n",
      "  5347: 3 [  850/ 1499], train_loss/perplexity = 5.55171347/257.6787109 secs/batch = 0.5394s, grad.norm=0.60262507\n",
      "  5352: 3 [  855/ 1499], train_loss/perplexity = 5.66822910/289.5213623 secs/batch = 0.5386s, grad.norm=0.52959162\n",
      "  5357: 3 [  860/ 1499], train_loss/perplexity = 5.17805576/177.3376923 secs/batch = 0.5369s, grad.norm=0.63034940\n",
      "  5362: 3 [  865/ 1499], train_loss/perplexity = 5.50815725/246.6961060 secs/batch = 0.5416s, grad.norm=0.51997513\n",
      "  5367: 3 [  870/ 1499], train_loss/perplexity = 5.49200726/242.7439728 secs/batch = 0.5364s, grad.norm=0.53551149\n",
      "  5372: 3 [  875/ 1499], train_loss/perplexity = 5.51569939/248.5637512 secs/batch = 0.5357s, grad.norm=0.51149476\n",
      "  5377: 3 [  880/ 1499], train_loss/perplexity = 5.43242836/228.7039490 secs/batch = 0.5451s, grad.norm=0.51443428\n",
      "  5382: 3 [  885/ 1499], train_loss/perplexity = 5.36536884/213.8701019 secs/batch = 0.5761s, grad.norm=0.62876439\n",
      "  5387: 3 [  890/ 1499], train_loss/perplexity = 5.65750790/286.4339294 secs/batch = 0.5452s, grad.norm=0.53448528\n",
      "  5392: 3 [  895/ 1499], train_loss/perplexity = 5.68950558/295.7473755 secs/batch = 0.5402s, grad.norm=0.50195438\n",
      "  5397: 3 [  900/ 1499], train_loss/perplexity = 5.45343924/233.5600586 secs/batch = 0.5403s, grad.norm=0.53052783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5402: 3 [  905/ 1499], train_loss/perplexity = 5.56944132/262.2875366 secs/batch = 0.5392s, grad.norm=0.58234221\n",
      "  5407: 3 [  910/ 1499], train_loss/perplexity = 5.72072554/305.1262207 secs/batch = 0.5473s, grad.norm=0.57953745\n",
      "  5412: 3 [  915/ 1499], train_loss/perplexity = 5.48584890/241.2536621 secs/batch = 0.5523s, grad.norm=0.59361750\n",
      "  5417: 3 [  920/ 1499], train_loss/perplexity = 5.19138479/179.7172546 secs/batch = 0.5415s, grad.norm=0.46387133\n",
      "  5422: 3 [  925/ 1499], train_loss/perplexity = 5.62708712/277.8515930 secs/batch = 0.5383s, grad.norm=0.65862906\n",
      "  5427: 3 [  930/ 1499], train_loss/perplexity = 5.56869030/262.0906067 secs/batch = 0.5473s, grad.norm=0.57407534\n",
      "  5432: 3 [  935/ 1499], train_loss/perplexity = 5.66202450/287.7305603 secs/batch = 0.5414s, grad.norm=0.52726960\n",
      "  5437: 3 [  940/ 1499], train_loss/perplexity = 5.28040934/196.4502716 secs/batch = 0.5382s, grad.norm=0.62236583\n",
      "  5442: 3 [  945/ 1499], train_loss/perplexity = 5.65296650/285.1360779 secs/batch = 0.5410s, grad.norm=0.52058882\n",
      "  5447: 3 [  950/ 1499], train_loss/perplexity = 5.56676197/261.5856934 secs/batch = 0.5469s, grad.norm=0.50892413\n",
      "  5452: 3 [  955/ 1499], train_loss/perplexity = 5.71200800/302.4778442 secs/batch = 0.5498s, grad.norm=0.48246646\n",
      "  5457: 3 [  960/ 1499], train_loss/perplexity = 5.45592308/234.1408997 secs/batch = 0.5466s, grad.norm=0.53849906\n",
      "  5462: 3 [  965/ 1499], train_loss/perplexity = 5.55439615/258.3709106 secs/batch = 0.5407s, grad.norm=0.70467055\n",
      "  5467: 3 [  970/ 1499], train_loss/perplexity = 5.77685928/322.7439575 secs/batch = 0.5402s, grad.norm=0.50380707\n",
      "  5472: 3 [  975/ 1499], train_loss/perplexity = 5.55975866/259.7601318 secs/batch = 0.5408s, grad.norm=0.54694128\n",
      "  5477: 3 [  980/ 1499], train_loss/perplexity = 5.62896776/278.3746338 secs/batch = 0.5413s, grad.norm=0.62610292\n",
      "  5482: 3 [  985/ 1499], train_loss/perplexity = 5.33875895/208.2540894 secs/batch = 0.5428s, grad.norm=0.50052315\n",
      "  5487: 3 [  990/ 1499], train_loss/perplexity = 5.48510265/241.0736847 secs/batch = 0.5392s, grad.norm=0.49243358\n",
      "  5492: 3 [  995/ 1499], train_loss/perplexity = 5.75991917/317.3226624 secs/batch = 0.5393s, grad.norm=0.59229243\n",
      "  5497: 3 [ 1000/ 1499], train_loss/perplexity = 5.42783356/227.6555023 secs/batch = 0.5769s, grad.norm=0.49957874\n",
      "  5502: 3 [ 1005/ 1499], train_loss/perplexity = 5.82017040/337.0294800 secs/batch = 0.5342s, grad.norm=0.54406041\n",
      "  5507: 3 [ 1010/ 1499], train_loss/perplexity = 5.70748949/301.1141663 secs/batch = 0.5533s, grad.norm=0.57764763\n",
      "  5512: 3 [ 1015/ 1499], train_loss/perplexity = 5.43952513/230.3327789 secs/batch = 0.5389s, grad.norm=0.50335336\n",
      "  5517: 3 [ 1020/ 1499], train_loss/perplexity = 5.56287718/260.5714722 secs/batch = 0.5408s, grad.norm=0.59105217\n",
      "  5522: 3 [ 1025/ 1499], train_loss/perplexity = 5.61499643/274.5123901 secs/batch = 0.5389s, grad.norm=0.68251199\n",
      "  5527: 3 [ 1030/ 1499], train_loss/perplexity = 5.71631384/303.7830505 secs/batch = 0.5409s, grad.norm=0.56451494\n",
      "  5532: 3 [ 1035/ 1499], train_loss/perplexity = 5.95155191/384.3493652 secs/batch = 0.5406s, grad.norm=0.57802230\n",
      "  5537: 3 [ 1040/ 1499], train_loss/perplexity = 5.37759638/216.5012665 secs/batch = 0.5415s, grad.norm=0.49928468\n",
      "  5542: 3 [ 1045/ 1499], train_loss/perplexity = 5.50873947/246.8397827 secs/batch = 0.5368s, grad.norm=0.51650381\n",
      "  5547: 3 [ 1050/ 1499], train_loss/perplexity = 5.19584417/180.5204620 secs/batch = 0.5435s, grad.norm=0.51458544\n",
      "  5552: 3 [ 1055/ 1499], train_loss/perplexity = 5.59849596/270.0199890 secs/batch = 0.5343s, grad.norm=0.61633331\n",
      "  5557: 3 [ 1060/ 1499], train_loss/perplexity = 5.76320076/318.3657227 secs/batch = 0.5426s, grad.norm=0.56936407\n",
      "  5562: 3 [ 1065/ 1499], train_loss/perplexity = 5.33691883/207.8712311 secs/batch = 0.5390s, grad.norm=0.55322891\n",
      "  5567: 3 [ 1070/ 1499], train_loss/perplexity = 5.78114891/324.1313782 secs/batch = 0.5372s, grad.norm=0.52494931\n",
      "  5572: 3 [ 1075/ 1499], train_loss/perplexity = 5.66015530/287.1932373 secs/batch = 0.5412s, grad.norm=0.57356465\n",
      "  5577: 3 [ 1080/ 1499], train_loss/perplexity = 5.84664869/346.0726318 secs/batch = 0.5419s, grad.norm=0.53630179\n",
      "  5582: 3 [ 1085/ 1499], train_loss/perplexity = 5.93878984/379.4754333 secs/batch = 0.5378s, grad.norm=0.53449416\n",
      "  5587: 3 [ 1090/ 1499], train_loss/perplexity = 5.49773455/244.1382141 secs/batch = 0.5388s, grad.norm=0.50963181\n",
      "  5592: 3 [ 1095/ 1499], train_loss/perplexity = 5.65194273/284.8442993 secs/batch = 0.5409s, grad.norm=0.48659292\n",
      "  5597: 3 [ 1100/ 1499], train_loss/perplexity = 5.72338581/305.9390259 secs/batch = 0.5413s, grad.norm=0.72809875\n",
      "  5602: 3 [ 1105/ 1499], train_loss/perplexity = 5.42948961/228.0328369 secs/batch = 0.5688s, grad.norm=0.52530599\n",
      "  5607: 3 [ 1110/ 1499], train_loss/perplexity = 5.61549330/274.6488342 secs/batch = 0.5482s, grad.norm=0.52142972\n",
      "  5612: 3 [ 1115/ 1499], train_loss/perplexity = 5.24003029/188.6758118 secs/batch = 0.5399s, grad.norm=0.55584133\n",
      "  5617: 3 [ 1120/ 1499], train_loss/perplexity = 5.45095778/232.9812012 secs/batch = 0.5402s, grad.norm=0.51809776\n",
      "  5622: 3 [ 1125/ 1499], train_loss/perplexity = 5.42459774/226.9200439 secs/batch = 0.5396s, grad.norm=0.53059661\n",
      "  5627: 3 [ 1130/ 1499], train_loss/perplexity = 5.22653484/186.1466522 secs/batch = 0.5469s, grad.norm=0.47845149\n",
      "  5632: 3 [ 1135/ 1499], train_loss/perplexity = 5.83067226/340.5875549 secs/batch = 0.5458s, grad.norm=0.48607072\n",
      "  5637: 3 [ 1140/ 1499], train_loss/perplexity = 5.71678400/303.9259338 secs/batch = 0.5418s, grad.norm=0.61493754\n",
      "  5642: 3 [ 1145/ 1499], train_loss/perplexity = 5.75410509/315.4830933 secs/batch = 0.5417s, grad.norm=0.54053962\n",
      "  5647: 3 [ 1150/ 1499], train_loss/perplexity = 5.68071222/293.1581421 secs/batch = 0.5399s, grad.norm=0.50791967\n",
      "  5652: 3 [ 1155/ 1499], train_loss/perplexity = 5.37542057/216.0307159 secs/batch = 0.5391s, grad.norm=0.54876715\n",
      "  5657: 3 [ 1160/ 1499], train_loss/perplexity = 5.54550648/256.0842590 secs/batch = 0.5456s, grad.norm=0.51614404\n",
      "  5662: 3 [ 1165/ 1499], train_loss/perplexity = 5.48205662/240.3404846 secs/batch = 0.5469s, grad.norm=0.59098828\n",
      "  5667: 3 [ 1170/ 1499], train_loss/perplexity = 5.72174406/305.4371643 secs/batch = 0.5440s, grad.norm=0.56707734\n",
      "  5672: 3 [ 1175/ 1499], train_loss/perplexity = 5.12265921/167.7809448 secs/batch = 0.5438s, grad.norm=0.60604030\n",
      "  5677: 3 [ 1180/ 1499], train_loss/perplexity = 5.41378117/224.4787750 secs/batch = 0.5475s, grad.norm=0.58640325\n",
      "  5682: 3 [ 1185/ 1499], train_loss/perplexity = 5.35184240/210.9966736 secs/batch = 0.5371s, grad.norm=0.61154056\n",
      "  5687: 3 [ 1190/ 1499], train_loss/perplexity = 5.57752037/264.4151306 secs/batch = 0.5331s, grad.norm=0.55871028\n",
      "  5692: 3 [ 1195/ 1499], train_loss/perplexity = 5.62399244/276.9930725 secs/batch = 0.5415s, grad.norm=0.56754827\n",
      "  5697: 3 [ 1200/ 1499], train_loss/perplexity = 5.49875546/244.3875885 secs/batch = 0.5357s, grad.norm=0.68462759\n",
      "  5702: 3 [ 1205/ 1499], train_loss/perplexity = 5.43284988/228.8003693 secs/batch = 0.5375s, grad.norm=0.62269950\n",
      "  5707: 3 [ 1210/ 1499], train_loss/perplexity = 5.39960527/221.3190460 secs/batch = 0.5422s, grad.norm=0.58615297\n",
      "  5712: 3 [ 1215/ 1499], train_loss/perplexity = 5.08522940/161.6170044 secs/batch = 0.5450s, grad.norm=0.58603758\n",
      "  5717: 3 [ 1220/ 1499], train_loss/perplexity = 5.48404980/240.8200073 secs/batch = 0.5389s, grad.norm=0.56519300\n",
      "  5722: 3 [ 1225/ 1499], train_loss/perplexity = 4.95992613/142.5832672 secs/batch = 0.5420s, grad.norm=0.68806249\n",
      "  5727: 3 [ 1230/ 1499], train_loss/perplexity = 5.50497198/245.9115601 secs/batch = 0.5361s, grad.norm=0.55845094\n",
      "  5732: 3 [ 1235/ 1499], train_loss/perplexity = 5.32447863/205.3013000 secs/batch = 0.5364s, grad.norm=0.63071799\n",
      "  5737: 3 [ 1240/ 1499], train_loss/perplexity = 5.38492966/218.0947571 secs/batch = 0.5442s, grad.norm=0.56442332\n",
      "  5742: 3 [ 1245/ 1499], train_loss/perplexity = 5.65553904/285.8705444 secs/batch = 0.5369s, grad.norm=0.50909817\n",
      "  5747: 3 [ 1250/ 1499], train_loss/perplexity = 5.88538551/359.7414246 secs/batch = 0.5428s, grad.norm=0.57751697\n",
      "  5752: 3 [ 1255/ 1499], train_loss/perplexity = 5.46918440/237.2666016 secs/batch = 0.5403s, grad.norm=0.54519761\n",
      "  5757: 3 [ 1260/ 1499], train_loss/perplexity = 5.53287983/252.8710938 secs/batch = 0.5437s, grad.norm=0.50159240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5762: 3 [ 1265/ 1499], train_loss/perplexity = 5.59605455/269.3615417 secs/batch = 0.5438s, grad.norm=0.48422015\n",
      "  5767: 3 [ 1270/ 1499], train_loss/perplexity = 5.57837152/264.6402893 secs/batch = 0.5966s, grad.norm=0.51380485\n",
      "  5772: 3 [ 1275/ 1499], train_loss/perplexity = 5.59074068/267.9339905 secs/batch = 0.5399s, grad.norm=0.58012235\n",
      "  5777: 3 [ 1280/ 1499], train_loss/perplexity = 5.22716475/186.2639465 secs/batch = 0.5368s, grad.norm=0.58040404\n",
      "  5782: 3 [ 1285/ 1499], train_loss/perplexity = 5.67457438/291.3642883 secs/batch = 0.5413s, grad.norm=0.60756665\n",
      "  5787: 3 [ 1290/ 1499], train_loss/perplexity = 5.61714792/275.1036377 secs/batch = 0.5387s, grad.norm=0.58219230\n",
      "  5792: 3 [ 1295/ 1499], train_loss/perplexity = 5.49642706/243.8192291 secs/batch = 0.5413s, grad.norm=0.51811647\n",
      "  5797: 3 [ 1300/ 1499], train_loss/perplexity = 5.68850279/295.4509277 secs/batch = 0.5385s, grad.norm=0.49984932\n",
      "  5802: 3 [ 1305/ 1499], train_loss/perplexity = 5.56954527/262.3147888 secs/batch = 0.5382s, grad.norm=0.59630805\n",
      "  5807: 3 [ 1310/ 1499], train_loss/perplexity = 5.64448261/282.7272339 secs/batch = 0.5457s, grad.norm=0.50513268\n",
      "  5812: 3 [ 1315/ 1499], train_loss/perplexity = 5.50042057/244.7948608 secs/batch = 0.5430s, grad.norm=0.69359660\n",
      "  5817: 3 [ 1320/ 1499], train_loss/perplexity = 5.19298601/180.0052490 secs/batch = 0.5434s, grad.norm=0.55668175\n",
      "  5822: 3 [ 1325/ 1499], train_loss/perplexity = 5.55327892/258.0823975 secs/batch = 0.5493s, grad.norm=0.54161888\n",
      "  5827: 3 [ 1330/ 1499], train_loss/perplexity = 5.69126654/296.2686157 secs/batch = 0.5377s, grad.norm=0.64126396\n",
      "  5832: 3 [ 1335/ 1499], train_loss/perplexity = 5.50494289/245.9044189 secs/batch = 0.5368s, grad.norm=0.55928552\n",
      "  5837: 3 [ 1340/ 1499], train_loss/perplexity = 4.96684933/143.5738220 secs/batch = 0.5407s, grad.norm=0.56869924\n",
      "  5842: 3 [ 1345/ 1499], train_loss/perplexity = 5.16903067/175.7444000 secs/batch = 0.5482s, grad.norm=0.66126108\n",
      "  5847: 3 [ 1350/ 1499], train_loss/perplexity = 5.26882219/194.1871185 secs/batch = 0.5439s, grad.norm=0.50971216\n",
      "  5852: 3 [ 1355/ 1499], train_loss/perplexity = 5.07398939/159.8106079 secs/batch = 0.5391s, grad.norm=0.68192774\n",
      "  5857: 3 [ 1360/ 1499], train_loss/perplexity = 5.13356352/169.6204834 secs/batch = 0.5466s, grad.norm=0.51464742\n",
      "  5862: 3 [ 1365/ 1499], train_loss/perplexity = 5.04599667/155.3991089 secs/batch = 0.5482s, grad.norm=0.52625948\n",
      "  5867: 3 [ 1370/ 1499], train_loss/perplexity = 5.01537991/150.7133789 secs/batch = 0.5425s, grad.norm=0.52419317\n",
      "  5872: 3 [ 1375/ 1499], train_loss/perplexity = 5.09824514/163.7343292 secs/batch = 0.5514s, grad.norm=0.65914863\n",
      "  5877: 3 [ 1380/ 1499], train_loss/perplexity = 5.53606892/253.6788025 secs/batch = 0.5432s, grad.norm=0.59510392\n",
      "  5882: 3 [ 1385/ 1499], train_loss/perplexity = 5.29935455/200.2075500 secs/batch = 0.5448s, grad.norm=0.52551323\n",
      "  5887: 3 [ 1390/ 1499], train_loss/perplexity = 5.33517313/207.5086670 secs/batch = 0.5449s, grad.norm=0.49870360\n",
      "  5892: 3 [ 1395/ 1499], train_loss/perplexity = 5.50408411/245.6933289 secs/batch = 0.5422s, grad.norm=0.48887309\n",
      "  5897: 3 [ 1400/ 1499], train_loss/perplexity = 5.52265739/250.2993011 secs/batch = 0.5432s, grad.norm=0.57325035\n",
      "  5902: 3 [ 1405/ 1499], train_loss/perplexity = 5.36182547/213.1136169 secs/batch = 0.5414s, grad.norm=0.52183163\n",
      "  5907: 3 [ 1410/ 1499], train_loss/perplexity = 5.61089182/273.3879395 secs/batch = 0.5445s, grad.norm=0.50791925\n",
      "  5912: 3 [ 1415/ 1499], train_loss/perplexity = 5.50720644/246.4616547 secs/batch = 0.5535s, grad.norm=0.55946594\n",
      "  5917: 3 [ 1420/ 1499], train_loss/perplexity = 5.49408293/243.2483521 secs/batch = 0.5433s, grad.norm=0.50835973\n",
      "  5922: 3 [ 1425/ 1499], train_loss/perplexity = 5.39794111/220.9510345 secs/batch = 0.5457s, grad.norm=0.60588568\n",
      "  5927: 3 [ 1430/ 1499], train_loss/perplexity = 5.60422468/271.5712891 secs/batch = 0.5362s, grad.norm=0.52459383\n",
      "  5932: 3 [ 1435/ 1499], train_loss/perplexity = 5.28330231/197.0194244 secs/batch = 0.5422s, grad.norm=0.56223297\n",
      "  5937: 3 [ 1440/ 1499], train_loss/perplexity = 5.00197935/148.7072144 secs/batch = 0.5411s, grad.norm=0.51436239\n",
      "  5942: 3 [ 1445/ 1499], train_loss/perplexity = 5.44563437/231.7442474 secs/batch = 0.5369s, grad.norm=0.61688989\n",
      "  5947: 3 [ 1450/ 1499], train_loss/perplexity = 5.50738430/246.5054932 secs/batch = 0.5419s, grad.norm=0.61654305\n",
      "  5952: 3 [ 1455/ 1499], train_loss/perplexity = 5.78301287/324.7361145 secs/batch = 0.5464s, grad.norm=0.53499854\n",
      "  5957: 3 [ 1460/ 1499], train_loss/perplexity = 5.76950073/320.3777466 secs/batch = 0.5530s, grad.norm=0.49560508\n",
      "  5962: 3 [ 1465/ 1499], train_loss/perplexity = 5.90678310/367.5219727 secs/batch = 0.5377s, grad.norm=0.51659501\n",
      "  5967: 3 [ 1470/ 1499], train_loss/perplexity = 5.57138300/262.7973022 secs/batch = 0.5415s, grad.norm=0.59964716\n",
      "  5972: 3 [ 1475/ 1499], train_loss/perplexity = 5.59959602/270.3171692 secs/batch = 0.5337s, grad.norm=0.50458288\n",
      "  5977: 3 [ 1480/ 1499], train_loss/perplexity = 5.59690952/269.5919495 secs/batch = 0.5424s, grad.norm=0.50908387\n",
      "  5982: 3 [ 1485/ 1499], train_loss/perplexity = 5.36680079/214.1765747 secs/batch = 0.5435s, grad.norm=0.65072000\n",
      "  5987: 3 [ 1490/ 1499], train_loss/perplexity = 5.44047260/230.5511169 secs/batch = 0.5407s, grad.norm=0.51260817\n",
      "  5992: 3 [ 1495/ 1499], train_loss/perplexity = 5.74737549/313.3671570 secs/batch = 0.5388s, grad.norm=0.55304027\n",
      "Epoch training time: 815.8495335578918\n",
      "Saved char model cv/epoch003_5.5638.model\n",
      "  6001: 4 [    5/ 1499], train_loss/perplexity = 5.63415623/279.8227234 secs/batch = 0.6153s, grad.norm=0.54224586\n",
      "  6006: 4 [   10/ 1499], train_loss/perplexity = 5.62969398/278.5768433 secs/batch = 0.6451s, grad.norm=0.53400046\n",
      "  6011: 4 [   15/ 1499], train_loss/perplexity = 5.49131584/242.5761871 secs/batch = 0.5686s, grad.norm=0.72756034\n",
      "  6016: 4 [   20/ 1499], train_loss/perplexity = 5.36019516/212.7664642 secs/batch = 0.5405s, grad.norm=0.49679118\n",
      "  6021: 4 [   25/ 1499], train_loss/perplexity = 5.77708721/322.8175354 secs/batch = 0.5419s, grad.norm=0.56842190\n",
      "  6026: 4 [   30/ 1499], train_loss/perplexity = 5.70085764/299.1238403 secs/batch = 0.5409s, grad.norm=0.59952074\n",
      "  6031: 4 [   35/ 1499], train_loss/perplexity = 5.54838324/256.8219910 secs/batch = 0.5729s, grad.norm=0.51538563\n",
      "  6036: 4 [   40/ 1499], train_loss/perplexity = 5.54300642/255.4448242 secs/batch = 0.5605s, grad.norm=0.51672941\n",
      "  6041: 4 [   45/ 1499], train_loss/perplexity = 5.52350521/250.5115967 secs/batch = 0.5625s, grad.norm=0.55909425\n",
      "  6046: 4 [   50/ 1499], train_loss/perplexity = 5.44219160/230.9477692 secs/batch = 0.5510s, grad.norm=0.52502894\n",
      "  6051: 4 [   55/ 1499], train_loss/perplexity = 5.38427877/217.9528503 secs/batch = 0.5505s, grad.norm=0.61251271\n",
      "  6056: 4 [   60/ 1499], train_loss/perplexity = 5.32437992/205.2810364 secs/batch = 0.5513s, grad.norm=0.53135574\n",
      "  6061: 4 [   65/ 1499], train_loss/perplexity = 5.37950182/216.9141846 secs/batch = 0.5441s, grad.norm=0.61276019\n",
      "  6066: 4 [   70/ 1499], train_loss/perplexity = 5.34815454/210.2199860 secs/batch = 0.5547s, grad.norm=0.63031423\n",
      "  6071: 4 [   75/ 1499], train_loss/perplexity = 5.13873148/170.4993439 secs/batch = 0.5519s, grad.norm=0.55024922\n",
      "  6076: 4 [   80/ 1499], train_loss/perplexity = 5.28943682/198.2317505 secs/batch = 0.5534s, grad.norm=0.50798792\n",
      "  6081: 4 [   85/ 1499], train_loss/perplexity = 5.21752882/184.4777374 secs/batch = 0.5575s, grad.norm=0.59512043\n",
      "  6086: 4 [   90/ 1499], train_loss/perplexity = 5.44738436/232.1501465 secs/batch = 0.5476s, grad.norm=0.61358780\n",
      "  6091: 4 [   95/ 1499], train_loss/perplexity = 5.19800711/180.9113464 secs/batch = 0.5533s, grad.norm=0.51568687\n",
      "  6096: 4 [  100/ 1499], train_loss/perplexity = 5.25930071/192.3469391 secs/batch = 0.5589s, grad.norm=0.49912837\n",
      "  6101: 4 [  105/ 1499], train_loss/perplexity = 5.19640636/180.6219788 secs/batch = 0.5503s, grad.norm=0.56702739\n",
      "  6106: 4 [  110/ 1499], train_loss/perplexity = 5.20786524/182.7036133 secs/batch = 0.5539s, grad.norm=0.56531775\n",
      "  6111: 4 [  115/ 1499], train_loss/perplexity = 5.42521477/227.0601044 secs/batch = 0.5601s, grad.norm=0.48129606\n",
      "  6116: 4 [  120/ 1499], train_loss/perplexity = 5.26957750/194.3338470 secs/batch = 0.5491s, grad.norm=0.55711544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6121: 4 [  125/ 1499], train_loss/perplexity = 5.50282669/245.3845825 secs/batch = 0.5560s, grad.norm=0.68629920\n",
      "  6126: 4 [  130/ 1499], train_loss/perplexity = 5.42908764/227.9411926 secs/batch = 0.5634s, grad.norm=0.53032947\n",
      "  6131: 4 [  135/ 1499], train_loss/perplexity = 5.43758011/229.8852081 secs/batch = 0.5509s, grad.norm=0.57254535\n",
      "  6136: 4 [  140/ 1499], train_loss/perplexity = 5.44690466/232.0388184 secs/batch = 0.5502s, grad.norm=0.60254824\n",
      "  6141: 4 [  145/ 1499], train_loss/perplexity = 5.18459749/178.5015869 secs/batch = 0.5537s, grad.norm=0.55803508\n",
      "  6146: 4 [  150/ 1499], train_loss/perplexity = 5.41601610/224.9810333 secs/batch = 0.5496s, grad.norm=0.55128151\n",
      "  6151: 4 [  155/ 1499], train_loss/perplexity = 5.54189444/255.1609344 secs/batch = 0.5480s, grad.norm=0.52777064\n",
      "  6156: 4 [  160/ 1499], train_loss/perplexity = 5.82739353/339.4726868 secs/batch = 0.6064s, grad.norm=0.57841212\n",
      "  6161: 4 [  165/ 1499], train_loss/perplexity = 5.26996422/194.4090118 secs/batch = 0.5525s, grad.norm=0.53231978\n",
      "  6166: 4 [  170/ 1499], train_loss/perplexity = 5.67019129/290.0900269 secs/batch = 0.5797s, grad.norm=0.55410159\n",
      "  6171: 4 [  175/ 1499], train_loss/perplexity = 5.41260958/224.2159271 secs/batch = 0.5664s, grad.norm=0.52698100\n",
      "  6176: 4 [  180/ 1499], train_loss/perplexity = 5.62693882/277.8103943 secs/batch = 0.5462s, grad.norm=0.60515720\n",
      "  6181: 4 [  185/ 1499], train_loss/perplexity = 5.37081718/215.0385284 secs/batch = 0.5842s, grad.norm=0.53645766\n",
      "  6186: 4 [  190/ 1499], train_loss/perplexity = 5.60730553/272.4092407 secs/batch = 0.7070s, grad.norm=0.62013197\n",
      "  6191: 4 [  195/ 1499], train_loss/perplexity = 5.67971230/292.8651733 secs/batch = 0.5526s, grad.norm=0.52736294\n",
      "  6196: 4 [  200/ 1499], train_loss/perplexity = 5.60297203/271.2313232 secs/batch = 0.5730s, grad.norm=0.56043047\n",
      "  6201: 4 [  205/ 1499], train_loss/perplexity = 5.49735165/244.0447540 secs/batch = 0.5515s, grad.norm=0.55084687\n",
      "  6206: 4 [  210/ 1499], train_loss/perplexity = 5.29857540/200.0516205 secs/batch = 0.5757s, grad.norm=0.53006786\n",
      "  6211: 4 [  215/ 1499], train_loss/perplexity = 5.51559305/248.5373230 secs/batch = 0.5870s, grad.norm=0.50953859\n",
      "  6216: 4 [  220/ 1499], train_loss/perplexity = 5.23576164/187.8721466 secs/batch = 0.5433s, grad.norm=0.47243357\n",
      "  6221: 4 [  225/ 1499], train_loss/perplexity = 5.45953417/234.9879303 secs/batch = 0.5861s, grad.norm=0.51290971\n",
      "  6226: 4 [  230/ 1499], train_loss/perplexity = 5.50085878/244.9021606 secs/batch = 0.5696s, grad.norm=0.49905920\n",
      "  6231: 4 [  235/ 1499], train_loss/perplexity = 5.52330112/250.4604797 secs/batch = 0.5360s, grad.norm=0.56056398\n",
      "  6236: 4 [  240/ 1499], train_loss/perplexity = 5.62566090/277.4555969 secs/batch = 0.5626s, grad.norm=0.53276920\n",
      "  6241: 4 [  245/ 1499], train_loss/perplexity = 5.33325243/207.1104889 secs/batch = 0.5508s, grad.norm=0.62619060\n",
      "  6246: 4 [  250/ 1499], train_loss/perplexity = 5.41314125/224.3351746 secs/batch = 0.5445s, grad.norm=0.62312299\n"
     ]
    }
   ],
   "source": [
    "lstm_char_cnn.Train_Char_Model(sess, char_train_graph, train_reader, saver, summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "data_X shape: (20, 31, 21)\n",
      "data_Y shape: (20, 3)\n",
      "sentX shape: Tensor(\"Placeholder_1:0\", shape=(?, 31, 21), dtype=int32)\n",
      "sent_y, shape: Tensor(\"Placeholder_2:0\", shape=(?, 3), dtype=float32)\n",
      "2019-08-27 14:24:43 Step: 10 Training loss: 0.09034851486794651 accuracy: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6da89ead7dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainSentiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenti_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainSentiModel\u001b[0;34m(sess, saver, train_model, senti_reader, train_batch, test_batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mret_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \u001b[0msum_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainSentiModel(sess, saver, senti_train_graph, sentiReader, FLAGS.batch_size, FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: said..hes\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: a\"cleric\n",
      "Unknown char: \n",
      "Word: ministre\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \"\n",
      "Word: been\"justifying\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: okevery\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: therefoolish\n",
      "Unknown char: \n",
      "Word: wellposted\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: deplorableright\n",
      "Unknown char: \n",
      "Word: nownow\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: apologizefor\n",
      "Unknown char: \"\n",
      "Word: t\"...back\n",
      "Unknown char: \n",
      "Word: dernires\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: dfendaient\n",
      "Unknown char: \n",
      "Word: if\n",
      "Unknown char: \n",
      "Word: zufllig\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \"\n",
      "Word: god\"...if\n",
      "Unknown char: \"\n",
      "Word: shameful\"for\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: =\n",
      "Word: that=point\n",
      "Unknown char: \n",
      "Word: artw\n",
      "Unknown char: \n",
      "Word: vlgame\n",
      "Unknown char: \n",
      "Word: uuyor\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: whoevers\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: perdn\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: gjr\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: lrt\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: disposicin\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: free\"-nsw\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: thats\n",
      "2019-08-27 12:25:59 Step: 10 Training loss: 32.675646018981936 accuracy: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2499f5393235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainRDMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainRDMModel\u001b[0;34m(sess, mm, t_acc, t_steps, new_data_len)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mret_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_curtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Step: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Training loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msum_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mt_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainRDMModel(sess, rdm_train_graph, 0.7, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in RL the begining\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: said..hes\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: a\"cleric\n",
      "Unknown char: \n",
      "Word: ministre\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: lintrieur\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \"\n",
      "Word: been\"justifying\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: happy\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: week\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: end\n",
      "Unknown char: \n",
      "Word: okevery\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: jewsnow\n",
      "Unknown char: \n",
      "Word: therefoolish\n",
      "Unknown char: \n",
      "Word: wellposted\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: notethen\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: jewssoi\n",
      "Unknown char: \n",
      "Word: deplorableright\n",
      "Unknown char: \n",
      "Word: nownow\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: apologizefor\n",
      "Unknown char: \"\n",
      "Word: t\"...back\n",
      "Unknown char: \n",
      "Word: dernires\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: dfendaient\n",
      "Unknown char: \n",
      "Word: if\n",
      "Unknown char: \n",
      "Word: zufllig\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \"\n",
      "Word: god\"...if\n",
      "Unknown char: \"\n",
      "Word: shameful\"for\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: =\n",
      "Word: that=point\n",
      "Unknown char: \n",
      "Word: artw\n",
      "Unknown char: \n",
      "Word: vlgame\n",
      "Unknown char: \n",
      "Word: uuyor\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: therell\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: whoevers\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: perdn\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: gjr\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: lrt\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: disposicin\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: claim-theyre-super-rich-because-theyre-a-brand\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: free\"-nsw\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: that\"s\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: verstndnis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: congressmen:\"i\n",
      "Unknown char: \n",
      "Word: dantay\n",
      "Unknown char: \n",
      "Word: dantay\n",
      "Unknown char: \"\n",
      "guess again\"...\n",
      "Unknown char: \n",
      "guess again\"...\n",
      "Unknown char: \n",
      "guess again\"...\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \"\n",
      "Word: trust\"ourselves\n",
      "Unknown char: \n",
      "Word: familire\n",
      "Unknown char: \n",
      "Word: vre\n",
      "Unknown char: \n",
      "Word: paranod\n",
      "Unknown char: =\n",
      "Word: black=sunni\n",
      "Unknown char: =\n",
      "Word: green=shia\n",
      "Unknown char: \"\n",
      "Word: just\"sick\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: samobjcy\n",
      "Unknown char: \n",
      "Word: jzykach\n",
      "Unknown char: \n",
      "Word: skadasz\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: erdoan\n",
      "Unknown char: \n",
      "Word: biiler\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thisit's\n",
      "Unknown char: \n",
      "Word: tasteless/vulgar/pettyinhuman\n",
      "Unknown char: \n",
      "Word: evchs\n",
      "Unknown char: \n",
      "Word: evchs\n",
      "Unknown char: \n",
      "Word: ministers\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: lger\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: a\"riot\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: attaqus\n",
      "Unknown char: \n",
      "Word: ncessaire\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: lches\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: autorits\n",
      "Unknown char: \n",
      "Word: d'enqute\n",
      "Unknown char: \n",
      "Word: fminin\n",
      "Unknown char: \n",
      "Word: rsistance\n",
      "Unknown char: \n",
      "Word: gnrale\n",
      "Unknown char: \n",
      "Word: gnrale\n",
      "Unknown char: \n",
      "Word: aperue\n",
      "Unknown char: \n",
      "Word: librt\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: franois\n",
      "Unknown char: \n",
      "Word: renvoye\n",
      "Unknown char: \n",
      "Word: zgnz\n",
      "Unknown char: \n",
      "Word: zgnz\n",
      "Unknown char: \n",
      "Word: belive\n",
      "Unknown char: \n",
      "Word: service\n",
      "Unknown char: \n",
      "Word: this\n",
      "Unknown char: \n",
      "Word: think\n",
      "Unknown char: \n",
      "Word: choque\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \"\n",
      "Word: hate\"defending\n",
      "Unknown char: \n",
      "Word: deberan\n",
      "Unknown char: \n",
      "Word: rservs\n",
      "Unknown char: \n",
      "Word: rservs\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \"\n",
      "Word: guest:\"all\n",
      "Unknown char: \"\n",
      "Word: host:\"great\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: terror..\"glorious\"past\n",
      "Unknown char: \"\n",
      "Word: terror..\"glorious\"past\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: cdric\n",
      "Unknown char: \n",
      "Word: erwhnt\n",
      "Unknown char: \n",
      "Word: hpital\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: themirror.amen\n",
      "Unknown char: \n",
      "Word: castros\n",
      "Unknown char: \n",
      "Word: cubas\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: omalley\n",
      "Unknown char: \n",
      "Word: carrment\n",
      "Unknown char: \n",
      "Word: dmesur\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dbloquer\n",
      "Unknown char: \n",
      "Word: prsence\n",
      "Unknown char: \n",
      "Word: d'opration\n",
      "Unknown char: \n",
      "Word: armes\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: opration\n",
      "Unknown char: \n",
      "Word: l'tat\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: intressante\n",
      "Unknown char: \n",
      "Word: prs\n",
      "Unknown char: \n",
      "Word: sret\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \n",
      "Word: ondaann\n",
      "Unknown char: \"\n",
      "Word: care.\"this\n",
      "Unknown char: \n",
      "Word: aplicacin\n",
      "Unknown char: \n",
      "Word: shara\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: theyd\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: tho.but\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: bsta\n",
      "Unknown char: \n",
      "Word: tnker\n",
      "Unknown char: \n",
      "Word: ngonsin\n",
      "Unknown char: \n",
      "Word: lser\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: ltsaskompis\n",
      "Unknown char: \n",
      "Word: hger\n",
      "Unknown char: \n",
      "Word: bekrftar\n",
      "Unknown char: \n",
      "Word: vnsters\n",
      "Unknown char: \n",
      "Word: vrldbild\n",
      "Unknown char: \n",
      "Word: nstan\n",
      "Unknown char: \n",
      "Word: sgas\n",
      "Unknown char: \n",
      "Word: gra\n",
      "Unknown char: \n",
      "Word: ppekar\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: jsses\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: hromdagen\n",
      "Unknown char: \n",
      "Word: vl\n",
      "Unknown char: \n",
      "Word: mrdaren\n",
      "Unknown char: \n",
      "Word: hmnas\n",
      "Unknown char: \n",
      "Word: gr\n",
      "Unknown char: \n",
      "Word: terrordden\n",
      "Unknown char: \n",
      "Word: utfrs\n",
      "Unknown char: \n",
      "Word: vnta\n",
      "Unknown char: \n",
      "Word: bestmmer\n",
      "Unknown char: \n",
      "Word: hller\n",
      "Unknown char: \n",
      "Word: str\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: bsta\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: hller\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: mnskliga\n",
      "Unknown char: \n",
      "Word: bda\n",
      "Unknown char: \n",
      "Word: lser\n",
      "Unknown char: \n",
      "Word: lsa\n",
      "Unknown char: \n",
      "Word: istllet\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: hrt\n",
      "Unknown char: \n",
      "Word: sjlvmordsbombare\n",
      "Unknown char: \n",
      "Word: mellanstern\n",
      "Unknown char: \n",
      "Word: religs\n",
      "Unknown char: \n",
      "Word: str\n",
      "Unknown char: \n",
      "Word: religs\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: ls\n",
      "Unknown char: \n",
      "Word: sga\n",
      "Unknown char: \n",
      "Word: frolmpning\n",
      "Unknown char: \n",
      "Word: frolmpning\n",
      "Unknown char: \n",
      "Word: dr\n",
      "Unknown char: \n",
      "Word: nr\n",
      "Unknown char: \n",
      "Word: avrttar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: =\n",
      "Word: massmedia=junkfood\n",
      "Unknown char: \n",
      "Word: grard\n",
      "Unknown char: \n",
      "Word: frenchfrog\n",
      "Unknown char: \n",
      "Word: dfendre\n",
      "Unknown char: \n",
      "Word: dmocratie\n",
      "Unknown char: \n",
      "Word: gefhrlich\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: clbrits\n",
      "Unknown char: \n",
      "Word: ramnent\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \"\n",
      "Word: issues.\"i\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \"\n",
      "Word: unemplymt.\"they\"deserve\n",
      "Unknown char: \"\n",
      "Word: unemplymt.\"they\"deserve\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: i\"m\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: protgs\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: displayas\n",
      "Unknown char: \n",
      "Word: staffpublish\n",
      "Unknown char: \n",
      "Word: week1m\n",
      "Unknown char: \"\n",
      "Word: important\"...more\n",
      "Unknown char: \"\n",
      "Word: the\"religion\n",
      "Unknown char: \n",
      "Word: manqus\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: pnico\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: vergenza\n",
      "Unknown char: \n",
      "Word: debera\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: mdchen\n",
      "Unknown char: =\n",
      "Word: paris=israeli\n",
      "Unknown char: \n",
      "Word: iin\n",
      "Unknown char: \n",
      "Word: baskn\n",
      "Unknown char: \n",
      "Word: yapyor\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: mornings\n",
      "Unknown char: \n",
      "Word: commmoratif\n",
      "Unknown char: \n",
      "Word: bahrains\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \\\n",
      "Word: little\\no\n",
      "Unknown char: \n",
      "Word: pministerin\n",
      "Unknown char: \n",
      "Word: pministerin\n",
      "Unknown char: \n",
      "Word: pministeri\n",
      "Unknown char: \n",
      "Word: pministeri\n",
      "Unknown char: \n",
      "Word: sentn\n",
      "Unknown char: \n",
      "Word: sentn\n",
      "Unknown char: \n",
      "Word: tst\n",
      "Unknown char: \n",
      "Word: hpsis\n",
      "Unknown char: \"\n",
      "Word: and.get.a.job\"ignorance/complacency\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: worlds\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: pilots\n",
      "Unknown char: =\n",
      "Word: about=minority\n",
      "Unknown char: =\n",
      "Word: thing=depending\n",
      "Unknown char: \n",
      "Word: tus\n",
      "Unknown char: \n",
      "Word: proccs\n",
      "Unknown char: \"\n",
      "Word: worked.\"multiculturalism\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: arme\n",
      "Unknown char: \n",
      "Word: ame\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleased\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \n",
      "Word: pleasep\n",
      "Unknown char: \"\n",
      "Word: translates\"by\n",
      "Unknown char: \n",
      "Word: mrdias\n",
      "Unknown char: \"\n",
      "Word: translates\"by\n",
      "Unknown char: \n",
      "Word: seal\n",
      "Unknown char: \n",
      "Word: poblacin\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: needy\"...such\n",
      "Unknown char: \n",
      "Word: hros\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: =\n",
      "Word: b===d\n",
      "Unknown char: \n",
      "Word: esprons\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: extrmistes\n",
      "Unknown char: \n",
      "Word: speechit\n",
      "Unknown char: \n",
      "Word: speechit\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: rvulse\n",
      "Unknown char: \n",
      "Word: franaise\n",
      "Unknown char: \n",
      "Word: vnement\n",
      "Unknown char: \n",
      "Word: l-bas\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \n",
      "Word: palhaada\n",
      "Unknown char: \n",
      "Word: choque\n",
      "Unknown char: \n",
      "Word: skl\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: frsvarar\n",
      "Unknown char: \n",
      "Word: tnkte\n",
      "Unknown char: \n",
      "Word: islams\n",
      "Unknown char: \n",
      "Word: zgina\n",
      "Unknown char: \n",
      "Word: zgina\n",
      "Unknown char: \n",
      "Word: dzikuj\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: wyprbowanie\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: myli\n",
      "Unknown char: \n",
      "Word: sowo\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: stracia\n",
      "Unknown char: \n",
      "Word: przyszo\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: obamawould\n",
      "Unknown char: \n",
      "Word: grda\n",
      "Unknown char: \n",
      "Word: saighdiir\n",
      "Unknown char: \n",
      "Word: difrocht\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: enseando\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: glen\n",
      "Unknown char: \n",
      "Word: balayn\n",
      "Unknown char: \n",
      "Word: balayn\n",
      "Unknown char: \n",
      "Word: grce\n",
      "Unknown char: \n",
      "Word: concrtement\n",
      "Unknown char: \n",
      "Word: ragissons\n",
      "Unknown char: \"\n",
      "Word: a\"dark\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: situao\n",
      "Unknown char: \n",
      "Word: situao\n",
      "Unknown char: \n",
      "Word: reporte\n",
      "Unknown char: \n",
      "Word: trouve\n",
      "Unknown char: \n",
      "Word: j'tais\n",
      "Unknown char: \n",
      "Word: colre\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: islamici\n",
      "Unknown char: \n",
      "Word: zych\n",
      "Unknown char: \n",
      "Word: zdjcie\n",
      "Unknown char: \n",
      "Word: wiatw\n",
      "Unknown char: \n",
      "Word: pomyslaam\n",
      "Unknown char: \n",
      "Word: d'andras\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: unharmedxo\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: espre\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: thanks\n",
      "Unknown char: \"\n",
      "Word: say\"he\n",
      "Unknown char: \n",
      "Word: arrtes\n",
      "Unknown char: \n",
      "Word: lislam\n",
      "Unknown char: \n",
      "Word: newsmedia\n",
      "Unknown char: \n",
      "Word: sme\n",
      "Unknown char: \n",
      "Word: rcolte\n",
      "Unknown char: \n",
      "Word: tempte\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: shithow\n",
      "Unknown char: =\n",
      "Word: brigade=terrorist's\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: sydneys\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: dcroch\n",
      "Unknown char: \n",
      "Word: alterao\n",
      "Unknown char: \n",
      "Word: alterao\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: magnficas\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: atencin\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: espaol\n",
      "Unknown char: \n",
      "Word: ingls\n",
      "Unknown char: \n",
      "Word: paradjico\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: liberacin\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: fcil\n",
      "Unknown char: \n",
      "prayinginish....\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: itd\n",
      "Unknown char: \n",
      "Word: twtd\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \"\n",
      "Word: their\"nations\n",
      "Unknown char: =\n",
      "Word: piece==&gt\n",
      "Unknown char: =\n",
      "Word: piece==&gt\n",
      "Unknown char: \n",
      "Word: tus\n",
      "Unknown char: \n",
      "Word: cur\n",
      "Unknown char: \n",
      "Word: n'taient\n",
      "Unknown char: \n",
      "Word: arms\n",
      "Unknown char: \n",
      "Word: annes\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: muulmanos\n",
      "Unknown char: \"\n",
      "Word: martyrs\".i\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: kytt\n",
      "Unknown char: \n",
      "Word: kytt\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: online-kntj\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: kntmn\n",
      "Unknown char: \n",
      "Word: lhteest\n",
      "Unknown char: \n",
      "Word: elif\n",
      "Unknown char: \n",
      "Word: ltfen\n",
      "Unknown char: \n",
      "Word: takip\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: misiniz\n",
      "Unknown char: \n",
      "Word: whove\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: saldrlarnda\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: drmt...ayn\n",
      "Unknown char: \n",
      "Word: kapya\n",
      "Unknown char: \n",
      "Word: kyor\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: hed\n",
      "Unknown char: \n",
      "Word: wheres\n",
      "Unknown char: \"\n",
      "Word: allah\"to\n",
      "Unknown char: \n",
      "Word: allah\"to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: spisil\n",
      "Unknown char: \n",
      "Word: spisil\n",
      "Unknown char: \n",
      "Word: fill\n",
      "Unknown char: \n",
      "Word: shuomh\n",
      "Unknown char: \n",
      "Word: inscurit\n",
      "Unknown char: \n",
      "Word: mds\n",
      "Unknown char: \n",
      "Word: astrix\n",
      "Unknown char: \n",
      "Word: rsistant\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: crtica\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: article\"how\n",
      "Unknown char: =\n",
      "Word: it=a\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: you're*....\"as\n",
      "Unknown char: \"\n",
      "Word: faith\"when\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: areos\n",
      "Unknown char: \n",
      "Word: arent\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: govt\n",
      "Unknown char: \n",
      "Word: norvgien\n",
      "Unknown char: \n",
      "Word: dgouts\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: allans\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: frres\n",
      "Unknown char: \n",
      "Word: arrter\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: c'est--dire\n",
      "Unknown char: \n",
      "Word: gzaltna\n",
      "Unknown char: \n",
      "Word: gzaltna\n",
      "Unknown char: \n",
      "Word: alnd\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: ento\n",
      "Unknown char: \n",
      "Word: padres\n",
      "Unknown char: \n",
      "Word: so\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \"\n",
      "Word: muslim...\"silent\n",
      "Unknown char: \"\n",
      "Word: majority\"...condemn\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: razn\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: bytoday's\n",
      "Unknown char: \n",
      "Word: stck\n",
      "Unknown char: \n",
      "Word: kse\n",
      "Unknown char: \n",
      "Word: peaceit's\n",
      "Unknown char: \n",
      "Word: peaceit's\n",
      "Unknown char: =\n",
      "Word: uber=opportunistic\n",
      "Unknown char: \"\n",
      "Word: khanjar.\"a\n",
      "Unknown char: \"\n",
      "Word: case\"send\n",
      "Unknown char: \n",
      "Word: tomorrows\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: l'htel\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: pases\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: prf\n",
      "Unknown char: \n",
      "Word: prsent\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: europens\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: chrtiens\n",
      "Unknown char: \n",
      "Word: rpublicains\n",
      "Unknown char: \n",
      "Word: rpondre\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: terrist\n",
      "Unknown char: \n",
      "Word: terrist\n",
      "Unknown char: \n",
      "Word: dnya\n",
      "Unknown char: \n",
      "Word: baritan\n",
      "Unknown char: \n",
      "Word: geilmez\n",
      "Unknown char: \n",
      "Word: geilmez\n",
      "Unknown char: \n",
      "Word: kalrm\n",
      "Unknown char: \n",
      "Word: yannda\n",
      "Unknown char: \n",
      "Word: dn\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: kyamam\n",
      "Unknown char: \n",
      "Word: biey\n",
      "Unknown char: \n",
      "Word: syleyemiyo\n",
      "Unknown char: \n",
      "Word: artk\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: stmze\n",
      "Unknown char: \n",
      "Word: anlyorum\n",
      "Unknown char: \n",
      "Word: konuuyorum\n",
      "Unknown char: \n",
      "Word: azndan\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: anlalmayalm\n",
      "Unknown char: \n",
      "Word: p-p\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: schieen\n",
      "Unknown char: \n",
      "Word: livebertragung\n",
      "Unknown char: \n",
      "Word: drfte\n",
      "Unknown char: \n",
      "Word: wre\n",
      "Unknown char: \n",
      "Word: tter\n",
      "Unknown char: \n",
      "Word: auszulschen\n",
      "Unknown char: \n",
      "Word: gbe\n",
      "Unknown char: \n",
      "Word: strenden\n",
      "Unknown char: \n",
      "Word: verrterischen\n",
      "Unknown char: \n",
      "Word: mrder\n",
      "Unknown char: \n",
      "Word: kmen\n",
      "Unknown char: \n",
      "Word: grber\n",
      "Unknown char: \n",
      "Word: flchtet\n",
      "Unknown char: \n",
      "Word: parittisch\n",
      "Unknown char: \n",
      "Word: australias\n",
      "Unknown char: \n",
      "Word: americas\n",
      "Unknown char: \n",
      "Word: n'tes\n",
      "Unknown char: \n",
      "Word: iin\n",
      "Unknown char: \n",
      "Word: basp\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \n",
      "Word: datmalsnz\n",
      "Unknown char: \"\n",
      "Word: afraid\"-thousands\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: tribresararivelesgentfaitesapoursuirejurtise\n",
      "Unknown char: \n",
      "Word: confiana\n",
      "Unknown char: \n",
      "Word: ns\n",
      "Unknown char: \n",
      "Word: to\n",
      "Unknown char: \n",
      "Word: difcil\n",
      "Unknown char: \n",
      "Word: condenao\n",
      "Unknown char: \n",
      "Word: condenao\n",
      "Unknown char: \n",
      "Word: habrn\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: gnration\n",
      "Unknown char: \n",
      "Word: gnration\n",
      "Unknown char: \n",
      "Word: hro\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: yoll\n",
      "Unknown char: \"\n",
      "Word: euh...\"holletje\n",
      "Unknown char: \n",
      "Word: vrios\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: thisssrt\n",
      "Unknown char: \n",
      "Word: thisssrt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: powana\n",
      "Unknown char: \n",
      "Word: zwyky\n",
      "Unknown char: \n",
      "Word: moe\n",
      "Unknown char: \n",
      "Word: moe\n",
      "Unknown char: \n",
      "Word: oczywicie\n",
      "Unknown char: \n",
      "Word: rne\n",
      "Unknown char: \n",
      "Word: rne\n",
      "Unknown char: \n",
      "Word: rda\n",
      "Unknown char: \n",
      "Word: rda\n",
      "Unknown char: \n",
      "Word: mona\n",
      "Unknown char: \n",
      "Word: wpadem\n",
      "Unknown char: \n",
      "Word: sida\n",
      "Unknown char: \n",
      "Word: wite\n",
      "Unknown char: \n",
      "Word: sowa\n",
      "Unknown char: \n",
      "Word: ostrzeenie\n",
      "Unknown char: \n",
      "Word: lotw\n",
      "Unknown char: \n",
      "Word: przeszkadzay\n",
      "Unknown char: \n",
      "Word: pkinois(nous\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: chre\n",
      "Unknown char: \n",
      "Word: prvert\n",
      "Unknown char: \n",
      "Word: youve\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \"\n",
      "Word: cellphoneshutdown\"evansolomonreporting\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: ar\n",
      "Unknown char: \n",
      "Word: ar\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: weve\n",
      "Unknown char: =\n",
      "Word: lt;=&gt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: rightno\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: gods\n",
      "Unknown char: =\n",
      "Word: religion=violence\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: translationno\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: situacin\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: konuuyor\n",
      "Unknown char: \n",
      "Word: sanyor\n",
      "Unknown char: \n",
      "Word: islam\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: wrong\"~~mjs\n",
      "Unknown char: \n",
      "Word: seora\n",
      "Unknown char: \n",
      "Word: constncia\n",
      "Unknown char: \n",
      "Word: xenofbia\n",
      "Unknown char: \n",
      "Word: ents\n",
      "Unknown char: \n",
      "Word: sn\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: tradut\n",
      "Unknown char: \n",
      "Word: dbarras\n",
      "Unknown char: \n",
      "Word: xrbi\n",
      "Unknown char: \n",
      "Word: xrbi\n",
      "Unknown char: \"\n",
      "Word: heroes.\"he\n",
      "Unknown char: \n",
      "Word: rpublique\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: indignacin\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "excellent\n",
      "Unknown char: \n",
      "butd: idea\n",
      "Unknown char: \n",
      "they: about\n",
      "Unknown char: \n",
      "Word: headlesschookmpersonatingghekowithnotail\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: a320rt\n",
      "Unknown char: \n",
      "Word: bata\n",
      "Unknown char: \n",
      "Word: hepinizi\n",
      "Unknown char: \n",
      "Word: hepinizi\n",
      "Unknown char: \n",
      "Word: ak\n",
      "Unknown char: \n",
      "Word: ak\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: uularnz\n",
      "Unknown char: \n",
      "Word: inileriniz\n",
      "Unknown char: \n",
      "Word: inileriniz\n",
      "Unknown char: \n",
      "Word: gvenli\n",
      "Unknown char: \n",
      "Word: wtend\n",
      "Unknown char: \n",
      "Word: hrt\n",
      "Unknown char: \n",
      "Word: storlengmlsch\n",
      "Unknown char: \n",
      "Word: mllerkrnerkleft\n",
      "Unknown char: \n",
      "Word: mllerkrnerkleft\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: guantnamo\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: methats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wolinskis\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dmocratie\n",
      "Unknown char: \n",
      "Word: dcidment\n",
      "Unknown char: \n",
      "Word: dcidment\n",
      "Unknown char: \n",
      "Word: convictionjust\n",
      "Unknown char: \n",
      "Word: convictionjust\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dammartin-en-gol\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: arent\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: \n",
      "Word: dailyyour\n",
      "Unknown char: =\n",
      "Word: violations=dismissal\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: glen's\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: mnner\n",
      "Unknown char: \n",
      "Word: wet\n",
      "Unknown char: \n",
      "Word: bozuuna\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \n",
      "Word: policemans\n",
      "Unknown char: \"\n",
      "Word: were\"gardien\n",
      "Unknown char: \n",
      "Word: wyraam\n",
      "Unknown char: \n",
      "Word: najgbsze\n",
      "Unknown char: \n",
      "Word: najgbsze\n",
      "Unknown char: \n",
      "Word: wspczucie\n",
      "Unknown char: \n",
      "Word: wspczucie\n",
      "Unknown char: \n",
      "Word: bdziemy\n",
      "Unknown char: \n",
      "Word: l'poque\n",
      "Unknown char: \n",
      "Word: isral\n",
      "Unknown char: \n",
      "Word: doru\n",
      "Unknown char: \n",
      "Word: sylemi\n",
      "Unknown char: \n",
      "Word: castros\n",
      "Unknown char: \n",
      "Word: cubas\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: d'arrter\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: nause\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: jocks\"?for\n",
      "Unknown char: \n",
      "Word: corn\n",
      "Unknown char: \n",
      "Word: provocacin\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: gg\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: priphrique\n",
      "Unknown char: \n",
      "Word: priphrique\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: democrticas\n",
      "Unknown char: \n",
      "Word: expulsin\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: confirmation?\"ask\n",
      "Unknown char: =\n",
      "Word: sorry=proven\n",
      "Unknown char: \n",
      "Word: cmon\n",
      "Unknown char: \"\n",
      "Word: topic-\"are\n",
      "Unknown char: \n",
      "Word: sad\n",
      "Unknown char: \n",
      "Word: cherf\n",
      "Unknown char: \n",
      "Word: crmer\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \\\n",
      "Word: way\\ave\\st\\rd\n",
      "Unknown char: \n",
      "Word: erdoan's\n",
      "Unknown char: \n",
      "Word: erdoan\n",
      "Unknown char: \n",
      "Word: trkiyenin\n",
      "Unknown char: \n",
      "Word: gvenilir\n",
      "Unknown char: \n",
      "Word: takipi\n",
      "Unknown char: \n",
      "Word: satn\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: provocacin\n",
      "Unknown char: \n",
      "Word: religin\n",
      "Unknown char: \n",
      "Word: sunni/shite\n",
      "Unknown char: \n",
      "Word: israeles\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: dsseldorf\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: a3204u\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: engage\n",
      "Unknown char: \n",
      "Word: dcrit\n",
      "Unknown char: \n",
      "Word: trs\n",
      "Unknown char: \n",
      "Word: occupe\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \"\n",
      "Word: allah\"...and\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: anti-zoroastrianthe\n",
      "Unknown char: =\n",
      "Word: report=legal\n",
      "Unknown char: \"\n",
      "Word: so\"..wen\n",
      "Unknown char: \"\n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "i'md: yourselves\"\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: mnner\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: ideologas\n",
      "Unknown char: \n",
      "Word: polticos\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \"\n",
      "Word: wifi\"...i\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: hlas\n",
      "Unknown char: \n",
      "Word: converthero\n",
      "Unknown char: \n",
      "ifrd: ism\n",
      "Unknown char: \"\n",
      "Word: here\"..generic\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: eraseris\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \"\n",
      "Word: memorial:\"pray\n",
      "Unknown char: \n",
      "ifrd: ism\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: prire\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: nis\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: irans\n",
      "Unknown char: \n",
      "Word: irans\n",
      "Unknown char: \n",
      "Word: paraso\n",
      "Unknown char: \n",
      "Word: childs\n",
      "Unknown char: \"\n",
      "Word: wasn\"t\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: =\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \"\n",
      "Word: flags\"=\"psyops\"true\n",
      "Unknown char: \n",
      "Word: mio\n",
      "Unknown char: \"\n",
      "Word: purposes\"is\n",
      "Unknown char: \n",
      "Word: schbig\n",
      "Unknown char: \n",
      "Word: gesprch\n",
      "Unknown char: \n",
      "Word: obrien\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: gldjande\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: matana\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: circulao\n",
      "Unknown char: \n",
      "Word: circulao\n",
      "Unknown char: \n",
      "Word: edio\n",
      "Unknown char: \n",
      "Word: edio\n",
      "Unknown char: \n",
      "Word: prxima\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: ill\n",
      "Unknown char: \"\n",
      "Word: dissidents/undesirables\"-run\n",
      "Unknown char: \"\n",
      "Word: date\"-the\n",
      "Unknown char: \n",
      "Word: ones\n",
      "Unknown char: \n",
      "Word: shouldnt\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: gouverns\n",
      "Unknown char: \n",
      "Word: gallrie\n",
      "Unknown char: \n",
      "Word: fua\n",
      "Unknown char: \n",
      "Word: rtje\n",
      "Unknown char: \n",
      "Word: neutralised\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \"\n",
      "Word: not\"our\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: brush\"-i\n",
      "Unknown char: \n",
      "Word: villefranche-sur-sane\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: hasnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wouldve\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: tt\n",
      "Unknown char: \n",
      "Word: hypothse\n",
      "Unknown char: \n",
      "Word: cre\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: \n",
      "Word: m'crire\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: l'tat\n",
      "Unknown char: \n",
      "Word: procderais\n",
      "Unknown char: \n",
      "Word: ct\n",
      "Unknown char: \n",
      "Word: d'coute\n",
      "Unknown char: \n",
      "Word: honntement\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: l'vnement\n",
      "Unknown char: \n",
      "Word: l'vnement\n",
      "Unknown char: \n",
      "Word: prire\n",
      "Unknown char: \n",
      "Word: avrer\n",
      "Unknown char: \n",
      "Word: pice\n",
      "Unknown char: \n",
      "Word: mne\n",
      "Unknown char: \n",
      "Word: qubec\n",
      "Unknown char: \n",
      "Word: rellement\n",
      "Unknown char: \n",
      "Word: ct\n",
      "Unknown char: \n",
      "Word: toi-mme\n",
      "Unknown char: \n",
      "Word: mdia\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: gre\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: contrle\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: l'intrt\n",
      "Unknown char: \n",
      "Word: crer\n",
      "Unknown char: \n",
      "Word: infonde\n",
      "Unknown char: \n",
      "Word: errones\n",
      "Unknown char: \n",
      "Word: communiques\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: \n",
      "Word: arrter\n",
      "Unknown char: \n",
      "Word: caractre\n",
      "Unknown char: \n",
      "Word: rponde\n",
      "Unknown char: \n",
      "Word: srement\n",
      "Unknown char: \n",
      "Word: t'inquite\n",
      "Unknown char: \n",
      "Word: plutt\n",
      "Unknown char: \n",
      "Word: boe\n",
      "Unknown char: \n",
      "Word: sauvaj\n",
      "Unknown char: \n",
      "Word: fantismo\n",
      "Unknown char: \n",
      "Word: brbaro\n",
      "Unknown char: \n",
      "Word: donnes\n",
      "Unknown char: \n",
      "Word: spcial\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: d'lite\n",
      "Unknown char: \n",
      "Word: diffrente\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: units\n",
      "Unknown char: \n",
      "Word: mriterait\n",
      "Unknown char: \n",
      "Word: rforme\n",
      "Unknown char: \n",
      "Word: dautres\n",
      "Unknown char: \n",
      "Word: units\n",
      "Unknown char: \n",
      "Word: dinterventions\n",
      "Unknown char: \n",
      "Word: spciales\n",
      "Unknown char: \n",
      "Word: prviens\n",
      "Unknown char: \n",
      "Word: dsinformation\n",
      "Unknown char: \n",
      "Word: peut-tre\n",
      "Unknown char: \n",
      "Word: exagrer\n",
      "Unknown char: \n",
      "Word: l'extrmisme\n",
      "Unknown char: \n",
      "Word: atpeaceful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: \n",
      "Word: priorits\n",
      "Unknown char: \n",
      "Word: dplac\n",
      "Unknown char: \n",
      "Word: mankind.ll\n",
      "Unknown char: =\n",
      "Word: sharialaw=hamas=isis\n",
      "Unknown char: =\n",
      "Word: sharialaw=hamas=isis\n",
      "Unknown char: =\n",
      "Word: haram=hezbollah\n",
      "Unknown char: \"\n",
      "Word: bold\"?...trying\n",
      "Unknown char: =\n",
      "Word: lies=lies=lies\n",
      "Unknown char: =\n",
      "Word: lies=lies=lies\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: uss\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: uss\n",
      "Unknown char: =\n",
      "Word: suspects=more\n",
      "Unknown char: \n",
      "Word: tmraire\n",
      "Unknown char: \n",
      "Word: tmraire\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: wasnt\n",
      "Unknown char: \n",
      "Word: frre\n",
      "Unknown char: \n",
      "Word: monte\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: polcia\n",
      "Unknown char: \n",
      "Word: austrlia\n",
      "Unknown char: \n",
      "Word: mantm\n",
      "Unknown char: \n",
      "Word: vrios\n",
      "Unknown char: \n",
      "Word: refns\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \"\n",
      "Word: w/\"reported\n",
      "Unknown char: \n",
      "Word: sper\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \n",
      "Word: sr\n",
      "Unknown char: \"\n",
      "Word: world\"....great\n",
      "Unknown char: \n",
      "Word: exploses\n",
      "Unknown char: \n",
      "Word: downtowncrazy\n",
      "Unknown char: \"\n",
      "Word: it\"workplace\n",
      "Unknown char: \n",
      "Word: that's\n",
      "Unknown char: =\n",
      "Word: repeat=&gt;if\n",
      "Unknown char: =\n",
      "Word: repeat=&gt;if\n",
      "Unknown char: \n",
      "Word: terristlerin\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: =\n",
      "Word: terrorismo=mafie\n",
      "Unknown char: \n",
      "Word: stphane\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \"\n",
      "Word: yes....\"accidents\n",
      "Unknown char: \"\n",
      "Word: i\"m\n",
      "Unknown char: \n",
      "Word: terrorfico\n",
      "Unknown char: \n",
      "Word: frmmestad\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \"\n",
      "Word: matter.\"..lol\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: tte\n",
      "Unknown char: \n",
      "Word: mdiatique\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \\\n",
      "Word: nat\\f\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: =\n",
      "Word: peace(globe)=58=all\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: offices\n",
      "Unknown char: \"\n",
      "Word: islamist\"attacks\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: consistentmuch\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: hows\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: guessim\n",
      "Unknown char: \n",
      "Word: guessim\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: rats\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \"\n",
      "Word: do\"it\n",
      "Unknown char: \n",
      "Word: respects\n",
      "Unknown char: \n",
      "Word: assasins\n",
      "Unknown char: \n",
      "Word: brler\n",
      "Unknown char: \n",
      "Word: dnoncer\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: kady\n",
      "Unknown char: \n",
      "Word: bdzie\n",
      "Unknown char: \n",
      "Word: prbowa\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: zaktkach\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: zaczo\n",
      "Unknown char: \n",
      "Word: kady\n",
      "Unknown char: \n",
      "Word: bdzie\n",
      "Unknown char: \n",
      "Word: prbowa\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: rnych\n",
      "Unknown char: \n",
      "Word: zaktkach\n",
      "Unknown char: \n",
      "Word: cytujc\n",
      "Unknown char: \"\n",
      "Word: returned:\"but\n",
      "Unknown char: \n",
      "Word: lches\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: cdez\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \n",
      "Word: dammartin-en-gole\n",
      "Unknown char: \"\n",
      "Word: him\"...what\n",
      "Unknown char: \"\n",
      "Word: security\"...from\n",
      "Unknown char: \"\n",
      "Word: him\"...what\n",
      "Unknown char: \"\n",
      "Word: saying...\"if\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \"\n",
      "Word: condulence\"-jebag\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: journe\n",
      "Unknown char: \n",
      "Word: yeahi\n",
      "Unknown char: \n",
      "Word: got\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wren\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: caazo\n",
      "Unknown char: \n",
      "Word: attaqus\n",
      "Unknown char: \n",
      "Word: dmissionne\n",
      "Unknown char: \n",
      "Word: l'indpendantiste\n",
      "Unknown char: \n",
      "Word: rviser\n",
      "Unknown char: \n",
      "Word: amricaine\n",
      "Unknown char: \"\n",
      "Word: attack:paris\"ll\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \"\n",
      "Word: cop/\"security\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: theydidn't\n",
      "Unknown char: \n",
      "Word: dport\n",
      "Unknown char: \"\n",
      "Word: believe\"from\n",
      "Unknown char: \"\n",
      "Word: order\":usjrno\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: too\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \"\n",
      "Word: martyrs\"french\n",
      "Unknown char: \n",
      "Word: podan\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: pasamontaas\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: inmolndose.pero\n",
      "Unknown char: \n",
      "Word: pelcula\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: slo\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: mrtires\n",
      "Unknown char: \n",
      "Word: das\n",
      "Unknown char: \n",
      "Word: sultn\n",
      "Unknown char: \n",
      "Word: algn\n",
      "Unknown char: \n",
      "Word: partindose\n",
      "Unknown char: \n",
      "Word: rehn\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: dcd\n",
      "Unknown char: \n",
      "Word: dcd\n",
      "Unknown char: \n",
      "Word: leiil\n",
      "Unknown char: \n",
      "Word: plutt\n",
      "Unknown char: \n",
      "Word: dernire\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: artculo\n",
      "Unknown char: \n",
      "Word: pars\n",
      "Unknown char: \n",
      "Word: fume\n",
      "Unknown char: \n",
      "Word: poasi\n",
      "Unknown char: \n",
      "Word: nesrena\n",
      "Unknown char: \n",
      "Word: sluaj\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: cuntas\n",
      "Unknown char: \n",
      "Word: pakistn\n",
      "Unknown char: \n",
      "Word: irn\n",
      "Unknown char: \n",
      "Word: hed\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: sera\n",
      "Unknown char: \n",
      "Word: dediimiz\n",
      "Unknown char: \n",
      "Word: ite\n",
      "Unknown char: \n",
      "Word: yazld\n",
      "Unknown char: \n",
      "Word: kardeim\n",
      "Unknown char: \n",
      "Word: kavgann\n",
      "Unknown char: \n",
      "Word: istanbul\n",
      "Unknown char: \n",
      "Word: kfr\n",
      "Unknown char: \n",
      "Word: kfr\n",
      "Unknown char: \n",
      "Word: eteine\n",
      "Unknown char: \n",
      "Word: ayaksn\n",
      "Unknown char: \n",
      "Word: gerek\n",
      "Unknown char: \n",
      "Word: nasl\n",
      "Unknown char: \n",
      "Word: yazk\n",
      "Unknown char: \n",
      "Word: yazk\n",
      "Unknown char: \n",
      "Word: karl\n",
      "Unknown char: \n",
      "Word: karl\n",
      "Unknown char: \n",
      "Word: satrlarnda\n",
      "Unknown char: \n",
      "Word: satrlarnda\n",
      "Unknown char: \n",
      "Word: trk\n",
      "Unknown char: \n",
      "Word: deil\n",
      "Unknown char: \n",
      "Word: hereyi\n",
      "Unknown char: \n",
      "Word: satyorlar\n",
      "Unknown char: \n",
      "Word: yz\n",
      "Unknown char: \n",
      "Word: trkl\n",
      "Unknown char: \n",
      "Word: hakknda\n",
      "Unknown char: \n",
      "Word: olmutur\n",
      "Unknown char: \n",
      "Word: manasyla\n",
      "Unknown char: \n",
      "Word: istediin\n",
      "Unknown char: \n",
      "Word: sakalndan\n",
      "Unknown char: \n",
      "Word: srkleneceksin\n",
      "Unknown char: \n",
      "Word: srkleneceksin\n",
      "Unknown char: \n",
      "Word: dnya'da\n",
      "Unknown char: \n",
      "Word: grelim\n",
      "Unknown char: \n",
      "Word: yoksunluu\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: mnafklk\n",
      "Unknown char: \n",
      "Word: hainlii\n",
      "Unknown char: \n",
      "Word: ahsn\n",
      "Unknown char: \n",
      "Word: tarafndan\n",
      "Unknown char: \n",
      "Word: istihbaratn\n",
      "Unknown char: \n",
      "Word: inaatna\n",
      "Unknown char: \n",
      "Word: inaatna\n",
      "Unknown char: \n",
      "Word: kat\n",
      "Unknown char: \n",
      "Word: anlayamadk\n",
      "Unknown char: \n",
      "Word: bulalm\n",
      "Unknown char: \n",
      "Word: yaptlar\n",
      "Unknown char: \n",
      "Word: baka\n",
      "Unknown char: \n",
      "Word: grmez\n",
      "Unknown char: \n",
      "Word: gzlerin\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: alrsan\n",
      "Unknown char: \n",
      "Word: ettiine\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: inandn\n",
      "Unknown char: \n",
      "Word: kiilerden(iki\n",
      "Unknown char: \n",
      "Word: kii\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: badatramyorum\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: lam\n",
      "Unknown char: \n",
      "Word: laflarn\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: yakksz\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: kfrlere\n",
      "Unknown char: \n",
      "Word: kfrlere\n",
      "Unknown char: \n",
      "Word: tenezzl\n",
      "Unknown char: \n",
      "Word: aklmda\n",
      "Unknown char: \n",
      "Word: kaldn\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: yazdan\n",
      "Unknown char: \n",
      "Word: bakp\n",
      "Unknown char: \n",
      "Word: ahlaksz\n",
      "Unknown char: \n",
      "Word: gndermelerini\n",
      "Unknown char: \n",
      "Word: okyacaksn\n",
      "Unknown char: \n",
      "Word: bakp\n",
      "Unknown char: \n",
      "Word: temil\n",
      "Unknown char: \n",
      "Word: alamazsn.kastl\n",
      "Unknown char: \n",
      "Word: alamazsn.kastl\n",
      "Unknown char: \n",
      "Word: yapyorsun\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: aryorsunuzki?emre\n",
      "Unknown char: \n",
      "Word: grevini\n",
      "Unknown char: \n",
      "Word: yapyor\n",
      "Unknown char: \n",
      "Word: yedii\n",
      "Unknown char: \n",
      "Word: ca\n",
      "Unknown char: \n",
      "Word: nn\n",
      "Unknown char: \n",
      "Word: ahlaksz\n",
      "Unknown char: \n",
      "Word: gnderme\n",
      "Unknown char: \n",
      "Word: yazmadm\n",
      "Unknown char: \n",
      "Word: yazdklarnla\n",
      "Unknown char: \n",
      "Word: yazdklarnla\n",
      "Unknown char: \n",
      "Word: noktasna\n",
      "Unknown char: \n",
      "Word: gidecein\n",
      "Unknown char: \n",
      "Word: uyarsnda\n",
      "Unknown char: \n",
      "Word: uyarsnda\n",
      "Unknown char: \n",
      "Word: hocasnn\n",
      "Unknown char: \n",
      "Word: hocasnn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: gtrdn\n",
      "Unknown char: \n",
      "Word: deilmi\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \n",
      "Word: dnr\n",
      "Unknown char: \"\n",
      "Word: bitches...\"unless\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: va\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: americans=full\n",
      "Unknown char: \n",
      "Word: vl\n",
      "Unknown char: \n",
      "Word: sjlvklarhet\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: lhistoire\n",
      "Unknown char: \n",
      "Word: envoys\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: indication\"of\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: peoples\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: ive\n",
      "Unknown char: \n",
      "Word: bushs\n",
      "Unknown char: \n",
      "Word: bushs\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: someones\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: someones\n",
      "Unknown char: \n",
      "Word: gun.its\n",
      "Unknown char: \n",
      "Word: sryor\n",
      "Unknown char: \n",
      "Word: sryor\n",
      "Unknown char: \n",
      "Word: dnonce\n",
      "Unknown char: \"\n",
      "Word: much\"im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: persons\n",
      "Unknown char: \n",
      "Word: qubec\n",
      "Unknown char: \n",
      "Word: govt\n",
      "Unknown char: =\n",
      "Word: ideas=fair\n",
      "Unknown char: =\n",
      "Word: this=ridiculous.do\n",
      "Unknown char: \n",
      "Word: d'tre\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: comunicacin\n",
      "Unknown char: \n",
      "Word: islam\n",
      "Unknown char: \n",
      "Word: adna\n",
      "Unknown char: \n",
      "Word: terrn\n",
      "Unknown char: \n",
      "Word: terrn\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: saldrsnda\n",
      "Unknown char: \n",
      "Word: hayatn\n",
      "Unknown char: \n",
      "Word: mslman\n",
      "Unknown char: \n",
      "Word: mslman\n",
      "Unknown char: \n",
      "Word: reporters\n",
      "Unknown char: \"\n",
      "Word: truth\"-du-jour\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "Word: tt\n",
      "Unknown char: \n",
      "herd: mention\n",
      "Unknown char: \n",
      "herd: mention\n",
      "Unknown char: \n",
      "Word: mans\n",
      "Unknown char: \n",
      "Word: hy\n",
      "Unknown char: \n",
      "Word: git\n",
      "Unknown char: \n",
      "Word: cht\n",
      "Unknown char: \n",
      "Word: bn\n",
      "Unknown char: \n",
      "Word: khng\n",
      "Unknown char: \n",
      "Word: youve\n",
      "Unknown char: =\n",
      "Word: innocent=kool\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: amrique\n",
      "Unknown char: \n",
      "Word: catstrofe\n",
      "Unknown char: \n",
      "Word: area\n",
      "Unknown char: \n",
      "Word: w/themiddle\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: polices\n",
      "Unknown char: \n",
      "Word: seor\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: econmico\n",
      "Unknown char: \n",
      "Word: previsvel\n",
      "Unknown char: \n",
      "Word: theyd\n",
      "Unknown char: \n",
      "Word: dorothe\n",
      "Unknown char: \n",
      "Word: enchan\n",
      "Unknown char: \n",
      "Word: bajn\n",
      "Unknown char: \n",
      "Word: psame\n",
      "Unknown char: \n",
      "Word: habls\n",
      "Unknown char: \n",
      "Word: tambin\n",
      "Unknown char: \n",
      "Word: obligacin\n",
      "Unknown char: \n",
      "Word: democrticos\n",
      "Unknown char: \"\n",
      "Word: skyline\"yourselves\n",
      "Unknown char: \"\n",
      "Word: christ-like\"....many\n",
      "Unknown char: \"\n",
      "Word: who\"the\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: mgalomane\n",
      "Unknown char: \n",
      "Word: order.the\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: naf\n",
      "Unknown char: \n",
      "Word: condolances\n",
      "Unknown char: \n",
      "Word: aprs\n",
      "Unknown char: =\n",
      "Word: religion=superstition\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: curs\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \n",
      "Word: prfre\n",
      "Unknown char: \"\n",
      "Word: rt\"why\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: =\n",
      "Word: act=intelligent\n",
      "Unknown char: \n",
      "Word: bni\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: dlires\n",
      "Unknown char: \n",
      "Word: islamise\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: franois\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: sptzle\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: ferguson\"monday\n",
      "Unknown char: \n",
      "Word: mxico\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: ningum\n",
      "Unknown char: \n",
      "Word: to\n",
      "Unknown char: \n",
      "Word: dbr\n",
      "Unknown char: \n",
      "Word: dbr\n",
      "Unknown char: \n",
      "Word: deilsin\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: bji\n",
      "Unknown char: \n",
      "Word: krdistan\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: wt\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: well\n",
      "Unknown char: \n",
      "Word: sacrbleu\n",
      "Unknown char: \n",
      "Word: prophte\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: =\n",
      "Word: ability=no\n",
      "Unknown char: \"\n",
      "Word: negros\"when\n",
      "Unknown char: \"\n",
      "Word: surveillance-\"terrorism\n",
      "Unknown char: \n",
      "Word: lmites\n",
      "Unknown char: \n",
      "Word: expresin\n",
      "Unknown char: \n",
      "Word: mxico\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: journe...c'est\n",
      "Unknown char: \n",
      "Word: scurit\n",
      "Unknown char: \n",
      "Word: hros\n",
      "Unknown char: \n",
      "Word: tiet-sp\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: england=jamaica=denmark=first\n",
      "Unknown char: =\n",
      "Word: aid=treasure\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: =\n",
      "Word: isis=saudi=taliban=al-nusra\n",
      "Unknown char: \n",
      "Word: wont\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: terrorists\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: youll\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: comunicacin\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: havent\n",
      "Unknown char: \n",
      "Word: heres\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: frankrich\n",
      "Unknown char: \"\n",
      "Word: theres\"dryer\n",
      "Unknown char: \"\n",
      "Word: american\"..who\n",
      "Unknown char: \n",
      "Word: sonot\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \"\n",
      "Word: to\"heaven\n",
      "Unknown char: \n",
      "Word: tawhd\n",
      "Unknown char: \n",
      "Word: lve\n",
      "Unknown char: \n",
      "Word: cafetera\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: shooting.that\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: glen's\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: iid\n",
      "Unknown char: \n",
      "Word: terristlerini\n",
      "Unknown char: \n",
      "Word: eiten\n",
      "Unknown char: \n",
      "Word: sayda\n",
      "Unknown char: \n",
      "Word: ottawas\n",
      "Unknown char: \"\n",
      "Word: says:\"ray\n",
      "Unknown char: \"\n",
      "Word: k\"c\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: quelquun\n",
      "Unknown char: \n",
      "Word: premire\n",
      "Unknown char: \n",
      "Word: perscutions\n",
      "Unknown char: \n",
      "Word: perscuts\n",
      "Unknown char: \n",
      "Word: perscuts\n",
      "Unknown char: \n",
      "Word: mdias\n",
      "Unknown char: \n",
      "Word: dalgriens\n",
      "Unknown char: \n",
      "Word: dalgriens\n",
      "Unknown char: \n",
      "Word: franais\n",
      "Unknown char: \n",
      "Word: algrie\n",
      "Unknown char: \n",
      "Word: sicle\n",
      "Unknown char: \n",
      "Word: isral\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: prt\n",
      "Unknown char: \n",
      "Word: extrmistes\n",
      "Unknown char: \n",
      "Word: uks\n",
      "Unknown char: \n",
      "Word: frmmestad\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: solder\n",
      "Unknown char: \n",
      "Word: ddn't\n",
      "Unknown char: \n",
      "Word: ths\n",
      "Unknown char: \"\n",
      "Word: mere\"politics\"to\n",
      "Unknown char: \"\n",
      "Word: mere\"politics\"to\n",
      "Unknown char: \n",
      "Word: servio\n",
      "Unknown char: \n",
      "Word: wre\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: grere\n",
      "Unknown char: \n",
      "Word: grere\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: murder=intentional\n",
      "Unknown char: \n",
      "Word: canadas\n",
      "Unknown char: \n",
      "Word: jmfra\n",
      "Unknown char: \n",
      "Word: jmfra\n",
      "Unknown char: \n",
      "Word: ngon\n",
      "Unknown char: \n",
      "Word: frstr\n",
      "Unknown char: \n",
      "Word: frstr\n",
      "Unknown char: \n",
      "Word: snt\n",
      "Unknown char: \n",
      "Word: hr\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: frlorare\n",
      "Unknown char: \n",
      "Word: vldigt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: derrire\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: mglich\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: islmico\n",
      "Unknown char: \n",
      "Word: so\n",
      "Unknown char: \n",
      "Word: esto\n",
      "Unknown char: \n",
      "Word: id\n",
      "Unknown char: \"\n",
      "Word: war.\"once\n",
      "Unknown char: \"\n",
      "Word: you\"re\n",
      "Unknown char: \"\n",
      "Word: in\"jean\n",
      "Unknown char: \n",
      "Word: barbrie\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: vietas\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: matarn\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: vendrn\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: pen.\"-stephane\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: =\n",
      "Word: is=not=a\n",
      "Unknown char: =\n",
      "Word: is=not=a\n",
      "Unknown char: =\n",
      "Word: western.......=not=here\n",
      "Unknown char: =\n",
      "Word: western.......=not=here\n",
      "Unknown char: \"\n",
      "Word: says..\"i\n",
      "Unknown char: \"\n",
      "Word: can\"t\n",
      "Unknown char: \"\n",
      "Word: breathe!\"...lol\n",
      "Unknown char: \n",
      "Word: lm\n",
      "Unknown char: \n",
      "Word: iekleri\n",
      "Unknown char: \n",
      "Word: iekte\n",
      "Unknown char: \n",
      "Word: klliyen\n",
      "Unknown char: \n",
      "Word: istiyorum\n",
      "Unknown char: \n",
      "Word: gzel\n",
      "Unknown char: \n",
      "Word: insanlarsnz\n",
      "Unknown char: \n",
      "Word: insanlarsnz\n",
      "Unknown char: \n",
      "Word: byle\n",
      "Unknown char: \n",
      "Word: gnlerde\n",
      "Unknown char: \n",
      "Word: tkanan\n",
      "Unknown char: \n",
      "Word: cierlerimize\n",
      "Unknown char: \n",
      "Word: varsnz\n",
      "Unknown char: \n",
      "Word: varsnz\n",
      "Unknown char: \n",
      "Word: olmayn\n",
      "Unknown char: \n",
      "Word: bey...sayglar\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \\\n",
      "Word: her\\his\n",
      "Unknown char: =\n",
      "Word: us=the\n",
      "Unknown char: \n",
      "Word: matterssophisticated\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: =\n",
      "Word: uber=ugly\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: =\n",
      "Word: snp=ira\n",
      "Unknown char: \n",
      "Word: krdish\n",
      "Unknown char: \n",
      "Word: bibis\n",
      "Unknown char: \"\n",
      "Word: muslim\"...never\n",
      "Unknown char: \"\n",
      "Word: false-flag\"...lol\n",
      "Unknown char: \"\n",
      "Word: danger\".and\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: lets\n",
      "Unknown char: \n",
      "Word: trocadro\n",
      "Unknown char: =\n",
      "Word: dss=grounds\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: atma\n",
      "Unknown char: \n",
      "Word: atma\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: muslims\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \"\n",
      "Word: the\"hijacking\"of\n",
      "Unknown char: \"\n",
      "Word: the\"hijacking\"of\n",
      "Unknown char: \"\n",
      "Word: insistent\"on\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: rsumer\n",
      "Unknown char: \n",
      "Word: aportacin\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \"\n",
      "Word: up?\"...\"find\n",
      "Unknown char: \"\n",
      "Word: up?\"...\"find\n",
      "Unknown char: \n",
      "Word: prophte\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thatso\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: mediaand\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: deportacin\n",
      "Unknown char: \n",
      "Word: prvu\n",
      "Unknown char: \"\n",
      "Word: rumors\"...should\n",
      "Unknown char: \n",
      "Word: todays\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: whats\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: =\n",
      "Word: crime=wall\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: oughtnt\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: polticos\n",
      "Unknown char: \n",
      "Word: seguirn\n",
      "Unknown char: \n",
      "Word: sbado\n",
      "Unknown char: \n",
      "Word: odos\n",
      "Unknown char: \n",
      "Word: aos\n",
      "Unknown char: \n",
      "Word: adems\n",
      "Unknown char: \n",
      "Word: das\n",
      "Unknown char: \"\n",
      "Word: his...er...\"attention\n",
      "Unknown char: \n",
      "Word: weve\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: larticle\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: couldnt\n",
      "Unknown char: \n",
      "Word: thiswell\n",
      "Unknown char: \n",
      "Word: iu\n",
      "Unknown char: \n",
      "Word: ha\n",
      "Unknown char: \n",
      "Word: trn\n",
      "Unknown char: =\n",
      "Word: muslims=no\n",
      "Unknown char: =\n",
      "Word: blacks=non\n",
      "Unknown char: =\n",
      "Word: muslims=no\n",
      "Unknown char: =\n",
      "Word: blacks=non\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \n",
      "Word: mme\n",
      "Unknown char: \"\n",
      "Word: his\"audience\"behind\n",
      "Unknown char: \"\n",
      "Word: his\"audience\"behind\n",
      "Unknown char: \"\n",
      "Word: and\"widescreen\"up\n",
      "Unknown char: \"\n",
      "Word: and\"widescreen\"up\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: yazyor\n",
      "Unknown char: \n",
      "Word: na\n",
      "Unknown char: \n",
      "Word: judasm\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: =\n",
      "Word: lt;&lt;&lt;==================jew\n",
      "Unknown char: \n",
      "Word: julians\n",
      "Unknown char: \n",
      "Word: vctms\n",
      "Unknown char: \n",
      "Word: vctms\n",
      "Unknown char: \n",
      "Word: ther\n",
      "Unknown char: \n",
      "Word: famles\n",
      "Unknown char: \n",
      "Word: famles\n",
      "Unknown char: \n",
      "Word: wshes\n",
      "Unknown char: \n",
      "Word: ther\n",
      "Unknown char: \n",
      "Word: quck\n",
      "Unknown char: \n",
      "Word: prayng\n",
      "Unknown char: \n",
      "Word: solder\n",
      "Unknown char: \n",
      "Word: solder's\n",
      "Unknown char: \n",
      "Word: famly\n",
      "Unknown char: \n",
      "Word: chre\n",
      "Unknown char: \n",
      "Word: lcido\n",
      "Unknown char: \n",
      "Word: mritent\n",
      "Unknown char: \n",
      "Word: religionsdeserve\n",
      "Unknown char: \n",
      "Word: andour\n",
      "Unknown char: \"\n",
      "Word: disrespect\"...you\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \"\n",
      "Word: be\"fixed\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: hebdos\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: tragdia\n",
      "Unknown char: \n",
      "Word: comearem\n",
      "Unknown char: \n",
      "Word: pases\n",
      "Unknown char: \n",
      "Word: wed\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: quran\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: quran\n",
      "Unknown char: \n",
      "Word: schn\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: auer\n",
      "Unknown char: \n",
      "Word: palstinenser\n",
      "Unknown char: \"\n",
      "Word: victim\".everywhere\n",
      "Unknown char: \n",
      "Word: naeve\n",
      "Unknown char: \n",
      "Word: fernndez\n",
      "Unknown char: \n",
      "Word: were\n",
      "Unknown char: \n",
      "Word: vous-tes\n",
      "Unknown char: \n",
      "Word: vergenza\n",
      "Unknown char: \n",
      "Word: comparacin\n",
      "Unknown char: \n",
      "Word: aportacin\n",
      "Unknown char: \n",
      "Word: ms\n",
      "Unknown char: \n",
      "Word: psame\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: da\n",
      "Unknown char: \n",
      "Word: hacis\n",
      "Unknown char: \n",
      "Word: penses\n",
      "Unknown char: \n",
      "Word: prires\n",
      "Unknown char: \n",
      "watd: feiten!\n",
      "Unknown char: \n",
      "Word: einsatzkrfte\n",
      "Unknown char: \"\n",
      "Word: paris\"pres.obama\n",
      "Unknown char: \"\n",
      "Word: off...\"i\n",
      "Unknown char: \n",
      "Word: estlles\n",
      "Unknown char: \n",
      "Word: sontdivischaque\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown char: =\n",
      "Word: dreadheads=savages\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: wouldnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: thats\n",
      "Unknown char: \n",
      "Word: chtiment\n",
      "Unknown char: \n",
      "Word: c'tait\n",
      "Unknown char: \n",
      "Word: vritable\n",
      "Unknown char: \n",
      "Word: succs\n",
      "Unknown char: =\n",
      "Word: hispanicsasians=democrat\n",
      "Unknown char: =\n",
      "Word: christians=republicans\n",
      "Unknown char: =\n",
      "Word: christians=dems\n",
      "Unknown char: \n",
      "Word: mitgefhl\n",
      "Unknown char: \n",
      "Word: gehrt\n",
      "Unknown char: \n",
      "Word: angehrigen\n",
      "Unknown char: \n",
      "Word: israilli\n",
      "Unknown char: \n",
      "Word: tmgeneral\n",
      "Unknown char: \n",
      "Word: koavi\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "Word: trkiyede\n",
      "Unknown char: \n",
      "make: gun\n",
      "Unknown char: \n",
      "rewardsun\n",
      "Unknown char: \n",
      "puttingead\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: kardelerim\n",
      "Unknown char: \n",
      "Word: cantrk\n",
      "Unknown char: \n",
      "Word: kardein\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: dmanlarna\n",
      "Unknown char: \n",
      "Word: lt;&lt;&lt;kardeimizin\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: ineallah\n",
      "Unknown char: \n",
      "Word: aln\n",
      "Unknown char: \n",
      "Word: terr\n",
      "Unknown char: \n",
      "Word: amberin\n",
      "Unknown char: \n",
      "Word: yaatsin\n",
      "Unknown char: \n",
      "Word: zalim\n",
      "Unknown char: \n",
      "Word: dimi\n",
      "Unknown char: \n",
      "Word: irmainin\n",
      "Unknown char: \n",
      "Word: baindadir\n",
      "Unknown char: \n",
      "Word: gnahsizlar\n",
      "Unknown char: \n",
      "Word: amn\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \n",
      "Word: isnt\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \n",
      "Word: astrix\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: cafs\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: =\n",
      "Word: search=ferguson\n",
      "Unknown char: =\n",
      "Word: tip=lesson\n",
      "Unknown char: \n",
      "Word: israels\n",
      "Unknown char: \n",
      "Word: polica\n",
      "Unknown char: \n",
      "Word: va\n",
      "Unknown char: \n",
      "Word: localizacin\n",
      "Unknown char: \n",
      "Word: despus\n",
      "Unknown char: \n",
      "Word: ridculos\n",
      "Unknown char: =\n",
      "Word: photos=boasting\n",
      "Unknown char: =\n",
      "Word: situation=acceptable\n",
      "Unknown char: \n",
      "Word: enneige\n",
      "Unknown char: \n",
      "Word: dbris\n",
      "Unknown char: \n",
      "Word: cest\n",
      "Unknown char: \n",
      "Word: valle\n",
      "Unknown char: \n",
      "Word: hlico\n",
      "Unknown char: \n",
      "Word: dcollent\n",
      "Unknown char: \n",
      "Word: dj\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: impresso\n",
      "Unknown char: \n",
      "Word: tambm\n",
      "Unknown char: \n",
      "Word: segurana\n",
      "Unknown char: \n",
      "Word: no\n",
      "Unknown char: \n",
      "Word: lamentvel\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: ameaas\n",
      "Unknown char: \n",
      "Word: islmicos\n",
      "Unknown char: \n",
      "Word: ameaas\n",
      "Unknown char: \n",
      "Word: tranqilas.tanto\n",
      "Unknown char: \n",
      "Word: sado\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: proteo\n",
      "Unknown char: \n",
      "Word: aps\n",
      "Unknown char: \n",
      "Word: lder\n",
      "Unknown char: \n",
      "Word: islmico\n",
      "Unknown char: \n",
      "Word: religies\n",
      "Unknown char: \n",
      "Word: redao\n",
      "Unknown char: \n",
      "Word: redao\n",
      "Unknown char: \n",
      "Word: horrio\n",
      "Unknown char: \n",
      "Word: reunio\n",
      "Unknown char: \n",
      "Word: vtimas\n",
      "Unknown char: \n",
      "Word: crashthoughts\n",
      "Unknown char: \"\n",
      "Word: radioshow\"=ain't\n",
      "Unknown char: =\n",
      "Word: radioshow\"=ain't\n",
      "Unknown char: =\n",
      "Word: account=prob\n",
      "Unknown char: \"\n",
      "Word: like\"riot\n",
      "Unknown char: \n",
      "Word: policires\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \"\n",
      "Word: recruits\"?first\n",
      "Unknown char: \"\n",
      "Word: terrorism\"&amp\n",
      "Unknown char: =\n",
      "Word: crime=police\n",
      "Unknown char: \n",
      "Word: theres\n",
      "Unknown char: \n",
      "Word: represin\n",
      "Unknown char: \n",
      "Word: l'amrique\n",
      "Unknown char: \"\n",
      "Word: to?\"you\n",
      "Unknown char: \n",
      "Word: cant\n",
      "Unknown char: \n",
      "Word: journe\n",
      "Unknown char: \n",
      "Word: smtliche\n",
      "Unknown char: \n",
      "Word: lt\n",
      "Unknown char: \n",
      "Word: lt\n",
      "Unknown char: \n",
      "Word: nmlich\n",
      "Unknown char: \n",
      "Word: didnt\n",
      "Unknown char: \n",
      "Word: im\n",
      "Unknown char: \n",
      "Word: luft\n",
      "Unknown char: \n",
      "Word: erzhlt\n",
      "Unknown char: \n",
      "Word: fr\n",
      "Unknown char: \n",
      "Word: lngst\n",
      "Unknown char: \n",
      "Word: berfllig\n",
      "Unknown char: \n",
      "Word: unglcke\n",
      "Unknown char: \n",
      "Word: whod\n",
      "Unknown char: \n",
      "Word: procs\n",
      "Unknown char: \n",
      "Word: pense\n",
      "Unknown char: \n",
      "Word: jrg\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: true?ilyy\n",
      "Unknown char: \n",
      "Word: francs\n",
      "Unknown char: \n",
      "Word: vctimas\n",
      "Unknown char: \n",
      "Word: juda\n",
      "Unknown char: \n",
      "Word: hes\n",
      "Unknown char: \n",
      "Word: youre\n",
      "Unknown char: \n",
      "Word: its\n",
      "Unknown char: \n",
      "Word: theyre\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: frances\n",
      "Unknown char: \n",
      "Word: estn\n",
      "Unknown char: \n",
      "Word: yesterdays\n",
      "Unknown char: \n",
      "Word: scholarmay\n",
      "Unknown char: =\n",
      "Word: words=action\n",
      "Unknown char: \n",
      "Word: mastersdo\n",
      "Unknown char: \n",
      "Word: card\n",
      "Unknown char: \n",
      "Word: dont\n",
      "Unknown char: \n",
      "Word: inquitant\n",
      "Unknown char: \n",
      "Word: doesnt\n",
      "Unknown char: \"\n",
      "Word: how\"...the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ee5c2c10c07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainCMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_train_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ERD/model.py\u001b[0m in \u001b[0;36mTrainCMModel\u001b[0;34m(sess, rdm_train, cm_train, t_rw, t_steps)\u001b[0m\n\u001b[1;32m    381\u001b[0m                      \u001b[0mrdm_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minit_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                      rdm_train.dropout_keep_prob: 1.0 }\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mt_ssq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdm_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# t_ssq = [batchsize, max_seq, scores]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mssq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_ssq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TF/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(model)\n",
    "model.TrainCMModel(sess, rdm_train_graph, cm_train_graph, 0.3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
